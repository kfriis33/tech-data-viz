Field1
Held virtually this year due to the COVID-19 pandemic, Brown CS welcomed students to our fifth annual CS Research Open House last week. Hosted by the Meta-Undergraduate Research Assistants (MURAs), it offered students an opportunity to learn about research in the department, no matter their year, concentration, or classes they're enrolled in. This year, more than 150 students attended!
You can watch brief introductions from each research group here.
At Brown CS and around the globe, interest in AI and related topics is soaring. CSCI 1470 Deep Learning, only a few years old, today has an enrollment of over 350 students, the department's second largest. But as computer scientists hope to expand the field to historically underrepresented groups (HUGs), students from demographics that have born the brunt of algorithmic bias and deepfakes may be understandably hesitant to take part. A new program aims to change that.
Thanks to an exploreCSR award from Google, Professors Amy Greenwald, Jeff Huang, Daniel Ritchie, and James Tompkin are launching a program for college students from HUGs in CS that will expose them to socially-responsible ways that AI can be used to realize creative visions. Working virtually at first due to the COVID-19 pandemic, students from around the country will become research associates, paired with Brown graduate student mentors (where possible, from similar HUGs) to conduct individualized research experiences, each culminating in a substantial artifact. In short, it's a behind-the-scenes look at how CS research operates, normalized and without pretentions, that students can use as a building block for future careers.

"Brown is uniquely qualified for this kind of experience," says Daniel, "not only due to our diversity and inclusion programs and our efforts to  integrate socially responsible computing across our entire curriculum, but because we're the headquarters of the Leadership Alliance. It's a nationwide consortium of higher educational institutions dedicated to training and mentoring a diverse population of students to take up research leadership positions in academia, industry, and government. CS faculty have mentored visiting students as part of Leadership Alliance programs in the past, and we'll be collaborating with them on this effort (and other upcoming outreach initiatives) to coordinate professional networking and other opportunities for participating students."
Thanks to the department's strong AI and visual computing groups, research topics are many and varied, including creative deepfake detection, mimicking high-end photography, structured image editing, adversarial shape generation, 3D augmented reality sketching, and building AI to play games. Opportunities for projects include literature reviews, replication studies, research apprenticeships, and independent research projects. Although activities will be conducted virtually, the program will culminate in an in-person visit to campus, centered around a symposium in which students present their findings alongside Brown undergrads. 


"I'm excited to see how this 'virtual research associate' approach goes," says Daniel. "It's really different from the typical outreach approach of bringing students in for a short workshop that lasts maybe for a long weekend. I'm hopeful that having a longer timeline, where students get to really experience a research culture over the span of a semester, can have a more lasting impact on attracting students to CS research careers. And we'll still bring students to campus at the end of this experience, for them to meet with their mentors and collaborators and present their research alongside Brown undergrads, as their research peers."
In the middle of a pandemic, some Brown CS alums found a new opportunity to give back. 
Inspired by a neighbor who bought a second monitor to help his mother teach her elementary school students remotely during the COVID-19 pandemic, alum Matt Lerner founded Two Screens for Teachers just two months ago. The nonprofit aims to make an immediate impact on the quality of learning during the current online teaching crisis by providing teachers with a second monitor. (Studies indicate that the additional equipment, which can allow teachers to see students on one screen and lesson plans on the other, can improve productivity by 20-30 percent.) His collaborators include two other Brown CS alums, Jesse Kocher and Dave Peck.
Their goal is to distribute 250,000 second monitors to teachers nationwide by the end of 2020, and they recently purchased monitors for all Seattle Public School teachers after raising more than $2M in funding. They're turning their attention now to Brown University's home: the city of Providence, Rhode Island.
Two Screens for Teachers is asking Brown CS alums and anyone else to help provide second monitors for all Providence Public School teachers. You can reach out directly to Matt at matt@twoscreensforteachers.org to donate. 
"There are 3.2 million K-12 public school teachers in the United States," Matt tells us, "and America risks losing a year of educational progress across an entire generation. We need to transform teaching to get our kids through this educational crisis, and we really appreciate your help."
Applying to a doctoral program can be challenging for anyone, and for applicants from historically underrepresented groups (HUGs) who may not have adequate access to research experiences or mentors, it can be especially daunting. It’s thought to be a considerable contributor to the lack of diversity in computer science departments nationwide.
In response, several PhD students from Brown CS have put together a pilot Application Feedback Program for Underrepresented Applicants. It’s an attempt to help address that lack of diversity by putting helpful application resources at the fingertips of applicants from these groups, sharing a variety of best practices and lessons learned.  
Maybe even more valuably, the program also provides extensive mentorship opportunities. They include possibilities such as one-on-one Zoom calls with applicants, graduate student and faculty panels, feedback on statements of purpose and application materials, and answering any questions that applicants may have about the PhD application process.
Since this year has seen a range of organisations and universities offering a variety of support programs for applicants from HUGs in CS, the students also compiled a list of them at the site above to provide easy access. 
“This makes for a very different application process,” say Kweku Kwegyir-Aggrey and Roma Patel, two of the students behind the pilot, “and it’s the kind of thing we wish had been available when we applied. Taking part in our program doesn’t guarantee admission, but we believe that qualified mentorship and ongoing support will prove to be a deciding factor for students from historically underrepresented groups.”
In his newly-released third book, The Alignment Problem, Brown CS alum Brian Christian returns to the intersection of computer science with human ethics and thought. Already winning praise from experts such as Cathy O'Neil, author of Weapons of Math Destruction, it focuses on the consequences of allowing machine learning systems an increasing power to make decisions on humanity's behalf. What ethical and even existential risks emerge when the systems we're attempting to teach act in unexpected and unwanted ways?
"The book," he tells us, "is rooted in four years of research and approximately one hundred interviews with machine-learning researchers, and covers the field's history as well as open problems and recent progress, emphasizing the recent and frequently interdisciplinary work on the ways in which ML systems intersect in complex ways with human norms and values."
Brown CS Professor Michael Littman, whose research in reinforcement learning features in the book, has been following Christian's work closely over the years. "I loved Brian's book on Algorithms to Live By," he says, "and am looking forward to his take on the future of AI." Christian's other prior book, The Most Human Human, explores the ways in which computers are shaping our ideas of what it means to be human. It was a Wall Street Journal national bestseller, a New York Times editors' choice, and was chosen as a favorite book of the year by The New Yorker.
More details about The Alignment Problem are available at Brian's website.
The Department of Computer Science at Brown University is seeking applicants for a tenure-track faculty member at the level of Assistant Professor and a faculty position at the rank of lecturer, senior lecturer, or distinguished senior lecturer. We strive to build a diverse and inclusive environment for all members of our community, and are particularly interested in candidates whose scholarship, teaching and service can further our efforts. Brown also aims to foster a diverse and inclusive environment; its detailed vision and action plan for realizing this commitment is articulated in Pathways to Diversity and Inclusion. 
Brown CS is glad to announce that applications are open for the Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, which provides $10,000 annually to support an undergraduate engaged in an intensive faculty-student summer research partnership with the Department of Computer Science.
The award is available thanks to a generous gift from Brown CS alum Peter Norvig, now a Director of Research at Google and a thought leader in the areas of artificial intelligence, natural language processing, information retrieval, and software engineering. He was drawn to Brown as an undergraduate by the open curriculum and his interests in computer science and linguistics, which he studied in high school. His gift honors the life and work of Randy F. Pausch '82, a renowned expert in computer science, human-computer interaction, and design who died of complications from pancreatic cancer in 2008 and whose "Last Lecture" has been widely praised. "I didn't know Randy when I was at Brown," Peter says, "but we met afterward and corresponded for many years. His story is inspiring, and this is an opportunity to remember him."
Norvig sees this award as a "multiplier" that will amplify the value of his gift and extend it through time. "I'm interested in students with a wide range of personalities and interests," he says, "and in putting students and faculty together. In the past, we had to build all our own tools, and we didn't have time to combine computer science with other fields. Now, there are so many opportunities to do so. I think it's a wise choice: you invest in things that you think will do good, and educating a student allows them to help add to the things that you're already trying to accomplish." 
Last year, the award went to Casey Nelson and Dylan Sam, who used the funds to work with Professors R. Iris Bahar and Michael Littman on near data processing and weakly supervised machine learning.
To apply, no later than February 14, 2021, students should email Associate Professor (Research) and Vice Chair Tom Doeppner either: (A) a copy of their summer UTRA application or (B) a two-page description of their proposed research and a letter of support from the Brown CS faculty member that they intend to work with.
The Computing Research Association (CRA) and Computing Community Consortium (CCC) have announced a new Computing Innovation Fellows (CIFellows) Program for 2020 that recognizes the significant disruption to the academic job search caused by the COVID-19 pandemic and aims to provide a career-enhancing bridge experience for recent and soon-to-be PhD graduates in computing. Nationwide, only 59 fellowships were awarded (out of 550 applications), and Brown CS has received the maximum number possible for a single institution: Evangelos Atlidakis, a recent Columbia University doctoral graduate, will work with Professor Vasileios P. Kemerlis, and Ben Greenman, a doctoral student at Northeastern University, will work with Professor Shriram Krishnamurthi.
There's an additional Brown CS connection: Morgan L. Turner, a PhD candidate at Brown's Department of Ecology and Evolutionary Biology and collaborator of Professor David Laidlaw, has won a Computing Innovation Fellowship to work with Brown CS alum Daniel F. Keefe, now at the University of Minnesota.
The goal of the program is to create career growth opportunities that support maintaining the computing research pipeline, and it offers two-year postdoctoral opportunities in computing, with cohort activities to support career development and community building for this group of Fellows. 
Last night, the Brown CS community started the semester with a virtual town hall where students learned about upcoming events and deadlines, met new faculty members who talked about the latest courses they’ll be teaching, found out more about research opportunities, and had questions answered.
You can watch a recording here (Brown login required).
Two students from Brown University, Andrea Greene-Horace and Yanling He, have just received the CrowdStrike Foundation's NextGen Scholarship, which supports the development of the next generation of talent and leadership in cybersecurity and artificial intelligence. No more than six of these scholarships are given each year, and Brown students have received at least one of them every year that the scholarship has been offered. Andrea and Yanling are students in the Executive Master's in Cybersecurity program, which recently evolved into the Master of Science (ScM) in Cybersecurity.
"I’m honored," says Yanling, "and humbled to receive this award as a Brown student."
"I am very humbled and appreciative for CrowdStrike’s selection as a 2020-21 NextGen scholarship award recipient for use in Brown University’s Cybersecurity degree program," says Andrea. "Brown Cybersecurity is a world-class program. It fosters industry leadership growth by incorporating both scientific methods and proven interdisciplinary frameworks as a basis for solving today’s critical issues, with an eye on emerging cybersecurity and privacy discoveries and disruptions. I've found the program powerful and challenging, which has led to more thoughtful discovery in my own research, application, and growth. It continues to impress me with how it provides regular access to highly relevant, recognized, and sought-after industry and government leaders, commensurate with the classes and topics at hand."
Founded in 2011, CrowdStrike combines advanced endpoint protection with expert intelligence to pinpoint the adversaries perpetrating cybersecurity attacks. Its philanthropic and volunteer arm, the CrowdStrike Foundation, offers a pro bono cybersecurity program for nonprofits, research grants, and the NextGen Scholarship, which provides financial assistance to select undergraduate and graduate students studying cybersecurity and/or artificial intelligence.
Andrea and Yanling were chosen based on a range of criteria that include academic record and courses, experience, and interest in cybersecurity and/or artificial intelligence. They join two prior Brown Cybersecurity winners of the scholarship: George "Donnie" Hasseltine '19 in 2018-2019 and Sarah Lachance '20 in 2019-2020.
"The Brown Cybersecurity faculty is very proud of Andrea and Yanling for their selection as NextGen Scholarship recipients," says Ernesto Zaldivar, Brown CS faculty member and Program Director of Brown's Cybersecurity degree. "The fact that two of our students were selected for this competitive honor – during the same selection period – is a testament to the strength of each of their applications. We're thrilled that the CrowdStrike Foundation recognizes the same promise that we see in Andrea's and Yanling's work."
“The most successful computer vision algorithms of the last few years,” says Chen Sun, “were all built on top of supervised deep learning, where big data and powerful computing devices are the key factors. I’m interested in making machines perceive and learn more naturally, like humans.” Currently a staff research scientist at Google, Chen has just joined Brown University’s Department of Computer Science as a visiting assistant professor, and he’ll become a full assistant professor next July. He’s the newest hire in the multi-year CS With Impact campaign, the largest expansion in Brown CS history. 
One of the reasons why Chen is hoping to change the way machines learn is personal. “When I’m joking with friends,” he says, “I tell them that I have one major goal before I retire: automating everything in my life as much as possible and building myself a robot assistant!” Self-driving cars have been a longtime interest, and one of his recent efforts in this area was an attempt to improve their decision-making through behavior prediction.
In his youth, Chen was an avid videogamer but didn’t find himself immediately drawn to computer science. “I wasn’t quite the standard ‘good student’ according to East Asian culture,” he says. “I often got bored during classes and fell asleep. On the other hand, I worked hard on the topics I was fascinated with.” That atypical experience has provided a lot of insight into pedagogy: “It made me believe that every student is unique and has their own way of learning, and a great teacher can provide a diverse set of tools for different students to choose from and learn most effectively.”
Not learning to code in high school made for a stressful start to college, and it was a year and a half before CS felt comfortable. The two turning points were an algorithm class taught by Professor Xiaoming Sun, who Chen describes as his role model for teaching, and two computer system classes where Chen wrote a compiler and then a mini operating system. “The labs were so nicely designed,” he says, “that after finishing the course I had the skills to build larger software systems and apply them to research projects.”
Chen graduated from Tsinghua University with a Bachelor’s degree in Computer Science, then went on to earn his PhD at the University of Southern California, advised by Professor Ram Nevatia. Video understanding was one of the first subjects to catch his eye. It remains his focus, but Chen says that his relationships with other experts keep his interests broad: “Computer science and AI in general are really interdisciplinary, and computer vision, natural language processing, machine learning, and even cognitive science are all connected – I draw inspiration from people in those areas, and I want to help them as well.”  
As Chen explains it, humans are easily able to distill rich and highly complex information from numerous kinds of videos (documentaries, instructional videos, movies) without supervision, but the process is much more difficult for machines. Inspired in part by his love of cooking alongside his wife, one of Chen’s recent projects (see the paper here and blog post here) involved using an algorithm that discovered human actions and object state transitions automatically by watching people in cooking videos do things and listening to them explain.
“Humans observe the world and then actively explore it,” he says. “I want robots to help us cook, but I want them to learn by watching YouTube, playing around in my kitchen, and asking me for tips, then apply skills that they learned virtually in the real world. I want to close the loop between their perception and their taking action.” 
Also critical, Chen says, is for machines to learn to anticipate multiple futures of human behaviors, whether it’s thinking about varying game strategies or different ways for autonomous vehicles to drive. “That ability to anticipate the future is critical for robots to interact with us,” he says. “It determines whether a self-driving car should yield to pedestrians or not.”
“In the long run,” Chen tells us, “ I’d like to develop algorithms that could discover structured, abstract knowledge automatically from multimodal data, such as videos, websites, speech or music. Such algorithms could lead to smarter softwares, such as an automatic editor for the video of Brown CS faculty singing to graduates. Of course, the ultimate goal is to transfer such knowledge to a robot that can cook me breakfast, drive me to work, and vacuum the house.” 
While that robotic helper is taking shape, Chen is happy to be landing somewhere that reminds him very much of his own undergraduate experience. “I’m getting started as an educator, and Brown is a great place for me.” he says. “The undergraduate teaching assistant and undergraduate research assistant programs at Brown CS are so strong, and I’m really looking forward to getting inspiration from students. The opportunities for collaboration are everywhere – I just read a paper on compositionality by Ellie Pavlick, and it was full of ideas that I could transfer to my research. I love that she’s just down the hall.”
Advising is one of the things that Chen’s looking forward to most: “I really want students to prosper. Taking my experience, helping them be successful – that’s really fulfilling for me. I can’t wait.”
by Kevin Stacey (Senior Writer, Physical Sciences)
Thanks to a grant from the National Science Foundation (NSF) to a Brown University professor, a high-flying new robotics curriculum is coming to high schools across Rhode Island. 
The curriculum teaches students the basics of robotics in the process of building and programming their own autonomous aerial drones. It was originally developed as a college class by Stefanie Tellex, a computer science professor at Brown. The NSF grant enabled Tellex to adapt the curriculum for high schoolers and provide schools at no cost with the drone parts needed to teach it. This past summer, Tellex trained teachers from 10 Rhode Island schools to teach the course, with the hope that each school will offer it this fall.
“There’s something about flight that captures the imagination,” she said. “The idea here is to use that excitement to introduce students to robotics and artificial intelligence, which can seem a little daunting at first. But these autonomous drones are really good for teaching the basics in a fun way, and we think it’s a great way to get kids interested in STEM.”
The new curriculum is a partnership with Duckietown, an existing robotics education initiative that uses simple, ground-based robots. The new aerial addition to the Duckietown curriculum  is dubbed DuckieSky. 
The DuckieDrone was first developed by Tellex and a group of undergraduates at Brown a few years ago, and Tellex began using the drones to teach introductory robotics in 2017. The drones themselves are small and fairly simple, which Tellex says is by design. She wanted a platform that would be safe and inexpensive, but still sophisticated enough to teach key concepts.
Each quad-rotor drone is equipped with a camera and a small computer processor called a Raspberry Pi. In addition to assembling the drone’s physical parts, students learn how to program the drone to be able to fly on its own. That involves algorithms that help the drone figure out where it is in space and adjust its flight path accordingly. Students also learn key concepts related to safety, networking and communications. 
“One of the key things in robotics is the interaction between the hardware and the software,” Tellex said. “These drones are a great way to bring that all together in a really tangible way.”
Rick Sinard, a computer science teacher at Mount Saint Charles Academy in Woonsocket who participated in the training last month, said he’s excited to bring the curriculum to his school. 
“What this will mean to our students is beyond words,” Sinard said. “Computer science was once thought to be the future, however, it is no longer the future, it is the ‘now.’ This drone program covers the entire STEM program, and is a great addition to our growing CS offerings.”
Other schools participating in the program are Charles E. Shea High School in Pawtucket, Cranston Area Career and Technical School, Cumberland High School, Exeter-West Greenwich High School, the MET High School in Providence, Rogers High School in Newport, Toll Gate High School in Warwick, William M. Davies. Jr. Career & Technical HS, and Smithfield High School. In addition to the 10 Rhode Island schools, Penn Yan Academy in New York State is also participating. In all, the grant will provide the parts to build more than 200 drones to students in these schools.
A group of nine Brown undergraduate students, along with students from other colleges, who are alumni of a drone program piloted at the Providence Career & Technical Academy,  played a key role in helping train teachers, and plan to help out during the school year when the instruction begins. 
“I can’t say enough good things about the Brown students who have been involved in this,” said Diane Silva Pimentel, director of the Master of Arts in Teaching program at Brown. “These students have taken their passion for robotics and are using it to bring more people into that world. The program is fostering tremendous engagement for these students.”
One of those students is Dev Ramesh, who worked on the project this summer as part of an Undergraduate Teaching and Research Award (UTRA) from Brown. Ramesh, a rising junior computer science concentrator, is thinking about a future career in academia, which makes this a golden opportunity for him.
“I have been given the opportunity both to develop the curriculum that we then used to train teachers, and to learn from these teachers about producing and implementing holistic and goal-driven lessons,” Ramesh said. “I believe that my understanding of academic teaching has tremendously improved due to both of these factors, and if I do end up going into academia, this UTRA has set up the foundations for my future success.”
Tellex says she looks forward to seeing a curriculum she’s worked on for years put into action for high school students.
"It's so exciting to empower a group of students with a flying robot and help them on their journey in the path of STEM,” she said. “Our goal is to provide another entry point into STEM that will engage a diverse group of students into autonomous robotics."
"As a disclaimer," says Professor Seny Kamara of Brown CS, "I want to say that this is not a technical talk...so what am I doing here? Why am I giving a talk at CRYPTO if I'm not talking about technical things? I'm here because Ahmaud Arbery was killed in February, because Breonna Taylor was killed by police officers in March, and because George Floyd was also killed by police officers in May."
CRYPTO 2020, which is being held virtually this week, is the world's flagship cryptography conference, sponsored by the International Association for Cryptographic Research. Seny's words above are the start to a very atypical keynote that was shaped by his experience of being Black, an immigrant, an applied cryptographer, and in particular, an outsider: one of perhaps only two or three Black cryptographers in the world.
"My goal is not to convince you that my perspective is the right one," Seny explains, but to share a point of view that he says has not yet been expressed within the cryptography community. In his talk, he takes a critical look at who benefits from cryptography as it currently stands and explores how it can be used to fight oppression and violence: both topics featured prominently in Seny's recent courses, CSCI 2952-V Algorithms for the People and CS 2950-V Topics in Applied Cryptography: Crypto for Social Good.
A recording of the keynote is available here.
Within days of earning a Greek Diaspora Fellowship for work with Internet of Things software hardening, Professor Vasileios P. Kemerlis of Brown CS is making news again. This time, he's won the DIMVA 2020 Outstanding Reviewer Award, which he shares with Fabio Pierazzi of King's College London. Held virtually this year due to the COVID-19 pandemic, the Detection of Intrusions and Malware & Vulnerability Assessment (DIMVA) conference is a premier security forum for advancing the state of the art in intrusion detection, malware detection, and vulnerability assessment. Each year, it brings together international experts from academia, industry, and government to present and discuss novel research in these areas.
The official announcement is available here.
Since the Greek Diaspora Fellowship Program (GDFP) began four years ago, it's provided 123 fellowships to create brain circulation between Greek universities and universities abroad, strengthening capacity at the host institutions and developing long-term, mutually-beneficial collaborations. This year, only three of the 36 awards were in the areas of computer science or computer science education, and one of them has gone to Brown CS Professor Vasileios P. Kemerlis, who shares it with George Polyzos of the Athens University of Economics and Business.
"The field of Internet of Things (IoT) is expanding rapidly," says Vasileios, "and it now includes cars, medical equipment, and other devices with the potential for life-threatening harm. This makes their security essential, but existing software hardening protection solutions may not apply to an IoT setting. Our research will combine my expertise in software hardening with George's expertise in IoT eco-systems, and we'll be working on real-world systems and applications instead of generic scenarios. That combination allows us to focus our effort on attacks that are more likely to occur and more destructive if successful."
Funded by the Stavros Niarchos Foundation and managed by the Institute of International Education in collaboration with the Fulbright Foundation in Greece, the GDFP pairs members of the Greek and Cypriot academic diaspora with higher education institutions and collaborators in Greece to work together on curriculum co-development, collaborative research, graduate and undergraduate research training, and mentoring activities. Fellowships match host universities with scholars and cover the expenses for project visits of between 14 and 90 days, including transportation, a daily stipend, materials allowance, and health insurance.
The collaboration will take place during two visits, with time before and between them used for preparatory and assessment activities, including the sharing of recent research results. During the first visit, Vasileios and George will go through possible software hardening and protection techniques for the IoT setting and make a preliminary assessment of their suitability via estimations and prototyping, while also considering the potential benefits and overheads of each mechanism. They'll select a set of proposed methods for each area (operating system, applications) and create a plan for applying them to the target software and evaluating the results. After the first visit, the two researchers will work on introducing the suggested protection mechanisms to the software components selected, debugging and testing the prototyped code, and then assessing its effectiveness against attacks and its overhead in real use. During the second visit, Vasileios and George will evaluate these results, perform adjustments to the prototyped code, assess their impact, and draw up a report on results that they'll expand on later in co-authored research papers.
"We're really excited," Vasileios says. "Any positive results will be of great value to designers and developers of IoT systems, with considerable impact on the research community that hasn't yet addressed the issues surrounding IoT software. We're looking forward to technology transfer in both directions."
A full list of funded projects is available here.
Held virtually this year, the USENIX Annual Technical Conference (USENIX ATC) brings together leading systems researchers for the presentation of cutting-edge systems research and the opportunity to gain insight into a wealth of must-know topics, including virtualization, system and network management and troubleshooting, cloud and edge computing, security, privacy, and trust, mobile and wireless, and more.
Brown CS Professor Rodrigo Fonseca was an attendee, and he and colleagues from Microsoft Azure and Microsoft Research (Mohammad Shahrad, Íñigo Goiri, Gohar Chaudhry, Paul Batum, Jason Cooke, Eduardo Laureano, Colby Tresness, Mark Russinovich, and Ricardo Bianchini), have won their Community Award, which is given for providing an important new dataset to the community and for the paper ("Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider") analyzing it. In presenting the award, the organizers remarked that "when such a release is the first of its kind and on a timely topic, as is the case in this paper, it marks an important milestone for the community".
"Function as a Service (FaaS)," the researchers explain, "has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e. fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics."
In their paper, Rodrigo and his colleagues first characterize the entire production FaaS workload of Azure Functions. They show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from their characterization, they then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies.
The paper and a presentation video are available here.
This fall, Srinath Sridhar joins Brown University’s Department of Computer Science as assistant professor. He’s the newest hire in the multi-year CS With Impact campaign, the largest expansion in Brown CS history. Currently a postdoctoral researcher at Stanford University, he credits his father, a self-taught user with contagious enthusiasm, for his first exposure to computing. But in some ways, that experience may have been less important than Srinath’s teenaged tinkering with cameras, which set the stage for his future research.
“I was compelled by the idea,” he explains, “of exploring cameras as mechanical devices.” A sense of wonder continues to be a motivating factor: Srinath has been photographing eclipses since 2003, and even traveled to Oregon to see one. “They’re celestial events I can’t postpone, something that’s much bigger than me, and that I’m not in control of. I find that fascinating.” And his early interest in the inner workings of things is still present. One recent personal project offered insight into how plants grow by taking time-lapse photos over many days, watching stems lengthen and leaves uncurl.
“I’ve always been interested in imaging,” Srinath says. “For me, deconstructing images is how I start to understand the world of humans and objects. They’re where my research begins.” 
His primary interests, he explains, lie in 3D computer vision and machine learning, including overlapping topics in robotics, computer graphics, and human–computer interaction. “I develop 3D spatio-temporal machine learning methods that provide human-centric, object-centric, and interaction-centric understanding of our world from videos and images.”
Initially hoping to study computer science after high school, he ended up in geoinformatics, a branch of civil engineering concerned with gathering, processing, and delivering geographic information. Assignments in photogrammetry (extracting 3D information from aerial images of cities and other locales) were a powerful glimpse of what computer vision could offer, and by the end of his undergraduate days in Chennai, Srinath had decided that it was a worthy research topic. His next stop was the University of Michigan, Ann Arbor, for a Master’s degree, followed by a doctorate at the Max Planck Institute for Informatics in Germany.
“I was interested in 3D computer vision first,” he says, “but my other interests, like machine learning, came fairly soon afterward.” Among other things, the varied experiences showed Srinath the importance of mentorship: “I was lucky enough to be working in augmented reality before it was considered cool, and in ML before the deep learning craze hit. Part of the reason why that happened was because I had the right kind of mentors to help me figure out what would be interesting to work on.”
That kind of foresight, Srinath explains, is one of the reasons why he’s always felt at home in academia, and it’s something he’s eager to find at Brown. “I want to be around people who are researching long-term problems,” he says, “and the opportunity to teach a new generation of students who have radical ideas that I didn’t think of is wonderful.” 
And the world that this new generation will inhabit, what does Srinath hope it will look like? It’s one in which robots would interact with their environment, and humans with robots, in the natural, unthinking way that we cook a meal or clean our house. We’ve gotten to the point, he says, where it’s easy for computers to beat us at chess or Go, but the challenge is human activities that we don’t associate with intelligence, like loading a dishwasher or folding laundry. 
“One of my drivers,” Srinath tells us, “is to learn from humans in order to help robots perform these everyday tasks, and that means digitizing and examining human physical interactions from visual data–images and videos–so we can build smarter robots, and more immersive virtual/augmented realities. We can’t have them do what we do until we understand how we do it.” It’s a sentiment that recalls his younger self: wanting robots to function like people, Srinath’s analyzing human motion in the same way that he once did the innards of a camera.
“Grasping and manipulation appear so simple,” he says, “but we don’t completely understand how they work. Our hands played a massively important role in the evolution of human intelligence, but there are no robotic hands with human-level dexterity yet.” Over the next five years, Srinath hopes to expand his work on understanding human and object motion to encompass increasingly complex interactions. He’s also looking forward to multidisciplinary work with colleagues in cognitive psychology and other areas, seeing their expertise as vital for the next steps in human-robot interaction.
“My research is often inspired by applications,” he says. “We have pretty good robots and AR/VR headsets, but there are still major computer vision and machine learning challenges left to solve before these devices can interact like humans.” We will eventually have self-driving cars, Srinath explains, as well as realistic AR/VR and robots helping the disabled, but he’s not sure how long it might take for robots to have enough humanlike capability to free their owners from daily chores and let us do something more creative with our lives.
But he likes a challenge. “It might be decades away,” he says. “but I’d like to shorten that!”
The COVID-19 pandemic has disrupted education at a seldom-seen scale, affecting students and teachers around the world and triggering a learning crisis. In response, next week's Futures Forum on Learning (organized by Schmidt Futures) will bring together leaders in global education and edtech virtually to discuss critical areas of need and opportunity. At 7 PM EDT on Monday, July 20, Professor Kathi Fisler of Brown CS and Bootstrap will take part in a panel discussion ("Should COVID-19 Change Our Expectations About What Math is Taught") with Steven Levitt, author of Freakonomics, and Ruthe Farmer of CS for All, former Senior Policy Advisor for Tech Inclusion at the White House Office of Science and Technology Policy.
“Even before COVID, people were talking about the need for more data science content in K-12”, Kathi says. “COVID has both inspired many people to look at data and illuminated the challenges of shifting instruction online. In our session, we’re looking forward to discussing how to infuse more data science into K-12, within the broader conference context of the learning environment.”
More information on the Futures Forum on Learning, including details on how to register for this free event, can be found here.
Brandon J. Woodard, a Brown CS doctoral student advised by Professor David H. Laidlaw, has just won the NASA RI Space Grant Fellowship for 2020-2021, which provides stipend support for young researchers working toward their Master's or PhD degrees in a NASA-related field of study. Given by the NASA Rhode Island Space Grant Consortium, it will fund his research involving novel methods of modeling data of Earth's forests collected with the Global Ecosystem Dynamics Investigation LiDAR (GEDI), a remote sensing instrument aboard the International Space Station (ISS). It's the first step, Brandon says, of a multi-year plan of working on CS research that supports NASA's strategic goals.
"Space exploration and planetary sciences have been lifelong interests," Brandon tells us, "and my curiosity was especially fostered after multiple field trips to NASA's Jet Propulsion Laboratory (JPL) in grade school. As I toured JPL's campus I was very interested and intrigued by the amazing projects, like the Mars exploration rovers and Cassini. I learned that at the core of each of these missions, computers and computer science research were imperative to the mission's success. When I finally got the opportunity to intern at JPL, I realized how critical data is to NASA and learned that the majority of decisions and inferences made are through the interpretation of scientific data. In order to interpret data correctly, researchers need to be able to visualize and intuitively understand the information accurately – otherwise, a lot can go wrong. I gradually developed a fascination for visualizing data and decided to research scientific visualization." 
In his research, Brandon and his advisor propose to develop inferential models that will improve LiDAR-derived estimates of forest structure. Their research will increase the utility of the Global Ecosystem Dynamics Investigation (GEDI) instrument aboard the International Space Station and other large-footprint (large LiDAR beam; common for space-borne devices) LiDAR instruments through better data modeling. Capturing the underlying geometry and biology by utilizing GEDI LiDAR data that is unused in current methods of forest structure estimation will make this possible. Their research will provide ecologists with a better understanding of important carbon processes, biodiversity, and habitat of forests. The resulting research will positively impact NASA’s efforts in remote sensing and ecology for overall terrestrial health.
"My hope," Brandon tells us, "is that developing better methods to model Earth surveying data will provide scientists with a better understanding of Earth as a system, in particular the carbon processes, biodiversity, and habitats of forests."
To learn more about the fellowship, click here.
Brown CS alums Deqing Sun ‘13 (currently a research scientist at Google) and Stefan Roth ‘07 (currently a professor of computer science at TU Darmstadt), along with Brown CS adjunct faculty member Michael Black (currently at the Max Planck Institute for Intelligent Systems), have won the Longuet-Higgins Prize for their paper on optical flow estimation. Deqinq has also won the PAMI Young Researcher award (shared with Jon Barron of Google Research) for his contributions to computer vision.
The PAMI Young Researcher award is given out by the IEEE Pattern Analysis and Machine Intelligence (PAMI) Technical Committee, and aims to recognize young researchers who have made groundbreaking advances in computer vision within seven years of completing their PhD. The Longuet-Higgins award is presented by the IEEE Pattern Analysis and Machine Intelligence (PAMI) Technical Committee at each year’s CVPR, a premier annual computer vision event, for fundamental contributions in computer vision. The award recognizes CVPR papers from ten years ago with significant impact on computer vision research. 
Deqing, Stefan, and Michael’s award-winning paper was written in 2010, when both Deqing and Michael were at Brown University, and focuses primarily on the reasons behind recent advancements in the accuracy of optical flow estimation algorithms. The trio attempted to uncover the drivers behind these advancements via a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy.
In a recent retrospective blog post, Michael explains the journey behind his research and reminisces about some of his key takeaways from the process. “I’m still happy with this one and I feel so very fortunate to have had great students like Deqing and Stefan as collaborators and friends,” he explains. “That is the final, true, 'secret' of this paper: work with great people who you like, and enjoy the process.”
Professor Pedro Felzenszwalb of Brown CS and Brown's School of Engineering was also a winner of the Longuet-Higgins Prize in 2018.
In recent years, software defined networking has exploded in popularity as programmers have made a concerted effort to move the control plane (which handles traffic sent to a device or generated on the device) to a logically centralized machine. This development has led to breakneck innovation in the field of computer networking, and Professor Theophilus "Theo" A. Benson of Brown CS has seized the opportunity to explore even newer horizons. 
“Programmable data planes are essentially Lego blocks that can be built together and made into whatever you want,” Theo explains, “and they are responsible for packet processing and forwarding on network devices.” Programmable data planes are used for high-speed packet processing when handling packets that pass through a device and the forwarding logic it's capable of executing is mutable. It's this aspect that Theo’s work focuses on – exploring the programmability of these data planes. 
“Recent efforts to develop reconfigurable data planes and high-level network programming languages have made it possible to truly program the data plane – in other words, to change the way packets are processed on network devices,” says Theo. “The ability to fully program the network – both control and data plane – is expected to have a profound impact on the field of networking in the coming years.”
What inspired Theo to work on this field of study? For him, it was the limitless potential afforded by such technologies: “I had this idea of having programmable networks, cloud infrastructures that are robots, clouds that automatically figure out what their problems are and fix themselves, figure out resource allocation – with programmable networks we are a step closer.” Theo’s research has increasingly focused on this topic, with recent work including a workshop paper laying out the vision for programmable data planes, another paper discussing methods for debugging the data planes, a recent Eurosys paper exploring possible speed enhancements for this networking technology, and a keynote delivered at the International Workshop on Edge Systems, Analytics, and Networking. 
With such rapid progress, where would Theo say this research is headed in the future? “Well, we’ve started to look at some machine learning,” he says, “and we’re looking into discovering intelligent algorithms to put things together, and into figuring out how to make the network configure itself to make this dream happen.”
Held virtually this year due to the COVID-19 pandemic, the ACM SIGMOD/PODS Conference is a leading international forum for database researchers, practitioners, developers, and users to explore cutting-edge ideas and results, and to exchange techniques, tools, and experiences in all aspects of data management. This year, Brown CS students, faculty, and alums published seven papers at SIGMOD, on topics ranging from deep semantic compression for tabular data to benchmarking interactive data exploration:
DBPal: A Fully Pluggable NL2SQL Training Pipeline
Nathaniel Weir (Johns Hopkins University), Prasetya Utama (Technische Universität Darmstadt), Alex Galakatos (Brown University), Andrew Crotty (Brown University), Amir Ilkhechi (Brown University), Shekar Ramaswamy (Brown University), Rohin Bhushan (Brown University), Nadja Geisler (Technische Universität Darmstadt), Benjamin Hättasch (Technische Universität Darmstadt), Steffen Eger (Technische Universität Darmstadt), Ugur Cetintemel (Brown University), Carsten Binnig (Technische Universität Darmstadt)
DeepSqueeze: Deep Semantic Compression for Tabular Data
Amir Ilkhechi (Brown University), Andrew Crotty (Brown University), Alex Galakatos (Brown University), Yicong Mao (Brown University), Grace Fan (Brown University), Xiran Shi (Brown University), Ugur Cetintemel (Brown University)
The Case for a Learned Sorting Algorithm
Ani Kristo (Brown University), Kapil Vaidya (Massachusetts Institute of Technology), Ugur Çetintemel (Brown University), Sanchit Misra (Intel Labs), Tim Kraska (Massachusetts Institute of Technology)
Chiller: Contention-centric Transaction Execution and Data Partitioning for Modern Networks
Erfan Zamanian (Brown University), Julian Shun (Massachusetts Institute of Technology), Carsten Binnig (TU Darmstadt), Tim Kraska (Massachusetts Institute of Technology)
IDEBench: A Benchmark for Interactive Data Exploration
Philipp Eichmann (Brown University), Emanuel Zgraggen (MIT), Carsten Binnig (TU Darmstadt), Tim Kraska (MIT)
Database Benchmarking for Supporting Real-Time Interactive Querying of Large Data
Leilani Battle (University of Maryland), Philipp Eichmann (Brown University), Marco Angelini (University of Rome “La Sapienza”), Tiziana Catarci (University of Rome “La Sapienza”), Giuseppe Santucci (University of Rome “La Sapienza”), Yukun Zheng (University of Maryland), Carsten Binnig (Technical University of Darmstadt), Jean-Daniel Fekete (Inria, Univ. Paris-Saclay, CNRS), Dominik Moritz (University of Washington)
Extending Graph Patterns with Conditions
Grace Fan (Brown University), Wenfei Fan (University of Edinburgh, Beihang University & Shenzhen University), Yuanhao Li (University of Edinburgh & Shenzhen University), Ping Lu (Beihang University), Chao Tian (Alibaba Group), Jingren Zhou (Alibaba Group)
A full list of papers is available here. 
In the inaugural Analog Devices Real-Time Sensor Fusion Challenge, sponsored by MassRobotics, teams from Massachusetts and Rhode Island were invited to compete and submit projects that fuse at least two sensor inputs, using the Robot Operating System (ROS) to create a system that provides better context awareness in robots. Brown’s Actuated Robotics Guidance Systems team, which consisted of Brown undergrads Jung Yeop (Steve) Kim, Mckenna Cisler, and Howon Lee, advised by Professors George Konidaris and Stefanie Tellex, won First Place and were awarded $40,000. Their competition included not only fellow undergraduates but teams made up of graduate and post-graduate students.
The team explains that although falling is the leading cause of death for people over the age of 65, the aid and care devices used to support them have not improved over the past few decades. They designed an actuated walker that allows a user to walk naturally while the walker adjusts itself to the user's pace, keeping them safe and stable. Their approach uses multiple sensors to detect the center of mass of the user, as well as their intent (sitting down, going uphill or downhill, and so on) to autonomously brake/accelerate the walker to adjust its position relative to the user.
Their work can be found on this website and a demo video can be found here.
"Advising this team was a real privilege. When everything had to go virtual because of the pandemic, they simply accelerated their timeline. The resulting video really showed how the walker prevents falls," says Stefanie.
"It's really wonderful to see students being so creative, and working so hard, to design something that could really improve people's lives," adds George.
Although only the three students participated in the competition, the Actuated Walker team has grown into an interdepartmental effort across College Hill, consisting of professors, PhD and Master’s students, and undergraduates in Brown’s School of Public Health, Medical School, Department of Computer Science, and School of Engineering, as well as RISD’s Department of Industrial Design.
“We were able to get input from Brown’s School of Public Health from early on in the project, and we wouldn’t be here if it weren’t for their continuous advice,” says Jung Yeop (Steve). “Brown’s culture of interdepartmental collaboration really pushed our project forward, and we’re not sure if we could have pulled this off anywhere else.”
“Our project is a unique combination of the hardware challenge of building a usable, sensor-equipped walker with the software and robotics challenge of intelligently actuating the walker to keep the user from falling,” says Mckenna. “Having team members and advisors from both these sides has really helped us build a well-integrated and capable walker system.”
Hannah Dunnigan is studying Industrial Design at RISD. “Our collaboration and breakthroughs on the walker project demonstrate a larger interdepartmental cooperation,” she says, “a strong partnership between Brown and RISD, and an ever-evolving connection between research and design.”
Facebook recently announced the recipients of their research awards on privacy-preserving technologies and the role of applied cryptography in a privacy-focused advertising ecosystem. Out of 164 proposals submitted, the company chose 14 winners, which include a Brown CS faculty member and two alums. A third alum was a finalist in the competition.
These interconnections demonstrate a long Brown CS tradition of research and teaching on all aspects of cryptography, anonymity, privacy and security, ranging from studying theoretical foundations to building real-world systems. Anna, for example, was Foteini's doctoral advisor, and Brown CS Professor Roberto Tamassia, also a faculty member with the Master of Science in Cybersecurity program, advised both Charalampos and Olya. Charalampos, in turn, was Yupeng's advisor.  
“Selecting winners from this impressive set of proposals was not easy,” says Sharon Ayalde, Facebook's Research Program Manager, Academic Relations. “We look forward to seeing the outcome of the research described in these outstanding proposals and to collaborating further with these experts.”
The full list of winners and finalists is available here.
Brown University's Department of Computer Science (Brown CS) is pleased to announce that two of its faculty members received named chairs at the May, 2020 meeting of the Corporation of Brown University. Michael Littman is now the Royce Family Professor of Teaching Excellence in Computer Science and Ellie Pavlick is now the Manning Assistant Professor of Computer Science.
Awarded for terms of three years each, and renewable for one successive term, Royce Family Professorships recognize, reward, and encourage innovation and excellence in teaching among the Brown University faculty. The Manning Assistant Professorships were established by Martha Sharp Joukowsky, Class of 1958, LHD 1985, and Artemis A. W. Joukowsky, Class of 1955, LLD 1985, parents of Michael W. Joukowsky, Class of 1987, and they honor James Manning, Brown's first president.
"I'm very flattered to have been chosen," Michael says.  Given Brown's deep commitment to education, it's quite humbling and a little intimidating to have an official title that shines a spotlight on 'teaching excellence'."
Littman earned his doctorate from Brown CS in 1996 and has been a member of the faculty since 2012. Currently co-directing Brown's Humanity-Centered Robotics Initiative, he works mainly in reinforcement learning, but has done research in machine learning, game theory, computer networking, partially observable Markov decision process solving, computer solving of analogy problems, and other areas.
Michael has earned multiple awards for teaching and research and has served on the editorial boards for The Journal of Machine Learning Research and The Journal of Artificial Intelligence Research. He served as General Chair of the International Conference on Machine Learning and Program Chair of the Association for the Advancement of Artificial Intelligence (AAAI) Conference in 2013. He's also an AAAI Fellow and was Co-Chair of the recent Reinforcement Learning and Decision Making Conference, held last year in Montreal. Mostly recently, he was named an ACM Fellow and an AAAS Leshner Fellow.
"I am extremely flattered," Ellie tells us. "It's such a privilege to be a part of Brown's community – the department is supportive, the students are bright and curious, I really couldn't imagine a better place to be building a lab."
Pavlick received her PhD in Computer and Information Science from University of Pennsylvania in 2017 and joined the Brown CS faculty in the same year. Her current research is in Natural Language Processing, specifically on computational models of semantics and pragmatics that emulate human inferences.
Also a research scientist at Google AI, Ellie won the *SEM Best Paper Award ("So-Called Non-Subsective Adjectives") and received a Facebook PhD Fellowship ("Meaning Variation in Paraphrase") in 2016. Most recently, she was a keynote speaker at the 2019 International Conference on Computational Semantics (IWCS) and gave invited talks at the 2019 New England Machine Learning and Harvard Linguistics Universals Colloquium Series. She was also Team Co-Lead for the Workshop on General Purpose Sentence Representation Learning at the 2018 Johns Hopkins Summer Workshop on Language Technology (JSALT) and Area Chair of Sentence-Level Semantics for the Association for Computational Linguistics in the same year.
Ellie and Michael join seven other holders of Brown CS endowed professorships: Eugene Charniak (University Professor of Computer Science), Maurice Herlihy (An Wang Professor of Computer Science), Sorin Istrail (Julie Nguyen Brown Professor of Computational and Mathematical Sciences and Professor of Computer Science), George Konidaris (John E. Savage Assistant Professorship in Computer Science), Roberto Tamassia (Plastech Professor of Computer Science), Eliezer Upfal (Rush Hawkins Professor of Computer Science), and Andries van Dam (Thomas J. Watson, Junior University Professor of Technology and Education and Professor of Computer Science).
Brown University's Department of Computer Science (Brown CS) is reporting another record-breaking graduating class. According to Vice Chair Tom Doeppner, Brown CS graduated 297 undergraduate students, its highest total ever, as well as 89 Master's students and eight PhDs.
"297 students is a 14.7 increase over last year," says Tom, "which in turn saw a 4% increase over the prior year. We're already Brown's most commonly chosen concentration, and enrollment is not only still growing but accelerating."
39% of the undergraduates are women, a figure that's almost twice the national average.
Due to the COVID-19 pandemic, Brown CS saluted its graduates virtually, creating a website of celebrations that included congratulatory posters, slideshows, historic photos from the Brown CS Digital Archive, and a video of faculty members singing a Gilbert-and-Sullivan-inspired song. The video has since gone viral, earning thousands of views and praise from Ruthe Farmer, formerly the Senior Policy Advisor for Tech Inclusion at the White House Office of Science and Technology Policy.
Brown CS PhD student Jing Qian (advised by Brown CS Professor Jeff Huang) has been working at Adobe Research to develop a means to bridge the gap between physical and digital documents with augmented reality (AR) technology. His work has culminated in the creation of Dually Noted, which gives users the ability to annotate documents and then view the annotations in real time as an AR overlay over the actual physical document. These annotations can be shared directly and immediately with other users, and users can then view the annotations by simply hovering their phones over the text in question.
“I really first became interested in this field when I collaborated on my first AR project in the MIT fluid interface group,” Jing explains. “And then I met Jeff at Brown, I found the ideas to be really interesting, and I decided to come work with him.” This has led to Jing specializing in Human Computer Interaction and subsequently joining the HCI Research Group. “Jing has been exploring the area of mixed reality interaction through his PhD,” Jeff says, “and he can build almost anything.”
But why did HCI appeal so much to Jing? For him, it was the unique opportunity to work at the intersection of design and technology, giving users the ability to unlock the full potential of the tools at their disposal. “In general, if we have a really cool piece of technology, whoever developed it can use it very well,” Jing says, “but for real-life impacts and real-life benefits we need to expose this technology to users. HCI works to provide a really good methodology and systematic set of ways so that the technology can be used by all kinds of people.” 
Jing brought those ideas of innovation to Adobe, where he played a pivotal role in the design and development of their Dually Noted software. “I noticed that people are shifting to digital document consumption, but at the same time this leads to a loss in the tangible experience of physical mediums,” says Jing. “I wanted to find a way to preserve and combine the physical reading experience and the digital experience.”
Jing has undoubtedly accomplished a great deal through his work at Adobe, but he has even greater plans for the future. One aspect that especially interests him is the integration of user behavior data with AR. “It would make sense to get user behavior data and actually customize AR interfaces based on their behavior,” he explains, “and I want to continue working on using AR to bridge the gap between technology and its users.”
You can read other articles about Dually Noted here, here, and here. 
Brown University has just announced the award of $350,000 in a new COVID-19 Research Seed Fund, and three Brown CS faculty members are among the recipients. Brown established the fund to fast track innovative research proposals that directly address the urgent needs of the COVID-19 pandemic. The grants focus on projects that best support research with potential significant, rapid impact on human health and create products of immediate need for the health care system in Rhode Island and the nation.
Along with the project lead, Dr. Harrison Bai (Diagnostic Imaging), Brown CS Professor and Department Chair Ugur Cetintemel and Professor Ritambhara Singh of Brown CS and Brown's Center for Computational Molecular Biology are the investigators of a grant for research that will develop an AI platform to differentiate COVID-19 from other viral pneumonia on chest CT, and use the information to identify early-stage patients who are likely to transition to severe disease.
Brown CS Professor Sorin Istrail is Co-PI of a grant led by Primary Investigator Elizabeth Chen of the Brown Center for Biomedical Informatics that will establish an inter-institutional informatics infrastructure to support COVID-19 research in Rhode Island through electronic health data, digital health technology, and data science techniques. The other CO-PIs are Philip Chan (Medicine), A. Rani Elwy (Psychiatry and Human Behavior), Fizza Gillani (Medicine), Joseph W. Hogan (Biostatistics), and Indra Neil Sarkar (Brown Center for Biomedical Informatics and Rhode Island Quality Institute).
A full list of recipients is here and a News from Brown story about the grants is here.
Brown CS has just announced that it will recognize 17 graduating seniors for their achievements this year. Evan Cater, Min Jean Cho, Tyler DeFroscia, Megan Gessner, Brad Guesman, Samuel Hinthorn, Shawna Huang, Brendan Le, Joshua Levin, Zhengyi Peng, Heila Precel, Lena Renshaw, Shoshana Simons, Henry Stone, Andrew Wagner, and Solomon Zitter will each receive the Senior Prize in Computer Science for their academic work as well as their service to Brown CS. The prize will also be awarded posthumously to Will Povell.
"Along with Eleanor Avril, who won the Norman K. Meyrowitz '81 Award, these are our top students," says Associate Professor (Research) and Vice Chair Tom Doeppner. "As a group, they've done superior work, but we were also very impressed with how generously they were able to give back to the department and their fellow students."
"Brown CS has been a home for me for the past four years," says Solomon. "Between all the late nights (which often stretched into early mornings) at the CIT, the fantastic friends I’ve made, and the incredible learning I’ve done, I couldn’t have asked for a better way to spend my time at Brown. I've especially cherished the opportunity to give back to the department and reinforce my own learning through TAing and HTAing a wide variety of courses, experiences which I believe would have been nearly impossible if it weren’t for the unique structure of this department. I also was lucky enough to have the chance to get involved with Professor Philip Klein’s research group, where I have been able to develop new skills and new ways of thinking."
Joshua and Shawna also mention the importance of community. "I’m so grateful for the opportunity," Joshua tells us, "to have learned to love CS in a community so welcoming, supportive, collaborative, and inspiring." Shawna says that she's made lifelong friends, taken challenging courses, discovered new passions, and grown as a person. "It's also been so rewarding to help other students as a TA, HTA, and MTA," she tells us. "It's truly been a privilege to give back to this community and make a difference for the next generation of students."
Zhengyi focuses on the transformational aspect of computer science: "What always fascinates me, both in being a TA and doing web development, is that we can make knowledge and service more accessible, liberating from discriminations and prejudices."
Finally, Lena echoes many of her fellow students with thoughts that turn to classmate Will Povell: "I’m incredibly thankful to have had such a supportive environment to learn computer science in. I’m humbled to share this award with my extraordinary peers, especially Will, who I know all of us miss incredibly."
Brown University's Department of Computer Science has just announced that Eleanor Avril, a Brown CS student and one of the four Meta-TAs who coordinate the Undergraduate Teaching Assistant program, has just won the Norman K. Meyrowitz '81 Award. Named for an alum known for his contributions to the department, the award recognizes exceptionally meritorious service to Brown CS and is accompanied by a cash prize of five hundred dollars.
"We are delighted to say that it is warranted this year," says Brown CS Professor Tom Doeppner, Vice Chair and Director of Undergraduate Studies, "for Eleanor's dependable eagerness to help out in any situation, and for doing so ever so competently and cheerfully."
Norm Meyrowitz feels similarly. "It was an honor to have this award named after me years ago in the Triassic period of CS," he says, "and it is even more of an honor to have someone as deserving as Eleanor receive it.”
Lorenzo De Stefani is the most recent hire in the multi-year CS With Impact campaign, the largest expansion in Brown CS history. One of the most fortunate things in his early life, he tells us, was the strong relationship that his father, an electrical engineer, had with his former university advisor. “It gave me this idea,” says Lorenzo, “of universities as being a little bit magical.” 
Having successfully defended his doctoral thesis last month, he’s returning to Brown CS in the fall as lecturer. It’s time for him to be part of that magic in a different way, and he’s looking forward to meeting his new students and sharing his love of intellectual challenge. “I like hard questions,” he says, “and the freedom of thinking that theory allows. It helps you appreciate the other things you do, and we lose something without it.”
A native of Treviso, in Italy, Lorenzo studied computer engineering at the nearby University of Padova in a class of 120 students, only six of whom were women. “There’s much more diversity and varying social dynamics in America,” he says. “For some Europeans, the differences can be scary at first, but I see them as interesting. They make me want to learn more about what’s happening and follow the political discourse. As an instructor, diversity leads me to make my course content as approachable and inclusive as possible.”
Originally drawn to computer engineering by his curiosity about how and why computers work, Lorenzo’s interest was soon piqued by the theoretical aspects of computer science, and he met Professor Gianfranco Bilardi, who later became his Master’s and PhD advisor. Most interesting to him, Lorenzo says, were the fundamental questions of how to characterize computation with restrictions in resources, particularly memory. “It’s not too fashionable these days,” he says, “and it can be intimidating to see how the list of experts in this area intersects with a list of Turing Award winners. But it’s a very good mental gym. I’m still actively working on these topics.”
Today, Lorenzo’s research focuses on statistical learning and algorithms for knowledge discovery from big data with probabilistic guarantees. He describes himself as a multitasker, interested in analysis of statistical properties of time series and designing algorithms for large graphs, as well as modeling user preferences given incomplete rankings and visually representing data. 
As someone with a wide scope of interests, he found Professor Eli Upfal of Brown CS to be an ideal advisor: “I’m used to having a broad range of research activities. Eli is great at fostering initiative and curiosity in his students, and he encourages them to pursue their research interests, provided that they’re sound and significant. Instead of being hyper-specialized, he’s a master of a very versatile and important toolkit, which is applying probability to computer science.”  
That being said, we hope Eli can forgive us for speculating that he was only the second most important person that Lorenzo met in Providence. Beginning his doctoral work in 2014, his very first conversation in the CIT was with fellow PhD student, Megumi Ando. They soon began dating and were married two years later: Lorenzo’s plans of getting his degree in four years and returning to Italy were quickly altered. Together, the couple enjoys watching Netflix documentaries and cooking food from around the world. Of course, Italian cuisine abounds in Rhode Island, but does it match what Lorenzo was used to at home? “The food is more like ‘Italian-inspired’ here,” he says charitably. “More of a fusion.”
Lorenzo will be teaching CSCI 1010 Theory of Computation and co-teaching CSCI 1570 Design and Analysis of Algorithms with Roberto Tamassia this fall. His prior experience with CSCI 1010 has left him eager for more. Last year, he explains, a fire alarm went off a half-hour before the final session of class ended. Expecting only a handful of students to return after the sirens were silenced, he was delighted to see that all but one came back inside for the final minutes. “Our students are very interested,” he says. “They engage with their TAs, and they want to be TAs. Given an opportunity, they always want to participate.”
Lorenzo also intends to continue researching and pursuing grants, seeing them as ideal ways to recruit undergraduate research assistants. “I’m lucky enough to have more ideas than time,” he says. “I want to share those ideas with students and see where they go with them.”
As a former Brown CS student, he also has the rare perspective of someone who’s seen our doctoral program from within. “Brown takes very good care of its students,” he tells us, “but being a PhD student is stressful. Providing them with a good support structure is so important, especially for international students – I knew that any time I had an issue, I could go to Lauren Clarke, who could give me advice or fix something for me in hours, not days. Students also need good advisors who are able to distinguish between effort and success.”
“Talking to students is something I really enjoy,” he says, “and I’m always happy to do it. I have a certain amount of expertise in various topics, and a lot of projects fall under those areas. I like talking about research, especially with people who have different interests, because I always learn something from how they approach things.” 
In particular, he says when we press him on the subject, he’s looking forward to sharing his love of theory and of truly challenging projects. “I have maximum respect for the more experimental tracks of computer science,” Lorenzo says, “but I prefer being able to think about my work at any time of day, without going into the lab. Not every problem I work on has to be a big one, but I like the ones that push us out of our comfort zones. When we get through them, we’re better equipped – and it’s more satisfying.” 
The longest continuously running graphics group in the known universe, Brown Visual Computing, has just released a new website, https://visual.cs.brown.edu/. Dating back to 1966, the group is currently composed of Brown CS Professors Jeff Huang, John "Spike" Hughes, David Laidlaw, Barbara Meier, Daniel Ritchie, James Tompkin, and Andries "Andy" van Dam and their student advisees and researchers. They develop technology to make and make sense of visual data, leveraging both principled physical models and data-driven methods to synthesize, edit, and explain visual data. The new site shares the group's research vision, lists their PhD alums and current courses, provides publication links and open-source code links, offers a Twitter feed to share the latest Brown Visual Computing news, and even contains a few graphics-related easter eggs.
Less than a year after his arrival at Brown, Professor Malte Schwarzkopf of Brown CS has received a Richard B. Salomon Faculty Research Award. The award, given annually by Brown’s Office of the Vice-President for Research, was established to support excellence in scholarly work by providing funding for selected faculty research projects of exceptional merit with preference given to junior faculty who are in the process of building their research portfolio.
Asked to situate his research, Malte explains that comprehensive data protection laws such as the European Union's recent General Data Protection Regulation (GDPR) better protect citizens' sensitive data, but impose a high cost for compliance on organizations that operate digital services. The expense, he says, comes partly because retrofitting compliance onto current computer systems is difficult, manual, and time-consuming.
"In this project," Malte tells us, "we seek to understand where current software abstractions and common practices come into conflict with laws like the GDPR, and to develop new computer systems designs that address these problems and make data protection a primary design concern. One example will restructure the databases that web services use today as federations of per-user micro-databases. Users can add or remove their personal micro-database at any time, and all data related to them are stored in their personal micro-database. Applications – such as a social network or an e-commerce site – then combine different users' micro-databases into what database literature calls 'views' computed over their combined information (e.g. a list of top rated posts). These views and the derived information they contain change automatically as the underlying set of micro-databases changes (e.g. because a user unsubscribes her data). One of the major challenges for this research is to design and build software systems that can build and maintain such views over hundreds of thousands or millions of micro-databases with the same performance that today's software delivers over a single database."
Malte hopes to build a Brown-based research group that turns this idea into a real system, and to build collaborations both with industry and non-CS academics interested in data protection. Currently, Brown has isolated centers of interest in data protection legislation and its impact, but this effort will seek to bring them together.
"This project," Malte says, "will unite these groups, introduce industry and off-campus perspectives, establish Brown as a household name in this emerging field of research, foster interdisciplinary thinking in the Brown spirit, and seek to create new technology that makes data protection a primary design goal."
A full list of awardees is available here. Malte joins multiple previous Brown CS winners, including (most recently) Theophilus A. Benson and George Konidaris. 
Brown CS PhD student Lucy Qin (advised by Brown CS Professor Seny Kamara) has just received an NSF Graduate Research Fellowship for her work in applied cryptography. The award is the oldest graduate fellowship of its kind, and aims to recognize and support outstanding graduate students in the fields of science, technology, engineering, and math.
Lucy’s previous work focused on the design and implementation of secure multi-party computation, which allows mutually distrustful parties to jointly compute functions over their private data. Her research has contributed to projects that span a wide variety of applications, including one that evaluates the pay gap in Boston and another that aims to build a solution for detecting serial perpetrators of sexual misconduct. Looking forward, Lucy is working with Seny and others in the Encrypted Systems Lab to build a novel cryptographic protocol that will aid policymakers in advocating for explicit privacy guarantees in an upcoming piece of legislation.
“I’m inspired by how cryptography can be used as a tool in rearranging power and would like to focus on projects that prioritize the privacy needs of groups traditionally overlooked in technology design,” Lucy says. “My hope is that this fellowship will open up greater opportunities to conduct interdisciplinary research and assess the technology I’m building through a more holistic and critical lens.” Lucy joins eight other prior Brown CS winners of the fellowship, the most recent winner being Jeff Rasley.
Professor Donald Stanford '72 ScM '77 of Brown CS has just been recognized for his dedication to student mentoring, particularly with aspiring entrepreneurs, with the Hazeltine Mentoring in Entrepreneurship Award. Founded by a group of alums, it honors Professor Barrett Hazeltine, who has been mentoring and inspiring Brown University students for decades.
"Don was one of the first people in my career to take me under his wings and show faith in me," remembers Adi Dhandania, now Vice-President of Strategy and Corporate Development at Twin River Worldwide Holdings and currently an Executive Master's student in EMSTL. "Going back 10+ years ago, I still remember the joy and excitement I would get from listening to the engaging stories that Don would share at intern lunches. His enthusiasm and commitment to teach and develop young professionals is unparalleled. His mentorship goes far beyond entrepreneurs and has made a significant difference for students as well as early-career, mid-career, and late-career professionals. His support of me has been invaluable to both my personal and professional growth, and I will always be grateful for that."
Jack Roswell, co-founder of Cloud Agronomics and another of Don's mentees, feels similarly: "Don is revered by many due to his technical expertise, kind heart, and keen eye for talent. My first encounter with Don was nothing short of serendipitous, and put simply, I wouldn't be where I am today if he had not taken me under his wing."
In addition to teaching and mentoring students for decades, Don is a founding member of GTECH (now IGT Global)  and began working for the start-up company in 1979 as Manager of Software Development when it had less than 10 employees. Over 40 years, he has held every technical leadership position in the organization, including Vice President of Advanced Development, Chief Technology Officer, and Chief Innovation Officer. Don guided the growth of GTECH’s technology organization from a software staff of four to its current worldwide deployment of over 1,200 technology professionals. Under his leadership, GTECH advanced the state of the art in transaction processing, secure retail systems, and wireless communications, enabling it to lead the Gaming industry worldwide and install systems in over 80 jurisdictions on six continents. Don serves on several boards, including YearUp Rhode Island, Spectra Systems, and the Business Innovation Factory. He is also a past member of the Rhode Island Science and Technology Advisory Council and a co-founder of Times2 STEM Academy, Rhode Island’s very first K-12 charter school.
In 2001, Don was appointed Adjunct Professor of the Practice in Computer Science at Brown, and he's been teaching undergraduates since 2002. He's also an Adjunct Professor in the School of Engineering and in the School of Professional Studies, where he teaches Science and Technology Leadership. In the past he has served on the Brown advisory councils to the President and the School of Engineering. In 1999, Don received the Black Engineer of the Year Award for Professional Achievement and the Honorable Thurgood Marshall Award for community service from the NAACP. In 2002, he received the Brown Graduate School’s Distinguished Graduate Award and the Rhode Island Professional Engineer’s Award for Community Service.
"I am truly honored," Don tells us, "to be mentioned in the same breath as the legendary Barrett Hazeltine. I have known Barrett since I was a freshman student at Brown in 1968 and have followed his meteoric success in fostering entrepreneurship studies and activities at Brown over the past decades. On many occasions I have been contacted by a Brown student whose first words were, ‘Barrett Hazeltine sent me.’ For me, it is a gift to be able to interact with students in developing their vision for ideas that have potential impact on our society. It is a great time to be at Brown and I am grateful for the opportunity in my retirement to participate in the overall effort and ecosystem that makes Brown such a special place. Being able to teach and work at Brown allows me to return to school and learn something new every day from colleagues and students. The slogan 'Ever True to Brown' are words that I take to heart.”
This year's other award winner is Professor Jennifer Nazareno of the School of Public Health and the Nelson Center for Entrepreneurship. Don and Jennifer will receive their awards in a virtual ceremony on Thursday, April 30, at 4 PM. If you're interested in attending, please RSVP here, and a Zoom link will be sent to you.
by Kevin Stacey (Science News Writer, Physical Sciences)
One of the keys to safely ending COVID-19 lockdowns across the country is contact tracing — identifying people who have been exposed to the virus to prevent them from spreading it. A Brown University computer scientist is part of an international research team trying to enable contact tracking with smartphones in a way that preserves everyone’s privacy.
Anna Lysyanskaya, a professor of computer science and a cryptography expert, is working on an MIT-based project called PACT: Private Automated Contact Tracing. The team is developing a system that uses the Bluetooth signals that smartphones exchange all the time to track which devices have come in close contact to each other. The system makes it possible to notify people that they may have been in contact with an infected person, but without revealing any private information to other individuals, the government, health care providers or cellular service companies.
Lysyanskaya says that a system like this helps to strike a balance between the need to track infections and the need to preserve people’s privacy.
“I think that this time when people are dying and everybody’s stuck at home, it’s tempting to say, ‘well let’s give up privacy; let’s give up human rights; let’s give up democracy’ — anything to stop this,” she said. “But we need to not yield to that impulse. We can do automated tracing, which could really improve outcomes for containing this disease, while preserving privacy.”
Lysyanskaya discussed the details of the project in an interview.
Tracing contacts without any assistance from an automated system is a very labor-intensive process. And relying on human memory to recall who have I interacted with, where have I been is less than perfect. That’s especially true when someone has just been diagnosed with a devastating disease. They might not be able to recall everything they’ve done in the last few days when they were spreading the infection. Anything that can relieve the burden from the sick person to have to recall things — and also from medical professionals who are extremely busy — is very advantageous. Automated contact tracing can do that.
There are many ways that you can do automated contact tracing that aren’t very private. You could just carry around a device that exchanges contact information every time it comes in a close enough range to another device. That way if you exchanged information with someone who was infected, you could find that out. But that would not be a good idea from the point of view of privacy because now you know everything about everybody you run into. 
We’ve been working on doing contact tracing in a way that, as much as possible, doesn’t intrude on people’s privacy. 
The goal of this project is to record interactions but without leaving any identifying information about yourself behind. The way it works is that instead of providing your contact information, you just provide a completely random string of bits, called chirps. These chirps are produced by BLE, or Bluetooth low energy. It’s a technology that’s similar to what you use, for example, to attach Bluetooth-enabled devices, like headphones. 
What we’re proposing is an app that stores all the chirps that people’s phones send and receive. So if I’m diagnosed, I can simply tell the app about it, and it will take all the chirps I’ve emitted over the past 14 days and publish them to a database. Everybody who has this app installed is connected to this database. At regular time intervals, the app compares the database of chirps from infected people with their locally stored chirps. If there’s an overlap, then that person knows they may have been exposed and they should contact a health care provider.
The important thing is that these chirps are just strings of random numbers that don’t identify you in any way. They’re meaningless in the absence of anything else happening in the system. But if somebody is diagnosed with the disease, and they let the app know that, then the chirps become useful. We can then tell if we’ve contacted an infected person without knowing who they were.
First of all, with GPS there are issues about whether you’re revealing someone’s location, which is problematic in terms of privacy. But there are other issues. My understanding is that GPS coordinates just don’t give you fine-grained enough information about how close you were to somebody. Bluetooth on the other hand is designed to work over a small range, so it’s a pretty decent proxy for how far you are from another person.
And your phone is automatically generating these random strings in connection with Bluetooth, so it’s something we can take advantage of.
Last week Apple and Google announced that they’re providing support in their operating systems for the type of contact tracing apps that rely on these Bluetooth chirps. They make the operating systems for the vast majority of devices out there, so that’s a big step. 
From there, the app has to be created, and we’re still thinking about the best technical approach for that. Once the app is ready, the question becomes how do we get it on peoples’ devices. One way is that it could be included in your next operating system update. If Apple and Google are willing, then it will happen by itself. If they weren’t willing to push an app like that, then maybe individual states or jurisdictions might say, ‘You need this app’ and put out advertisements telling people to download it. That would be much more difficult, because people don’t just install apps because you tell them to.
But first we need to have an app and it needs to be usable. Then we can worry about how we get people to use it.
The group I’m involved with has its gravity center at MIT, specifically Ron Rivest, who was my Ph.D. advisor a long time ago. At some point I was one of those people who said, “Oh you’re doing this? Here are my ideas for how to do this even better.” The group just kept growing and growing. 
This is an amazing effort and really inspiring. For the last few weeks I've been stuck at home getting stir crazy and a little bit depressed. But this has jerked me out of that. Not everybody has an outlet like this. I’m very lucky that I work in a field that gives me this kind of an outlet where I can feel happy, even in these circumstances. For those people who have an outlet like this, use it. It’s going to make you feel great.
A recent Brown Daily Herald article provides the story behind a new online replica of the Brown University campus. A joint effort between Brown Esports and Geopipe, it's already providing a valuable service to prospective students during the COVID-19 crisis.
Thomas Dickerson, a Brown CS alum, is a co-founder of Geopipe, which builds algorithms to turn 2D and 3D data into highly detailed 3D virtual models. "We initially heard what Brown Esports was planning from an undergraduate intern over the summer," he tells us. "At the time, there was no pandemic on the horizon, so it mostly seemed like a fun and quirky way to participate in the Brown community. That's especially true because Geopipe's roots are in a project that my co-founder and I collaborated on several years ago to  reproduce Manhattan at a 1:1 scale in Minecraft."
When the pandemic hit, Thomas says, the project took on new urgency: "We're hoping it will provide prospective students with a way to get a feel for Brown. Obviously, the CIT is my favorite building on campus, so I took special care to make sure it was reproduced well."
The original article can be found here.
To explore an online map of the replica, click here.
To access the replica in Minecraft (Java Edition only), launch the Minecraft client for version 1.15.2, then click Multiplayer –> Add Server –> enter mc.brownesports.org for the IP address –> click Join.
Brown University undergraduate Nishanth Kumar, a Computer Engineering concentrator and Brown CS researcher, has recently received the Barry Goldwater Scholarship for his research into Learning from Demonstration (LfD). The scholarship was established by Congress in 1986 to identify and support the next generation of research leaders, and is widely regarded as one of the most prestigious undergraduate scholarships in the natural sciences, mathematics, and engineering in America. Nishanth joins fellow Brown students Adam Tropper (Physics and Astronomy concentrator) and Lucas Sanchez (Chemistry concentrator) this year as a scholarship recipient.
Nishanth’s research focuses on teaching robots to learn real-world skills directly from observing demonstrations. “LfD allows non-expert operators to program skills simply by demonstrating them many times,” he explains, “and these learned skills are more general: they are able to handle slight variations of a task, such as if an object to be placed is slightly misplaced.” The issue with current LfD techniques, however, is that they train skills that are unable to target specific goals from many possible choices (i.e. targeting a specific button within a grid) without copious amounts of training data. 
“To combat this issue, I helped propose a method that learns skills that are parameterized by a goal parameter,” Nishanth says, “such that altering this parameter correctly alters the skill. In the button pressing scenario, instead of training a new skill for each button, we train one general skill that adapts itself depending on where the button is.”
Looking forward, Nishanth is ready to continue solving some of the most practical problems in Artificial Intelligence: “After winning the scholarship, I’ve felt a deep responsibility and motivation to continue my research into AI and robotics. I believe the advent of intelligent, collaborative robots can massively change the world for the better and I hope to play some part in making this dream a reality.”
Chosen by an esteemed panel of Brown University faculty, Professor Michael Littman of Brown CS has just been given Brown's Distinguished Research Achievement Award. Now in its fourth year, the Research Achievement Award program is supported by the Office of the President and the Provost to nurture and recognize the extraordinary research contributions of faculty. Michael is the first Brown CS recipient of the award, which comes with a one-time $5,000 research stipend.
Michael has earned multiple awards for teaching and research and has served on the editorial boards for The Journal of Machine Learning Research and The Journal of Artificial Intelligence Research. He served as General Chair of the International Conference on Machine Learning and Program Chair of the Association for the Advancement of Artificial Intelligence (AAAI) Conference in 2013. He's also an AAAI Fellow and was Co-Chair of the recent Reinforcement Learning and Decision Making Conference, held last year in Montreal. Mostly recently, he was named an ACM Fellow and an AAAS Leshner Fellow.
More information about the Research Achievement Awards and the other winners is available here.
Game playing has long been a subject of fascination within the AI field, and Brown CS Professor Amy Greenwald recently hosted a AAAI Panel, “Advancing AI By Playing Games”, with some of the most influential figures in this specialty. The speakers included Dr. Michael Bowling of the University of Alberta (leader of the research group that built the first poker playing AI to beat professional players), Dr. Murray Campbell of IBM (member of the team that developed Deep Blue, the first computer chess program to defeat a world champion), Garry Kasparov (world-renowned chess grandmaster), Dr. Hiroaki Kitano (CEO of Sony Computer Science Laboratories), and Dr. David Silver of University College London (leader of the AlphaGo project, the first computer Go program to defeat a world champion). These individuals are responsible for some of the most pivotal advancements in Game-Playing AI. 
“They are incredibly inspiring people,” says Amy when asked about her experience hosting such a star-studded panel. “All of them are really deep thinkers.” This natural curiosity for learning is clearly evident in their work, research that has been crucial in driving the break-neck speed of discovery in AI. “They work on problems for which success is not guaranteed,” explains Amy. “It is a high risk/high reward enterprise.”
Why has there been such a fascination with game-playing AI in particular over the past several decades? “Well, we all love playing games,” laughs Amy, “but beyond that, being good at playing games and solving puzzles has always been a sign of intelligence.” She explains that success at game-playing serves as a benchmark of human intelligence; likewise, it provides a definitive way to measure just how “smart” an AI really is. “The speed of discovery is increasing at an astronomical rate. There are multitudes of people in countries like China and India, and these countries prioritize and emphasize computer technology.”
Even with such rapid development, game-playing AI has encountered a number of challenges that have yet to be overcome. “Most of the successes have been in two-player zero-sum games, where there is a winner and a loser. Games with more than two agents, and general-sum games –that have both competitive and cooperative elements– are very, very challenging,” explains Amy. Even with these obstacles, however, the future of AI is bright. “Dr. Kitano’s Robocup organization has set a goal of robots winning the world cup in soccer by 2050. The kind of teamwork required to play soccer well may translate well, for example, to disaster response, and there are many other real-world applications for game-playing AI.”
The Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, given this year to Casey Nelson and Dylan Sam to support their work with Brown CS Professors R. Iris Bahar and Stephen Bach, respectively, recognizes strong achievement from undergraduate researchers and offers them the opportunity to continue their work over the summer.
A generous gift from Peter Norvig '78 (a Director of Research at Google and a thought leader in the areas of artificial intelligence, natural language processing, information retrieval, and software engineering) established the award, which provides $10,000 annually to support an undergraduate engaged in an intensive faculty-student summer research partnership. The gift honors the life and work of Randy F. Pausch '82, a renowned expert in computer science, human-computer interaction, and design who died of complications from pancreatic cancer in 2008. "His story is inspiring," Peter says, "and this is an opportunity to remember him."
Casey explains that she began collaborating with Iris last summer. “She was my first year advisor, and talking with her about her work during our advisor meetings led to me joining her project last summer.” Casey’s research focuses on using near data processing (NDP) as a solution to the memory wall problem, which is the bottleneck in performance due to the increasing gap between memory access speeds and processor speeds. “Using NDP involves making hardware modifications (like moving processing closer to memory) and software modifications to take advantage of these hardware changes,” she describes, “and we specifically focus on using NDP to optimize concurrent, pointer chasing data structures, whose poor cache locality causes bottlenecks which NDP might be able to eliminate.”
How might Casey’s work be used in the real world? It seems like there are a variety of possibilities. “Currently, I’m working on applying these NDP data structures to realistic applications to get a measure of the real world benefits,” she says, “and I’m specifically looking into applications in databases and security.”
Dylan, on the other hand, began working with Steve his sophomore spring. “I started out reading papers and discussing them, focusing on the deeper underlying theories behind machine learning models,” he says, “and I gained an appreciation of his background in weakly supervised machine learning, which I was immediately interested in.” This interest led to Dylan focusing on weakly supervised machine learning for his research project. “In modern machine learning applications, models require large amounts of labeled data,” he explains, “and weakly supervised learning looks to solve this bottleneck and find other solutions when labeled data is not readily available.” 
More specifically, Dylan’s work focuses on the underlying theory behind why weakly supervised strategies work. “Steve and his collaborators at Stanford have worked on projects including Snorkel, which empirically show that these weak supervision strategies perform well,” Dylan explains. The biggest issue, Dylan says, is that these strategies miss a theoretical explanation as to why they work. How exactly does Dylan plan to solve this problem? Well, it really requires a multifaceted approach. “My project first looks to survey the field, searching for inspiration for a new theoretical approach. Next, I will apply their strategies on difficult computer vision tasks to fully understand and demonstrate the workings of the algorithms. Finally, I’ll develop a theoretical justification for why these strategies work.”
With summer coming soon, both Casey and Dylan tell us that they can’t wait to begin work on their proposals. “I’m very excited to pursue this research over the summer, and I’m very grateful to Iris for allowing me to get involved on this project,” says Casey. Likewise, Dylan tells us that he plans to use this project as a springboard to really explore his interests. “This award will allow me to further dive into my research in preparation for PhD programs,” he says, “and I’m thankful to Steve, the BATS lab, and the TA2 group for continuously inspiring me and driving me to pursue my research.”
Casey and Dylan’s eagerness and curiosity are exactly what Peter Norvig is looking for. He sees this award as a multiplier that will amplify the value of his gift and extend it through time. "In the past," he says, "we had to build all our own tools, and we didn't have time to combine computer science with other fields. Now, there are so many opportunities to do so. I think it's a wise choice: you invest in things that you think will do good, and educating a student allows them to help add to the things that you're already trying to accomplish."
"High-quality virtual 3D objects," says Professor Daniel Ritchie of Brown CS, "are a critical resource for many academic disciplines, as well as for industries such as CAD and furniture and displays like the Hololens and MagicLeap. Unfortunately, traditional processes for creating new 3D objects are ill-equipped to meet this demand, and alternate approaches can be time-consuming, expensive, and require expertise, or fail to produce high-quality geometry."
To help remedy this problem, Daniel has just received a National Science Foundation (NSF) CAREER Award to develop 3D generative models that satisfy the criteria of quality, variety, efficiency, interpretability, economy, and universality, enabling scalable synthesis of high-quality objects for the expanding set of applications that demand them. His solution is neurosymbolic 3D models, a new class of generative representation for 3D objects that combines the best features of both symbolic and neural models.
"The main insight," he explains, "is to use a symbolic program to model the logical part structure of a 3D object, and then to use neural networks to refine this structure into high-quality geometry. This representation supports synthesis of new objects, reconstruction of objects from real-world sensor input, and high-level editing of object structure and geometry. It also supports modeling of higher-order object properties, including kinematics and physics."
The end goal is not only to create these high-quality objects but to help democratize 3D content creation and enable massive-scale generation of synthetic 3D training data for vision and robotics. This includes releasing augmented datasets with neurosymbolic versions of objects, improvements to robotic perception systems, and educating a new generation of diverse researchers with the interdisciplinary skillset needed for neurosymbolic 3D modeling.
CAREER Awards are given in support of outstanding junior faculty teacher-scholars who excel at research, education, and integration of the two within the context of an organizational mission, and Daniel joins multiple previous Brown CS winners of the award, including (most recently) George Konidaris, Theophilus A. Benson, Stefanie Tellex, and Jeff Huang.
Brown CS is happy to announce that Jeff Huang has been promoted to Associate Professor with tenure (pending the approval of Brown's Corporation), effective as of July 1, 2020.
Jeff came to Brown in 2013 after receiving his PhD and Master of Science in Information Science from University of Washington at Seattle. He also worked in various industry positions over the course of a decade, including Senior Software Engineer at Yahoo and co-founder of WorldBlender, a Techstars-funded startup, and he holds a record with ten tech internships at Google, Microsoft, and Motorola.
Alexandra Papoutsaki, one of Jeff's doctoral advisees, is now an Assistant Professor of Computer Science at Pomona College, and she's delighted by the news. "The first thing that pops in my mind," she says, "is that he's an incredibly kind human, always seeing the best in the people around him. Jeff gave me the room to grow and experiment and was always willing to help me and encourage me during our meetings. Seeing how he approaches research was eye-opening: he pursues hard questions that have a larger societal impact instead of looking for immediate gratification and the easy road. His energy in the classroom is palpable and infectious and I often look to his dedication to high-quality teaching for inspiration for my own teaching. Even after my graduation, he's always there for me: sharing teaching and research material and offering his perspective on how to navigate my career."
Jeff's Brown HCI research group specializes in creating personalized applications from remotely-captured user behavior data. Some of their recent projects include Sochiatrist (a social extractor that automatically dumps and anonymizes social communication data and combines it with biomarker data and qualitative interviews done by clinical researchers to find correlations), Self-E (a guided self-experimentation app), and WebGazer (an eye tracking library that uses common webcams to infer the eye-gaze locations of web visitors on a page in real time).
In addition to giving more than twenty invited talks over the past five years, Jeff has served as Conference Program Chair of CHIIR 2018, Conference Track Chair of SIGIR 2018, Conference Local Chair of WSDM 2012, and served on the Editorial Board of the Journal of the American Society for Information Science and Technology (JASIST). Some of his recent honors include an Army Research Office Young Investigator Award, being chosen as a CHIIR 2017 Best Paper Finalist, and an NSF CAREER Award.
Brown CS Professor Theophilus “Theo” A. Benson has just won the Eurosys  Best Student Paper Award for co-authoring the paper, “Efficient and Safe Network Updates with Suffix Causal Consistency” (authored by University of North Carolina Chapel Hill student Shen Liu and also co-authored by UNC Professor Michael Reiter), presented at EuroSys 2019. EuroSys is one of the premier systems conferences worldwide, focusing on issues related to systems software research and development, network middleware, database systems, real-time systems, and distributed, parallel, or embedded computing systems.
“Though centrally managed by a controller,” the authors explain, “a software-defined network can still encounter routing inconsistencies among its switches due to the non-atomic updates to their forwarding table. These inconsistencies can allow attackers to circumvent security appliances or lead to dropped packets which impact performance. In this paper, we proposed a new method to rectify these inconsistencies that is inspired by causal consistency, a consistency model for shared-memory systems. Applied to SDNs, causal consistency would imply that once a packet is matched to a forwarding rule in a switch, it can be matched in downstream switches only to rules that are equally or more up-to-date.” The authors focused on a functionally equivalent version of this property called suffix causal consistency, and showed that SCC provided greater efficiency than other competing consistent-update alternatives. 
The International Automated Negotiation Agents Competition (ANAC) is now in its tenth year of bringing together researchers from the negotiation community and spawning novel research in the field of autonomous agent design. Most recently, it was held at the International Joint Conferences on Artificial Intelligence (IJCAI) in Macao in August of 2019, and Brown CS PhD student Enrique Areyan and Professor Amy Greenwald returned home with the second place award in the competition's Supply Chain Management League.
The goal of this particular league is to design and build an autonomous agent that acts as the virtual manager of a factory who needs to buy raw materials, turn them into finished products, and then sell them. The goal is to maximize profit given private production capabilities, but the challenge for the researchers is that all transactions happen via automated negotiations between autonomous artificial agents.
"Our agent," says Enrique, "takes inspiration from the newsvendor model, a classic model in operations research used to model the choice of an optimal inventory level for a perishable product (for example, a newspaper). Analogous to newsvendor models, our agent tries not to over-produce or under-produce during its planning horizon. The ideas is not to stock too many products, as any excess (whatever does not sell) will go to waste; but at the same time not stock too few, as any shortage will result in lost sales. The goal of our agent is to maximize its total expected profits over a finite time horizon, in the face of uncertain and non-stationary elastic demand." 
The competition is returning again in 2020, and the SCML rules have been simplified so that agents can focus less on factory scheduling and production, which made up most of Enrique and Amy's focus in 2019, and more on agent negotiation strategies. The SCML 2020 game description can be found here.
More information about Enrique and Amy's agent is available here.
Brown CS alum Feng-Hao Liu, now assistant professor at the College of Engineering and Computer Science at Florida Atlantic University, has just received an NSF CAREER Award, titled “CAREER: Towards Efficient Cryptography for Next Generation Applications”. The topic of his research, he explains, is homomorphic computation, physical attacks, lattices, and post-quantum cryptography. "I would like to develop more analytical techniques and mathematical tools," he tells us, "to improve efficiency of the current states of the arts, hopefully making some of the constructions practical."
Feng-Hao would like to acknowledge Brown CS as a wonderful place where he developed as a researcher. In a message to Professor Anna Lysyanskaya, his PhD thesis advisor, and Professor John Savage, Feng-Hao says, "I appreciate the time when I was studying at Brown. Thank both of you for creating such a great environment that makes doing science a cool thing!"  
Looking back on the past few years, it’s difficult to remember a month or even a week when blockchains weren’t making news. But in the wake of various cryptocurrency crashes, have we reached Peak Blockchain? Not even close, say three computer scientists whose work may help fuel considerable speed increases for the technology as its reach expands far beyond Bitcoin and its competitors.
“With blockchains,” says Maurice Herlihy, An Wang Professor of Computer Science at Brown University, “we have a Volkswagen engine because that’s the way things evolved out of necessity. Our research is trying to replace it with a Maserati.”
That speedup, he says, is particularly important when we consider the many industries looking to benefit from blockchain’s reliability and decentralization. “Most people aren’t going to become cryptocurrency speculators, but blockchains could someday be used for everything from monitoring the freshness of produce to trading stocks to tracking items that move through the planet’s biggest supply chains. They’re conceivably a good fit for any interaction in which parties have a discussion process and are interested in coming to a conclusion.”
But first, there’s a big hurdle to overcome: there’s only one bathroom, so to speak, and one key.
“If we use Bitcoin as an example,” says Assistant Professor Eric Koskinen of Stevens Institute of Technology, one of Maurice’s collaborators, “it’s simple: you have some money, you can break it into smaller pieces, and you can transfer those pieces to other people. Other cryptocurrencies, such as Ethereum, have built on this by offering smart contracts.” 
Smart contracts are small programs associated with transactions, he explains, so you could have your money transfer automatically when the moon is full on a Thursday. Because they don’t require functionaries to get involved, they add a great deal of versatility. Unfortunately, they also present a massive bottleneck. To avoid conflicts, all smart contracts, even unrelated ones, only operate one at a time.
“In the beginning,” Maurice explains, “blockchains worked that way for simplicity’s sake, because it was easier to program. For an analogy, think of a thousand people wanting to buy concert tickets at the same time, and the software doesn’t want two people getting the same seat. When you’re picking your seat, you have a lock on the database, just like a bathroom lock that only lets one person in at a time.”
So, how to get around that limitation? Inspiration came in the form of transactional memory techniques, which allow for easier and faster development of programs that operate simultaneously. Maurice and Eric had collaborated on some of these methods, such as transactional boosting, as far back as 2008.
“We felt like the ideas that went into transactional memory would be a good fit for adding concurrency to smart contracts,” says Eric. Ethereum data, he explains, is stored in a structure known as a hash table, and Ethereum transactions are built by reading from and writing to the hash table. “Our work detects when simultaneous transactions might try to access the same part of the hash table, and pauses smart contracts to ensure that they proceed carefully. In this way, you can make high-level statements like ‘X and Y are independent’ and then the system finds a way to let them happen at the same time.” A bit of advance planning and coordination, in other words, can ensure that smart contracts interact with minimal fuss. 
This idea led to a paper (and a patent application) with Thomas Dickerson (Brown CS PhD '19, advised by Maurice with Eric also on his committee) and Paul Gazzillo.
Maurice and Eric needed to evaluate the effectiveness of their methods, but implementing them across the entire Ethereum virtual machine would have required a massive effort. The next best option? A simulation, which could model the effect that transactional memory techniques and concurrency might have. That’s where Vikram Saraph (now at Facebook, he earned his PhD from Brown with Maurice as his advisor) entered the picture.  
“A blockchain’s transactional data is never thrown out,” he says, “which makes it a historian’s paradise. So to evaluate how concurrent execution techniques might work in the future, I applied them to smart contracts from earlier in Ethereum’s history.” 
It was a bit of archaeology in the service of better engineering, focusing on the period of time from July of 2016 to December of 2017. After looking at the Ethereum source code, Vikram thought about what he wanted to re-execute in parallel and then created his simulation, which walks through the instruction sets of the smart contracts to see what would affect simultaneous execution. The goal was to see a speed increase.
“But in simulations,” he explains, “you don’t have an obvious measure for time. As a proxy, you can think about what instructions cost, because certain instructions require a payment to Ethereum. It’s a hypothetical way of evaluating the speedup.”
The results were striking: in the earliest part of the simulation, when conflicts were minimal, speed increased eightfold. But then the adorable kittens interfered.
Initial coin offerings (ICOs) are the blockchain version of stock offerings, and one of the most popular was CryptoKitties, which let users trade virtual cats. Due to the amount of contention as the same smart contracts were used again and again, some ICOs reduced the speedup from eight times to two times. But workarounds were still possible: by ignoring a small set of high-contention contracts and focusing on running others concurrently, Vikram was able to increase the speedup back to four or five times.
“That showed us a possible way forward,” Maurice says. “In the future, we can take steps to mitigate high-contention activity and add instructions to the Ethereum instruction set to allow some operations that otherwise wouldn’t be able to run in parallel to do so. For example, instead of two contracts trying to increment the same variable, the contracts could communicate with each other beforehand, so the database is only accessed once.” 
Eric and Maurice are looking forward to refining their techniques in the days ahead. Historical code, they explain, doesn’t provide a complete understanding of what was actually going on, so there’s more work to be done to evaluate whether a perceived conflict actually is one. It’s that idea of replacing a Volkwagen engine with the Maserati, says Maurice: making things run faster with a minimum of fuss. And to do it transparently, in a way that will work for any blockchain that uses smart contracts.
As time goes by, Maurice sees the importance of blockchains growing in parallel with the expansion of machine learning and an increasing interest in personal data ownership. “Blockchains could provide a more explicit and transparent way for people to control their data,” he says. “For example, they could serve as a data market where you selectively sell data from your smart devices to insurance companies.”
And more and more, the researchers believe, the public’s need for speed will motivate their work and those of their colleagues in the years to come. “As blockchain usage continues to grow,” Eric says, “people will be less and less happy to wait for someone else’s transaction to finish. Over time, speed will only become more important.”
Brown CS Professor Michael Littman has just been named a Leshner Fellow focusing on Artificial Intelligence by the American Association for the Advancement of Science. Each year, the AAAS selects leaders from disciplines at the forefront of important science-society issues, recognizing them for their contributions and commitment to public engagement in the field. Fellows are provided with the opportunity to convene for a week of intensive public-engagement and science-communication collaboration with the rest of the cohort, with the goal of increasing public engagement and enacting institutional change during the fellowship year. 
“I am most interested in helping people whose lives are being impacted by computing technology,” Michael explains, “to understand how that technology works and how we can best enhance its ability to empower us while minimizing its risks.” As a founding member of “AI Hub” (an organization with the goal of providing free, high-quality information about AI to the public), and co-host of the monthly podcast “Computing Up”,  Michael continues to be a pioneer in sharing the wonders of AI and computing with the public. 
AAAI-20 is the thirty-fourth AAAI Conference on Artificial Intelligence, one of the world's most prominent international conferences on the subject. Held this year in New York from February 7-12, AAAI promotes theoretical and applied AI research as well as intellectual interchange among researchers and practitioners. As with previous years, this year's AAAI included a Student Abstract Program that allows undergraduate, Master's, and PhD students to submit short, extended abstracts. Brown CS undergraduate Nishanth Kumar's abstract was one of the few chosen from numerous submissions, and it was included in the conference proceedings.
Nishanth is an Undergraduate Research Assistant in Brown CS Professor Stefanie Tellex's Humans 2 Robots Lab whose interests include robotics, artificial intelligence, and using hardware and software to create truly useful products. Primarily advised by Stefanie, his other advisors include Professor George Konidaris and Michael Littman of Brown CS.
"My work for the conference," Nishanth explains, "was about discerning relevance within large world-models. AI agents, such as intelligent robots, possess models of the world that they use to perform specific tasks. It is common practice for programmers to hand-design models to only contain relevant information. However, generally-intelligent agents must automatically discern this relevance for themselves based on the particular task they are given. My work proposes an algorithm to do this, which will hopefully help AI agents solve difficult, large-problems like Minecraft in the future."
AAAI-20 is the thirty-fourth AAAI Conference on Artificial Intelligence, one of the world's most prominent international conferences on the subject. Held this year in New York from February 7-12, AAAI promotes theoretical and applied AI research as well as intellectual interchange among researchers and practitioners. This year, the conference also included an Undergraduate Consortium (AAAI-UC), at which Brown CS undergraduates Jessica Dai and Pazia Bermudez-Silverman presented their recent research. Both were advised by Sarah M. Brown, a postdoctoral research associate at Brown's Data Science Initiative.
"My work," Jessica says, "is about how to understand and interpret the performance of 'fair' machine learning algorithms, particularly in situations where the data used for training a model is unrepresentative of the real-world domain in which it will be applied. My research finds that simply changing the demographics of datasets (e.g. to reflect real-world proportions) has unpredictable effects on 'fair' algorithms, and proposes a model for understanding 'ML bias' that encompasses historical or systemic inequity; poor data collection, sampling, or labeling; or any combination of these potential 'sources of unfairness'."  
AAAI-UC offered undergraduate students an opportunity to enrich their conference experience by (1) presenting and receiving critical feedback about their work in a professional, academic setting; (2) meeting prospective graduate advisors; (3) receiving mentoring about the advantages (and disadvantages) of pursuing graduate studies in AI as well as practical early career advice; (4) expanding their professional network to include both AI experts, current graduate students, and undergraduate peers; and (5) providing advice, tools, and resources for successfully applying to and attending graduate school in an AI-related field.
Pazia explains that her research focuses on the harm (racial, gender-based, class-based, and so on) that AI models and algorithms are causing in our society. "While these models," she says, "are used in many areas, including employment, housing, and welfare, I am focusing particularly on AI systems used in criminal justice, including predictive policing, recidivism, and facial recognition algorithms, which have all been shown to be racially biased. My work synthesizes previous analyses of this urgent topic and recommendations to make change in this area, including auditing these systems, spreading awareness, and putting pressure on those using them, as well as demonstrates how these algorithms feed each other and contribute to both a culture of poverty and structural racism." 
Their applications were reviewed according to criteria that included evidence of significant personal contributions to an AI research project, assessment of contribution to and benefit from participating in the Undergraduate Consortium, and input from their advisors. The acceptance rate for applications was only 16 percent.
Not only do we spend most of our lives indoors, says Brown CS Professor Daniel Ritchie, we spend a sizable percentage of our time virtually indoors, exploring computer-generated interiors. Some uses for these spaces are well known, like architects creating digital representations of buildings that don't yet exist, but others (moving furniture around a virtual living room before making a purchase, or training robots) are only beginning to become familiar to the layperson. 
Test yourself by looking at the images throughout this story: did a machine or a person design each room? Answers are at the bottom.
"The demand for virtual bedrooms, living rooms, offices, kitchens, and so on has never been higher," Daniel says. "Not only established fields like architecture and interior design and gaming but newer and rapidly-growing ones like robotics, virtual reality, and augmented reality – all of these need to create high-fidelity digital instances of real-world indoor scenes." 
To fully meet the demands of those industries, Ritchie explains, we need algorithms to generate data-driven scenes, producing a variety of plausible and visually appealing results that are generated quickly and user-controllable. At the moment, no existing approach satisfies all of these requirements. But his latest project presents a scene synthesis system that aims to do all these things, as well as a plan to demonstrate the system’s effectiveness by developing or improving real-world applications.
In the short term, we might soon be using Daniel's work to design our next living rooms. Slightly further down the road, we might use it to help train our household robots to navigate those same spaces.
The project has three main thrusts:
This project, which will be funded by a recently-awarded NSF Small Grant, continues work that Daniel has collaborated on with Kai Wang, his PhD student, and published together in three papers (one, two, three) at top conferences for graphics and vision like SIGGRAPH and CVPR. Kai recently received an Adobe Research Fellowship for this work.
"I'm really enthused that we'll be sharing interactive demos with the general public," Daniel says, "and it's very important to me that we're improving representation in CS. Our collaboration with Wayfair is being led by a female grad student, and last summer, I hosted a student from the University of Southern Mississippi who's also working in this area. We're still collaborating remotely. Two other female students will join my lab next semester, and I'll also be presenting our work to students in the Artemis Project, a Brown CS program for ninth-grade girls who are interested in STEM."
Answers for the three scenes above: (1) person on the left and machine on the right, (2) person on the left and machine on the right, and (3) machine on the left and person on the right.
Brown CS PhD student Kai Wang has just received an Adobe Research Fellowship for his research in automating design of structures and layouts. His work currently centers on bridging computer graphics and machine learning to create data-driven algorithms to achieve this automation. 
The Adobe Research Fellowship program was created to recognize outstanding graduate students carrying out exceptional research. This research spans a broad array of subjects, including Computer Graphics, Computer Vision, Machine Learning, Natural Language Processing, and Programming Languages. The fellowship includes a monetary award, a chance to interview for an internship at Adobe, and an opportunity to work with an Adobe Research mentor. This year, Kai is one of ten winners of the award, joining fellow graduate students from MIT, Cornell, UC Berkeley, and other universities around the world. 
Kai’s research thus far, he explains, has focused on the problem of creating algorithms that can automatically design indoor scenes. “We spend a lot of time in indoor spaces designed to support functionalities such as sleeping, cooking, meeting, etc,” Kai says. “To create the layouts of such spaces, a lot of human expertise and labor is often needed.” Kai's algorithms use deep neural networks to learn indoor scene layout patterns from large collections of existing layouts, enabling them to create plausible new scenes without ever being explicitly told how to do so. These algorithms can rapidly generate many new scene layouts, making them useful for design inspiration or to provide novel virtual training environments for autonomous robots. Going forward, Kai is interested in extending these machine learning algorithms to other types of layout problems (floor plans, for example) and other design problems that involve making sequences of decisions.
Over the past two years, Kai’s lab –led by Brown CS Professor Daniel Ritchie– has made significant inroads into this field of automatic indoor scene layout via machine learning. “We call this the scene synthesis problem,” Daniel explains. “Given the architectural specification of a room –its walls, doors, and windows– the goal is to create a plausible instance of a particular  type of room, say a bedroom, by filling it with the right types of objects in the right arrangements. In doing so, we aim to build a system that can support a range of use cases.” Coming off the heels of a recent grant award, the researchers in Daniel’s lab continue to be recognized for their groundbreaking work. 
The Computing Research Association (CRA) is a coalition of more than 200 organizations with the mission of enhancing innovation by joining with industry, government, and academia to strengthen research and advance education in computing. Every year, they recognize North American students who show phenomenal research potential with their Outstanding Undergraduate Researcher Award, and in 2020, Brown CS made one of the strongest showings in the Honorable Mentions category. Out of 103 students who received Honorable Mentions, four of them are Brown CS students: Deniz Bayazit, Rigel Galgana, Nishanth Kumar, and Esteban Safranchik. Last year, Brown CS students received three Honorable Mentions.
Asked to desribe her work, Deniz says that it lies in the intersection of Natural Language Processing (NLP) and robotics. "Humans," she says, "can convey instructions to other humans via different means of communication. They have the ability to enter a new environment and follow through a navigation task when given a partial or full map, and hinted with a form of natural language (NL). Reproducing a similar ability in a robot would be extremely useful for untrained users, who do not have an in-depth knowledge of robot programming. To achieve grounding language to arbitrary landmarks in any outdoor environment (or in other words, without needing to re-train the language model for a new environment), I worked with Matthew Berg, Rebecca Mathew, Ariel Rotter-Aboyoun, Professor Pavlick, and Professor Tellex in summer, 2019. We are currently investigating spatial language understanding models that could help guide agents in partially observable environments with natural language hints such as 'The car is behind the CIT on Waterman Street.'"
"Let's play a game," says Rigel when posed with questions about his research. "Say that you are in dire need of TA assistance on a project, so you decide to register for every single TA hours slot for the next month in hopes of getting seen at least once. While you would ideally want a high spot on SignMeUp so as to avoid a long wait, the queue is shuffled randomly at the beginning of each set of hours. After this shuffling, you see how long you'll have to wait before getting seen and decide whether or not to wait it out or delete your ticket. Assuming you knew in advance how crowded each session was ahead of time, how do you decide when to wait it out? How long do you expect to wait? What if you wanted to get seen at least twice? Thrice? This variant of the secretary problem is only one of many applications of order statistics. Other applications include auction design and analysis, Bayesian-optimal pricing, and more. For the last year and a half, I've been working with Professor Amy Greenwald on efficient algorithms to compute joint distributions of order statistics with the goal of answering some of the above questions."
Nishanth's research has focused on two interrelated components of AI for Robotics: intelligence and collaboration. "On the collaboration front," he says, "I've worked on using Augmented and Virtual Reality (AR/VR) to allow humans to effectively and intuitively give robots all the information required to accomplish complex tasks. On the intelligence front, I've helped make state-of-the-art Learning from Demonstration (LfD) algorithms more data-efficient. I've also worked on bridging the fields of Reinforcement Learning and Formal Methods to allow agents to reason about which parts of the world are relevant to their decision making, and thus ignore irrelevant factors."
"I'm incredibly lucky and grateful," says Nishanth, "to be part of such an amazing community within Brown CS: none of my work would be possible without it. Specifically, I want to thank my amazing mentors and collaborators in the H2R, IRL, and RLAB groups, especially Professors Stefanie Tellex, George Konidaris, and Michael Littman. I also want to thank TStaff and CIS for never failing to support the unique needs that come with running experiments on robots. I couldn't imagine a better environment in which to produce great research!"
"My primary area of research," Esteban tells us, "is weakly supervised machine learning, a paradigm for machine learning featuring noisy or limited information sources as substitutes to full supervision. My work with Prof. Stephen Bach addresses the challenges of building classifiers and structured predictors for NLP in the absence of hand-labeled data. During the summer, I worked on WISER, a framework for training deep sequence taggers with weak supervision from user-written rules. Our work introduces a novel approach to modeling weak supervision sources that achieves state-of-the-art performance on several NLP tasks. WISER allows users to build powerful machine learning systems without relying on ground-truth labeled data."
The full list of Outstanding Undergraduate Researcher winners is available here.
Over the past few years, The Brown Daily Herald has published several articles and op-eds about growing enrollment in computer science, including suggestions on how to address the resulting challenges that students face. This morning, Professor and Chair Ugur Çetintemel and Professor Shriram Krishnamurthi continued the dialogue with an op-ed explaining the constraints that Brown CS faces as it tries to balance the tradeoffs between access and an intimate feel.
You can read the op-ed here.
Please note that unfortunately, the SIRoS conference planned for March 20 will be postponed to a later date. If you have any questions, please contact hcri@brown.edu.
SIRoS (Societal Implications of Robotics Symposium) 3: Robots Entering Society will be held on March 20, 2020. The article below looks back at SIRoS 1 and 2 in anticipation of a symposium that will bring together scholars and practitioners from multiple disciplines to examine the difficult questions: What are our obligations to shape the transformative impact of robotics on our society to be positive? How can we contribute to such a positive shaping? And what legal and ethical norms may have to be established to foster a harmonious growth toward a future society with robots?
“SIRoS 1 was when the question really struck me,” remembers HCRI Co-Director, Professor Michael Littman of Brown CS. “How do we keep the value of humans, of people, from declining? We’ve known for a long time that we’re at an inflection point, and the first SIRoS really drove the point home for me.” He’s talking about the Societal Implications of Robotics Symposium, first held by Brown’s Humanity-Centered Robotics Initiative (HCRI) in 2015. Every two years, the conference has been bringing together scholars and practitioners from multiple disciplines to examine the difficult questions of a future society where robots are part of everyday life. SIRoS 3 is scheduled for 2019.
“For so long,” explains HCRI Associate Director Peter Haas, “Robotics had always been a research endeavor, with ethics and other issues as more of an afterthought, but SIRoS 1 saw people who are making a big difference in the field talking to each other and considering ethical implications for the first time.”
The ideas that Michael references above are highlighted in a keynote address (“Robotics, Empowerment, and Equity”) delivered at SIRoS 1 by Illah Nourbakhsh of the Robotics Institute at Carnegie Mellon University. Littman explains that it included an early mention of a concept that’s been repeatedly making news in 2017: guaranteed basic income. “This was an extremely important realization," Michael says. "Because of robots, the work that had been done by people is being done by infrastructure. Some politicians rail against immigrants, but you could argue that computer scientists are causing the problem: the value of people is going down. This is exactly the kind of idea that arises when the research community broadens its outlook to include societal issues."
A second keynote (“How the Law Will Think About Robots (And Why You Should Care)”) by Bill Smart of Oregon State University, a Brown CS PhD alum, bookended three panel discussions with small groups of leading robotics researchers, economists, philosophers, psychologists, legal scholars, and even representatives of funding agencies. “It was really exciting to see the field changing,” says Peter. “SIRoS 1 helped us understand that robotics isn’t just research: it’s becoming applied research, with sociological issues rising to the forefront.”
You can watch videos of both keynotes here: http://bit.ly/2wajwPu and http://bit.ly/2hrxAkx
SIRos 2: Seeing Through Their Eyes
A small, padded cube is flying toward you from across the auditorium. There’s a microphone inside: you catch it, use the voice amplification to share your thoughts with the group, and then throw it to someone else. It’s a good illustration of a key difference between SIRoS 1 and 2. Like its predecessor, the second conference dealt with the difficult questions of how to ensure that robotics has positive effects on society, with talks on healthcare robot policy, algorithmic bias, the legality of autonomous weapons systems, and more, but the approach was a bit less traditional.
“We had more panel discussions,” says Michael, “but we really focused on having people make shorter position statements so we could allow the audience to push back. It helped create a shared perspective, and it felt outside the norm of academic meetings — much more playful. Increasing the size and diversity of our audience let us see problems and challenges through their eyes. It was extremely satisfying to bring this community together.”
As with SIRoS 1, the topics discussed have only increased in relevance in the days since. As just one example, the juxtaposition of the ethical dilemma known as the Trolley Problem with the rise of self-driving cars, first proposed by SIRoS 2 speaker Gary Marcus in 2012, has entered the public consciousness sufficiently that it spawned a popular Internet meme and eventually entire web sites devoted to collecting variants of that meme. And research is already underway, including some by HCRI Co-Director, Professor Bertram Malle of Brown’s Department of Cognitive, Linguistic, and Psychological Sciences (CLIPS).
You can read Bertram’s paper here: http://bit.ly/2fn7yyd
“Problems like this have been discussed in academic circles before,” says Peter, “but it was wonderful to see strategies and possible answers presented. People see the future coming, and they’re thinking about when robots will be prolific, and they’re proposing solutions.”
“This year's SIROS 2 also served as a preconference to the We Robot conference, which has discussed legal and policy questions concerning robots for five years now,” adds Bertram. “This is an important partnership we hope to build on. Thoughtful science should inspire thoughtful law and policy initiatives.”
You can watch videos of ten SIRoS 2 talks here: http://bit.ly/2uz2Q2c
SIRoS 3: Showing Leadership
“One of the important things we have to do for SIRoS 3 is make sure that we have enough seating,” laughs Peter. “The quality of our speakers has really been fantastic, and we’re looking to continue finding the best ideas from around the world. We’re planning for growth.”
And the growth of the conference mirrors the expansion of the Humanity-Centered Robotics Initiative as a whole. It’s continuing to facilitate collaboration among Brown researchers by providing access to robots, robotic research, and building spaces, and Peter explains that HCRI has recently formed corporate partnerships with Hasbro and Sproutel, along with external funding to support them. The goal is to serve as an interface with Brown Robotics for the two companies, to help them understand our evolving robotic future from HCRI’s unique perspective.
“Robotics is at an inflection point,” he says, “like computing was in the 1970s. It’s not just the purview of a select few any longer, so we have to keep looking toward the future in the broadest sense. It’s a real opportunity for Brown to show leadership: interdisciplinary work is what we do well, and HCRI and SIRos are great examples.”
Michael agrees: “All the pieces are in place for robots to have a dramatic effect on individuals, but we don’t have a crisp structure yet. SIRoS is about aiming the rocket before it goes off the launch pad. It might sound playful, but it’s deadly serious to me. Robotics needs more roboticists, but just as much, it needs more sociologists. Right now is when we need the interdisciplinary approach.”
"There seems to be some kind of magic," says Professor Ellie Pavlick of Brown CS, "that happens in humans' heads that allows us to conjure up a staggering amount of information in order to make inferences: not just information about language, but assumption and intents. The main questions I'm interested in in this area are about representation and learning. What is 'common sense' and what does it 'look like'? How do humans represent our knowledge about the world, and how do we build representations through the experience of living rather than supervised training? And how can we make the answers to these questions play well within a computational system?"
Two upcoming projects will allow Ellie and her collaborators to explore those answers in depth. They're being funded by two grants, one from the Defense Advanced Research Projects Agency (DARPA) and one from the Intelligence Advanced Research Projects Activity (IARPA). The latter of these, at six million dollars, is the largest Brown CS grant to date. 
One of Ellie's partners, Professor Carsten Eickhoff of Brown CS, who leads the AI Lab at Brown's Center for Biomedical Informatics, explains that he looks at both grants from an Information Retrieval perspective. "Information Retrieval engineers and researchers," he says, "have spent decades devising formal models describing the connection between search queries and the best possible results to answer them. In particular, we care about the true underlying intention behind the ill-formed strings of keywords that all of us hammer into Google and Co. on a daily basis. For many queries this interpretation will be straightforward; for a significant number of others, however, it becomes veritable detective work. While modern natural language processing techniques have become indispensable in this pursuit of meaning, intentions, and goals, there still is a gap between the way in which people and machines read and generate language. These projects try to close this gap and truly enable search engines to understand what you are looking for."
Better Extraction from Text Toward Enhanced Retrieval
In the IARPA grant project ("Better Extraction from Text Towards Enhanced Retrieval"), the researchers will be studying methods for improving search that work across languages and which account for fine-grained differences in users' interests.
"Probably the most familiar of all natural language processing applications," Ellie says, "is information retrieval. We can think of Google search: products like it seem to perform extremely well, but they're actually only good at answering queries that a huge number of people have already asked. It seems surprising, but when you search for 'what was that movie with the box of chocolates', you're probably one of thousands of people who have searched for the same thing."
However, when people need to do very specific searches over very specialized sets of documents, search quality is far worse: think of the daily frustration from trying to find something in a massive pile of old email. In this project, Ellie and her team will attempt to build better search methods that quickly recognize a fine-grained topic by asking a small number of disambiguating questions and reorganizing the entire representation of language on the basis of the answers. The goal is for users to get better results from searches on very specialized topics, such as finding evidence for and against a little-known scientific theory. Collaborators include Carsten as well as colleagues from Ohio State University and University of Pennsylvania.
Grounded Artificial Intelligence Language Acquisition
To situate the project ("Grounded Artificial Intelligence Language Acquisition") funded by the second grant, Ellie explains that current approaches for teaching language to computers work by reading large volumes of text over and over.
"This means that they can produce normal-sounding sentences," she says, "but have zero knowledge of what they actually mean. Instead, we'll be teaching computers the meaning of words by emulating the way we talk to toddlers. They learn language by interacting with the world and humans, hearing something like this: 'Let's get ready for lunch. We should get a plate and glass to put on the table so we can eat.' We'll be doing that in virtual reality, letting you talk to a computer just like you would with a child." 
Over time, the computer will learn to connect the words the person says to the objects and actions it observes. Later, it will generate the same words when it sees similar objects and actions.
"Asking questions and dialog is essential to robotics and language grounding," says Professor Stefanie Tellex of Brown CS, one of Ellie's collaborators. "Moreover, as a robot gains long-term memory, the information retrieval questions and applications for this grant are important, too. Think about asking a robot in your home when Grandma last took her medicine, or where she left her keys."
"We have an amazingly multidisciplinary group of collaborators right here at Brown," says Ellie, "including Carsten Eickhoff in Natural Language Processing, Roman Feiman in Developmental Psychology, Stefanie Tellex in Robotics, and Daniel Ritchie in Graphics. It's a dream team."
"Computer scientists," says Roman, "are trying to engineer systems that understand and use language. Cognitive scientists are trying to reverse-engineer how humans do the same thing. The two fields can learn a lot from each other, because engineering solutions can inspire scientific hypotheses and scientific findings can be the basis for new approaches in engineering. What I think is really special about our group is that we all have deep interests in the other's field, so that we can make meaningful connections from our own areas of expertise. That's rare, and it's an outstanding opportunity to make progress together."
Among other things, Brown CS history is a treasure trove of stories, and sometimes even the sequence of events that led to a recent symposium is a tale worth telling.
As last year began, Marc Weber ‘84 had hypertext on the brain. The Computer History Museum (CHM) was planning a conference on pioneering inventor Douglas Engelbart, and in Marc’s role as the Curatorial Director of CHM’s Internet History Program, he knew about an effort by two alums to revive Brown’s late 1960s hypertext system, FRESS.
Steven DeRose ‘81 AM ‘86 PhD ‘89 had been the final director of the FRESS project, building its last binary from 82,000 lines of assembly source. David Durand ‘83 had been a FRESS user in high school due to a policy that gave access to the children of faculty members. In 1989, they created a FRESS demo for the ACM Hypertext Conference after building an emulator for its original vector graphics display.
Marc asked Professor Andries “Andy” van Dam to revive the FRESS demo for CHM’s conference, but the emulator no longer worked, so Tyler Schicke ‘18 ScM ‘19 was pressed into service, working with David to create a new one. That’s where Norm Meyrowitz ‘81 enters our story.
“Andy, in his classic ‘the more the merrier’ style,” Norm remembers, “thought I should try to revive Intermedia, Brown’s hypertext system from the late 1980s. We thought it had been lost due to a degraded disk, but someone had saved our demo copy.” And so he started buying up 30-year-old hardware: mice, displays, video cards, cables.
“And it worked!” he says. “It got me thinking two things. The first was that we should find more of our old systems and create a symposium to celebrate Brown’s impact on the online universe, and the second was that I was about to create a lot of work for myself.”   
A symposium was born, and on May 23, Brown CS held A Half-Century of Hypertext to recognize Brown’s many contributions to the linked world. The event featured a half-dozen demos and almost thirty presenters, ranging in age from their early twenties to late eighties.
In the article here, our largest feature to date, we retrace the symposium’s chronology to provide a brief history of five decades of research with a lasting impact.
“I’ve never really been interested in technology just for technology’s sake,” explains Brown CS alum Michael Frederickson ‘08, “but rather in using technology to make somebody feel something.” Currently working as a Lead Technical Director at Pixar Animation Studios, Michael has indeed dedicated his life to this passion, and he has made an impact on the lives of so many Brown CS students who look to follow this dream.
Michael’s love of the “emotion of awe”, as he calls it, began very early on. “I remember watching Jurassic Park,” he laughs, “and my nine-year-old brain was just completely amazed. The graphics were so awe-inspiring and I knew I wanted to help do that.” This intellectual curiosity and passion naturally led Michael to Brown, where he immediately immersed himself in the computer science department. “There’s always been a conflict for me between the artistic side and the technical side,” he explains, “because technical involves abstraction, and sometimes as an artist you need to forget the structure and explore without following a specific path. I was attracted to Brown as a place where both these modes of exploration and discovery were encouraged, and often blended.” And it was at Brown that Michael really thrived. 
“I remember back in 2003, we had this small capped animation class,” laughs Brown CS Professor Barb Meier, “and Michael showed up to class with three books, and just sat in the center front row of the class as a first-year student. He always had a really good impact on the class, he could work with anybody, had tons of new ideas.” These ideas, this desire to make people feel emotions through art, was what eventually drew Michael to work at Pixar. “Brown and Pixar are both really compatible with who I am. Pixar has always been a place that understands there is deep creativity in technology – that at the deepest levels of technical or artistic sophistication, there isn't much difference between an ‘artist’ and a ‘scientist’.”
But what really makes Michael special is how much he’s given back to the Brown community, even a decade after graduating from the program. Ranging from holding “office-hours” in which he personally guides students on their CS projects, to organizing recruitment trips for Pixar, to giving constructive critiques in Barb’s computer animation class, he has remained personally vested in the lives of so many Brown CS students, and his advice has proven to be invaluable.
What motivates Michael to come back even now? “I suppose some combination of empathy and passion,” he answers. “There’s something about this place that just feels right to me.” As he reminisces about his years as an undergraduate, Michael distinctly remembers the excitement he felt as he ventured into the world of CS. “I remember when Pixar came to recruit when I was a freshman and I just freaked out,” he laughs. “I remember how unthinkably off the map excited I was to get the opportunity to ask questions about that. I remember how good that felt, and I want someone else to get to feel that good. I really just love doing this stuff, and I'm always excited to help out when I see a student with an echo of the passion that got me into graphics in the first place.” 
This desire to help others has always been there for Michael (he was a TA for Barb’s computer animation class for six semesters), and he explains this desire as akin to a movie. “Telling somebody the conclusion has very little impact,” he explains, “but watching a movie puts you through this choreography of emotions over time. I always loved the challenge of teaching as a TA – trying to come up with the right way to explain or the right experience to put different students through to help them come to their own understanding of filmmaking is a really satisfying challenge.” 
Given that Michael has had such a fulfilling and impactful career, what advice would he have for current students who look to follow in his footsteps? “If you’re fortunate enough to have the chance,” he says, “think very hard about what you find yourself wanting to do when no one is watching. Brown is an environment full of intrinsically motivated people. The open curriculum lets you run all these little experiments. ‘Do I like this subject? Am I happy in big groups? Alone? Do I like structure? Improvisation?’ You can use Brown as a little lab to notice what you really love and just can’t stop yourself from doing.” It’s clear that Michael has certainly found his passion, and will undoubtedly help generations of Brown CS students do the same. 
Hoping to encourage young authors to tackle emerging business themes, the Financial Times and McKinsey and Company first awarded the Bracken Bower Prize in 2014. The award is given to the best business book proposal by an author younger than 35, and it's already helped a number of young business writers bring their ideas from proposal to publication.
This year, Ernesto Zaldivar, a graduate of Brown University's Executive Master in Cybersecurity (EMCS) program, was one of three finalists for the award with his proposal for a book titled InfoSec. "InfoSec is based on the field research I completed for my EMCS Critical Challenge Project. The unique research opportunities I had were possible because of my EMCS cohort, Brown faculty, concurrent EMCS cohorts, and Brown alums. Their thoughtful introductions to leading cybersecurity practitioners were instrumental and granted me unprecedented access to high-profile information security teams," said Zaldivar.
In addition to citing the importance of the support he received from his classmates and professors, he added: "The Executive Master in Cybersecurity curriculum made me a better cybersecurity professional and scholar by providing me with an interdisciplinary knowledge base and scientific frameworks that I can apply to solving novel cybersecurity problems."  
A panel of expert judges, looking for work that provides a compelling and enjoyable insight into future trends in business, economics, finance, or management, selected Ernesto's entry as one of the top three from a pool of over 80 proposals from 22 countries. Jonathan Hillman, the winner of the prize, is also a Brown alum.
You can read an excerpt from Zaldivar's proposal here.
"Two weeks before the OOPSLA 2009 deadline," remembers Brown CS alum Arjun Guha (now Associate Professor of Computer Science at University of Massachusetts, Amherst), "Shriram declared that it was time to actually write a Flapjax paper. That isn't much time, but since we had so much experience, the system was so polished, and because I had decided to drop a class to focus on writing, it actually came together easily."
Co-written with Brown CS alums Leo A. Meyerovich (now at Graphistry), Jacob Baskin (now at Coord), Gregory H. Cooper (now at Google), Michael Greenberg (now at Pomona College), and Aleksandra N. Culver (now at Google) and Brown CS Professor Shriram Krishnamurthi, that paper ("Flapjax: A Programming Language for Ajax Applications"), which presented a new language designed for contemporary web applications, has won the OOPSLA Most Influential Paper Award.
First released in 2006, Flapjax added now-ubiquitous reactive abstractions to Web programming. Specifically, it borrowed from functional reactive programming to (1) provide event streams, a uniform abstraction for communication within a program as well as with external Web services, and (2) automatically track dependencies and propagating updates along those dataflows. This allows developers to write reactive interfaces in a declarative and compositional style. Built on top of JavaScript, Flapjax runs on unmodified browsers and readily interoperates with existing JavaScript code, usable as either a programming language (that is compiled to JavaScript) or as a JavaScript library.
As Arjun noted in his acceptance speech, a paper about Flapjax didn't come until three years later. During those three years, the language was part of multiple research projects: undergraduate honors theses for Jacob, Leo, and Michael; a PhD thesis from Gregory; and two papers by Arjun.
"The lesson that I took away from it as a junior graduate student," Arjun said of the Flapjax project, "was that it is worthwhile and rewarding to take the time to build systems that work, and that the approach leads to better and more research."
The International Collegiate Programming Contest (ICPC) is believed to be oldest, largest, and perhaps the most prestigious programming competition in the world. An algorithmic programming contest for college students, it requires teams of three to solve real-world problems, fostering collaboration, creativity, innovation, and the ability to perform under pressure. 
Last month, a team from Brown CS (undergraduate students Khemarat Boonyapaluk, Rutchathon Chairattana-Apirom, and Shreeyash Gotmare, coached by Professor Rodrigo Fonseca) returned from the ICPC's 2019 Northeast North America Regional Contest with a Third Place award. Their competitors were drawn from states and provinces that included Quebec, New Brunswick, Nova Scotia, Prince Edward Island, Newfoundland, Labrador, Maine, New Hampshire, Vermont, Massachusetts, Rhode Island, Connecticut, and New York, with the exception of New York City.
After working with a five-hour deadline to solve the greatest number of problems in the fewest attempts in the least cumulative time, the Brown team was awarded the opportunity to advance to the ICPC North American Championship, which will be held in Atlanta, Georgia, in February, 2020. Winners of that event will continue onward to the World Finals in Moscow, Russia. In last year’s regionals, 49,935 contestants from 3,098 universities in 111 countries on six continents competed at over 530 sites to advance to the World Finals.
Last week, Professor Seny Kamara of Brown CS testified at a Congressional hearing ("Banking on Your Data: The Role of Big Data in Financial Services"), explaining that cryptography solutions to improve consumer financial health can be implemented without endangering privacy. Invited by the U.S. House Financial Services Committee Task Force on Financial Technology, Seny was part of a panel of data security and cryptography experts. 
At various times, Seny notes, the discussion expanded to a larger examination of equity and bias, reminding him of topics addressed in Brown's Socially Responsible Computing program and his own course (CS 2950-V Topics in Applied Cryptography: Crypto for Social Good), which examines how surveillance is used to suppress dissent and how cryptography can be used to protect marginalized groups. One example was when Representative Rashida Tlaib asked about the use of biometrics in low-income Detroit communities, and how it affects immigrants and people of color.
The rise of big data, Seny explained, has created a host of new financial products, but many of them pose serious privacy concerns because of the data they collect. For example, cash advance apps and micro-credit apps collect and use a variety of sensitive data including location data, text messages, Facebook friends and financial transactions. But since some of these apps are more often used by lower-income communities, they will be disproportionately affected by the loss of privacy.  "Cryptography researchers in academia and in industry labs," he said, "have developed a wide array of cryptographic techniques to process encrypted data." By leveraging those advances, he noted, they could improve the financial health of customers without sacrificing their privacy.
"I find it very significant," Seny tells us, "that I wasn't just sharing algorithms or code or commenting purely on technical issues but thinking seriously about the effects of technology. For our students, as future computer scientists, it's a real illustration of evaluating the impact that our work has on society. Even the act of testifying can be part of practicing responsible computer science."
You can read Seny's testimony here and a video of the proceedings here.
“Energy efficiency is now a critically important design constraint for most computing systems today,” says Professor R. Iris Bahar of Brown CS, “and as applications become more and more memory- and compute-intensive, energy-efficiency and reliability become harder to manage.”
In a recent keynote address (“Energy-efficient and Sustainable Computing Across the Hardware/Software Stack”) at the International Green and Sustainable Computing Conference, Iris presented techniques across the Hardware/Software stack for energy-efficient and reliable computing, and discussed how these techniques may be used to achieve more sustainable computing in the future. 
“In the 1990’s,” Iris explains, “computer architects and chip designers invested huge amounts of resources to improve processor performance by supporting aggressive speculation with superscalar out-of-order execution. Eventually, power issues caught up with designers and we had to rethink designs that were more power-aware.”
With machine learning (and learning through deep neural networks) exploding in popularity, Iris’ research has been exploring the costs of deep learning and energy-efficient neural network design. “Deep neural networks have been shown to be very accurate for many applications, but in the end there are costs of deep learning that haven’t been fully accounted for (in terms of training, data set requirements, energy usage, and others),” she states, “and we may need to think twice if deep learning is the best use of our resources. Otherwise, we may hit a wall again, just like we did in the 1990’s with processor design.”
“My big vision is making design accessible to everyone,” explains co-founder/CEO of Figma, Dylan Fields, “and I want the tools to be simple and powerful.” This dream is quickly becoming a reality, as both Dylan and Brown CS alum Evan Wallace ‘12 have been recognized by INC Magazine as “Rising Stars of 2019” for founding Figma. (Dylan was also a Brown CS student.) This award aims to highlight the most impactful and world-changing startups around the world, and Figma certainly fits the bill with its innovative approach to collaborative design. The San-Francisco based startup is for those who create diagrams, illustrations, and web-based applications, and gives users the ability to work on designs together in real-time.
What sparked the idea for the company? “Well, it all started in elementary school,” laughs Dylan. “I’ve always been really excited about coding and computers, and the first time I got to do any programming in elementary school was when I had access to a basic compiler.” The creative spark certainly began early, as Dylan was designing PHP applications and even making websites for his school, and carried over to Dylan’s time at Brown. And it was at Brown that he met Evan and began exploring his entrepreneurial side. 
“Evan and I had completed several internships at major tech companies,” explains Dylan, “and we were really getting frustrated with the lack of tools available for actual design.” Growing up with Google Docs, the pair had grown accustomed to the suite of cloud-based collaborative tools that allowed multiple users to work together in real-time, and were shocked that there was no such viable tool available for web-based design. 
“Evan was doing work as a graphics TA,” Dylan remembers, “and it was so surprising that our current tools didn’t work in cloud-enabled ways. We wanted to make tools for designers that was more interactive.” Ultimately the pair ended up choosing to focus on interface design, because they saw the big trend of design becoming more important than ever. “Ten years ago at IBM, there were 72 engineers to one designer, five years ago it became eight engineers to one designer, on mobile it’s now three engineers to one designer,” explains Dylan, “and because of that, a lot of different companies started realizing that they needed to catch up and that design was becoming more important.”
The company launched in 2012, and immediately experienced explosive growth. The company now has over one million sign-ups, over 118 employees, and a customer base including Microsoft, Dropbox, Uber, Twitter, and Slack. Over the past seven years, the company has raised over $85 million in venture capital funding, and is valued at over $400 million today. “We’ve made it possible for everyone to work in one place, for designers to work with designers, marketing managers, and others, so that more people are able to access the design process,” explains Dylan.
With such an exhilarating journey into startup success, what advice does Dylan have for those students who want to follow in his shoes? “Internships are the single greatest time to explore,” he says, “and there’s no other time in your life where you have the opportunity to figure out exactly what kind of management style and cultures you like. Take on as many projects as you can, go through the process of making and supporting your own ideas – the more launches you have under your belt, the more you’re able to do the next one better.” This approach has certainly worked for Dylan and his team, and his company has made the dream of truly making design accessible a reality. 
The Department of Computer Science at Brown University is hiring tenure-track faculty at all levels. These positions are a part of a major expansion plan for the department as it works to increase its faculty roster by close to 50% over a five-year period. While many of these positions will be used to strengthen and expand core CS areas, some will be used to build bridges with other campus disciplines to facilitate interdisciplinary research and teaching. The department is particularly interested in candidates whose research addresses at least one of the following:
We will also consider additional areas of department need. Applicants whose research may relate to our other open position in Data Science (apply.interfolio.com/68207) are encouraged to apply to both searches.
The department has 31 tenure-stream and 3 research faculty members, 2 lecturers, and several adjunct and visiting faculty members. In addition to its strong graduate program, the department has a strong undergraduate culture, anchored by a mature, endowed program for undergraduate teaching assistants and research assistants. Department members frequently take advantage of Brown's interdisciplinary culture via collaborations with numerous other Brown units including Applied Mathematics, Biology, Brain Sciences, Cognitive Linguistic and Psychological Sciences, Economics, Engineering, Mathematics, Medicine, Public Health, Public Policy, and Visual Arts, as well as the Rhode Island School of Design. CS is a founding partner and plays key roles in major university-wide programs and initiatives including Data Science, Humanity Centered Robotics, Cybersecurity, and Computational and Molecular Biology.
Brown University is committed to fostering a diverse, inclusive, and global academic community. As an EEO/AA employer, Brown considers applicants for employment without discrimination on the basis of gender, race, protected veteran status, disability, or any other legally protected status. The department is similarly committed to building a diverse faculty and strongly encourages women, underrepresented minorities and those who can contribute to the excellence, diversity, and inclusivity of our academic community. We strongly encourage the candidates to report any relevant experience, including work with diverse constituents, and plans in their teaching statements or in a separate diversity statement. Brown University is located in Providence, RI, an hour from Boston and about three hours from New York City, both accessible via frequent rail service, and close to Narragansett Bay. Providence has been consistently rated among the Northeast's most livable cities and is home to diverse intellectual, artistic, and business communities.
Junior applicants must have completed all requirements for the doctoral degree by the start of the position. The initial appointment as assistant professor is for four years and is renewable. Applicants for a junior position must submit three letters of reference, and applicants for senior positions should submit five names of references whom the committee may contact. Please provide a diversity statement, in which you summarize your past or planned contributions to diversity and inclusion. These contributions may arise from teaching/mentoring, outreach activities, lived experience, or other activities. (For additional information about the university's and department's commitment to diversity and inclusion, see www.brown.edu/about/administration/institutional-diversity/pathways and
www.cs.brown.edu/about/diversity.) We are eager to try to accommodate the needs of, and welcome applications from, dual career couples.
Applications will be considered until the position(s) are filled but we strongly encourage the candidates to submit complete applications (including reference letters) by December 15, 2019 for full consideration. We will start application reviews and interviewing immediately and highly encourage early applications. Applicants who would like confidentiality should explicitly mention this desire in the first paragraph of their cover letters. To apply, please use Interfolio: (https://apply.interfolio.com/70568). Inquiries may be addressed to: faculty_search_2020@lists.cs.brown.edu.
Brown CS PhD student Jiwon Choe has just won the Best Student Presentation Award for her presentation “Attacking Memory-Hard Scrypt with Near-Data-Processing” (co-authored with Brown CS Professors Maurice Herlihy and R. Iris Bahar and Boston University Professor Tali Moreshet) at the International Symposium on Memory Systems (MEMSYS ‘19). Held recently in Washington, DC, MEMSYS is an annual conference that aims to bring together leading researchers and practitioners interested in emerging memory technologies.
“In a traditional DRAM-based main memory architecture,” the authors explain, “a memory access operation requires much more time and energy than a simple logic operation. This fact is exploited to build time-consuming and power-hungry cryptographic functions that serve the purpose of deterring brute-force security attacks.” Jiwon’s presentation focuses on the impact of near-data processing (NDP) on scrypt, a widely used memory-hard password-based key-derivation function, and discusses the opportunities to undermine scrypt using compute-capable memory.
The IEEE VIS Conference is an annual conference held by the Institute of Electrical and Electronics Engineers and is regarded as the foremost gathering of researchers focused on scientific visualization, information visualization, and visual analytics. This year, the conference’s Visualization Academy Selection Committee has announced that Professor David Laidlaw of  Brown CS has been named to the IEEE Visualization Academy, a group of research leaders in the field of visualization that is credited with underscoring the accomplishments of the entire field. The academy was established in 2018 by the IEEE Executive Committee with an inaugural class of 32 individuals, and induction is considered to be one of the highest honors in the field of visualization.
“I’m flattered and honored to be inducted into the IEEE Visualization Academy,” David says, “and when I look at the group being inducted in this initial year, I feel very fortunate.” With this honor coming on the heels of the 2018 Best Poster Award at IEEE Vis, David continues to have an outsized impact on the exciting field of visualization.
"Mechanism design," says Professor Amy Greenwald of Brown CS, "is the engineering branch of game theory, where knobs can be adjusted to achieve certain goals in multi-agent systems inhabited by strategic agents. For example, a network designer might seek a design that minimizes congestion assuming selfish agents."
In a recent Research Keynote Series address ("Learning Equilibria in Simulation-Based Games ... and the Ensuing Empirical Design of Mechanisms") at the annual IEEE UEMCON (the Institute of Electrical and Electronics Engineers Ubiquitous Computing, Electronics and Mobile Communication Conference), she put forth a methodology to design these mechanisms. It applies under two key conditions:
"Under these conditions," Amy says, "we use the probably approximately correct learning framework to construct algorithms that learn equilibria.  We show experimentally that our methodology can be used to design effective mechanisms that capture stylized, but rich multiagent systems, such as advertisement exchanges, which are not generally amenable to analytical mechanism design."
Amy's research on focuses game-theoretic and economic interactions among computational agents, applied to areas like autonomous bidding in wireless spectrum auctions and ad exchanges. During the 2018-19 academic year, she was a visiting researcher at the Artificial Intelligence Research Center at the Japanese National Institute of Advanced Industrial Science and Technology in Tokyo, and in 2017, she gave a keynote address at Microsoft's Faculty Summit.
"I had the misfortune of having early success," says Professor Shriram Krishnamurthi of Brown CS. In a recent invited talk ("On the Expressive Power of Programming Languages") at Papers We Love Conference 2019, he shares some personal history with the audience, saying that academic achievement at the start of his career brought with it a profound experience of impostor syndrome.
During that identity crisis, he read a paper of the same title by Matthias Felleisen. "It was the most stunning paper I had ever read," Shriram says, "and remains so. It's like the poem that never leaves your soul." Explaining that his listeners will benefit from "cultural knowledge" of programming language theory, Shriram lifts them above the fray of linguistic claims and language wars to show how a fellow scientist's devotion to a clean approach, correspondence of formalism and intuition, and tautness of execution can be personally revelatory.  
"I hope to peel off some of those layers," he says, "and help you, too, understand the paper – hopefully while preserving the joy and beauty I experienced."
A recording of the talk is available here.
Shriram's other recent invited talks include a plenary talk at FCRC 2019 (recording here) and keynotes at École Polytechnique Fédérale de Lausanne's Teaching Computational Thinking Workshop (recording here) and Programming 2018.
Brown CS is happy to announce that Stefanie Tellex has been promoted to Associate Professor with Tenure, effective as of July 1, 2019. "This achievement," she says, "could not have happened without my amazing students and collaborators. I'm privileged to work with some of the best people in the world, and my research success would not be possible without them. I have truly found an academic home in the Brown CS Department, and I greatly appreciate finding such a supportive, inclusive, and friendly
environment."
Stefanie came to Brown in 2013 after working as a research scientist at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL). She was also a postdoctoral research associate at MIT, serving as the technical lead for the Interpretation of Spatial Language Project, which worked to develop a language understanding system for robotic mobile manipulators.
Stefanie's Humans 2 Robots Laboratory aims to create robots that collaborate with people to meet their needs, so that human-robot collaboration approaches the ease of human-human collaboration. They apply probabilistic methods, corpus-based training, and decision theory to develop interactive robotic systems that can understand and generate natural language. Their focus is on three key challenges: 1) perceiving the world using the robot’s sensors, 2) communicating with people to understand their needs and how to meet them, and 3) acting to change the world in ways that meet people’s needs.
In addition to her many honors (they include an NSF CAREER Award, Sloan Fellowship, DARPA Director's Fellowship, and a DARPA Young Faculty Award), Stefanie has served as an advisor for a children's book about robots and contributed to an exhibit at London's Science Museum. Most recently, she received a Seed Award from Brown's Office of the Vice President for Research and Brown's Early Career Research Achievement Award.
“I had a choice to take a high-paying job in Silicon Valley,” laughs Brown CS alum Victoria Chávez ‘18, “but I chose to follow my passions and make a difference.” Making a difference seems to be the hallmark of so many Brown students, and Victoria is no different. Currently working as part-time faculty for the University of Rhode Island computer science department and serving as a consultant for the Computer Science for Rhode Island (CS4RI) education initiative, she’s devoted her career to helping others, and her impacts can be felt far beyond beyond campus.
Victoria discovered her love for computer science very early on. “I took computer science in high school,” she remembers, “and I fell in love.” This love for problem solving and tinkering, combined with her passion for making an impact, naturally led her to Brown. But at Brown, she decided to take a bit of an unconventional path. “I was super involved as a TA,” she explains, “and I wanted to help students from a similar background to mine.” Growing up in a single-parent household in the West Side of Chicago, Victoria knew the struggles and challenges often faced by those from disadvantaged backgrounds, and wanted to help those students. 
“Low-income students end up having much less preparation for college, and are much more likely to leave STEM fields,” describes Victoria. So with this realization in mind, Victoria chose to follow a path that diverged from the typical high-paying Bay Area development job that so many CS students decide to pursue. “I couldn’t find a Master of Arts in teaching computer science,” she laughs, “so I went for a Master’s in Urban Education Policy.” Now armed with a background in both STEM and Urban Policy, Victoria took on a role as a researcher with CS4RI. CS4RI is one of the most comprehensive computer science initiatives in the country, with the main goal of ensuring that CS opportunities and access are available to all Rhode Island students in the years ahead. Specializing in providing access to students with learning disabilities, Victoria immersed herself in the program.
It was during her time at the program that one of her colleagues brought up a suggestion. There was an open teaching position in the URI computer science department, and Victoria seemed like the perfect fit. Naturally, she jumped at the opportunity to get involved in a grass-roots movement to improve computer science access. 
Why has she chosen this route? “I don’t want to spend my life debugging code,” laughs Victoria. “I like big-picture solutions. I don’t really care about producing products for a corporation, I would much rather work on making education more equitable, on helping low-income students succeed.” Victoria has brought this philosophy to URI, where she currently teaches Web Design and Programming, a course for students with all levels of programming experience. “I want to create an environment where everyone can prosper,” she remarks, “an environment where I can really get to know my students and there’s an emphasis on learning and not just grades.” 
When asked what prepared her the most for her eventual career, Victoria has an immediate answer: the UTA program. “I felt like working as a TA really prepared me for the work,” she explains. “It shaped my view of computer science and helped me understand what I believe to be true about computer science education.” As an HTA for an introductory CS class, one of the developers of CS 17’s workshops, and a founding member of Mosaic+ –a program to increase the participation and retention of CS students of underrepresented backgrounds– Victoria has clearly made the most of her time at Brown to make an impact on the Rhode Island community.
The Theory of Cryptography Conference (TCC) is an International Association for Cryptologic Research (IACR) conference that focuses on paradigms, approaches, and techniques used to conceptualize, define, and provide solutions to natural cryptographic problems and computer security concerns. With a single author paper from 2008 ("Incrementally Verifiable Computation or Proofs of Knowledge Imply Time/Space Efficiency"), Professor Paul Valiant of Brown CS has just won the conference's fifth Test of Time Award, which recognizes outstanding papers that have made a significant contribution to the theory of cryptography, preferably with influence also in other areas of cryptography, computer science theory, and beyond. He joins a distinguished list of previous winners that includes Dan Boneh, Cynthia Dwork, and Silvio Micali.
Paul's paper asks the question: can long computations be easily adapted to produce a certificate of their correctness? While each individual step of a trillion-step computation can be easily certified, no one would read through a trillion certificates. In response, in an analogy to cryptographic "hash functions" that transform any long file into a 256-bit digest, Paul shows how to compress long proofs into short yet robust certificates. The key insight, Paul says, is that while regular proofs lose their security when compressed, this is not true for a variant called "proofs of knowledge". Paul's ideas have found crucial use in recent cryptocurrency developments, under the name "zk-SNARKs", where a "SNARK", named in honor of a mythical beast in a Lewis Carroll poem, stands for "Succinct Non-interactive ARgument of Knowledge". The Test of Time Award, to be presented in December in Nuremberg, Germany, cites Paul's paper "for demonstrating the power of recursive composition of proofs of knowledge and enabling the development of efficiently verifiable proofs of correctness for complex computations".
You can find a full version of Paul's paper here.
Brown CS is glad to announce that applications are open for the Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, which provides $10,000 annually to support an undergraduate engaged in an intensive faculty-student summer research partnership with the Department of Computer Science.
The award is available thanks to a generous gift from Brown CS alum Peter Norvig, now a Director of Research at Google and a thought leader in the areas of artificial intelligence, natural language processing, information retrieval, and software engineering, was drawn to Brown as an undergraduate by the open curriculum and his interests in computer science and linguistics, which he studied in high school. His gift honors the life and work of Randy F. Pausch '82, a renowned expert in computer science, human-computer interaction, and design who died of complications from pancreatic cancer in 2008 and whose "Last Lecture" has been widely praised. "I didn't know Randy when I was at Brown," Peter says, "but we met afterward and corresponded for many years. His story is inspiring, and this is an opportunity to remember him."
Norvig sees this award as a "multiplier" that will amplify the value of his gift and extend it through time. "I'm interested in students with a wide range of personalities and interests," he says, "and in putting students and faculty together. In the past, we had to build all our own tools, and we didn't have time to combine computer science with other fields. Now, there are so many opportunities to do so. I think it's a wise choice: you invest in things that you think will do good, and educating a student allows them to help add to the things that you're already trying to accomplish." 
Last year, the award went to Evan Cater, who used the funds to explore autonomous agents that operate under uncertain conditions with Professor Michael LiIttman.
To apply, no later than February 9, 2020, students should email Associate Professor (Research) and Vice Chair Tom Doeppner either: (A) a copy of their summer UTRA application or (B) a two-page description of their proposed research and a letter of support from the Brown CS faculty member that they intend to work with.
Brown CS PhD student Evangelia Anna (Lilika) Markatou has just won the Best Student Paper Award for her paper “Full Database Reconstruction with Access and Search Pattern Leakage” (coauthored with her advisor, Professor Roberto Tamassia) presented at the International Security Conference (ISC). They also coauthored a second paper on “Mitigation Techniques for Attacks on 1-Dimensional Databases that Support Range Queries” presented at the same conference. Held last week in New York City, ISC is an annual international conference covering research in theory and applications of information security.
“The widespread use of cloud computing,” the authors explain, “has enabled several database providers to store their data on servers in the cloud and answer queries from those servers. In order to protect the confidentiality of data in the cloud, a database can be stored in encrypted form and all queries can be executed on the encrypted database. Recent research results suggest that a curious cloud provider may be able to decrypt some of the items in the database after seeing a large number of queries and their (encrypted) results.” In this paper, Lilika and Roberto focus on one-dimensional databases that support range queries and develop an attack that can achieve full database reconstruction, inferring the exact value of every element in the database. 
“You’re going to be in a room someday,” says Sydney Skybetter, Lecturer at Brown’s Department of Theatre Arts and Performance Studies. “Where your colleagues are designing technologies intent on targeting other people’s bodies, with the aim of manipulating them, and potentially causing harm. As a computer scientist, you will have a voice in that room. When that happens, I want you to be able to speak in the interests of broader civic good, not institutional inertia or surveillance capital.”
A choreographer, speaker, and producer, Sydney is the founder of the Conference for Research on Choreographic Interfaces, and he’s giving a talk (“Dark Elegies: The Choreographics of Surveillant Systems and National Defense”) at Brown CS on October 4. His interest in the relationship between choreography and surveillance dates back to 2015, when, while teaching at Harvard University, he learned about “Marauder’s Map,” a web browser extension that revealed the extent to which the Facebook platform was tracking users’ movements and physical locations.
“I was getting ready to teach students about seventeenth-century pirate dances, as you do,” Sydney remembers, “and this just blew me away, the social gamifying of private location data. Who was watching this? What was the platform gleaning from it? A precondition for most dances is an audience, and with Facebook we had human movement being mined for meaning not only by people, but machines.”
Before long, Skybetter was deeply engaged with what he calls the intertwined histories of computer science and contemporary dance. Examples abound: Bell Labs scientist, A. Michael Noll, created the first digital dance in 1965 after seeing the New York City Ballet in concert, and several commonly-used motion capture systems contain movement data obtained from choreographers Bill T. Jones and Merce Cunningham in context of the AIDS crisis of the 1990s.
As a recent case study, Sydney mentions the Apple Watch: “The scientists who created the product weren’t just trying to track bodies, they were distinguishing between the intentionality of certain gestures. They tried to define what certain, supposedly universal movements meant, but meanwhile, lots of folks weren’t (and aren’t) invited to contribute their embodied experiences: folks of color, folks with disabilities, folks with varying cultural expectations pertaining to bodily expressiveness.” 
Given what Sydney sees as a troubling history of people’s bodies and movement being seen as objects, as data, he sees great value in computer science students thinking of the body specifically not as an abstraction. “We’re in a moment where CS is reconsidering its relationship to the arts and humanities,” he says, “and choreography frames this eloquently. What happens when we put bodies first?”
Which brings us to his talk (“Dark Elegies: The Choreographics of Surveillant Systems and National Defense”) on October 4, which he hopes students will see as an opportunity to broach questions of bodily ethics but also as a doorway into a line of study that can be social and fun. “Dance and choreography,” he says, “are unlike what many students have seriously studied before. They help us understand the tangible ways the world affects our selves, and develop an understanding of our own embodiment that contributes to understanding others.”
Some of his best students, Sydney notes, have been those with CS experience. “There are specific kinds of research problems,” he says, “that people with backgrounds in computation and dance are uniquely able to address.” Monica Roy is a sophomore with experience in ballet and modern dance who is concentrating in CS and serving as an Undergraduate Teaching Assistant for CSCI 0170. “Over the summer, Monica helped parse some of the technical papers from the Snowden leaks, and kept specific tabs on how the NSA understands time, space, and body movement. This material is uniquely legible to her because of her background in dance and tech.”
Sydney hopes that Brown CS students will also be interested in the Conference for Research on Choreographic Interfaces, which will take place on March 6 and 7. “We’ll have people there working through questions of bodies and emerging technologies, representing an incredibly wide spectrum of expertise. Every year we facilitate conversations between artists and computer scientists that value equally the expertise of both. It’s a dorky fun nerd gaggle.” But also deadly serious, he says, as the participants at times consider the dystopic possibilities of their work.
“None of this is easy,” he says. “But scientists need to be able to talk about bodies. It’s something we can practice. Dance helps us rehearse these negotiations. As a future participant in a room where decisions about surveillant technologies will be made, you can either go with the sway of the group, or you can force consideration of how your decisions affect others. This kind of tech moves fast. Chances are good that if you don’t add friction in the moment, nobody else will.”
Founded almost thirty years ago, the Electronic Frontier Foundation (EFF) is one of the leading nonprofit organizations defending civil liberties in the digital world. In 1992, they established their Pioneer Award to recognize leaders on the electronic frontier who are extending freedom and innovation in the realm of information technology. The list of previous winners is formidable (it includes Anita Borg, Tim Berners-Lee, Douglas Engelbart, and Limor Fried), and on September 12, 2019, Brown CS alum and Advisory Board member danah boyd joined them alongside author William Gibson and surveillance-fighting organization Oakland Privacy.
In her acceptance speech, danah addressed what she called a "great reckoning" that's begun to occur in response to a patriarchial and exclusionist industry whose values, she says, have been lost or violated. "People can change," she said. "Institutions can change. But doing so requires all who harmed — and all who benefited from harm — to come forward, admit their mistakes, and actively take steps to change the power dynamics. It requires everyone to hold each other accountable, but also to aim for reconciliation not simply retribution."
She also offered advice on how to achieve this: "Let’s stop designing the technologies envisioned in dystopian novels. We need to heed the warnings of artists, not race head-on into their nightmares. Let’s focus on hearing the voices and experiences of those who have been harmed because of the technologies that made this industry so powerful. And let’s collaborate with and design alongside those communities to fix these wrongs, to build just and empowering technologies rather than those that reify the status quo."
Currently working as a Principal Researcher at Microsoft Research, danah is the founder and president of the Data and Society Research Institute and serves as a Visiting Professor at New York University. She also serves on the boards of Crisis Text Line and Social Science Research Council and is a Trustee of the National Museum of the American Indian. Widely recognized as an authority in the field of social media research, her recent work focuses on how data-driven technologies can amplify inequities, mistrust, and hate. This recognition by the EFF is the latest in a long list of accomplishments, including (most recently) being named to the Forbes Top 50 Women in Tech, winning the Award for Public Sociology from the American Sociological Association, and being named the “smartest academic in the technology field” by Fortune.
"Here we are," said Brown CS Professor Andries "Andy" van Dam in 1987, "and we have to ask, perhaps rhetorically, has hypertext arrived?" He was delivering a keynote address at the first ACM Hypertext Conference (text available here) in which he credited visionaries Doug Engelbart and Ted Nelson, reviewed Brown's hypertext experiments to date, and talked about nine issues facing the linked world, ranging from standards and accessibility to sociopolitico-economic problems. "Hypertext is basically clay, and we have to mold it; that is what this workshop is all about: starting to mold that clay."
Three decades later, Andy will return to the conference for its thirtieth anniversary to again deliver a keynote ("Reflections on a Half-Century of Hypertext"), this time in Hof, Germany, on September 17. As part of his remarks, he'll be looking back on five decades of Brown hypermedia systems and demoing three of them (FRESS, from the late ‘60s, TAG, from 2017, and Dash, under development), presenting them in the context of the research trends that ultimately led to the interconnected society in which we live.
"What I did not predict," he says, referring back to the visions of pioneers like Engelbart, "is that 50 years later the revolution in human-centered computing would remain far too unfinished in terms of its positive societal impact. Indeed, that impact and utility are increasingly in jeopardy from a variety of forces, both economic and political. I will close with some thoughts on both deliberately designed and unanticipated societal issues of social media that I feel we technologists must urgently help address."
Andy's keynote will be livestreamed on September 17: see the event's website for details.
Computer science that benefits humanity, science that’s grounded in an awareness of its societal impact, has long been a Brown CS hallmark. Today, more than forty years after the founding of the department, global concern about the ethical and societal issues surrounding computing is greater than ever. This semester brings a new departmental initiative, Socially Responsible Computing, which will place the exploration of those issues broadly at the heart of the Brown CS undergraduate experience. 
In the last few years, the department has started to offer a number of courses that exclusively address the ethical and societal impact of CS. These include Cybersecurity Ethics (new this fall); Data, Ethics and Society; and CS for Social Change. While these courses allow students to explore these subjects in depth, Brown CS felt it was important to cover ethical and societal issues in as many courses as possible and as frequently as possible.
“Computer science doesn’t happen in a vacuum,” says Professor Ugur Cetintemel. “Having individual courses on these topics can make it look like ethics is just a box to check, and that can sometimes cause students to think they’re 'done' after completing a single course.” Several CS classes have already been incorporating ethical and societal issues in their course material, he explains, and instructors from other classes had expressed an interest in doing so with more institutional support. That’s what led to the creation of the Socially Responsible Computing program.
Looking to benefit from the insights of other disciplines, as well as to trigger excitement around these topics across the campus, Brown CS began engaging with relevant campus units that include Philosophy, Science and Technology Studies, and Modern Culture and Media (MCM). Their knowledge is being put to use by a newly-formed group of Undergraduate Teaching Assistants (UTAs), the STAs. Under the supervision of Brown CS faculty, the STAs are working with course staff and graduate students from the partner units to create customized content for each class that examines ethical and societal issues in computer science. 
It’s a pilot, and one whose results will be closely watched, with an eye to expanding the initiative in the semesters to follow.
“The goal,” says Jessica Dai, one of the Head STAs, “is that Brown CS grads will be thinking about and driving responsible tech wherever their work may take them. We aren't prescribing absolute morality judgments, but hope that the additional questions we raise and context we provide will help lead students to think about ethics in the long term. We're partnering with courses directly, embedding ethics education in technical course content.”
Strong Interest
The first step was to recruit the initial cohort of STAs, and interest was immediate and strong. Brown CS received more than double the number of applications needed to fill the positions, and eventually chose ten students for the pilot: Lena Cohen, Jessica Dai, Signe Golash, Heila Precel, Andrew Rickert, TzuHwan Seet, Kendrick Tan, Hal Triedman, Stanley Yip, and Rebecca Zuo. Jessica and Stanley are serving as Head STAs. It’s a multidisciplinary group, with most of the students pursuing or planning to pursue double concentrations in areas such as History, Philosophy, and MCM.
Enthusiasm from faculty members and course staff was equally emphatic, with numerous courses hoping to take part. Professor Jeff Huang says that ethics has always been a component of his classes. “I’m interested,” he explains, “in helping students articulate their own principles for determining the ethical boundaries of the products they design.”
In the end, based on STA familiarity with the course material, Brown CS chose five classes, prioritizing introductory courses and ones with high enrollment in order to maximize early exposure, number of students affected, and impact. They include the following, with two STAs assigned per class: CS 15 Introduction to Object-Oriented Programming, CS 17 CS: An Integrated Introduction, CS 111 Computing Foundations: Data, CS 130 User Interfaces and User Experience, and CS 147 Deep Learning. The STAs have been working over the summer with the staff of their assigned courses to create integration plans for the new content. During the semester, the STAs will continue to play a supporting role, working with course staff to develop, deliver, and grade the content.
“In CS 15,” says Professor Andries “Andy” van Dam, “over the last few years I’ve had ‘IT in the News’ segments at the beginning of each class, mostly devoted to social impact and issues of our technology. This year we plan to have our assigned STAs, both former CS 15 TAs, lecture on ethics and social-impact-related topics during these segments.”
Getting Started
Over the summer, the STAs have been busy. With a focus on three general areas (ethical software development and data analysis, biases and other ethical pitfalls, and using computing for societal good), they’ve been doing the following:
The STAs identified the following principles to organize the coverage of this material in courses:
“My goal,” says Professor Doug Woos, “is to treat ethics as much as possible as part of the core curriculum, rather than as something extra or external that we've bolted on. In particular, I'd love to see students really putting themselves in the shoes of practitioners who are making engineering decisions with social consequences, in the hopes that they’ll understand how those decisions get made, what the pressures are, and how to make ethical choices.”
“I want every CS 17 student to understand,” says Professor John Hughes, “that every program they write (especially those they release to the world) has an impact, and that the extent of the impact may be far from obvious; that every program specification involves subtle choices that have consequences –did my choice of string representation mean that this program is usable only by people who use US English and the associated character sets?– and that even with ethical guidelines, many of the decisions we make in computer science may be very difficult.“
Looking Ahead
As the semester progresses, Brown CS will evaluate the pilot’s success and plan the initiative’s future. Goals for the next phase of Socially Responsible Computing include: 
Professor Daniel Ritchie is eager to see students thinking critically about the ethical issues of deep learning. “My goal,” he says, “is for them to leave the course having developed an 'ethics-first' mindset: that they naturally consider the implications of the models they’re building as they’re building them, rather than as an afterthought.”
“I'm incredibly excited about the mission of this program,” says Stanley Yip, one of the two Head STAs. “It's of utmost importance, especially now, for computer scientists to have the skills necessary to think critically about ethical choices and answer ethical dilemmas that they will experience throughout their career. Ideally, every student leaves Brown CS with these abilities, regardless of which courses they took.”
It could be said that Malte Schwarzkopf, who joins Brown CS as Assistant Professor this fall, prefers the long view, and perhaps the wide-angle lens: “Efficiency of datacenter systems is one of the major tracks of my research because the data center stack is important to all of us. Awake or asleep, hundreds of computers are doing things on our behalf, and if they do them inefficiently, that can be expensive and wasteful. How can humanity improve on that? With today’s systems, it’s easy to do a lot of computation at the flick of a switch, but many of these systems are not very discerning in their resource use, and we should aim to do better.”
One way to begin that process, Malte finds, is by excavating the past, and his favorite possessions include a layperson’s guide to technology from 1828. “It illustrates how progress has happened, and creates a connection with people who lived in a very different world,” he says, “and that’s fascinating to me.” On one page, electricity is listed as a curiosity for which no practical use has been found, but one of the book’s owners worked to keep it current by adding annotations and newspaper clippings, including one of the Wright brothers taking to the air for the first time.
“History helps explain why things are the way they are and why we think the way we do. One great example from computer systems research is that only two decades ago,” Malte explains, “we didn’t have journaling file systems, and I have a vivid memory of Windows 95 crashing during an OS upgrade and running ScanDisk to figure out what had happened and repair the file system. It’s totally different now: all major operating systems use journaling file systems to prevent these problems from arising in the first place. I find it very satisfying to learn from the past when I try to invent new things, and to see what impact computer systems research can have even in, by the dimensions of history, a very short term.”
Most recently, Malte’s research produced Noria, a new system that makes websites more efficient by reducing the redundant computation done at their backend. Noria uses a technique called “dataflow computing” to efficiently precompute the data that the website’s frontend might request. This ensures that reads – which are much more common than writes in these applications – are maximally efficient, improving the load served with the same hardware by 5-74x over common industry practice. In his prior work on datacenter systems, Malte developed cluster schedulers, systems for data science on big data, and a new datacenter network design. His work has received best paper awards at EuroSys 2013 (for the Omega cluster scheduler) and NSDI 2015 (for the QJump datacenter network design).
Schwarzkopf’s introduction to computers came at age seven or eight, in the form of an IBM PC with a 386 processor running at 25 MHz. It was a pretty typical first experience, he says: “I was growing up in Hamburg, and it had been surplused at the insurance company where my father worked, so he brought it home for us. We kept it in the basement and it was a lot of fun.”
Talk of the era before flat screens and smartphones brings us to the second track of Malte’s research. “Unlike in those days,” he says, “our data lives remotely now, and our computers and devices are really just portals. This creates trustworthiness problems about our data being lost, destroyed, or sold that are very much unresolved. Systems play an unappreciated role in this. If we create them in a free-for-all environment, we often end up with bad outcomes, but when we define some parameters, we may get a degree of assurance, a reason to trust.”
By the end of primary school, Malte had started programming in the form of writing macros for Microsoft Office. Self-taught, he eventually became the systems administrator for his high school’s computer system. It was a grassroots effort, he says, with students running the show and building computers themselves, and it was likely the origin for his applied computer science perspective. In the years that followed at the University of Cambridge, Schwarzkopf credits Steven Hand, who later became his PhD advisor, with inspiring his love of systems.
“Broadly speaking,” he says, “I’m interested in all flavors of computer systems, but my recent work has focused on data centers. It’s an interesting area and things are still in flux, with hardware and software always presenting new demands.” Malte’s doctoral research focused on operating systems for large data centers, including work on cluster schedulers, which provide a kind of brain for the data center by deciding which servers should process which work in order to be most efficient.  “Currently, I’m very interested in dataflow systems, as I believe they’re a great abstraction for building scalable infrastructure that is easy to use, but also to reason about.” 
As his research continues, Malte sees Brown as a perfect home for someone who wants to take risks and look ten years down the road instead of being bound to a six-month industry cycle. “And I love teaching,” he says. “I really enjoy seeing students I mentor grow into researchers, and I’m passionate about diversity because we need to move beyond the stereotypes. Even though I work in systems, you don’t need to have been hacking on a kernel in your basement since you were nine. You need to be curious about how things work, keen to build new things, and enjoy tackling intellectual challenges. My wife is a political philosopher, and she provides a lot of input for my research. The best questions come from people who see links and bring a different perspective and are curious – that’s more important than the number of lines of code you have written. We need that broad spectrum.”
The most enjoyable moments of teaching, Malte says, are when neither nor he nor his students know a particular answer, but they approach the problem together. He’s hoping for many such moments in his new course (CSCI 2390 Privacy-Conscious Computer Systems), which focuses on systems that protect user data by design through enforcing predefined rules. “We need that kind of two-way learning because so many of these problems are unsolved, even for simple web apps. We need to find ways for companies to run back-end systems that won’t harm users, and a lot of theoretical work has been done in this area, but many of the concepts are impractical and will never be deployed. Legislators are waking up, and they’re asking for system designs that better protect sensitive information at a cost where people can use them.”
Ultimately, Schwarzkopf hopes that he and his students will respond to that need with feasible, affordable, and efficient solutions. The way to build them, he says, is by challenging each other to be better engineers, and more responsible ones: “Building practical systems is in my DNA, and I want to keep doing that with my students, making things that work and are worthy of trust and have some certainty of becoming widely used.” 
Perhaps someday they’ll be able to write their own names among the marginalia of that book of his. “As academic researchers,” Malte says, “we’re part of history. We’re telling our future selves why they do what they do as scientists, as creative humans.”
This year's ACM SIGMOD (Association for Computing Machinery's Special Interest Group on Management of Data) conference was held in Amsterdam, and members of the Brown CS community returned from the event with two notable awards. PhD candidates Philipp Eichmann, Franco Solleza, and Junjay Tan; alum Nesime Tatbul; and Professor Stan Zdonik received the Best Demonstration Award and alums Nathaniel Weir and P. Ajie Utama earned Second Place in the Undergraduate category of the Student Research Competition.
Philipp, Franco, Junjay, Nesime, and Stan's research ("Visual Exploration of Time Series Anomalies with Metro-Viz") presents a novel data visualization solution for exploring the results of time series anomaly detection systems. "When anomalies are reported," they explain, "there's a need to reason about the results, and so we introduce Metro-Viz, a visual tool to assist data scientists in performing this analysis." Metro-Viz offers a rich set of interaction features, including comparative analysis and what-if testing, backed by data management strategies specifically tailored to the workload. In their demo, they showed the tool in action via multiple time series datasets and anomaly detectors.
The project's web site is here, and the researchers are recognized in the first three minutes of the award session video here. 
Nathaniel and P. Ajie's research ("Bootstrapping an End-to-End, Cross-Domain NLI for Databases") offers an end-to-end Natural Language Interfaces for Databases (NLIDB) framework in which a neural translation model is trained for any new database schema with minimal manual overhead. "The ability to extract insights from data," they write, "is critical for decision making. Intuitive natural language interfaces to databases provide non-technical users with an effective way to formulate complex questions and information needs efficiently and effectively." In addition to being the first off-the-shelf, neural machine translation based system of its kind, their project's other contribution is its use of a synthetic training set generation pipeline used to bootstrap a translation model without requiring manually curated data. In experiments, they show that their system can achieve competitive performance on the recently released benchmarks for nl-to-sql translation.
We're seeking applicants for a faculty position at the rank of lecturer, senior lecturer, or distinguished senior lecturer. The initial appointment would be for a 3-year period (renewable with potential for promotion and longer-term contracts). This position is part of a major expansion plan for the department as it is increasing its roster by 50% over the next few years. The position involves teaching four undergraduate courses per year and advising undergraduate CS majors. At least some of the teaching would be in first- and second-year courses. Candidates will also teach some upper-level undergraduate courses, based on their expertise and department needs.
The department seeks candidates who will contribute to its overall intellectual culture; lecturers are included in faculty meetings, advise undergraduate research projects, and participate in graduate research with the rest of the faculty. Lecturers with substantial research participation and supporting funds may be eligible for periodic course release. The department values teaching and educational innovation, and welcomes candidates interested in formally researching computing education in the context of their teaching. The department’s commitment to diversity and inclusion is reflected in several activities both within and beyond the scope of our courses.
Brown offers a vibrant community for both teaching and research, with 34 tenured and tenure-track faculty members, two lecturers, three research faculty and several affiliated, adjunct, and visiting faculty members. The department has a strong undergraduate culture, anchored by a mature program for undergraduate teaching assistants (endowed at $10 million), as well as a long history of top-caliber published undergraduate research. Research and graduate programs leverage disciplinary strengths in CS as well as Brown’s broader interdisciplinary culture. CS is a founding partner in multiple university-wide initiatives including Data Science, Computational and Molecular Biology, Cybersecurity, and Human-Centered Robotics.
The position is expected to start in the fall of 2020. In selecting candidates, we will consider quality of teaching, evidence of effective teaching, commitment to diversity and inclusion, and compatibility with the area needs and interests of the department, as well as potential for effective participation in department or university activities. For all applicants, we will consider potential for impact beyond Brown (through teaching, research, significant system building, outreach, or other professional activities, as appropriate for the candidate). Applicants must have a Ph.D. by the start of the position. Applicants must submit a CV, a teaching statement, and a research statement (or a statement describing other significant professional activities beyond classroom instruction). Candidates must also arrange for at least three letters of reference to be submitted through the application website.
Brown University is committed to fostering a diverse, inclusive, and global academic community. As an EEO/AA employer, Brown considers applicants for employment without discrimination on the basis of gender, race, protected veteran status, disability, or any other legally protected status. The department is similarly committed to building a diverse faculty and strongly encourages women, underrepresented minorities and those who can contribute to the excellence, diversity, and inclusivity of our academic community. We strongly encourage the candidates to report any relevant experience, including work with diverse constituents, and plans in their teaching statements.
Brown University is located in Providence, RI, close to Narragansett Bay, an hour from Boston and about three hours from New York City. Providence has been consistently rated among the Northeast's most livable cities and is home to diverse intellectual, artistic, and business communities.
To apply, please use Interfolio (https://apply.interfolio.com/66319). Review will begin on October 1, 2019, but applications will be considered until the position is filled. Inquiries may be addressed to: teaching_faculty_search_2020@lists.cs.brown.edu.
Nishanth Kumar is an Undergraduate Research Assistant in Brown CS Professor Stefanie Tellex's Humans 2 Robots Lab whose interests include robotics, artificial intelligence, and using hardware and software to create truly useful products. Last semester, he was selected as a Plenary Speaker for the Ivy League Undergraduate Research Symposium (ILURS) 2019 for his research ("Action-Oriented Semantic Maps via Mixed Reality") with alum Eric Rosen, Brown CLPS PhD student Daniel Ullman, alum David Whitney, Professor George Konidaris, and Stefanie. While there, he was recognized as Best Plenary Presentation, the conference's top honor.
Asked about his research, Nishanth explains that if we want robots to operate robustly in human-centered environments, they need to bridge the gab between their internal models and objects, actions, and tasks in the real world. "Semantic maps," he says, "are the prototypical representation for grounded robot knowledge about objects and attributes, but they don't naturally include information about actions. In our work, we present a novel approach to bridging the semantic gap via Action-Oriented Semantic Maps (AOSMs): semantic maps that also model the possible consequences of actions."
Nishanth's presentation can be found here and the official research demonstration video is here.
Professor Jeff Huang of Brown CS has just received an Army Research Office Young Investigator Award, which is given to support the research and encourage the careers of outstanding young university faculty members. Jeff's Brown HCI research group focuses on behavior-powered systems, and their goal is to invent new, personalized applications from remotely-captured user behavioral data. The award will support Jeff and his students in research investigating how emotional cues can be inferred from large-scale social messages, particularly on mobile devices.
The goal is to identify which aspects of social messaging affect emotion, and in what way do different social features such as frequency, content, timing, and recipients of messages influence affect. This research builds on existing work the HCI group has done with Sochiatrist, a social extractor that automatically dumps and anonymizes social communication data (like texting and Facebook messages) and combines it with biomarker data and qualitative interviews done by clinical researchers to find correlations. The project is in collaboration with clinical psychiatrists from Rhode Island Hospital who have collected data about mental health patients and the funded research proposes to work with veterans, who are particularly at risk of mental health issues.
“I’m eager to start this research, and we’ve already started exploring the unique nature of messaging, which is noticeably different from formal English, and how to find signals of social support and moments of reveal,” Jeff explains. “I’m excited about working on this with students in my group who have thought about the machine learning aspects of messaging, together with graduate students admitted into our group in the fall who have both computer science and psychology backgrounds.”
Jeff has previously received another early CAREER award from the NSF for research on modeling user behaviors on mobile devices.  
"Computer science education," says Professor Shriram Krishnamurthi of Brown CS, "is a difficult and fascinating problem, sitting at the intersection of the technical and human." In a recent plenary talk ("The Role of Computer Science in Computer Science Education") on June 26 at ACM FCRC, he showed how solving even seemingly simple educational problems requires applying results from several areas of technical computer science.
A recording of the plenary talk is available here.
Shriram's other recent invited talks include keynotes at École Polytechnique Fédérale de Lausanne's Teaching Computational Thinking Workshop (video available here) and Programming 2018.
Professor Sohini Ramachandran of Brown University's Center for Computational Molecular Biology (she also serves as its Director) and Department of Computer Science has just received a Presidential Early Career Award for Scientists and Engineers (PECASE). Sohini's Ramachandran Group of researchers addresses problems in population genetics and evolutionary theory, generally using humans as a study system. Their work uses mathematical modeling, applied statistical methods, and computer simulations to make inferences from genetic data. 
The PECASE is the highest honor bestowed by the United States Government to outstanding scientists and engineers who are beginning their independent research careers and who show exceptional promise for leadership in science and technology. Established in 1996, it acknowledges the contributions scientists and engineers have made to the advancement of science, technology, education, and mathematics education and to community service as demonstrated through scientific leadership, public education, and community outreach.
"I’m very honored," Sohini says, "to have been nominated by the National Institutes of Health in the Department of Health and Human Services for this award. My research program focuses on studying the causes and consequences of human genomic variation, entirely from a mathematical and computational perspective. My becoming a PECASE awardee underscores that quantitative approaches in biology are equal partners with molecular, cellular, and organismal biomedical research in helping us understand the origins and diversity of life, as well as how to improve human health."
Sohini was one of just three PECASE winners from Rhode Island, along with Professor Anita Shukla of Brown's School of Engineering and Center for Biological Engineering. She joins Professor Amy Greenwald of Brown CS, a previous PECASE winner.
A full list of recipients is available here.
"In today's security environment," says Professor Seny Kamara of Brown University's Department of Computer Science (Brown CS), "we believe it's necessary to have systems that fully encrypt the user's data at every point in its life cycle." He's talking about end-to-end encryption, the focus of his Encrypted Systems Lab and new startup Aroki Systems. Last year, the Encrypted Systems Lab's research saw fruit in the release of an app, Pixek, which uses structured encryption (a kind of encryption Seny co-invented in 2010 with Brown PhD Melissa Chase) to provide a simple solution for keeping photos private while in the cloud. More recently, Seny and Aroki co-founder Tarik Moataz have been putting their expertise to good use at an entirely different scale.
Due to Seny and Tarik's expertise in designing and cryptanalyzing encrypted search algorithms, MongoDB asked for an analysis of a new product feature called Field Level Encryption. MongoDB, co-founded by Brown CS alum Eliot Horowitz, is a general-purpose, document-based, distributed database built for modern applications in the cloud era. It's used by millions of developers worldwide and some of the largest organizations in the world including Adobe, Facebook, Google, and Verizon. Field Level Encryption uses local keys on a client to ensure that data will be inaccessible even to servers run by cloud providers and database managers. It's an increase in security for database users who previously had been vulnerable to hackers and rogue agents abusing access.
"It’s exciting to see industry involving the research community and leveraging our expertise," Tarik says. "MongoDB are great to work with and are very receptive to our findings."
An algorithm developed by Brown University computer scientists enables robots to put pen to paper, writing words using stroke patterns similar to human handwriting. It’s a step, the researchers say, toward robots that are able to communicate more fluently with human co-workers and collaborators.
“Just by looking at a target image of a word or sketch, the robot can reproduce each stroke as one continuous action,” said Atsunobu Kotani, an undergraduate student at Brown who led the algorithm’s development. “That makes it hard for people to distinguish if it was written by the robot or actually written by a human.”
The algorithm makes use of deep learning networks that analyze images of handwritten words or sketches and can deduce the likely series of pen strokes that created them. The robot can then reproduce the words or sketches using the pen strokes it learned. In a paper presented at the International Conference on Robotics and Automation in May, the researchers demonstrated a robot that was able to write “hello” in ten languages that employ different character sets. The robot was also able to reproduce rough sketches, including one of the Mona Lisa.
Stefanie Tellex, an assistant professor of computer science at Brown and Kotani’s advisor, says that what makes this work unique is the ability of the robot to learn stroke order from scratch.
“A lot of the existing work in this area requires the robot to have information about the stroke order in advance,” Tellex said. “If you wanted the robot to write something, somebody would have to program the stroke orders each time. With what Atsu has done, you can draw whatever you want and the robot can reproduce it. It doesn’t always do the perfect stroke order, but it gets pretty close.”
Another remarkable aspect of the work, Tellex says, is how the algorithm was able to generalize its ability to reproduce strokes. Kotani trained his deep learning algorithm using a set of Japanese characters, and showed that it could reproduce the characters and the strokes that created them with around 93 percent accuracy. But much to the researchers’ surprise, the algorithm wound up being able to reproduce very different character types it had never seen before – English print and cursive, for example.
“We would have been happy if it had only learned the Japanese characters,” Tellex said. “But once it started working on English, we were amazed. Then we decided to see how far we could take it.”
Tellex and Kotani asked everyone who works in Tellex’s Humans to Robots lab to write “hello” in their native languages, which included Greek, Hindi, Urdu, Chinese and Yiddish among others. The robot was able to reproduce them all with reasonable stroke accuracy.
“I feel like there’s something really beautiful about the robot writing in so many different languages,” Tellex said. “I thought that was really cool.”
But the system’s masterwork may be its copy of Kotani’s Mona Lisa sketch. He drew his sketch on a dry erase board in Tellex’s lab, and then allowed the robot to copy it –fairly faithfully on the same board– just below Kotani’s original.
“It was early morning that our robot finally drew the Mona Lisa on the whiteboard,” Kotani said. “When I came back to the lab, everybody was standing around the whiteboard looking at the Mona Lisa and asking me if [the robot] drew this. They couldn’t believe it.”
It was a big moment for Kotani because “it was the moment that our robot defined what’s beyond mere printing.” An ink jet printer can recreate an image, but it does so with a print head that goes back in forth building the image line by line. But this was the robot creating an image with human-like strokes, which to Kotani is “something much more humane and expressive.”
Key to making the system work, Kotani says, is that the algorithm uses two distinct models of the image it’s trying to reproduce. Using a global model that considers the image as a whole, the algorithm identifies a likely starting point for making the first stroke. Once that stroke has begun, the algorithm zooms in, looking at the image pixel by pixel to determine where that stroke should go and how long it should be. When it reaches the end of the stroke, the algorithm again calls the global model to determine where the next stroke should start, then it’s back to the zoomed-in model. This process is repeated until the image is complete.
Both Kotani and Tellex say the work is a step toward better communication between people and robots. Ultimately, they envision robots that can leave Post-it Notes, take dictation or sketch diagrams for their human coworkers and collaborators.
“I want a robot to be able to do everything a person can do,” Tellex said. “I’m particularly interested in a robot that can use language. Writing is a way that people use language, so we thought we should try this.”
At this spring's Commencement ceremony in the Olney-Margolies Athletic Center, Brown CS presented undergraduate diplomas to 259 students. It was another historic number, beating previous records of 249 last year, 205 the year before, and 168 in 2016. Here are just a few of the places where our newest alums will be working:
Congratulations to everyone! We have tremendous respect for your talents and effort, and we wish you all the best in the days ahead.
by Rujul Singh
From the IBM punch cards of the 1960’s to the machine learning labs of today, Brown University's Department of Computer Science has undergone decades of transformational change since its founding. The Brown CS Digital Archive (BCSDA) is a crowdsourced effort to preserve this history, curating items from faculty, staff, students, and alums that have contributed to Brown CS and preserving them permanently online, where they're accessible by all.
Started just a few months ago, the BCSDA has grown rapidly, with over 100 artifacts submitted to date. Ranging from handouts for the original Applied Math 101-102 computer science sequence to photos of the Brown University IBM System/360 model 67 (the main campus computing facility from 1970 to 1976), the archive encompasses a broad variety of photos, audio, PDFs, and even code that attempts to capture the story of Brown CS.
What’s contributed to the success of the archive? The efforts of Brown alums have surely played a huge role. Paul Anagnostopoulos ‘74 (thanks to alphabetical order, he was Brown's first undergraduate to receive a pure CS degree instead of a degree in Applied Mathematics with a concentration in CS) has been the top contributor to the archive, with over 80 items submitted. “I’ve coordinated many of our class reunions,” he explains, “and it really is a group effort to provide these things.” But with the archives at such an early stage, more submissions are needed to fully achieve the goals of the archive. “We need more artifacts from the 80s and 90s, the more obscure the better,” he laughs.
As participation increases, each new asset adds color and a bit of insight into the community that is Brown CS. “We're in the technology business and the change in technology is fascinating,” says Paul. “Remembering people is important. Everybody enjoyed what they were doing. People need to know what they did, and what projects they worked on.”
If you have something that you’d like to contribute to the archive, submitting an item is easy:
Within the span of a single year, Brown CS faculty members have won two of the field's highest distinctions for teaching: Professor Shriram Krishnamurthi received the SIGSOFT Influential Educator Award and Professor Andy van Dam earned the inaugural ACM SIGGRAPH Distinguished Educator Award. ("We're teaching twins," commented Andy when the two gathered for the photo above.) The honors are international recognition of continued strength in education for the Department of Computer Science, and they follow recent awards for three faculty members and two PhD students that highlight campus-wide recognition of Brown CS teaching excellence.
Presented annually, the SIGSOFT Influential Educator Award is given to an educator or educators who have made significant contributions to, and impact on, the field of software engineering through accomplishments as teachers, mentors, researchers (in education or learning), authors, and/or policy makers. In particular, the award committee noted Shriram's contributions to the advancement of the research and practice of software engineering. With this win, he joins Brown alum Barbara Ryder and the late David Notkin, a Brown CS alum, who won the prestigious award in 2015 and 2012, respectively. Both studied under Andy and David was one of van Dam's Head TAs.
Established less than a year ago, the SIGGRAPH Distinguished Educator Award is given annually to a member for outstanding pedagogical contributions to computer graphics and interactive techniques at any educational level or within the context of any discipline. It recognizes contributions in both innovative content and delivery. Nominated by the SIGGRAPH community, Andy was chosen based on award criteria that include impact on research and practice of education as it relates to computer graphics and interactive techniques, cumulative contributions to the field both directly and through leadership of others, and innovation in education.
The full list of SIGSOFT Influential Educator Award winners dating back to 2009 is available here.
Additional details about the ACM SIGGRAPH Distinguished Educator Award are available here.
by Ajula Van Ness-Otunnu
On April 30, 2019, Brown CS Professor Seny Kamara was announced as a winner of The Responsible Computer Science Challenge from Omidyar Network, Mozilla, Schmidt Futures, and Craig Newmark Philanthropies. Seny worked in collaboration with colleagues Suresh Venkatasubramanian (University of Utah) and Sorelle A. Friedler (Haverford College) to “integrate ethics and responsibility into undergraduate computer science curricula and pedagogy at U.S. colleges and universities” according to Mozilla.
The Omidyar Network, Mozilla, Schmidt Futures, and Craig Newmark Philanthropies give this award to acknowledge the power computer scientists hold in society. This power comes with the responsibility to integrate ethics into the education of prospective computer scientists. “I think it is clear to everyone now that CS is having a major impact on individuals and societies. This impact, however, is not always positive,” Seny explains, “and we need computer scientists to understand this and to strive to design and build better technologies for everyone. We need to teach our students more than how to design fast algorithms and how to build profitable companies.”
Seny and his colleagues were inspired to apply for this award due to their experience and understanding of the current limits of CS curricula. “All three of us had been thinking individually about the fact that the traditional CS curriculum really needed to be expanded to include the ethics and social responsibility of CS,” he explains. Seny is well-versed in privacy and surveillance while his colleagues, Suresh and Sorelle, have experience with algorithmic fairness which has informed their awareness of technology’s potential to harmfully impact society. These experiences made the decision to apply for the challenge simple. The process of writing their proposal was challenging yet enjoyable for all three since they had the opportunity to discuss various socially impactful topics.
Recognizing that CS students are first introduced to problem solving methods when evaluating their solutions and must keep in mind the resources they have available to them, Seny and his colleagues plan to use the funds from the prize to normalize curriculum design. This provides students with a strong technical foundation to actively integrate questions about how marginalized groups in different communities are impacted by technology. This curriculum aims to condition students to think about the inequality in global society as a natural part of their studies in CS. He believes the performance motivated culture of CS education results in a lack of ethical designs.
Seny and his colleagues plan to approach this issue by equipping students with a direct knowledge of the impact the tech field has on society. By integrating questions involving issues in society into the data structures and algorithms curriculum, students begin their CS education with a foundation that allows them to ethically problem solve. This sets the stage for a more ethically based CS community in education at Brown University.
"This award reinforces our own departmental initiative that aims to integrate in our curriculum material on the ethical and societal implications of computing.” says Ugur Cetintemel, Department Chair. “As a first step, ten ‘Ethics TAs’ will be working with a number of courses this fall to develop and deliver such content. We plan to grow the number of Ethics TAs and the number of supported courses in subsequent semesters. Congratulations to Seny and his collaborators!"
In light of the award, Seny says: “I’m excited to work on expanding and changing the algorithms and data structure curriculum. I teach CS 16 and although we’ve had material about algorithmic fairness in the course since 2018, we will now be able to add a lot more content and assignments related to responsible CS. I’m really happy that Mozilla and its partners put this together and I’m excited to get started.”
In a ceremony on June 15, 2019, the Association for Computing Machinery (ACM), the world’s largest educational and professional computing society, elevated Professor Michael Littman of Brown CS to the rank of Fellow, the organization's highest membership grade, for contributions to the design and analysis of sequential decision-making algorithms in artificial intelligence. The ACM Fellows Program, initiated in 1993, celebrates the exceptional contributions of leading members of the computing field, and Michael joins a distinguished list of colleagues to whom the ACM and its members look for guidance and leadership in computing and information technology.
"It was an honor to be selected as a Fellow," Michael says, "and I really enjoyed the ceremony and hearing about all the remarkable achievements of the big award winners. Computing is such an exciting field right now and getting to see so many important figures in the field in one place was a total trip."  
Michael earned his doctorate from Brown CS in 1996 and has been a member of the faculty since 2012. Currently co-directing Brown’s Humanity-Centered Robotics Initiative, he works mainly in reinforcement learning, but has done research in machine learning, game theory, computer networking, partially observable Markov decision process solving, computer solving of analogy problems, and other areas. He's earned multiple awards for teaching and research and has served on the editorial boards for The Journal of Machine Learning Research and The Journal of Artificial Intelligence Research. He served as General Chair of the International Conference on Machine Learning and Program Chair of the Association for the Advancement of Artificial Intelligence (AAAI) Conference in 2013. He's also an AAAI Fellow and Co-Chair of the upcoming Reinforcement Learning and Decision Making Conference, to be held in 2019 in Montreal.
In a ceremony on June 15, 2019, the Association for Computing Machinery (ACM), the world’s largest educational and professional computing society, presented Robert Sedgewick (William O. Baker '39 Professor at Princeton University) with the Karl V. Karlstrom Outstanding Educator Award for developing classic textbooks and online materials for the study of algorithms, analytic combinatorics, and introductory computer science that have educated generations of students worldwide. Robert is not only a Brown University alum (Bachelor of Science in 1968 and Master's of Science in 1970) but one of the initial seven Brown CS faculty members. He's the second Brown CS winner of the award, following Andy van Dam, whom he thanked during his acceptance speech.
Robert is best known for his series of algorithms textbooks, which have been bestsellers for four decades (12 books in four editions covering five programming languages). The books develop a scientific approach to the study of algorithms, based on experiments with real code to validate hypotheses about performance based on mathematical analysis. His recent book (with Kevin Wayne), Computer Science: An Interdisciplinary Approach, is a comprehensive introduction to the field and was named by ACM Computing Reviews as a “Best of Computing Notable Book” for 2017. His book Analytic Combinatorics (with Philippe Flajolet) is an advanced graduate text that has been recognized with the 2019 Leroy P. Steele Prize for Mathematical Exposition. More recently, Sedgewick has been extremely active as a pioneer and innovator in online education. He has co-developed extensive online content associated with his books that attract millions of visitors annually. Robert has also recorded over 100 hours of online lectures on programming, computer science, and algorithms that reach hundreds of thousands of people around the world. The Sedgewick-Wayne Algorithms online course has been listed as one of the top 10 Massive Open Online Courses (MOOCs) of all time.
The Karl V. Karlstrom Outstanding Educator Award is presented annually to an educator who has been appointed to a recognized educational baccalaureate institution, recognized for advancing new teaching methodologies or effecting new curriculum development or expansion in computer science and engineering, or who has made a significant contribution to the educational mission of the ACM. It's accompanied by a prize of ten thousand dollars, with financial support provided by Pearson Education.
Brown University's Department of Computer Science (Brown CS) is pleased to announce that Assistant Professor George Konidaris has become the inaugural holder of the John E. Savage Endowed Professorship in Computer Science. First announced by President Christina Paxson at John's fiftieth anniversary celebration, the professorship was funded with very generous gifts from John's family and friends, and is now the first of what will be ten new endowed professorships in the CS With Impact expansion. As part of this effort, Brown CS will raise a minimum of $40 million to create ten new endowed chairs, which will be supplemented with the creation of five lecturer positions, for a total of fifteen new faculty slots.
"John has been someone I've looked up to since I joined the department that he co-founded," said Konidaris. "He has an incredible record –over half a century!– of technical innovation, dedicated service to Brown, and real public impact. I feel immensely honored to be the first holder of the chair endowed in his name."
A member of the Brown CS faculty since 2016, George is the director of the Intelligent Robot Lab, which aims to build intelligent, autonomous, general-purpose robots that are generally capable in a wide variety of tasks and environments. His many  honors include, most recently, an NSF CAREER Award, a Salomon Award, a DARPA Director's Fellowship, and an AFOSR Young Investigator Research Award.
"I am very pleased that George Konidaris is the inaugural holder of a chair in my name," says John. "George has a bright career ahead of him."
George joins seven other holders of Brown CS endowed professorships: Eugene Charniak (University Professor of Computer Science), Maurice Herlihy (An Wang Professor of Computer Science), Sorin Istrail (Julie Nguyen Brown Professor of Computational and Mathematical Sciences and Professor of Computer Science), Roberto Tamassia (Plastech Professor of Computer Science), Stefanie Tellex (Joukowsky Family Assistant Professor of Computer Science), Eliezer Upfal (Rush Hawkins Professor of Computer Science), and Andries van Dam (Thomas J. Watson, Jr., University Professor of Technology and Education and Professor of Computer Science).
Now in his ninth decade, it remains to be seen whether Brown CS Professor Andries van Dam is slowing down. He's just become the inaugural recipient of the ACM SIGGRAPH Distinguished Educator Award, one of the organization's highest honors. 
Established less than a year ago, the award is given annually to a member for outstanding pedagogical contributions to computer graphics and interactive techniques at any educational level or within the context of any discipline. It recognizes contributions in both innovative content and delivery. Nominated by the SIGGRAPH community, Andy was chosen based on award criteria that include impact on research and practice of education as it relates to computer graphics and interactive techniques, cumulative contributions to the field both directly and through leadership of others, and innovation in education.
Last week, Brown CS celebrated a half-century of pioneering hypertext research by Andy and his collaborators, and the Distinguished Educator Award offers yet another reason to raise a glass to van Dam's innovation. He was also named to the SIGGRAPH Academy just last year. Andy will receive this latest award at either the North American SIGGRAPH conference or the SIGGRAPH Asia conference.
PhD candidate David Abel of Brown CS, who just recently proposed his thesis and expects to graduate with a PhD in Computer Science and a Master’s in Philosophy next spring, has been recognized for an accomplishment beyond his achievements in research. Chosen out of hundreds of graduate students with teaching appointments, Dave was one of only four to win the Presidential Award for Excellence in Teaching.
The award, given annually at the University Awards ceremony, recognizes outstanding pedagogical achievement. Its criteria span from teaching that influences and inspires students to learn to development of curriculum and resources that promote student learning.
Dave began his teaching journey in 2014 as a TA for Stefanie Tellex, teaching CS 1410 (an undergraduate Artificial Intelligence class). After being nominated as a “great TA” by the students in the class, he became a TA for CS 8 (A First Byte of Computer Science), an introductory computer science class for non-majors taught by Professor Michael Littman with enrollment of 109 students. During his semester of teaching the course, Dave was consistently praised by his students, with many citing his “energy, availability, and thoughtfulness” as being key to fostering an environment for intellectual curiosity. Dave was instrumental in implementing an optional python unit in the class that gave students the opportunity to learn a language used widely in industry. As a testament to his teaching abilities, a full 98.5% of respondents rated the class as effective or very effective when Michael took a sabbatical and Dave ran the class on his own.
Not limited to the classroom, Dave has been involved in a variety of activities that may very well have had an even greater impact on the Brown community. Along with fellow CS PhD students Nediyana Daskalova and Amariah Becker, Dave has been heavily involved in designing and running peer mentorship program in the department. His initiative pairs up post-candidacy PhD students with first year PhD students, ensuring that new students have proper guidance regarding finding research, working with their advisor, and establishing work-life balance. Keeping with the spirit of mentorship, Dave has been a primary research advisor for several Brown undergraduates as well. Over the past few years, he has co-authored 11 papers with many undergraduate students, guiding them through the research process.
Dave has clearly shown himself to be a remarkable teacher, both in and outside the classroom. As he finishes up his graduate studies, it's evident that his work has made a personal impact on the many dozens of students with whom he has worked.
Professor of Medical Science and Computer Science Carsten Eickhoff and Professors Daniel Ritchie, Stefanie Tellex, and James Tompkin of Brown CS have just received Seed Awards from Brown’s Office of the Vice President for Research (OVPR) to help them compete more successfully for large-scale, interdisciplinary, multi-investigator grants. They join numerous previous Brown CS recipients of OVPR Seed Awards, including (most recently) Ian Gonsher, Jeff Huang, and Stefanie Tellex.
Carsten Eickhoff
"Delirium is common after acute stroke," Carsten says, "and likely represents an impediment to recovery. However, the concrete manifestations of delirium comprise a spectrum, and it is unclear whether various patterns of symptoms may have differential effects on outcomes."
Many of these symptoms, he says, are intimately connected, including arousal, attention, and activity level, and as a result, delirium phenotypes have been traditionally labeled as hyperactive, hypoactive, and mixed. Unfortunately, patients with hypoactive delirium are known to be underdiagnosed using standard screening tools, and the presence of pre-existing neurological symptoms only magnifies this challenge.
In his research, Carsten proposes an innovative approach aimed at diagnosing and categorizing delirium using wearable sensors capable of measuring activity on a granular scale. Activity data will then be analyzed using machine learning techniques to identify delirium phenotypes corresponding to patient activity patterns. He hypothesizes that such patterns may also be predictive of early motor recovery after stroke, and proposes to apply similar machine learning techniques to identify activity-based phenotypes corresponding to post-stroke functional outcomes.
Daniel Ritchie
"People spend a large percentage of their lives indoors," Daniel explains, "in bedrooms, living rooms, offices, kitchens, etc. The demand for virtual versions of these spaces has never been higher, with virtual reality, augmented reality, online furniture retail, computer vision, and robotics applications all requiring high-fidelity virtual environments."
To be truly compelling, he says, a virtual interior space must support the same interactions as its real-world counterpart: VR users expect to interact with the scene around them, and interaction with the surrounding environment is crucial for training autonomous robots (e.g. opening doors and cabinets). Most object interactions are characterized by the way the object's parts move or articulate. Unfortunately, it's difficult to create interactive scenes at the scale demanded by the applications above because there do not exist enough articulated 3D object models. Large static object databases exist, but the few existing articulated shape databases are several orders of magnitude smaller.
Daniel intends to address this critical need by creating a large dataset of articulated 3D object models: that is, each model in the dataset has a type and a range of motion annotated for each of its movable parts. This dataset will be of the same order of magnitude as the largest existing static shape databases. Hr plans to accomplish the goal by aggregating 3D models from existing static shape databases and then annotating them with part articulations, conducting the annotation process at scale using crowdsourcing tools (such as Amazon Mechanical Turk) by developing an easy-to-use, web-based annotation interface.
James Tompkin and Stefanie Tellex
"Teleoperation is an important means of robot control," James and Stefanie say, "with virtual reality (VR) teleoperation being a promising avenue for immersive control or ‘first-person’ view control. VR teleoperation is also especially promising for teaching robots how to learn from demonstration, as it enables a human to control robotic arm end effectors via VR wands to demonstrate how to conduct a task."
However, for the sense of sight, they explain that there's often a large difference between the freedom of view movement afforded to the human operator by the VR headset tracking system and the freedom of view movement afforded by the camera system: the robot typically has one or two cameras mounted on slowly-moving articulated heads or limbs, whereas the human’s head-mounted VR display provides rotation, binocular stereo, and motion parallax with very fast head pose changes (six degree of freedom, or ‘6DoF’). In effect, the robot is a camera operator trying to mimic the movements of the human to always match the desired view onto the scene. Even if the robot is mobile and articulated, it can rarely ‘keep up’ with the human’s fast motion and show the correct view to the VR headset. This mismatch makes it very easy to induce VR sickness, which limits comfort and teleoperation duration.
Their research will attempt to use deep learning methods to generate photorealistic image synthesis methods. These will adapt the fixed camera views to the human's current VR view by understanding the scene geometry and plausibly filling in any view discrepancies. It also aims to be fast to finally keep up with the human and improve teleoperation quality and reduce sickness. This will extend Brown's ROS Reality open source package for Unity-based VR robotic teleoperation.
Every year, Brown University's Graduate School recognizes four students who are receiving doctoral degrees for superior achievements in research: one each in the humanities, life sciences, physical sciences, and social sciences. This year, one of the recipients of the Joukowsky Family Foundation Outstanding Dissertation Award is Evgenios Kornaropoulos of Brown CS, who successfully defended his thesis two weeks ago. He is the Department of Computer Science's second winner of this prestigious award, following Stefan Roth. The award and an honorarium will be given out at the Graduate School Commencement ceremony on Sunday, May 26.
"As the volume and complexity of generated data grow," Evgenios explains, "users would like to maintain the ability to issue expressive queries on their data without sacrificing privacy. Encrypted databases are one of the most promising approaches towards this direction. However, this efficiency comes with the price of leaking information about the plaintext data. In my thesis, we use an algorithmic approach to develop rigorous attacks on encrypted databases and secure protocols." 
Specifically, Evgenios's work addresses the limitation of standard leakage profiles in encrypted databases under widely-used expressive queries such as range queries and k-nearest neighbor queries. "In the works published from my thesis," he says, "we show that even though we have cryptographic proofs that guarantee that the interaction between a client and a server leaks nothing more than a well-defined piece of information, we are still discovering what an adversary can infer from the leaked information. Using a plethora of algorithmic tools from areas such as computational geometry, statistics, learning theory, probabilistic analysis, and optimization, we devise new attacks that recover the plaintext values of encrypted databases under minimal assumptions about the query and the data distribution. Hopefully, our findings will pave the way towards new efficient cryptographic designs that defend against our attacks." 
When asked about the experience of doing a PhD at Brown CS, Evgenios says, "I am thrilled to call Prof. Roberto Tamassia my academic father. His experience, rigor, and patience helped me sharpen my technical skills as well as my research taste. I also feel very fortunate to interact frequently with our professors that lead by example such as Prof. Vasileios Kemerlis and the rest of our Security Group. Finally, a big part of my thesis came out of my interaction with my academic sibling Prof. Charalampos Papamanthou from University of Maryland who is doing outstanding research and always sets the bar high. It is an honor to receive this prestigious award from our Graduate School and I am thankful for our professors and staff for making my graduate studies such a rewarding experience!" 
A full list of winners is available here.
The image above is © 2019 by Kirtley Righi and used with permission.
As the academic year drew to a close, Brown CS held its annual Undergraduate Computer Science Research Symposium on May 2, 2019, organized by Professors Jeff Huang and Stefanie Tellex and Meta-URAs Mary Dong, Marshall Lerner, and Alan Yu. Now in its fifth year, the event features presentations of student research with the goal of showcasing cutting-edge work and encouraging other undergraduates to undertake research projects of their own.
Out of more than two dozen participants, the winners were:
Overall #1: Atsunobu Kotani ("Writing Robot")
"Atsunobi's work with Stefanie is about writing robots. The idea is to show the image of already drawn handwriting characters to the robot, and it will immediately infer a drawing policy to reproduce the image, while preserving details such as stroke orders and continuity. He also had a second project about handwriting synthesis. Imagine that you are asked to write 'hello' in your handwriting. The model Atsunobi trained can predict how you would write other words such as 'world' in your handwriting style, without ever asking you to write the characters."
Overall #2: Alex Jang ("Phonologically Informed Low-Resource Speech Recognition for Foreign Accents")
"Traditionally, dealing with foreign accents in automatic speech recognition (ASR) requires training data from speakers with each accent, which can be expensive to collect, take up lots of memory to store, and require lots of time to process. Instead, by using phonology to inform a model, we can remove the need for foreign accented training data, using only data from native English speakers."
Overall #3: Rohin Bhushan, Nathaniel Weir, and Shekar Ramaswamy ("DBPal")
"The team presents DBPal, a novel data exploration tool with a natural language interface. DBPal uses a deep model to translate natural language statements to SQL, making the translation process more robust to paraphrasing and other linguistic variations."
People’s Choice: Alexander Chase ("Examining Lexical Differences In South America")
"With the rise of massive amounts of easily accessible data, linguistics is shifting from a social science to a data science. Alexander is intrigued by this concept, and this project is an exploration into how data science can be applied to visualize how language is being used. He aims to build upon past research done in Spanish linguistics to provide a unique perspective of the language variation in South America."
A total of $2,000 in prizes was generously donated by Mitsubishi and Amazon Robotics.
The image above is © 2019 by Joseph Spiegel and used with permission.
Professor Josh Tenenbaum of MIT visited Brown CS last month to deliver the thirty-eighth lecture ("Building Machines That Learn And Think Like People") in the Distinguished Lecture Series.
After an introduction by Brown CS Professor Daniel Ritchie, who hosted the lecture, Tenenbaum moved quickly to the central theme of his talk, saying that we have artificial intelligence technologies "but no artificial intelligence, no flexible, general-purpose common sense". The challenge for scientists, he said, is to "reverse-engineer how intelligence works" in the human brain, and despite all our advances, we have yet to create artificial intelligence that can match the model-building abilities of an eighteen-month-old child.
Inspired by human cognitive development, which he called "the only known scaling path to intelligence that actually works," Tenenbaum explained his belief that the goal of artificial intelligence isn't mere pattern recognition but to grow into sophisticated behaviors (understanding what we see, explaining it, imagining the unseen, making plans) through small steps. A large crowd that more than filled CIT 368 listened as he walked through some of the tools he's employed in the pursuit of machines that learn and think like people: a bi-directional loop of CS and engineering to "reverse-engineer the common-sense core," probabilistic programming to create an intuitive psychics engine, and work with psychologists to understand social interactions.
"The child is the ultimate coder," he said, "and learning is programming the game engine of your life....The goal of learning is to make your code more awesome." 
Afterward, we caught up with Professor Ellie Pavlick of Brown CS, who attended the talk, and she highlighted Josh's eagerness to tackle difficult problems as well as his unorthodox perspective. "I think the point that Josh hit on at the end –the question 'what do we build in versus what do we learn from scratch'– is possibly the biggest question right now in AI and cognitive science," she says. "Deep learning, taken as a whole, is often seen as representing the 'build nothing, learn everything' stance. Josh’s work exemplifies something closer to the other side, where learning happens on top of a foundation of existing structured knowledge about the world. I’ve never met anyone who doesn’t know and admire the work Josh is doing. It’s theoretically well motivated and the empirical results are impressive."
"I'm very honored to be receiving this award," says R. Iris Bahar, Professor of Engineering and Computer Science at Brown University, upon hearing that she's just won the Marie R. Pistilli Women in Electronic Design Award. "I'm passionate about helping to support and promote women in electronic design automation, and in computing more generally." 
Given annually since 2000 by the Design Automation Conference (DAC), the award, which is named for a former DAC organizer, recognizes individuals who have visibly helped to advance women in the field of electronic design. In the official press release, the conference's organizers cite contributions by Iris that are "as impactful as they are diverse" as well as her numerous papers and citations, and name her one of the pioneers in identifying power consumption as the premier constraint for computer architects and system designers.
“Iris has walked and bridged the boundary between electronic design and computer architecture and has become a leader in both fields," says Diana Marculescu, David Edward Schramm Professor and past Marie R. Pistilli Award winner. "Given Iris’ technical contributions to the field of EDA and her dedication to the goal of advancing the status of women in electronic design, I cannot think of a more deserving recipient."
For Iris, advancing women in computing has included high school outreach, innovative course development, dedicated efforts to include female students in her research, and organizing as well as speaking at career development workshops. And the work continues.
"When I attend conferences in my research area," she says, "I usually find women make up only around 10-15% of the total number of attendees. I would like to see this number grow significantly in the near term, which means continuing to help females, from high school to long-time professionals, feel welcomed, recognized, and valued in in the greater computing community (including both academia and industry). I'm excited to continue my efforts in the years to come."
Iris will receive the award during the 56th DAC, which will be held from June 2-6, 2019, in Las Vegas.
You can read the full DAC press release about Iris and her award here.
Doug Woos sees some of the rationale for his interest in academia originating with his passion for moral and political philosophy. "The best attitude for a philosophy class," he says, "is to read a text and buy into it for the entire duration, accepting that it's correct throughout. Then, when you're done, ask what the problems were. Taking that step of really buying in is so important: starting with a critique when reading a research paper or studying a new subject can be a huge mistake." 
Doug is joining Brown CS as lecturer next semester (he's one of the first hires in the landmark CS With Impact expansion), and his ability to immerse himself in a subject and suspend critique may stem from hours upon hours spent among the ever-new planets and cultures of science fiction. "I was obsessed with SF as a kid," says Doug, remembering a family basement filled with classics: Clarke, Heinlein, Asimov, Dick, and the eccentric but visionary Cordwainer Smith. "I don't know where I'd be without it."
His first exposure to computer science came early. The son of a programmer, Doug learned Python at age eleven or twelve and worked for his father in middle school, enjoying himself but not expecting to make it a career. As an undergraduate at Swarthmore College, he took computer science classes to avoid atrophy of his programming skills, but his initial plans were to major in physics.
But research with Professor Tia Newhall, Doug's undergraduate advisor, changed his mind. Initial projects with memory allocation in Linux clusters were followed by graduate work at University of Pennsylvania with formal verification and programming languages. "I was still interested in systems," he says, "but I wanted to apply PL tools and techniques."
Doug's interests are unabashedly broad: "CS is such a young field, and it wasn't that many years ago when systems wasn't a separate area from programming languages or artificial intelligence. We've divided ourselves semi-artificially, and I'm skeptical that all of the divisions are useful. I think it pays to be interested in a really broad swath of computer science."
And his recent research, he explains, was directly by inspired by his teaching. Already interested in formally verifying the safety properties of distributed systems ("Verdi: A Framework for Implementing and Verifying Distributed Systems"), he noticed that students in his Distributed Systems class were having a difficult time debugging their work. This led to his current project, Oddity, a graphical debugger for distributed systems.  
"It was a great example of what I love about academia," Doug says. "Seeing students thinking about interesting problems, really understanding the material, and asking questions that you can't answer."
In just a few months, he'll be standing up in front of those students, and Doug says he's excited by courses like Tim Nelson's CSCI 1950-Y Logic for Systems and the new sequence of Brown CS introductory courses: "I'd love to help develop the new sequence, and to teach broadly about programming languages, systems, OSes, or compilers."
Or possibly formal methods or verification, he says, explaining that he envisions them growing in prominence in the days ahead. What does he see in the crystal ball? "From my perspective as someone who likes applying PL techniques to systems," he laughs, "more of that!" 
Giving the examples of Amazon Web Services and Oracle, Doug says that he sees widespread application of at least lightweight formal methods techniques to distributed systems. "These systems are so big and so complex," he says, "that you find bugs at very surprising levels. We need to understand these systems not just at the level of a diagram on a whiteboard: we need an FM understanding. As we use certain languages more and more, we can only avoid some really serious security vulnerabilities by having language-level guarantees of correctness."
As we wind down our conversation with Doug, we move through science fiction and philosophy to arrive at another of his great loves, coffee. ("I'm drinking it during this conversation," he notes.) A bit spoiled by the world-class scene in Seattle, he notes that Providence has some "pretty tasty" beans of its own that he's ready to put to use.
And he's eager to get started at Brown CS: "I really can't wait. Brown just jumped out at me because of the impressive commitment to undergraduate education. Everyone, even people who are focused on research, are passionately involved in what the undergraduate curriculum is like. I had a huge number of amazing conversations with faculty and students when I was in Providence, and I'm looking forward to more."
Brown CS has just announced that it will recognize 27 graduating seniors for their achievements at Commencement in May. Michael Ball, Laura Blackstone, Isaiah Brand, Leslie Bresnahan, Jonathan Chemburkar, Joshua Chipman, Loudon Cohen, Grant Fong, Montana Fowler, Alex Fratila, Purvi Goel, Mae Heitmann, Elaine Jiang, Atsunobu Kotani, Zach Kirschenbaum, Benjamin Murphy, Shivam Nadimpalli, Nina Polshakova, Silei Ren, Lucas Rosenblatt, Josh Roy, Sumit Sohani, Chantal Toupin, Chinenye Uduji, Nathan Umbanhowar, Nathaniel Weir, and Alan Yu will each receive the Senior Prize in Computer Science for their academic work as well as their service to Brown CS. It's the largest number of Senior Prizes ever awarded in one year, reflecting both the increased size of the department and the many contributions of these individuals.
"These are our top students," says Associate Professor (Research) and Vice Chair Tom Doeppner. "As a group, they've done superior work, but we were also very impressed with how generously they were able to give back to the department and their fellow students." 
Montana Fowler will be starting a PhD program at UC Santa Cruz in the Computational Media department. "I discovered my passion for teaching in the Undergraduate TA program at Brown and my passion for research in Daniel Ritchie’s lab," she says. "It’s thanks to Brown’s encouragement for undergraduates to be involved in the department that I discovered my dream to become a professor."
"Brown Computer Science has been everything I could have asked for in an undergraduate education," says Benjamin Murphy. "I’ve gotten to take challenging classes in topics from theory to systems, and have gained a great deal of experience in industry. I’m honored to have won the prize, and to have had the opportunity to give back towards the department even a small portion of what I’ve gained – the department uniquely values undergraduates, and the teaching I’ve received from my peers contributed an incalculable amount to my education. Brown has also given me the confidence to explore outside of computer science, and consequently I will be heading to Google for a couple years after graduation, before matriculating at Harvard Law School to study privacy law."
Isaiah Brand tells us that he's always felt that teaching is instrumental to learning, and Brown CS has consistently enabled him to tutor and collaborate with other students. “I feel incredibly lucky to have been able to design and help teach a robotics course as an undergrad," he says. "Where could an undergrad design a course that has every student build a drone?”
"Robotics is no longer primarily a hardware problem, but largely one of software," says Professor George Konidaris of Brown CS. "We already have highly physically capable robots, but our ability to program them is still quite limited. Ideally, we’d like to be able to give the robot a what goal (e.g. the light should be switched on), and have the robot figure the how part of the problem out itself. But we’re a long way from that, and a major reason why is that we don’t know how to get away from the plethora of tiny low-level details a robot has to deal with –every pixel, and every motor signal– to the kinds of high-level planning at which humans excel."
Konidaris’s recent work has been focused on the theoretical foundations of the problem of abstraction, and last year he published a paper that introduced a new theoretical framework for helping robots learn to reason abstractly. “That work felt like a great step,” says George, “but it wasn’t yet really practical, because the robot must learn every aspect of every new task from scratch.”
Earlier this month, George won a National Science Foundation (NSF) CAREER Award that will help him tackle that challenge by applying his new theoretical framework to the problem of designing robots that view the world in an object-centric way: they will learn to manipulate objects, build abstract internal representations of those objects, and reuse these representations to generalize across similar objects. His aim is to build the foundation for a new family of algorithms that will enable robots to reason and plan in complex scenarios in the real world by getting the most mileage out of the learning they’ve already done.
CAREER Awards are given in support of outstanding junior faculty teacher-scholars who excel at research, education, and integration of the two within the context of an organizational mission, and George joins multiple previous Brown CS winners of the award, including (most recently) Theophilus Benson, Stefanie Tellex, Jeff Huang, and Rodrigo Fonseca.
On March 21, team Brown Secure '19, composed of Brown CS students Nicole Cheng, Manuel Gorotiza, Willem Speckmann, and Angela Zhuo, and coached by Professor John Savage, competed in the seventh annual Atlantic Council Cyber 9/12 Strategy Challenge in Washington, DC. The event featured a record-setting 47 teams from institutions across the country and was designed to give students a taste of the challenges that face White House policy makers when responding to national cybersecurity threats. This is John's sixth year of participating in the competition, and his previous teams were winners of first place in 2015 and a prize for Best Teamwork in 2014.
The competition began weeks in advance, when the students prepared a decision document, operating as if they were members of the National Security Council staff. This year's cybersecurity threat involved both software and hardware vulnerabilities at the United States Census Bureau, and the team proposed multiple policy recommendations, including the creation of a task force and the declaration of a national emergency in order to make additional funds available and require intelligence services to assess the extent of foreign interference.
On the morning of March 21, teams had ten minutes to present their ideas orally, took questions for ten minutes, recessed, and then returned for feedback. Judges included a wide selection of individuals from both the public and private sector. During lunch, students attended presentations and career panels led by experts from the Cybersecurity and Infrastructure Security Agency, the Fletcher School of Law and Diplomacy at Tufts University, the Scowcroft Center for Strategy and Security, the US Department of Homeland Security, and other organizations.
That evening, the teams were hosted by the Baker & McKenzie global law firm at their offices on Connecticut Avenue, overlooking Lafayette Park and the White House. Speakers included David Lashway (Co-chair of the Global Cybersecurity Practice and Partner at Baker & McKenzie), Barry Pavel (Senior Vice President, Arnold Kanter Chair, and Director of the Scowcroft Center for Strategy and Security at the Atlantic Council), and Christopher Krebs (Director of the Cybersecurity and Infrastructure Security Agency at the US Department of Homeland Security.  
This year, Brown's team wasn't among the qualifiers for the next round, so they spent their second day touring the city, including a visit to the Air and Space Museum. Nevertheless, John is very proud of their hard work.
"The competition was fierce," he says, "and I was really pleased with our team's solutions. One of the judges questioned whether a national emergency was necessary, but given the unprecedented nature of the threat, I don't think it's an unreasonable solution. The students were very well prepared, they presented well, and I'm glad they had the opportunity to meet some true experts and hear about cutting-edge developments in the field." 
From left to right: John Savage, Manuel Gorotiza, Angela Zhuo, Nicole Cheng, and Willem Speckmann


From left to right: Manuel Gorotiza, Willem Speckmann, Angela Zhuo, John Savage, and Nicole Cheng
“There are a lot of aspects of language that are really complex and humans are really good at picking up on those nuances, and machines are really not,” says Brown CS Professor Ellie Pavlick. “I love working on [something] so fundamentally human and so complex.”
In a recent essay, ("Nothing Ventured, Nothing Gained") undergraduate Liyaan Maskati interviews Ellie about the complexities and nuances of Pavlick's entry into computer science, diverse interests, and even the relationship between music and math. It's a wide-ranging piece that she organizes thematically, opening each section with the literal and figurative definitions of various international proverbs, and it won her First Place in the Undergraduate category of the Association for Women In Mathematics (AWM) Student Essay Contest.
To increase awareness of women’s ongoing contributions to the mathematical sciences, AWM and Math for America co-sponsor an essay contest for biographies of contemporary women mathematicians and statisticians in academic, industrial, and government careers. This year, they received 307 entries across the three categories (Grades 6-8, Grades 9-12, and Undergraduate) from 46 states, the District of Columbia, and three other countries.
In the current day and age, self-driving cars are perhaps one of the most promising technologies on the horizon. With the potential to dramatically improve traffic planning, fuel efficiency, and productivity, it’s easy to understand the excitement surrounding this innovation. One of the largest challenges facing the field, however, is safety. Brown CS alums Jacob Beck ‘18 and Zoe Papakipos ‘18 have eagerly taken up this challenge. Currently working at Microsoft Research and Facebook AI Research respectively, the duo’s research on the training of autonomous driving software has recently been recognized in New Scientist, one of the leading publications recognizing those at the cutting edge of technological progress.
What exactly does the research entail? “Autonomous vehicle research is about making computers drive cars,” remarks Jake, “and it’s important because computers have a fast reaction time, infinite attention, and are consistent. The focus of our work is to do this in a scalable way by teaching the computers from real world examples.” Currently, one common way to teach autonomous vehicles to drive is to train the AI off hours of footage of human driving that is deemed to depict perfect behavior. The problem with this approach, however, is that it restricts the software to only one side of the spectrum. It learns all about good driving, but it doesn’t get exposure to poor driving and the dangerous states it brings about. The car would have no idea what to do in a “bad state”, hindering its response in the real world.  
“One alternative to this approach is a hybrid version of reinforcement learning – having a human driver try actions, and telling the computer how good or bad each action was,” says Zoe. The team recorded a human driving well, but also showed examples of swerving and erratic driving, and a backseat driver labelled the driving as positive or negative. The software then used this information to drive the car itself, and performed over 1.5 times better than the behavioral cloning method with perfect driving behavior. The research clearly showed a marked improvement over prior approaches, but this new approach also came with its own set of challenges.
“It was really difficult giving good feedback to the car,” explains Jake, “it’s really hard to know the exact angle for the steering wheel without turning the car yourself.” Eventually, the pair overcame this problem by having a human labeller turn the wheel to show approximately how much the car should change its action. For example, if the car should be turning a little bit more to the left, the human labeller turns the wheel slightly to the left. If the car is turning sharply left off the road, the labeller turns the wheel considerably further to the right.
Having made such an impact so early on their careers, what do they think made it possible for them to achieve so much so quickly? “It all really began in Professor Michael Littman’s autonomous driving lab at Brown,” explains Zoe, “and we were really interested in the applications of machine learning.” “It’s pretty amazing how willing to help professors are at Brown,” adds Jake, “and my work in the lab really prepared me well.” Although the pair have made much progress in the field, there is clearly much work to be done. “It feels really great, being recognized for a project that we were both so interested in,” laughs Zoe, “but we definitely need to put more work on it to be conference ready. The core ideas are there.” There’s no doubt that the duo’s research may prove to be vitally important to this exciting field, and the possibilities are endless.  
"Even today," says Diba Mirza, faculty in Computer Science at the University of California, Santa Barbara (UCSB), "there is a prevailing assumption that only graduate students are qualified to work as Teaching Assistants in undergraduate courses. Brown CS proves that assumption wrong, and that was very interesting to us."
Diba joined UCSB in 2017, and one of her first tasks was to develop their Computer Science Undergraduate Tutor (UT) Program. Previously, she explains, only graduate students served as teaching assistants, and her department was looking to create a model that allowed undergraduates to take part as well. After a pilot with six UTs in 2017, the program doubled in size the following semester, and now includes more than thirty tutors. Hoping for more growth, and inspired by the Brown CS UTA program (Professor Andy van Dam of Brown CS is a member of the UCSB Department of Computer Science's Advisory Board), Diba came to Providence last summer to learn more, meeting with students, UTAs, Andy, and other faculty members.
"Andy started the UTA program at Brown CS in the 1960s, and amazingly, he's still part of it," Diba says. "That's so rare! It's a huge learning opportunity, and I really wanted to benefit from all those years of experience."
UCSB has more than 450 undergraduate students majoring on Computer Science, she explains, with another 200 or more in Computer Engineering. Much like at Brown, interest in CS courses is increasing exponentially, with strong demand even from students who don't intend to major in the subject, but one key difference is that certain introductory courses have an attendance cap, restricting their enrollment.
"We want to grow our program, Diba says, "but it's a complicated, campus-level discussion about what undergraduates can and can't do. We had a lot of faculty and student support, so I came to Brown to see what might be involved to scale our program and what mechanisms would allow it to scale successfully."
One of the key points that Diba took away from the visit was the use of different roles within the program: UTAs, Head TAs, and Meta TAs. "All of them were so deeply engaged," she says. "I asked them how they run labs, how they train other TAs, and I could see the care and attention. They had been very carefully selected and they were really able to take charge without constant supervision, with a real sense of ownership. For instance, I was very interested to find out how involved the HTAs are with interviewing. Even if we don't emulate this exact model at UCSB, it's definitely going to inform what we do."
Diba was also impressed by the week of intense training that helps prepare UTAs just before the semester begins. UCSB's tutors are required to take a "Teaching Computer Science" course but she believes there's a lot to be gained from having at least some of the content delivered as Brown CS does, from students to students. Diba says that she also appreciates the close attention from faculty: "I was fortunate to hear Andy's signature address to his UTAs, in which he not only sets expectations but also explores themes of professionalism and empathy in a way that freshmen can relate to. Other faculty I spoke to support the program wholeheartedly, but I like how they keep a critical eye, making sure that students don't overwork themselves, and maintain the right balance of ownership for the course."    
According to Diba, one of the greatest similarities between UCSB and Brown CS is student enthusiasm for helping their peers: "Our students are very excited about making an impact and improving the experience of students that follow after them." And the focus, she maintains, is much larger than a single course or even a single degree.
"It's about mentorship with lasting impact," she says. "First, you have a terrific educator like Andy mentoring the HTAs, and that continues from the HTAs to the UTAs, and then to the students. It's true professional development: for example, if they go on to grad school, they've learned to do the kind of meticulous, error-free work that matters when working with a faculty member. We're really trying to help our students succeed not just in the short term but over many years." 
At this point, Diba says, UCSB is close to being able to expand the use of Undergraduate Tutors across a series of first-year courses taught by multiple instructors. "We're looking to scale," she says, "and Brown CS has been a great inspiration for how to run a very beneficial program at a very large scale.  Ultimately, we hope that any of our courses that would like to include UTs should be able to include them. That's our goal."
“The 90’s were a really interesting time to be a computer science undergraduate,” laughs Brown CS alum Michael Horn ‘97, as he remembers his earlier years at the University. “The web was just beginning to come into its own and the entire field was progressing at lightning pace, it certainly was an extraordinarily exciting time.” The field has advanced far beyond those early days of computing, but Michael remains at the forefront of cutting-edge research in the subject. Currently a professor in the Learning Sciences and Computer Science Department at Northwestern University, with a primary focus on Human Computer Interaction, his work has continued to make an impact on generations of students.
When Michael first began his career, his ambitions were as far from academia  as one can imagine. Like so many other CS students, he took a software engineering job in San Francisco doing business, task management, and software on mobile devices. But Michael soon realized that this line of work wasn’t for him. “I got sick of it pretty quickly,” he remarks, “and I knew I really loved learning.” This realization led Michael to Classroom Connect, which was an online curriculum company selling learning products to states and school districts. But after 5 years, Michael knew that this line of work still wasn’t for him. “I really felt that it was too tech driven, we were selling these learning products but I had no idea how they were actually being used by students. There was just a lack of incentive to explore the bigger questions that were difficult to make profitable.” With such an approach, Michael knew that graduate school would be his best route moving forward.
“I started looking into grad schools, specifically working in human computer interaction, but it was difficult finding a grad school that would take me,” remembers Michael. Fortunately, Michael was able to earn admission into the PhD program at Tufts, working in Human Computer Interaction, precisely what his aims had been. With work ranging from the development of exhibits at the Boston Museum of Science and involvement with the Child Development Project, Michael finally realized where his passions lay. Academia was truly the best avenue to continue making an impact on the field, while satisfying a lifelong love of learning.
Driven by a passion for learning, Michael made the decision to stay in the field of academia following his graduate years. This decision, however, did not come without its own set of challenges. He was offered a newly created position at Northwestern, with a joint appointment in the the departments of Computer Science and the Learning Sciences. “I didn’t have a formal background,” he explains, “and this was really a brand new field.” But Michael quickly found his home in this unique intersection, as he dived into the field of learning sciences. “I gradually began understanding the theoretical foundation of how people learn,” he remarks, “and this was really important for HCI. We were kind of inventing this field, which is really difficult to work on.” Beginning with work on tangible programming languages in kindergarten classrooms and science museums, Michael’s projects have expanded into a diverse range of fields with a focus on the thoughtful uses of emerging technologies in diverse learning settings.
With such a diverse and impactful career, what would Michael say best prepared him for his eventual path? “It was definitely the undergraduate TA program,” he says. “There really is just a culture of undergrads having the freedom to run the show. It was amazing to see people care so much about learning and education.” This love of learning certainly remains with Michael to this day, as he continues working on advancing this field.
"The Bootstrap curriculum stands out from the numerous computer science and computational thinking curricula that have cropped up in recent years," says Thea Charles, Head of Knowledge and Impact at Siegel Family Endowment (SFE), which is making a gift to support the K-12 CS curriculum directed by Kathi Fisler, Shriram Krishnamurthi, and Emmanuel Schanzer of Brown CS.
The project is a natural fit for SFE, which supports people who do work at the intersection of learning, workforce, and infrastructure, and champions organizations whose work contributes to making a world in which all people have the tools, skills, and context necessary to engage meaningfully in a rapidly changing society. Bootstrap, now reaching about 25,000 students each year (approximately forty-three percent are girls and forty-six percent are minorities), will be using SFE's support for general operation and to grow capacity. It's a promising start to the year for the organization, which recently joined a national coalition of educational groups to create comprehensive K-12 CS pathways and was praised as being part of a "wave of the future" by Internet pioneer Vint Cerf.
"The members of the Bootstrap team are leaders in the field," says Charles, "and their expertise and commitment have helped so many teachers incorporate computational thinking to their curricula in a way that is both creative and backed by research. We're eager to keep learning from them."
In the days since we went to press with the story below, Brown CS alums Dave Simons ’90 and Daniel Wilk ’92 have won yet another award for their work on Adobe Character Animator. The National Academy of Television Arts and Sciences has recognized the tool as a Pioneering System for Live Performance-Based Animation Using Facial Recognition. Developed by Dave’s team, Character Animator allows artists to animate characters in real time via live tracking and has changed the landscape of character animation. “We are honored to work on software that allows artists to tell their stories in new ways,” says Dave when asked about his motivations for the project. The tool was used for the first-ever live episode of The Simpsons, live cartoon interviews on The Late Show with Stephen Colbert, and the quick-turnaround production of Our Cartoon President, and will undoubtedly continue to make waves in the animation industry.
“We knew the odds were against us,” laughs Brown CS alum Dave Simons ’90, “but the idealism of four recent college graduates trumped the 90% failure rate we were warned about.” Nearly 30 years later, it seems this idealism may not have been misplaced, as Dave, along with fellow Brown CS alums Daniel Wilk ’92 and Michael Natkin ’89, have just won an Academy Award in scientific and technical achievement for their work on Adobe After Effects. Awarded to those who provide extraordinary contributions to the science of filmmaking and a proven record of contributing significant value to the process of making motion pictures, the honor truly recognized the critical role that After Effects has come to play in the motion graphics industry.
What motivated Dave to begin his work on this pioneering project? Well, it all started in the graphics group of Brown CS Professor Andy van Dam many years ago, at a time when undergraduate contributions to research were far less common. (Michael also mentioned both Andy and his group in his acceptance speech, thanking van Dam for "giving me a chance" and a first introduction to graphics.) “When I first began my work in this field, all the tools were command-line tools,” explains Dave, “and I actually did my senior thesis on distribution ray-tracing.” Friendships at Brown naturally led to the birth of The Company of Science & Art, otherwise known as CoSA for short. Founded by Dave and three other Brown graduates in June of 1990 –Greg Deocampo ’88, David Foster (DaveF) ’90, and David Herbstman (DaveH) ’90– the company planned to become the next world-class content provider for the new electronic age. “After searching all around Providence, DaveF found a great place near downtown,” he remembers. “DaveH negotiated the rent down to $1000 a month, and we were in business.”
With such lofty expectations, it was no surprise that the fledgling company inevitably faced a myriad of setbacks in its early years. The original plan was to have artists and programmers working side by side to produce multimedia content, and CD-ROM production was the first task. Named Connections: The CoSA Journal, this first hypermedia publication was designed to show off the new medium, but garnered little interest. This was followed by PACo (PICS Animation Compiler), which allowed platform-independent low-bandwidth streaming animation playback with synched sound. What initially seemed like a promising idea, however, quickly changed as Apple announced QuickTime a mere few months later. Running low on funds, the team knew that they needed to come up with something fast. And it was from this that Egg (the first codename for After Effects) was born. This is where Dave’s graphics-group training would really start to pay off.
With Egg development in full swing, it was at this point that Dan Wilk ’92 joined the team, helping to write effect plug-ins. The first press demos of the brand-new software were held in a private suite at MacWorld Boston. After receiving positive reviews from the public, the team quickly realized that the project needed a real name – After Effects.
“Showing After Effects 1.0 to the public for the first time was an exhilarating experience,” remembers Dave, “We had a tiny booth and people were packed ten-deep at times trying to get a glimpse.” The software exploded in popularity, and CoSA was bought out a mere six months later by Aldus Corporation, followed by another merger of Aldus into Adobe Systems a year later. This set into motion the chain of events that led to After Effects becoming the dominant motion graphics and visual effects application used in the post-production process of film and television production.
“It’s easily the most common tool now to do motion graphics in the film industry,” explains Dave, “and it really lets you create anything you want in this field.” Brown CS has certainly left its mark on the industry, as over 15 Brown graduates worked at CoSA or were involved in the After Effects project. Used for most of the iconic Pixar movies’ opening and closing credits, After Effects has quickly become an indispensable tool in the artist’s toolkit.
With such an accomplished career, what does Dave believe prepared him best from his years at Brown? Well, it may very well have been his experiences TAing a myriad of classes in Brown CS. “I was a head TA for Andy van Dam in CS 11 and CS 192,” he remembers, “and Andy’s high bar and criticalness made me learn a lot.” Dave has certainly come a long way from the old, refurbished apartment in which the project first began, and his work has undoubtedly made it possible for countless artists to enable their dreams.
The robots return! Rhode Island Robot Block Party will take place on Saturday, April 13, 2019, from 12-4 PM at the WaterFire Arts Center (475 Valley Street).
The event, an expo founded by the Rhode Island Students of the Future in partnership with Brown University's Humanity Centered Robotics Initiative and Brown CS, highlights the innovation of our state's robotic community. Bringing together industry, universities, community organizations, and K-12 schools, it's open to the public and includes numerous pieces of robotic equipment that range from ocean exploration devices to animatronic toys. It was a Rhode Island Monthly Editor's Choice "Best of" Award winner in 2015.
Recent exhibitors included:
University Exhibits and Demonstrations
Manufacturing and Community Organizations
Student Exhibits
The Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, given this year to Evan Cater to support his work with Brown CS Professor Michael Littman, recognizes strong achievement from young students and offers them the opportunity to partner with faculty and advance work that began in the  undergraduate research program.
A generous gift from Peter Norvig '78 (a Director of Research at Google and a thought leader in the areas of artificial intelligence, natural language processing, information retrieval, and software engineering) established the award, which provides $10,000 annually to support an undergraduate engaged in an intensive faculty-student summer research partnership. The gift honors the life and work of Randy F. Pausch '82, a renowned expert in computer science, human-computer interaction, and design who died of complications from pancreatic cancer in 2008. "His story is inspiring," Peter says, "and this is an opportunity to remember him."
Evan explains that he began collaborating with Michael as a first-year student, working alongside a Master's student on Variational Autoencoders with Deep Q-Networks, a type of reinforcement learning (RL) algorithm. "We couldn't get the idea working," he says, "and the project fizzled out, but it piqued my interest in both RL and Variational Inference. I've revisited these ideas in conjunction over the past couple of years, and this proposal and research project will hopefully synthesize my thoughts on the subject."
And what might that look like? First, some background.
"Any time we have robotics or computer programs interacting with the real world," Evan says, "there exists a lot of uncertainty. When deploying a program that, for example, modulates the power grid, or a self-driving bus that carts children to school, a lot is at stake. Storms can impede driving, hard drives can fail, car accidents happen, and programs can seg-fault. When faced with uncertainty, we want to make sure our Artificial Intelligence can make decisions. The agents interact with the world over time, so we can formalize this problem as learning good policies (or actions) over time, under uncertainty."
To construct agents that operate under uncertainty, he says, it's necessary to categorize uncertainties. For example, epistemic uncertainty is created by a lack of data, whereas aleatoric uncertainty is the result of random changes in the environment.
"My work," Evan says, "is a research study on a unified view of uncertainty in Deep Reinforcement Learning. First, we'll provide a taxonomy and vocabulary to describe the various uncertainties used in reinforcement learning. Next, we'll compare and contrast different contemporary papers, clearly distinguishing the types of uncertainty each paper considers and how the papers interact. The study will also provide some theoretical comparisons between the types of uncertainty. Finally, we'll use the study to inform the design of a new set of algorithms based on the insights ascertained. My hypothesis is that techniques from Variational Inference can be used as a unifying tool for the competing techniques."
With the summer only months away, Evan tells us that he can't wait to explore, read, and tinker: "I'm ecstatic and really thankful to all the students and professors that got me here. I remember being a first-year student sitting in on Michael's lab meetings and catching every tenth word. Flustered, a couple of seniors and PhD students took me under their wings and pointed me in the right direction. I owe everything to those student mentors and Michael for nurturing me as a researcher. Michael has been helpful and understanding, supportive during the rough weeks, and he sparks creativity in all of his students. I often look forward to our exploratory debates and deep dives into 'what is really going on'." 
Evan's eagerness and excitement is exactly what Peter Norvig is looking for. He sees this award as a multiplier that will amplify the value of his gift and extend it through time. "In the past," he says, "we had to build all our own tools, and we didn't have time to combine computer science with other fields. Now, there are so many opportunities to do so. I think it's a wise choice: you invest in things that you think will do good, and educating a student allows them to help add to the things that you're already trying to accomplish." 
Integral to the success of Brown CS are the dedicated and hard-working staff members who show tireless commitment to improving the CS community. Recently, this effort was recognized when Communications Outreach Specialist Jesse Polhemus was awarded the 2018 Brown Excellence Award in Efficiency. This award recognizes the accomplishments of Brown staff members, and Jesse is one of the thirty-five honored this year. Nominated for his unwavering dedication to the department, Jesse was specifically highlighted for his work with alums for the UTA fundraising campaign.
Jesse, now in his fifth year at the University, fulfills a vital function for the team. In his current position, Jesse shares information and tells the stories of what makes Brown CS special, whether it’s innovative research, teaching excellence, or the tight-knit community of faculty and students.
What makes him a great fit for the job? Well, it may very well be the unique perspective and diverse set of skills that Jesse brings to the table. “I see myself as a craftsperson (my heroes are the Roycrofters, a community of makers from last century’s Arts and Crafts movement),” explains Jesse, “as someone who has worked hard to develop of set of skills and is trusted to use them with my mind and heart to produce meaningful work.”
What attracted Jesse to Brown in particular? “One of the things I like about working for Brown is that if you take five minutes to get to know someone, they have another interest, or even expertise, in something that isn’t immediately obvious,” he says. “I got a Master’s degree in Old English riddles and enigmata because I’m interested in how other people see the world, and I wanted to prove to myself that I had a solid understanding of something I care about.”
There may not be very many Old English specialists in the ranks of computer science communication departments, but Jesse has clearly proved that his unique perspective provides an invaluable asset to our community. Ranging from his work on Conduit (the annual Brown CS magazine), to our first-ever Faculty Handbook, to his work with alums for the UTA endowment campaign, Jesse’s accomplishments have left his mark on the program.
‘I’m still smiling about it,” laughs Jesse when asked about his recent win. “I’ve worked at private companies where awards were given out in response to low morale, and this is the exact opposite – I’m thrilled that the quality of my work is important to Brown.” With such a dedicated team, it’s no surprise that this award comes on the heels of numerous other honors for Brown CS staff. Most recently, Laura Dobler (Financial and Outreach Coordinator) was named an Office of Institutional Diversity and Inclusion Fellow and Lauren Clarke (Academic and IP Programs Manager) was awarded the Brown Excellence Award.
Since the earliest days of Brown, our students have been eager to combine the pursuit of societal change with rigorous academic study, then use what they’ve learned to benefit humankind at large. In today’s world, where computation has become ubiquitous, this obligation takes on new importance. Users of social media, owners of Internet-enabled appliances, and other inhabitants of the Big Data world are increasingly being confronted with the notion of responsibility for how they use technology or allow it to be used on their behalf.
Here at Brown CS, this responsibility requires acknowledging that computer science can’t be solely a technical issue, and that its use must go beyond profit-making to address the societal issues of the day. One recent example is our participation in the Swearer Center’s Engaged Scholars Program, which in the context of CS challenges students to consider the ethical implications and societal impact of emerging technologies. Another is our Industry Partners Program, which now offers complimentary memberships to nonprofits and organizations working in the social good sector.
Last year saw yet another example when a group of CS undergraduates formed a group called CS for Social Change with the realization that computation gave them the perfect tools to serve a greater good. Their interests found a common cause with Professor and Department Chair Ugur Çetintemel, and they worked with him over a period of months to create a new a course that debuted last semester: CSCI 1951-I CS for Social Change.
This pioneering course now takes its place in a group of classes that includes DATA 0080 Data, Ethics and Society, a course offered by Brown’s Data Science Initiative; CSCI 1870 Cybersecurity Ethics; and several CS courses that include modules on responsible use of CS. Together, we can think of them as a growing emphasis within the Brown CS curriculum that demonstrates computer science’s potential to bring about greater social awareness and societal good. We’ll examine each course in more depth below and devote a series of future articles to the other programs above.
CSCI 1951-I CS for Social Change
“Even before many of us came to Brown,” says Head TA Nikita Ramoji, “we were thinking about the societal impact of CS. Software is very naturalized in the Western world, and it has the potential for reaching all levels of society equally.” Together with student Elaine Jiang, she started coming up with ideas for material that would unite computer science and social good soon after arriving in Providence.
Initially, Nikita and Elaine had thought that the content might work well as an extracurricular activity. But after talking to Brown seniors, the two students found that many of them wanted to be involved with social change but hadn’t found a clear avenue to pursue it. Many students who were anticipating careers in industry explained that if they had learned more about the societal impact of computer science, they might have chosen a different path.
And so they decided to approach Ugur, offering their assistance with material that he’d been working on for some time. The result was a course with student input at every level, from the syllabus to the assignments. “We didn’t know how things would work out,” Nikita says, “but Brown students are passionate about causes, and they want to use their privilege to make a positive impact. That’s really rare. We were so happy to find a cohort of students who agree that a fundamental part of CS education should be learning about the responsible use of products that will affect so many people.”
The course focused on ongoing project collaborations (essentially, remote internships) with three nonprofits: UTA Elaine Jiang’s group worked with Baker Ripley, building a web portal that aims to improve communications between the elderly and people who have been recently placed in the stressful, emotional role of caregivers, Head TA Nikita Ramoji’s team worked with Uplift to build a Chrome extension to autofilter content and prevent online harassment, and UTA Yuta Arai led a group of students who worked with YGA, a Turkish non-profit organization, to build a web portal accessible by a mobile app. The app interfaces with kits containing electronic hardware to help Syrian refugee children learn STEM concepts.
The class alternated between hands-on work and an ongoing dialogue in which students asked each other questions and examined issues surrounding taking responsibility for their creations. “We wanted the course to be intersectional,” explains Nikita, “because we wanted everyone to reflect on how applying what they’ve learned makes it real and concrete. We had incredibly talented students in the class, and they had a real interest in social impact.”
One of the most powerful moments, Nikita says, was looking at the midpoint presentations from each team. “It was amazing to see three separate products that will genuinely help people in the real world, outside of the classroom. As computer scientists, when we don’t remind ourselves about the rationale for what we’re doing, it’s very easy to forget the impact of our creations.”
Elaine, who will be one of the HTAs this spring, says, “This year, one of our primary goals is to ensure that all projects completed during the course will be immediately useful to the organizations we’re collaborating with.” Heila Precel, who took the course last spring and will also serve as an HTA this spring, adds, “We’re focusing on increasing the course size and constructing readings and discussions that have greater practical relevance to students.”
CSCI 1870 Cybersecurity Ethics
“Confluence” is the word that Adjunct Professor Deborah Hurley uses to describe the multiple factors that led her to create this class, but first among them was an “overwhelming student interest” in how society is being impacted by security and ethical issues. “Internet-enabled systems are ubiquitous,” she says, “even embedded in human beings. We’re training the future creators of these systems, and we need them to be built in service of humanity.”
Debuting this past summer, the course used a number of different formats, from lectures to discussions to student presentations, with carefully-selected readings as accompaniment throughout. Students ranged from first-years through seniors, plus some high school students, and to Deborah’s delight (“Interdisciplinarity is one of my religions!”), they came with varied interests: computer science, materials engineering, applied math, and more. There were multiple students who hadn’t chosen a concentration yet and others from RISD. “Having this variety lets us cross-educate one another,” Deborah says, “and we need an interdisciplinary approach to solve societal problems. This was a cohesive group of students that mixed really well.”
One of them was Alec Fujii, an international student from Tokyo who’s concentrating in engineering. “We had people from all over the world,” he says, “with many different backgrounds. It was really great to see what their perspectives were, and we need that because these problems are so complicated. For me, the main idea of the course is that as technology continues to evolve, how do we cope? How do we keep our rights and values as human beings when bioanalytics and surveillance try to turn us into numbers or make it harder to keep our identities to ourselves?”
And surveillance and bioanalytics are just two of the topics covered in the course. Deborah explains that most students come to Brown with very little ethical or philosophical training, so the first part of the class is devoted to examining the dominant ethical frameworks of this century and the last: utilitarianism, Kantianism, Aristotelian value ethics. With that under their belt, students move through a wide selection of issues at the intersection of technology and ethics, including privacy, access to government information, hate speech, fake news, encryption, and more.
To move the focus away from lecturing, student presentations make up a considerable portion of the course, which is structured around a series of milestones and Deborah providing feedback after each. “The last thing I want is for students to just be inhaling and regurgitating information,” she says. “My big emphasis is making sure that students learn, that they have one of those aha moments of real insight. I want their projects to be memorable for them, and I was really impressed with the quality and depth that I saw.”
According to Alec, the format of the class had a huge impact. “Most of my engineering classes are lecture-based and this was really my first discussion-based class at Brown. The issues aren’t as black-and-white, so we had to analyze how we look at a problem. And there are always multiple solutions, so we had to think about which would benefit or affect the most people.”
Deborah will continue the class in the summer as well as during the academic year. “Brown really emphasizes the liberal arts as our crown jewel,” she says, “and students from across the university need this material as they choose concentrations. It’s eye-opening, it’s full of things they’d never even thought or heard about. I feel very strongly about ensuring that the next generation both lives happily and benefits society, and this is an important set of issues. It’s vital that students have exposure to them before they leave. Something’s missing in their education if they don’t have that.”
DATA 0080 Data, Ethics and Society
Early in the semester, Professor Roger Blumberg asked his class to write an answer to the following question: “Imagine your typical day at Brown, from the time you wake up and the time you go to sleep; and then imagine you were at Brown thirty years ago. What do you think would be the three most significant differences in your experience as a student at Brown?” What’s really striking, he says, is that if you posed this question in 1987, looking back at to 1957, very few answers would have to do with communication. In 2018, they all do: nearly all the students mention their use of phones and laptops.
Designed as an undergraduate complement to a component of the Data Science Initiative’s Master’s program, Roger describes Data 80 as a liberal arts seminar: students read some of the great work that’s been done on technology and society, write responses to prompts, discuss and debate issues in class.
“I like to start with something concrete,” Roger says, “because I want students to become critical of not just theory but everyday practices. Introducing humanities texts in computer science is challenging – getting people to suspend their disbelief and consider technology in different ways. Sometimes students confuse being critical with being cynical, but important texts help make the distinction clear.”
“I’d never taken any classes in formalized philosophy or morality,” says Erin Bugbee, one of Roger’s students. (She’s pursuing a double concentration in Statistics and Behavioral Decision Sciences.) “Personally, I really enjoyed reading texts in a completely new area. It’s not just about having more material, it’s about having a new perspective, and I wondered why more students in the hard sciences aren’t interested in these issues. I definitely feel like they’d benefit from learning about them.”
Roger notes that some of the older students, whose perspective has already been shaped by time spent in industry, had some interesting observations that differed from those of their younger peers. “It may seem like a harsh word to use these days, but the conflict of ideas is essential for the liberal arts. Is a smartphone a wonderful portal to a wider world or a vortex from which energy and attention will never emerge? The notion of conflict may be difficult for us, but if we minimize it, we eliminate the thresholds for understanding that are so important in education. My goal for the class is for everyone to develop or improve on a free relationship to technology, to take control of technology and not be unwittingly dominated by it.”
Ideas in conflict necessarily produce some discomfort, but Erin didn’t find that to be a problem. “I like how the course exposed you to both the good and the ugly,” she says. “I think our general consensus was that data science isn’t inherently dangerous, but it can be if certain things aren’t accounted for. Since I took the class, I look at projects I’m approaching in a different way and really looking at their possible effects on society. When I was a first-year student, I wanted to be a data scientist, and I still do, but I see it differently.”
That change of perspective from someone who intends to use data in their professional life is exactly what Roger is hoping for. “This class looks at how professionals think about ethics,” he says, “and in the case of data science professionals, the consequences of their actions can be significant. Yes, the possibility of using computer science for social good is very real, but big questions need to be talked about between fields. Fortunately, we have wonderful students. The open curriculum encourages them to work across disciplines, and they do. Thirty years ago, I don’t think computer science departments had students of this caliber – they would’ve gone into medicine or law. Now they gravitate to CS.”
For Ugur, there’s a real thrill in being able to work with those students, and to see an interest in societal good shared by Brown, our faculty, and the student population. “I’m delighted to see us emphasize the use of technology to benefit society,” he says. “Our students are concerned about societal change, and so are we. In my course, and in all of these courses, we’re creating models for how to teach applied computer science and how to use CS to have tangible, meaningful results that can have a profound impact on people’s lives. We are planning to expand our offerings in this area and integrate these topics deeper and more comprehensively in our curriculum.”
As we look back on 40 years of Brown CS, successfully funding our UTA Endowment has been one of our most important but also our most gratifying recent milestones. UTAships are tremendous gifts, but in many cases, they’re also historical documents. Each of them tells a story, and some are funny (“Hotel CIT UTAship in honor of Tom Doeppner”), but others are much more. This is the story of a computer science pioneer whose tragic death inspired her classmate, alum Stephen H. Beck, to create a lasting memorial: the UTAship for Underrepresented Minority Women in memory of Marie Moses ‘82.
For privacy reasons, Brown CS records contain only limited demographic information about our students, but Marie Moses is believed to be one of our first two black female graduates. (Donna L. Woodall graduated in the same year.)
Born in Hollis, Queens, she was the eldest of three children in what her sister, Valarie, describes as an incredibly close-knit family: “When our friends would come over, sometimes they’d just sort of watch us, enjoying our family’s jokes and stories. We knew we were different and even unusual.”
“We also knew we were special because of how important our father was,” Valarie remembers. He had been James Brown’s manager in the 1960s, later was the first black partner in the entire country for Cooper and Lybrand (now Price Waterhouse), and eventually became CEO of Omnicare. “That belief drew us closer together as a family. It was us against the world.”
That closeness was clearly needed. Marie excelled at math and skipped the fifth grade, but a series of relocations exposed the family to prejudice and abuse. After a move to Westport, Connecticut, they were one of only two black families in the entire town, and Marie commuted each day to a high school in Stamford. “Marie had a tough time there,” Valarie says. “Many of the white students stayed away from her at best, and the black students taunted her and threw rocks at her for being ‘too white’ and ‘acting smart’.” Trying to protect her daughter, Marie’s mother had her learn needlepoint as a way to keep her home on weekends.
“In true Marie fashion,” says Valarie, “she not only learned needlepoint, she became adept at it.” Hard work and the ability to make hard work seem effortless had already become one of Marie’s trademarks: “She was organized, driven, and excelled at all she did, but she was also a lot of fun, incredibly loyal. Even as a child she had the characteristics that were on on full display as an adult.”
After graduating from high school at age sixteen, Valarie says that Marie was delighted to be admitted to her first-choice school, gaining a new social life and being surrounded by people who saw intelligence as something to be proud of. She’d been interested in computers before college, but this was years before the era of widespread home computing, so her first in-depth exposure to them was at Brown.
Our Father’s Daughter
Did Marie realize that she was going to become one of the first two black women with a Brown CS degree?
“It’s hard to say,” Valarie answers. “Marie didn’t talk about that, but she knew she was special and was very aware of what she’d been given in life – she volunteered for years in local nursing homes. We may have known how unusual it was for an African-American woman to major in CS at Brown, but our parents had both gone to CCNY and her success seemed like a natural continuation of our father’s legacy. She was our father’s daughter: an overachiever, very analytical, and she could command any room she walked into.”
What was Brown like then? Let’s turn to Steve Beck, who explains that he graduated at the top of his high school class but then suffered the all-too-common shock of arriving on campus and no longer being the smartest person in the room. “It was terrifying but wonderful,” he says. “I loved the Department’s rigor and discipline, but also the humor. People were expected to stand up and deliver.” Disco still ruled the airwaves, and Steve surprised himself by diving into the dance music scene and becoming a DJ. He can still be found mixing songs, only this time on his computer. “It was a passion, and it’s one I still have.”
Steve and Marie met through shared classes: “We saw each other in the lab all the time and eventually became friends. I really enjoyed our interactions – Marie had a great sense of humor, but she was even-keeled. Some of us were really wild, but she was more mature. She was unflappable.”
One of the interactions with Marie that Steve remembers most clearly was a challenging but revelatory moment. At the time, they were co-UTAs in what was then CS 11. During a group discussion with a mix of students and other UTAs on a block letters project, Steve impulsively (and, he admits in hindsight, immaturely) made a spontaneous decision on everyone’s behalf. Marie instantly called him out, mincing no words as she asked whether he was aware of the challenges facing her and other women of color on a college campus in that era.
“It was an awakening for me,” he says. “I saw a whole world that I didn’t know about, and it made a real impact.” Steve may modestly disagree, but it seems to say something about a person’s character when an encounter that could have led to resentment and anger becomes a learning moment and a catalyst for change.
Helping People
Ocie James Irons ‘81 met Marie at Brown through a shared calculus class. An only child and, he says, independent to a fault, he’d transferred to a private school in the tenth grade because he wanted to attend an Ivy League school. He describes himself as curious: “I’m not necessarily academic, but I enjoy learning. This class was more conceptual, and I wasn’t doing well. Marie was kicking ass.”
So he asked for help, walking across campus to her dorm room for tutoring with his calculus book and notes. And also, tucked under his arm, something that suggests a more than academic interest: a backgammon board. (The two later married and had a son together.)
Ocie remembers being astounded by the records and books that Marie showed him: Jonathan Livingston Seagull, Neil Diamond. “We were both black, and they were all things that I would have associated with white people. They were things I hadn’t read, or heard, and it was interesting to me, it impressed me.” Despite her formidable intelligence, there was more to Marie than studying. She was gifted at dance, Ocie says, and she had dreams of becoming a professional dancer in the same way that some of her male peers hoped for careers as athletes.
In addition to helping Ocie with calculus, Marie was looking out for other people as well, becoming a resident assistant and very much a resource for her fellow students: “She was open, helpful, very attractive, inquisitive, extremely friendly, but in a way that wasn’t off-putting. She expressed her opinions forcefully, but not aggressively, and she didn’t dominate the dialogue. She listened.”
And she listened with a real excitement, Valarie says, to groundbreaking instructors like Robert “Brother Ah” Northern, a jazz musician who had students repeat the phrase “Egypt is in Africa” aloud in class. Marie was always meticulous, marking her stack of punch cards with a diagonal line across the side (this allowed for easy reordering if they were dropped), but the boldness of statements like Northern’s appealed to her. “She was just so excited by her classes and the new ideas they exposed her to,” Valarie says. “She loved how eccentric some of her professors were. She loved all of it.”
Making A Big Impact
Marie and Ocie eventually divorced, but their son, Gregory, remained her world. “She took him everywhere,” Valarie remembers, “so he could have the broadest background possible. To me, the fact that she was such a dedicated mother is just as important as her intelligence and drive. She restructured her world for her kids.” As a single mother, Marie sometimes had to have someone watch her children, and Valarie still has the notebooks full of handwritten instructions that she kept for babysitters: “Marie wanted their every need taken care of. She wrote out in meticulous detail what she wanted the sitters to do when she wasn’t there so her kids would grow and learn.”
After graduating, Marie’s career rapidly took shape. One of her important early jobs was in programming. She was working for Genesis, a company developing something called Lifecard, a revolutionary product that would allow someone to carry around their entire medical history on a wallet-sized piece of plastic. “I was living with Marie that summer,” Valarie says, “and I was impressed that she could work long hours and still mop the kitchen floor before bedtime. On weekends, she would volunteer with Big Sisters or at a nursing home. All of this, and she still had the social life that she’d coveted for so long.”
Later, she returned to academia, becoming an adjunct professor at Walsh College and Wayne State University. But even as she traveled from city to city, earning a Master’s degree as a working single mother, her belief in giving back continued. After a move to Detroit, she started teaching high school, passionate about inspiring kids who wouldn’t otherwise be interested in math.
“From a very young age,” Ocie says, “Marie was making a big impact as an African- American woman, and she never stopped. The need to volunteer really stands out in her life.” He remembers a trip to Japan where Marie stood out physically with her height and skin color, but in other ways as well. “It was her personality and her acumen. She had a kind of edge, but not just for the sake of being noticed. She was daring enough to express who she was in the things she did and who she did them with.”
Valarie remembers that Marie never lost contact with her mentee from the Big Sisters program, even years later: “I have to say, I looked at her in amazement. ‘Marie, when do you sleep?’ And she was always so present. She always kept very late nights so the papers could be graded and the housework completed. She had that overall dedication while still being present for her family.”
The Best Kind Of Leader
In late December of 2002, something unimaginable occurred: Marie was killed by her second husband ten days after he’d been released from a Michigan emergency room. She’d brought him there after he began making threats and showing signs of mental illness, but a doctor ignored another physician’s recommendation that he be transferred to a psychiatric unit, and he was discharged. Immediately, there was an enormous hole left behind. Somehow, everything Marie had done (the volunteer work, getting her degrees, the dancing, the travel, being a family member and having a family of her own, her Lifecard innovations, the kids she’d inspired to study math) had been accomplished in just 41 years.
“He took out someone at the top of her game,” Valarie says, “and her death at Christmastime was especially cruel. The holidays were important for her because they were a time for family and friends to get together. Marie was the reason I came home for the holidays from the Peace Corps. She was the glue that held our family together as we grew older.”
“It was a huge shock,” Ocie remembers. “We don’t always think of domestic violence as something that affects everyone, but Marie’s murder calls to our attention that it can impact people regardless of their socioeconomic level.”
Finding out that Marie was gone was particularly difficult for Steve because they hadn’t been in contact for quite some time: “After school, we didn’t really stay in touch, and I deeply regret that. One of our classmates, Patty Lutsky, told me about her death, and she rallied the troops to donate to charity in Marie’s name, but I wanted to do more.”
Another person that Steve had considered honoring was Randy F. Pausch, the Brown CS alum and renowned expert in computer science, human-computer interaction, and design who died from pancreatic cancer in 2008. He and Randy had been sophomore roommates, and Steve had already set up a fundraising site for the Pancreatic Cancer Action Network (PanCAN) that ended up being quite successful. He thought about charitable giving to the UTA program in Pausch’s name, but other people had beaten him to it, so he turned his attention elsewhere. Marie’s UTAship was the result: he founded it, became the primary donor, and convinced others to give as well. (A full list of donors is at the end of this article.)
“I really wanted to honor her memory as a pioneer,” he says. “She led the way with grace, dignity, raw competence, intelligence, wit – all the things Brown CS is known for, but with humility. She was the best kind of leader.”
A Statement About Who Marie Was
“When I found out about the UTAship,” Valarie says, “I felt it was perfect. A gift like that – Marie was so generous with her knowledge and time, so I thought immediately that she would’ve loved the idea.” Ocie agrees: “I also feel a lot of joy and pride to know that Gregory will benefit from being reminded who his mother was, how she was thought of both on and off campus. That’s huge, personally, and I really appreciate that recognition.”
“And seeing people giving to her UTAship is telling,” he adds. “It lets them make a statement about who Marie was for them. If she could hear me and others talking about her today, she’d be very humble. Even in divorce, we remained friends. To this day I appreciate our relationship, all of it.”
Ocie is also glad to see how a UTAship in her honor demonstrates the importance of her alma mater to Marie: “She loved Brown, and the relationships she made there were so important. I can’t count or begin to imagine how many times I heard her say Andy van Dam’s name. And now she’s giving something to future Brown students.”
Through a gift that will renew itself each year for a different young woman, Marie’s name will always be associated with one of her lifelong priorities, the idea of giving back.
“I love that link,” Steve says. “All you have to do is follow the news cycle of the day, and for all the strides we’ve made, we keep suffering setbacks. People still look at each other differently. We have a moral imperative to be inclusive in our field, and the goal isn’t to reach a number. It should be a given that we bring in people unlike ourselves in order to change the world. I love the idea that a UTAship for underrepresented minority women marries the idea of diversity with Marie’s pioneering spirit.”
In addition to Stephen H. Beck, other donors to the UTAship for Underrepresented Minority Women in memory of Marie Moses '82 include Isaac Berkowitz; Karen Smith Catlin ‘85, P ‘18, P ‘20 and Timothy J. O. Catlin; Eugene Charniak; Justin Cohen; Maria Francesca Fernandez ‘85 ScM ‘89; Sarah Michelle Filman; Lisa Gelobter; Gregory Irons; Ocie James Irons ‘81; Patricia E. Lutsky ‘82; Norm Meyrowitz ‘81; Valarie Moses; Carol Rosenstock; Stephanie Singer; Annie Steele; and Neha Zope.
Do you remember when Donald Knuth gave the keynote at our inaugural symposium? Or when the CIT was built? What about when Tom smashed a laptop with a samurai sword or Andy walked into the Sunlab in a Darth Vader costume?
Faster than a rubber chicken flies, our fortieth anniversary has come upon us, and one of the ways we’re celebrating is by taking a major step to preserve the many artifacts (large and small, rigorous and silly) of Brown CS history. Our solution is a grassroots effort powered by homegrown technology: the Brown CS Digital Archive (BCSDA), an extension of the Brown Digital Repository. Put simply, it allows anyone on the planet to submit a piece of Brown CS history for permanent preservation online, accessible to all.
Launched this week, the BCSDA accepts more than a dozen different formats: photographs and other graphic files, audio, video, PDFs, and even code. (We’re guessing that more than a few of you have a favorite software project sitting around on a floppy disk somewhere.) All submissions will go through a virus check and be verified for accuracy, and authors/creators will retain all copyright.
We’ve populated the BCSDA with some initial assets, and we’ll be adding a lot more, but we don’t have a staff historian, so we’re depending on you. We’re looking for items with real historical interest, so please don’t send us your lecture notes, but if you take a look at the archive, you’ll see the wide variety of items that we’re looking for. After verification from an appropriate Brown CS staff or faculty member, your asset will enter the BCSDA and be available in perpetuity to anyone with an Internet connection. (Please be patient if we get a lot of submissions all at once after this article appears.)
As we’ve all seen, digital media can deteriorate much faster than expected, so we hope you’ll share your pieces of history as soon as you can. As participation increases, each new asset will add color and detail to a growing picture of what makes us the community that we are. “One of the things that truly sets Brown CS apart is our people,” says Ugur Çetintemel, Brown CS Professor and Department Chair. “We treasure these artifacts of our history because we understand and we continue the spirit of imagination, teamwork, discovery, and commitment that they embody. This archive helps us remember who we are, and it’s our gift to many future generations of computer scientists.”
Sending us something is an easy process:
The Computing Research Association (CRA) is a coalition of more than 200 organizations with the mission of enhancing innovation by joining with industry, government and academia to strengthen research and advanced education in computing. Every year, they recognize North American students who show phenomenal research potential with their Outstanding Undergraduate Researcher Award, and in 2019, Brown CS made one of the strongest showings in the Honorable Mentions category. Out of fifty-two students who received Honorable Mentions, three of them are Brown CS students: Grant Fong, Silei Ren, and Nathaniel Weir. Last year, Brown CS students received three Honorable Mentions as well.
Grant explains that his work with Professor Jeff Huang is focused on the Sochiatrist project. "The overarching goal is to predict mental health issues from naturally-occurring social media data," he says. "I have been recently researching how messaging data can be used to predict mood in an ethical way. My work here is split mainly into two areas. The first is to develop robust ways to extract data from multiple platforms that can be difficult for an end user to access and provide these tools to clinical professionals so they can better understand how people communicate. The second aspect is the analysis of this data, which combines natural language processing, time series analysis, and social network analysis to deliver powerful insights."
"My primary research area," says Silei, "is security. Currently, I'm working with Professor Roberto Tamassia on designing a poisoning attack against learned index structures. Learned Index Structures are database indexing structures that uses machine learning models for faster indexing. In the attack, we showed that an adversary can slow down learned index structures by inserting a small amount of crafted data. Another project I worked on is a leakage attack against encrypted databases. Specifically, we answered the question of what an adversary can infer from an encrypted two-dimensional database based solely on the responses of k-nearest neighbors queries. Besides security, I also worked with Professor Theophilus A. Benson on software defined network (SDN) fuzz testing. We designed an algorithm that introduces systematic failures into networks for resiliency testing."
Nathaniel's work with Professor and Department Chair Ugur Çetintemel and Professor Ellie Pavlick is mostly focused on natural language processing and machine learning. "My main project," he says, "is on building an interface for users to explore databases using natural language text queries. I’ve specifically focused on how to get deep learning models to understand language in arbitrary contextual domains. Our aim is to have an interface that can be plugged into any database containing information from any context – for example, data about hospital patients or about airport flights. However, a model trained to understand language related to a hospital really struggles to understand language about the airport flights since it hasn't seen much of the language patterns in the latter domain. Our challenge is figuring out how to bootstrap this cross-domain generalization process without needing to spend the time collecting lots of training examples of queries in the new domain."
You can see the full list of Outstanding Undergraduate Researcher winners here. Congratulations, Grant, Nathaniel, and Silei!
The Forbes 2018 Inaugural Top 50 Women In Technology identifies three generations of forward-thinking technologists leading more than a dozen tech sectors across the globe. Honoring the top tech “Moguls, Founders, Engineers, Innovators and Warriors” in the world, this list recognizes those who are truly at the cutting edge of advancement within the STEM fields. This year, Forbes has included Brown CS alum danah boyd, highlighting her work on the intersection of technology and society.
danah is currently working as a Principal Researcher at Microsoft Research, is the founder and president of the Data & Society Research Institute, and serves as a Visiting Professor at New York University. She also serves on the boards of Crisis Text Line and Social Science Research Council, and is a Trustee of the National Museum of the American Indian. Currently, danah’s research focuses on how data-driven technologies can amplify inequities, mistrust, and hate; she is also widely recognized as an authority in the field of social media research. In her book It’s Complicated: the social lives of networked teens, danah explores and uncovers some of the major myths regarding teens’ use of social media. She has made an enormous impact through her work on this subject, and this recognition follows a long list of accomplishments including the Award for Public Sociology from the American Sociological Association, the naming by Fortune as the “smartest academic in the technology field” and the naming as a “Top 100 Global Thinker” by Foreign Policy.
The Hyundai Visionary Challenge is a competition to ignite learning, exploration, and development in the realm of smart mobility. It’s meant to accelerate research innovations in smart mobility that drive the creation of sustainable cities across the globe, and awards teams across a broad spectrum of categories. Encompassing biology-inspired mobility, digital phenotyping, and man-machine partnership, these categories all aim to inspire new ways of thinking to impact the world. This year, Brown CS PhD students Nakul Gopalan, Eric Rosen, and David Whitney along with Brown CLPS PhD student Daniel Ullman (whose team was advised by Stefanie Tellex) have won the annual award for their proposal, “Improving Man-Machine Partnership using Mixed Reality Social Feedback from Robots.”
“I really enjoyed being able to talk about and explore the applications of our work in the business world,” explains David as he describes the group’s project, “and I saw the potential for VR to be truly transformative for society.”
In essence, the team’s goal was to increase the efficiency of communication between robots and human workers in the workplace. As it stands, robots are very unsafe to human coworkers. Humans can’t predict the future actions of automated robots, which vastly reduces robot effectiveness on the job. The idea to solve this issue is to have workers in warehouses wear augmented reality headsets to see the future actions of moving robots in real time, minimizing collisions and significantly improving workplace output. This tool would foster interaction between humans and machines, making for a much safer work environment. It may sound futuristic, but the team has set high goals for this project, and plans to continue working and refining this man-machine partnership idea in the upcoming years.
“We live in an era of fake news,” explains Brown CS  alum Meredith Ringel Morris ‘01, “and there’s this huge idea of trustworthiness when searching for information, and my research focuses on this process of search.” Currently working as a researcher at Microsoft, Merrie works on a broad spectrum of projects including human-computer interaction, computer-supported cooperative work, social computing, and accessibility. Investigating with fellow researcher Eric Horvitz, the pair’s work on SearchTogether (an interface for collaborative web search) has recently won the ACM UIST  (User Interface Software and Technology) Lasting Impact award, recognizing the significance of their work in this field.
Merrie’s aptitude for discovery began very early on, during her undergraduate years at Brown. She completed an independent study under the guidance of Brown CS Professors Steve Reiss and David Laidlaw on visualizing the runtime of Java programs to help people find bugs. “It wasn’t anything major,” she laughs, “but it really sparked my interest in research.” This passion grew, when she found herself at Stanford working in the lab of Professor Terry Winograd on interactive workspaces.
Her work on interactive workspaces continued after graduating from Brown as Merrie completed her PhD program at Stanford and focused on gesture interaction techniques – supporting groups of people working on displays. Merrie began working at Microsoft Research in 2006, helping the company’s continuous search for innovation. But her work quickly deviated from that of years past. “When I first joined, Eric had expected that I would do more work on gestures,” Merrie remembers, “but I really started to think about search and how inefficient it is.” This led Merrie to SearchTogether, a project that has redefined the way that people collaborate and has a range of applications.
Previously, searching and querying had been a primarily isolated and individual activity. SearchTogether changed that by focusing on the specific scenario of remote collaboration when stakeholders are not physically present. Essentially by supporting awareness of search processes when others are separated, it allows for groups to query and search through information simultaneously, dramatically reducing undesired redundancies and improving work efficiency. Featuring new ideas such as search result persistence and “split search” (allowing one person to issue a query and multiple people to triage unique subsets of the search results), the system dramatically improves the division of labor in the workspace with countless applications.
What makes this work so important? Well, not only does it provide a competitive edge for Microsoft’s own search services, it really taps into the potential of collaborative information seeking and sharing. This potential is highlighted by the UIST Lasting Impact Award, which honors a UIST paper from at least ten years earlier with long-lasting impact on the UIST community and beyond. “Microsoft research tends to focus on a portfolio of long-term research, and sometimes they come up with ideas so far out of the box that they seem so futuristic at the time,” Eric explains, “and the Lasting Impact Award is one of the best affirmations you can get for your research, since it means that a segment of researchers believes the topic is meaningful, and it serves as an affirmation of a crazy idea.”
Looking back at her four years at Brown, what was most valuable? For Merrie, it was truly the abundance of opportunities for undergrads. “Being a TA in Brown’s undergrad TA program was amazing,” she says , “and it taught me to be an effective public speaker, helped my communications skills, and was just a great experience.”
In the photo above, Merrie is being given her award by Brown CS PhD Alum Steven K. Feiner, Professor of Computer Science at Columbia University. 
Adjunct Professor Donald L. Stanford of Brown CS is making a major transition in a career that's been marked by profound impact in the technology sector, education, and service to his community. Two weeks ago, he retired from his role as Chief Innovation Officer of IGT (formerly GTECH), which he joined in 1979 as its seventh employee.
In an open letter, Marco Sala, CEO of IGT, praised Don's crucial contributions to research, development, sales, marketing, and project management activities, noting that during Stanford's first 22 years with GTECH, the company grew from a market share of five percent and sales of less than $1 million to a dominant worldwide market share of 70 percent, and sales in excess of $1 billion by the end of 2002. That was also the year in which Don began teaching for Brown CS and Brown's School of Engineering, and he now lectures for the Executive Master's program at the School of Professional Studies as well.
"Don has had an enormous impact on our Company," Sala says, "as a founding leader of the technology group, and later as an innovation mentor. He has worked tirelessly over four decades to ensure our lottery products and solutions have continuously eclipsed our competition while helping our customers grow their businesses."
Don's career has also been marked by considerable service to the community. He received the Black Engineer of the Year Award for Professional Achievement and the Honorable Thurgood Marshall Award for community service in 1999. Three years later, he received Brown University Graduate School's Distinguished Alumni Award. He also serves on the board of three Rhode Island organizations: the R.I. Business Innovation Factory, Year Up Providence, and Spectra Systems.
“With 2019 marking my 40th year with IGT," Don says, "I felt it was fitting to close this part of my career on this anniversary, and at a time when the organization is experiencing its greatest successes to date. I’m looking forward to enjoying more time with my family, pursuing my hobbies, and continuing to teach at Brown, where I plan to keep working with our university partners to identify new talent.”
You can watch Don reflect on his career at IGT and share his future plans in this short video.
The Department of Computer Science at Brown University seeks applicants for a Grants and Financial Specialist who will independently manage all matters pertaining to grants administration, both financial and administrative, and will have financial oversight and administrative responsibility for the sponsored projects in Computer Science and for the Bootstrap Program. This position provides high-level comprehensive pre- and post- award management, subcontract administration, and financial analysis. This position is the administrative liaison between the Division/Department, Office of Sponsored Projects (OSP), collaborating institutions, sponsoring agencies and principal investigators (PIs). This position also offers grant, financial and administrative management support for the Bootstrap Program in the Department of Computer Science. The Bootstrap Program is responsible for the management of grants and gifts for conduct of national program that develops curricula, trains teachers, and conducts research on computer science education for K-12 teachers and students. The incumbent will have a broad scope of responsibilities and must be knowledgeable of research administration, including financial management.
To apply for this position, please go here to submit the relevant materials.
Brown University is committed to fostering a diverse and inclusive academic global community; as an EEO/AA employer, Brown considers applicants for employment without regard to, and does not discriminate on the basis of, gender, race, protected veteran status, disability, or any other legally protected status.
"The CS community has talked about Exascale computing for a decade," says Brown CS Master's alum Prabhat, who recently received the ACM Gordon Bell Prize with colleagues from Lawrence Berkeley National Laboratory and NVIDIA, "and I'm thrilled that we were among the first ones to get there with a project that addresses one of the most important problems for all of us and our children."
Strongly contested on an international scale, the Gordon Bell Prize is awarded each year to recognize outstanding achievement in high-performance computing. It represents the highest honor for supercomputing experts, second perhaps only to the Turing Award. Prabhat and his team earned the prize by training a deep neural network to identify extreme weather patterns from high-resolution climate simulations, showing that accurate segmentation masks can be extracted for patterns such as tropical cyclones and atmospheric rivers.
"In the popular media, climate change is typically characterized by highly simplified quantities such as global, annual mean temperature or global sea level rise. Increasingly, the general public is interested in the impact of climate change and how climate change will affect them where they live,” Prabhat explains. “In order to address such questions, climate scientists need to develop capabilities for precision analytics: can we examine a 100-year climate simulation and automatically extract pixel-level segmentation masks corresponding to Hurricanes, Nor’easters and Atmospheric Rivers? It turns out that AI (and in particular Deep Learning) are up to the task, but there is a major performance bottleneck to overcome first. Our world-class team, comprising of researchers from NERSC, NVIDIA, and OLCF was able to adapt state-of-the-art segmentation architectures to work on million-pixel, 16-channel climate datasets, obtaining ~40TF performance on Volta GPUs, and 1.13 EF (16-bit) performance on the OLCF Summit system."
Prabhat began his career at Brown by earning his Master's degree in 2001, then spent more than a half-decade working at the Center for Computation and Visualization. "IMHO, there's no better school than Brown for breadth, depth, and exposure to other domain sciences," he says. "I was attracted to Brown by pioneers and strong mentors like Andy van Dam, who was always enthusiastic about learning himself and teaching others. David Laidlaw opened my eyes to the amazing, diverse possibilities of scientific visualization. I had the unique privilege of working with Thomas Banchoff and David on visualizing 4D mathematical objects in the CAVE for my Masters thesis. Upon graduation, Sam Fulcomer presented me with the  opportunity to work on diverse scientific projects at the CAVE, and CCV staff gave me room to grow and explore my interests. Working with Jim Head and Andy Forsberg on interactive visualization of large scale Martian topography and imagery data was probably the highlight of my time at CCV; I’d probably credit Jim the most for inspiring me to pursue a PhD in Earth Sciences at UC Berkeley."
Upon transitioning to Lawrence Berkeley National Lab, Prabhat's research interests shifted towards scientific visualization of large data sets, data management and supercomputing. He is currently the Data and Analytics Services Group Lead for the National Energy Research Scientific Computing Center (NERSC), where his group is responsible for deploying the Big Data stack for over 7000 science users.
“Our team is very proud of breaking the exaflop barrier and winning the Gordon Bell Prize. We’ve charted a new course for extreme scaling of Deep Learning architectures on HPC platforms. More than just a performance number, a research paper and an award, I’m personally most excited about the long-term possibilities for climate science. We now have a precision analytics tool that can help in assessing the impact of climate change for society at large. That is what really counts." Prabhat concludes.
You can also read the official ACM announcement, Lawrence Berkeley National Laboratory press release, and an NVIDIA blog post.
"The world is changing rapidly," says Brown University's John Savage, An Wang Professor Emeritus of Computer Science, "and to address our society's grand challenges, we need to engage in outreach."
"Take the example," he says, "of national security in the early 1950s: the US was challenged by the Soviet Union. President Eisenhower, wanting to formulate a policy toward this aggressive nuclear power, convened foreign policy specialists in meetings now referred to as Project Solarium. Today, we are confronted with serious new international challenges in which computers, networks, and cyber weapons play a key role. To understand and address the problems created by these new technologies, computer scientists must engage with political scientists, sociologists, psychologists, historians, area specialists, and others. While the hardest problems cross disciplinary boundaries, Brown is poised to continue its long tradition of multidisciplinary research and outreach in addressing the new problems of the cyber age."
We caught up with John just after his recent talk ("Cyber Security: A Societal Grand Challenge"), which was a retirement lecture in name only. (John is currently teaching in Brown's new Executive Master in Cyber Security degree program and will be teaching CSCI 1800 Cyber Security and International Relations to more than 185 students next semester.) At the reception afterward, Provost Richard Locke announced an important milestone: the establishment of the John E. Savage Endowed Professorship in Computer Science, funded with very generous gifts from John's family and friends.
Intended to honor John's legacy and support future scholars, the gift marks the beginning of a major fundraising effort in support of CS With Impact, the biggest expansion in Brown CS history and part of the BrownTogether campaign. As part of this effort, CS will seek to raise a minimum of $40 million to create ten new endowed chairs, which will be supplemented with the creation of five lecturer positions, for a total of fifteen new faculty slots.
Fifty years ago, Locke explained, even the idea of computer science as a department was highly controversial. Today, CS is Brown's largest concentration, acting as an intellectual hub for researchers who are helping decipher disease, understand the human brain, keep our data secure, and develop new technologies that improve lives.
To ensure that the University remains at the forefront of developing these tools, and that its students have access to the most advanced computer science education, Brown recently launched CS With Impact, which will dramatically expand Brown CS over the next five years. New faculty will help mitigate enrollment pressures and build bridges to other parts of Brown through joint appointments and with other disciplines.
First announced by President Christina Paxson less than two years ago, at John's fiftieth anniversary celebration, the John E. Savage Endowed Professorship in Computer Science was recently established and is now the first of what will be ten new endowed professorships in the CS With Impact expansion. At this time, the Savage chair will support an assistant professor. Brown is seeking a matching amount in additional donations, so that the chair can support a full professor.  
"When I came to Brown in 1967," John says, "I could see that the students were bright, curious, and wanted to take charge of their education. I was also impressed by the large amount of faculty interaction. For example, a chance interaction with a cognitive psychologist in the 1970s led to my co-teaching a human memory models course in the Psychology Department for four years. The freedom to pursue research worth doing has been very stimulating for me, and I'm delighted that we're adding these new faculty members via endowed professorships. It's the best way to build on our strength in CS and create opportunities for new directions as well."
On the afternoon of December 13, 2018, friends and colleagues gathered in the CIT to honor Professor John Savage, one of the co-founders of Brown CS, as he gave a lecture ("Cyber Security: A Societal Grand Challenge") that drew from a career of more than five decades. Although billed as a retirement lecture, John continues to teach in Brown's new Executive Master in Cyber Security degree program and has more than 185 students enrolled in his CSCI 1800 Cyber Security and International Relations class next semester. John's wife, Patricia, was also in attendance, and the talk was followed by a special reception at which colleagues offered toasts in his honor.
Department Chair Ugur Çetintemel opened the lecture by praising John's research and teaching and citing an article that John wrote more than twenty years ago, in which he had the foresight to encourage computer scientists to "become more outward directed" and "understand the problems that arise in computationally demanding science and business applications so we can help solve them". He thanked John for an extraordinary set of contributions to Brown: Savage served as Department Chair of Brown CS, Chair of the Task Force on Faculty Governance, and Chair of the Faculty Executive Committee, among other leadership roles. John received the President's Award for Faculty Governance in 2009.
His service to professional societies and in the field of cyber security and Internet governance are equally storied: John has been a member of the NSF Review Panel on Emerging Technologies and the Program Committee for the IEEE/ACM International Symposium On Nanoscale Architectures, a Jefferson Science Fellow for the US Department of State, an honored advisor for the government of Vietnam, and a member of the Scientific and Technical Intelligence Committee. He is a Fellow of AAAS and ACM, a Life Fellow of IEEE, a Guggenheim Fellow, a Professorial Fellow of the EastWest Institute, and the recipient of a Fulbright-Hays Research Award.
But as John himself pointed out two years ago, one of the most remarkable aspects of his career is how he, Brown CS, and the field of computer science co-evolved. Savage co-founded the Department, led the effort that established a multi-decade partnership with Sun Microsystems, played a major role in seeing the CIT built, and founded the Industry Partners Program, but even as he was serving both Brown and the public sector at large, he was publishing three books and pursuing diverse interests that included computational complexity, scientific computation, computational nanotechnology, and cyber security policy and technology. 
In his lecture, John drew on the full spectrum of his life's research and teaching to make an exacting case that cyber security dangers pose a threat to the very existence of our nation and our democracy, and that a "whole society approach" from a comprehensive set of experts and everyday users is necessary to combat it. He began with a balanced picture of a world that has benefited enormously from technological advances (currently, 55% of people worldwide are connected to the Internet and extreme poverty has plummeted globally to 9.1%) but is gravely threatened by technological misuse in the form of audio and visual fakery, lethal autonomous weapons, privacy violations, and numerous other dangers. Gone are the days, John said, in which the Internet could be seen as a "threatening but not terribly threatening" Wild West.
The facts of the case, Savage maintained, are stark: computer scientists can't secure technology alone, hardware and software errors will always exist, and security requires constant attention. "Above all," he said, "humans have important cognitive limitations" and share the same biases as our ancestors of 70,000 years ago: "To succeed, we must engage multiple human perspectives." Much of the responsibility lies with us as computer scientists, John said, offering solutions such as increasing the amount of security content in CS curricula, using memory-managed languages, and experimenting with segmented system architectures.
But perhaps most importantly, he argued that other domain experts are needed for true and lasting security by helping to counter social engineering strategies, examine misaligned incentives, assess the impact of technology on our privacy, and explore the role of history and culture on diplomacy. This includes the work of psychologists, economists, social scientists, political scientists, historians, and others. "Where is the wisdom required for this huge effort?" John asked in his conclusion. "I don't think it's just in computer science." 
At the reception afterward, colleague after colleague stepped up to the lectern to offer a toast. "Versatility, service, and discipline," were the three words that Professor Andy van Dam used to describe John; Professor Roberto Tamassia mentioned John's commitment to collaborative work, remembering the days when Savage was instrumental in envisioning and launching projects at the largest scale, including grants for more than ten million dollars in which practically the entire faculty were listed as investigators. "Because of John," said Professor Anna Lysyanskaya, the Brown CS culture of warmth and mentoring is still unmistakable. "We're like a family, and that hasn't changed."
The final tribute was from Provost Richard M. Locke, who joked about envying Savage's discipline every time he arrived at the gym at 6:30 in the morning, only to see John and Patricia leaving the facility, their workouts already complete. But he quickly moved to words of high praise: "John is an example of everything wonderful about Brown...the scholarship, the teaching, but also as a person." He recognized Savage for his warmth, intellectual curiosity, and ability to make connections, but most of all for having the sort of vision that guides "not just what it means to be a great university, but how we conduct our lives". John and Brown CS, Locke said, are truly intertwined.
And that connection was clearly in John's thoughts as he rose for a few final remarks, saying that he was proud and pleased of the work that "many individuals" had done to help Brown CS grow: "We truly are a community, so we must have done something right...I thank all of you as well."
A recording of the lecture is available here.
In an endeavor to increase opportunities for students and broaden excellence in research and teaching, Brown University will expand its Department of Computer Science over the next five years, adding 10 tenure track faculty members and five lecturers, as well as boosting support for collaborative, high-impact research.
The expansion, the largest in the 40-year history of the department, comes in the context of rapid growth in student enrollment in computer science, which is now the largest undergraduate concentration at Brown. The plan also reflects a recognition of the ever-expanding role of computation in the modern world.
“Computation is central to the modern university and applies to many fields," said Brown Provost Richard M. Locke. “Tools and insights from computer science are helping to decipher disease, understand the human brain and develop new technologies that improve lives. We want to make sure Brown remains at the forefront of developing those tools and insights, and that our students have access to the most advanced computer science education.”
Locke added that the strengths of computational research—particularly at Brown—are well aligned with the areas of integrated scholarship identified in Brown’s strategic plan, Building on Distinction.
Ugur Cetintemel, a professor and chair of computer science, says the department’s CS With Impact expansion will have far-reaching benefits to the University community as a whole.
“The goal is to make sure that all students — concentrators and non-concentrators alike — have a chance to interface with computer science,” Cetintemel said. “At the same time, the effort will expand our department’s engagement with other units on campus that draw upon computer science resources and expertise. Our goal is to elevate Brown's scholarly impact by increasing collaboration between CS and other disciplines.”
A Rapid Expansion
After its founding in 1979, the Brown Department of Computer Science quickly established itself as one of the top departments in the nation. Brown faculty members have made pioneering contributions in a wide array of research areas including computer graphics, object-oriented programming languages (a paradigm that gave rise to modern computer languages like Java and Python) and databases, and the synchronization of programs in multi-processor computer systems. Brown alums have gone on to chair computer science departments around the country, including powerhouse departments at MIT and the University of Washington, and hold top jobs at industry giants like Google, Intel and Microsoft.
Interest in computer science among Brown students has skyrocketed in recent years. In 2010, Brown awarded 48 undergraduate degrees in computer science. In 2018, that number was 248. For the 2018-19 academic year, 588 students have declared a concentration in computer science — one-sixth of the undergraduate population.
Yet in a rigorous Open Curriculum environment that encourages undergraduates to study what they love and architect their own education, interest extends well beyond students who have chosen a specific focus in computer science. More than half of the undergraduate Brown student body took at least one computer science course during the last academic year.
Junior Eleanor Avril’s journey into a computer science concentration is similar to that of many of her peers. When she arrived at Brown, she hadn’t considered CS, but like many of her friends, decided to take an introductory CS class. She was instantly hooked by the problem-solving aspect of the classwork, she says, and intrigued by how broadly CS tools could be applied. That broad applicability is a motivator for many new CS concentrators, Avril says.
“There’s a definite sense that you can take computer science and apply it in some way to almost any other area,” she said. “I know a lot of people who are studying computer science and visual arts, because they’re interested in design. I know people who are doing natural language processing because they’re interested in linguistics. There are so many ways to apply CS, and I think it’s very compelling for students.
“Having the problem-solving ability and the technical background is useful even for people who don’t end up going directly into software engineering,” Avril added.
Teaching And Research
The addition of 15 new professors and lecturers will help meet student demand for a broad selection of classes, keep class sizes in check and ensure that undergraduates have ample opportunities for research in collaboration with faculty. The five lecturer positions will be dedicated to teaching. The 10 tenure-track positions will have teaching and research responsibilities.
Cetintemel says the tenure-track hires will bolster existing and emerging strengths in the department in areas like artificial intelligence and robotics, digital liberties, computer education research, and visual computing with a focus on societal impact.
“We envision a third of the tenure-track hires having one foot in computer science and one foot in another academic discipline,” he said. “These would be people who bring CS tools to bear in another field of study — perhaps economics, health or visual arts as examples. Or perhaps someone who works in public policy matters that apply to digital liberties or cybersecurity.”
That kind of collaboration would continue an ethos that has existed in the Brown computer science department from the beginning, Cetintemel says. Current collaborative projects include:
Cetintemel says that the CS With Impact expansion will enhance these existing projects and create new programs. Expanded research also means more opportunity for students at both the graduate and undergraduate levels, as student research is a hallmark of the Brown CS education.
“Brown students want to solve problems for the public good,” said Stefanie Tellex, an assistant professor of computer science who specializes in robotics. “They see courses like Computer Science for Social Impact, and they know that CS is a powerful tool for any profession. My undergraduate researchers are already doing world-class work, teaching robots through social feedback and interacting with them in virtual reality. I’m confident that they’ll help build a future where robots are a valuable part of everyday life.”
Cetintemel says the initiative also aims to increase alum engagement and fundraising for the department, expanding upon recent successes. In May 2018, the University successfully raised $10 million in support of the department’s Undergraduate Teaching Assistant program. A cornerstone of the Brown CS education, the program engages experienced students to help teach their peers. The department hopes to build on the teaching assistant fundraising success by raising additional funding for named faculty positions and research programs.
Ultimately, Cetintemel says, CS With Impact is centered upon making sure that Brown students will be intellectual leaders in bringing data and computational tools to address the problems of the 21st century.
“In many ways, Brown is turning out the kind of technology leaders the world needs,” Cetintemel said. “Our students tend to be entrepreneurial and steeped in a variety of perspectives that are gleaned in exploring many areas of study. It’s these kinds of people who will make sure computational tools are used in ways that make people’s lives and society better.”
The IEEE International Conference on Data Mining (ICDM) has established itself as the world’s premier research conference in data mining and analytics. The conference covers aspects of data mining, including algorithms, software, systems and applications, and presents the Best Student Paper Award annually to a published paper presenting innovative solutions to challenging data mining problems, whose first author is a student. This year, “ProSecCo: Progressive Sequence Mining with Convergence Guarantees”, a paper by  Brown CS student Sacha Servan-Schreiber and Brown PhD alums Matteo Riondato and Emanuel Zgraggen, was  named the runner-up for this award.
Sacha is currently a senior at Brown pursuing an Sc.B in Computer Science, with academic interests in applied cryptography, decentralized networks, and data science. Matteo is a research scientist in the Labs group at Two Sigma, an innovative investment firm, and also holds an appointment as Adjunct Assistant Professor of Computer Science at Brown. Emanuel is a PostDoctoral Associate at MIT’s CSAIL DataBase Group with research interests including HCI, InfoVis, and Data Science. In their paper, the trio present an algorithm for progressive mining of frequent sequences from large transactional datasets. “The algorithm processes the datasets in blocks and outputs, after having analyzed each block, a high-quality approximation of the collection of frequent sequences,” they explain. Thanks to its properties, ProSecCo enables interactive data exploration, allowing analysts to remain engaged and continuously discover new insights from data.
Sacha, Matteo, and Emanuel’s paper can be found here.
“In recent years, many techniques have been developed to improve the performance and efficiency of data center networks,” writes Professor Theophilus “Theo” A. Benson of Brown CS, as he explains the significance of his most recent paper on Deep Learning. This paper was recently recognized by ACM SIGCOMM, and was a runner-up at a large workshop just this year. SIGCOMM is the ACM’s professional forum for the discussion of topics in the field of communications and computer networks, with a particular focus in systems engineering and architectural questions of communication.
Currently, Theo’s work is precisely in this field as his specialty is in solving practical networking and systems problems, with a focus on software defined networking, data centers, clouds, and configuration management. This paper centers on how many data center networking techniques (routing, topology augmentation, energy savings) share design and architectural similarities. He presents a framework for developing general representations of network topologies using deep learning to solve a large class of data center problems, and to simplify the process of configuring and training deep learning agents. This framework was used to implement a DeepConf-agent that tackles the data center topology augmentation problem, and resulted in performance comparable to the optimal solution. 
This paper follows a natural evolution of Theo’s research, as he works on understanding and designing techniques for data analysis. His paper “Network Traffic Characteristics of Data Centers in the Wild” won the best paper award at IMC 2010. This was followed by his paper “MicroTE: Fine Grained Traffic Engineering for Data Centers,” which involved work with Microsoft on domain specific algorithm to improve the performance of data centers, and his paper “YTrace: End-to-End Performance Diagnosis in Large Cloud and Content Providers,” which explored domain specific techniques for diagnosis at Yahoo data centers. His research has progressed from large scale analysis, to domain specific heuristics, to application of statistical techniques, before finally evolving to machine learning.
We're proud to announce that a team of four members from the Brown University Executive Master in Cybersecurity (EMCS) Class of 2019 won first place in the policy category at CSAW’18, the world’s largest student-led hacking and security competition. The New York University Tandon School of Engineering hosted the policy portion of CSAW in the U.S., one of the event’s six international venues which combined totaled over 20,000 participants worldwide.
Winning team members from EMCS ’19 include Adam DiPetrillo, George Hasseltine, Joshua Snavely, and Chad Thiemann.
The Team’s Winning Proposal: Securing U.S. Election Infrastructure
Competing at NYU against 28 teams comprised of PhD, Master's, law school, and undergraduate students, the team chose as their topic, Policy Recommendations for Securing the U.S. Elections. In the wake of the 2016 U.S. Presidential Election, and its exposure of the vulnerability of the nation’s election infrastructure and voter registration databases at the state and municipal levels, the team asserted, “the question is not if, but when they will be breached,” and called on Congress to act immediately to ensure safe and secure elections.
In their memo to the U.S. House Committee on Homeland Security, the team proposed legislation that would improve protection of the election infrastructure through:
As part of CSAW’s submission requirement, the team created a three-minute video summarizing the proposed legislation and the urgency for it. In this jointly crafted quote, team members shared, "It was a great honor for all of us on the team to develop a winning proposal that addresses one of the biggest threats to our nation’s democracy – the vulnerability of our voter infrastructure system. Our success can be directly attributed to our teamwork and the support of our EMCS cohort and staff. Many kudos to the competition – their efforts significantly raised the bar. While the competition is over, our collective efforts are not complete. We must strive to continuously engage legislators and election officials to refine and implement such policies to secure our democracy."
Commenting on the team’s success, Brown University EMCS Program Director and Adjunct Professor of Computer Science Alan Usas said, "Placing first in the CSAW competition is a tremendous achievement by this outstanding team from the EMCS Class of 2019. They created a winning proposal that reflects the major tenant of the EMCS program that cybersecurity leadership lives at the intersection of policy, technology, privacy and human factors.  I look forward to seeing the team progress this plan to the next level."
In Brooklyn, where the EMCS team competed, some 100 professionals and faculty worked with NYU Tandon student leaders to create, judge, and organize the giant event, supported by 30 industry and government partners. The Borough President proclaimed November 8 as NYU Tandon CSAW 15th Anniversary Day in Brooklyn.
Last week, Professor Kathi Fisler of Brown CS was in Boston to give the November 8 keynote address for OOPSLA (ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications). OOPSLA is one of the main international conferences on programming languages and software engineering, with a current focus beyond object-oriented programming. It's part of the umbrella SPLASH conference (Systems, Programming, Languages, and Applications: Software for Humanity), which brings together OOPSLA and several smaller conferences and tracks with related themes.
Kathi's talk ("In Defense of Little Code") addressed the interplay between quantitative big-data methods and qualitative small-data methods in computing education around programming and programming languages. She also promoted the idea of bringing introductory computer science and introductory data science together under the theme of "data-centric computing", an approach that she's been piloting in Brown's new CSCI 0111 course this fall.
Her talk presented work that was jointly done with Brown CS Professor Shriram Krishnamurthi, Brown CS PhD students Preston Tunnell Wilson and Sam Saarinen, and Worcester Polytechnic Institute PhD student Francis Castro, who is finishing his dissertation with Kathi in the coming year. 
Network performance has been one of Brown CS Professor Theophilus "Theo" A. Benson's core interests for years. And network performance in emerging regions, he says, is particularly interesting because it requires us to question assumptions about what works and what doesn't: "We can't just transplant network protocols designed for massive tech companies to developing countries, where there are huge infrastructure discrepancies." Earlier this year, he won a Google Faculty Award to develop network optimizations that can be brought to bear in developing countries, often overlooked, where mobile devices have become the primary mode of Internet access. The full list of recipients is available here.
The project begins in Pakistan, where Theo explains that mobile devices are often equipped with small memory sizes and slow CPUs. "For example," he says, "according to a recent study of mobile devices in Pakistan, 90% of devices were equipped with at most 1024 MB of RAM and 89% had 1 GHz or slower processors. Despite the prevalence of such devices in these markets, there are few systematic studies of the differences between web performance on low-end and high-end smartphones."
The first step of the project will involve using low-end smartphones popular in Pakistan to conduct a large-scale measurement study to identify bottleneck resources (for example, CPU, memory, and network) in the page load process and analyze how these bottlenecks may change over time based on device characteristics, network connectivity, and page structure. Insights from this study, Theo believes, will help in understanding the effectiveness of various infrastructure design choices (such as the role of CDNs) and page load optimization techniques (for example, Polaris) for developing regions.
"We also believe from anecdotal evidence," Theo says, "that small memory sizes in existing low-end smartphones are a significant pain point. We're going to determine when, where, and why memory bloats —potentially leading to stalls and crashes— occur for different popular web pages. Using these insights, we'll develop optimizations that would aim to throttle the usage of memory-heavy web page resources."
This latest research continues work for which Theo earned a Richard B. Salomon Faculty Research Award earlier this year.
Finally, this Google Faculty Award is one of Beson's two recent grants. In late summer, he also received an unrelated National Science Foundation grant for a project that aims to improve network performance by tailoring web server configurations to match the specifics of each application and end user's technology. If successful, this proposal will fundamentally impact widely accepted principles for managing services and will enable a broad range of users to benefit from efficient operation of the growing set of essential web services.
Toymaker, a seven-minute animated short created in 2017 by Brown CS Professor Barbara Meier and her students, has already been accepted at multiple national and international film festivals and won the Music Award in the Student Competition of the 13th Athens Animfest. Just this month, it's been accepted at four more festivals:
All events are open to the public. You can also read an in-depth feature on the making of Toymaker that includes a link to the film.
Professor Sohini Ramachandran of Brown University's Center for Computational Molecular Biology (she also serves as its Director) and Department of Computer Science has just had one of her recent projects, SWIF(r), mentioned in Nature, the multidisciplinary science journal. Originally published ("Localization of adaptive variants in human genomes using averaged one-dependence estimation") in Nature Communications, the research involves creating a machine learning algorithm to examine the genomes of a group of hunter-gatherers from southern Africa, then flag variations near genes associated with metabolism. Sohini and her collaborators speculate that the genetic changes they observed may allow their holders to store body fat at times when sufficient food isn't available.  
"Using statistical methods for identifying adaptive mutations from population genetic data is challenging," Sohini says, "There are several obstacles: assessing the significance of genomic outliers, integrating correlated measures of selection into one analytic framework, and distinguishing adaptive variants from hitchhiking neutral variants. SWIF(r) is a probabilistic method that detects selective sweeps by learning the distributions of multiple selection statistics under different evolutionary scenarios and calculating the posterior probability of a sweep at each genomic site. It explicitly models the joint distributions of selection statistics, thereby increasing its power to both identify regions undergoing sweeps and localize adaptive mutations. And because it provides a transparent probabilistic framework for localizing beneficial mutations, it's also extensible to a variety of evolutionary scenarios."
In addition to our many other lectures throughout the semester, Brown CS is excited to announce that Professor Robert Tarjan of Princeton University will be delivering the annual Paris C. Kanellakis Memorial Lecture on December 6. This talk comes on the heels of the recent Distinguished Lecture Series (featuring Professor Jennifer Rexford of Princeton University and Professor Dawn Song of UC Berkeley), and will be taking place at 4:00 PM at CIT 368.
Professor Rexford explored the application of research from a variety of established disciplines to the field of Computer Networking through her lecture, “Hitting the Nail on the Head: Interdisciplinary Research in Computer Networking.” Rexford is the Gordon Y.S. Wu Professor of Engineering and the Chair of Computer Science at Princeton University, and holds a PhD from the University of Michigan. Her lecture focused primarily on the innovating work within the field of computer networking, ranging from data-center networks, to cellular networks, to vehicular networks. This breakneck innovation necessarily requires unique solutions to the biggest problems facing the field, and Rexford illustrated how these solutions are often found beyond the field of computer networking.
Professor Song investigated challenges and opportunities at the intersection of AI and Security through her talk, “AI and Security: Lessons, Challenges, and Future Directions”, on October 3. Song is a professor in the Department of Electrical Engineering and Computer Science at UC Berkeley and holds a PhD from UC Berkeley. Her research lies within security and privacy issues in computer systems, and she has received many awards for her work in computer security and deep learning. In this talk, Song explained the role of AI and Deep learning in improving computer security systems, and delved into how machine learning must be balanced with privacy preservation.
Professor Tarjan will be analyzing theoretical approaches to algorithm design through his lecture, “Algorithms: Theory Meets Practice.” Professor Tarjan is the Distinguished University Professor of Computer Science at Princeton University as well as the Chief Scientist at Intertrust Technologies, and holds a PhD from Stanford University. He has held academic positions at Cornell, UC Berkeley, Stanford, and NYU, and industrial research positions at Bell Labs, NEC, HP, and Microsoft. His work focuses on the design and analysis of data structures and graph and network algorithms, and he has published over 250 papers. This lecture will be delving into how faster speeds and larger memory sizes of current computers vastly increase the size of the design space for even simple problems, and the role that theoretical approaches to algorithm design can help systematically explore this space. Such approaches can produce algorithms algorithms that are correct and efficient as well as simple to program, and Tarjan aims to illustrate this point through real-world applic
The ACM Karlstrom Award is presented annually to outstanding educators in recognition for the advancement of new teaching methodologies and for significant contributions to the development and expansion of  computer science and engineering. Professor Andy van Dam of Brown CS received the award for his impact on shaping the next generation of computer scientists in 1993, and this year, his fifty-third at Brown, marks the twenty-fifth anniversary of that recognition.
Although widely recognized in the field of computer graphics, Andy has always retained a unique passion for inspiring many thousands of students through his many years teaching. Even when Computer Science was still in its infancy and a mere shadow of its current form, Andy was at the forefront of guiding young minds through the subject. He taught the world’s first high school program in Computer Science in Philadelphia in 1962, and cites this experience as being pivotal in shaping his path towards academia. Andy went on to co-found the Department of Computer Science with John Savage at Brown, chairing the department from 1979-1985 and playing a central role in its development over the years.
When asked about his ability to impact so many students in such a meaningful way, Andy cites his unrelenting passion and commitment to teaching as being of utmost importance to him: “I have recorded nearly all my lectures so I can listen to them, and learn what to do better.” Andy has had a predominant role in retaining this kind of focus and attention on undergraduate education that remains a hallmark of the Brown CS program. From instituting a comprehensive undergraduate Teaching Assistant program (currently with 46 UTAs and 4 head UTAs in his introductory CS course alone) to helping to establish the departmental culture of considering teaching a central responsibility of all Brown CS faculty, this emphasis on quality undergraduate education has continued to foster a community of learning to this day.
In a testament to his influence and impact on so many within the Computer Science field, one of Andy’s undergraduate teaching and research assistants, the late CMU professor Randy Pausch himself went on to win the very same Karlstrom award for his work inventing the Alice 3D programming environment for creating VR experiences. The two were the only advisor-advisee pair to win the award, and when asked about Randy, Andy said that he was marked by “his huge investment in undergraduate teaching, his unrelenting optimism, and his overall energy”.
Although Andy continues to be recognized for his work in Computer Science (recently being named to the SIGGRAPH Academy for contributions to computer graphics), it is his commitment to teaching that has touched the lives of so many students over the years.
Brown CS will host Professor John Savage on Thursday, December 13, 2018, at 4 PM in CIT 368. John has spent 51 years at Brown and became Professor Emeritus in July, 2018. To commemorate this occasion, he will give a lecture ("Cybersecurity: A Grand Challenge for the Society") hosted by Professor and Department Chair Ugur Çetintemel. A reception will follow.
The cyber security problem, John says, is now a grand challenge problem that impacts all aspects of life. Making cyberspace safe is a complex problem that requires the engagement of all segments of society.
Dr. John E. Savage is the An Wang Emeritus Professor of Computer Science at Brown University. He earned his PhD in Electrical Engineering at MIT in 1965, joining Bell Laboratories in the same year. He moved to Brown University in 1967, co-founded the Department of Computer Science, and served as its second chair from 1985 to 1991. He has served Brown in many capacities including as Chair of the Faculty Executive Committee and Chair of the 2002-2003 Task Force on Faculty Governance. He was a member of the MIT Corporation Visiting Committee for EECS from 1991-2002. In 2011 he gave Congressional testimony on cyber security before the Judiciary Subcommittee on Crime and Terrorism of the United States Senate. He has done research on coding and communication theory, theoretical computer science, VLSI theory, silicon compilation, scientific computing, computational nanotechnology, the performance of multicore chips, reliable computing with unreliable elements, blockchain governance, and cyber security policy.
He is a Guggenheim Fellow and a Fellow of AAAS and ACM, and a Life Fellow of IEEE. He served as Jefferson Science Fellow in the U.S. Department of State in 2009-2010. He is a Professorial Fellow at the EastWest Institute, a member of the Board of the Michael Dukakis Institute, and a member of the Board of Advisors of the Verified Voting Foundation. He served as a member of the Rhode Island Cybersecurity Commission and participated in the 2016 Munich Security Conference as a member of the McCain-Whitehouse U.S. Congressional Delegation.
K-12 schools and districts planning to add computer science to their offerings face the challenge of creating sequences of classes out of many standalone curricular options, each targeting different contexts and content. Earlier this month, the National Math and Science Institute announced a coalition of national education groups that includes Bootstrap (a K-12 CS curriculum used worldwide that's directed by Kathi Fisler, Shriram Krishnamurthi, and Emmanuel Schanzer of Brown CS) to address this problem with a new approach.
In the current phase, coalition members are working together to define and explain the connections among their programs to educators. Members will offer coordinated professional development to participating schools, then work with schools and districts to design customized, cohesive CS programs that span multiple grades by stitching together existing curricula in meaningful ways.
“By helping schools incorporate computer science across courses and grade levels, we are increasing students’ opportunities to develop these skills within the context of the math, science, arts and language courses they already are taking,” says Emmanuel Schanzer of Bootstrap. “Education research confirms that students and teachers excel when classwork is tied to familiar experiences. This approach supports that type of learning.”
The new coalition is made of up of seven national educational groups and will start training teachers in three San Antonio districts in 2019.
The full press release is available here.
IEEE VIS, the world's largest conference on scientific visualization, information visualization, and visual analytics is now over, and Brown CS PhD candidate Johannes Novotny and his collaborators have returned with the Best Poster Award for their work ("Developing Virtual Reality Visualizations of Dinosaur Track Creation with Scientific Sketching") in using virtual reality (VR) to analyze dinosaur footprints. His co-authors include Brown CS Master's alum Joshua Tveite, Brown PhD candidate Morgan L. Turner, Professor Stephen Gatesy of Brown's Division of Biology and Medicine, Professor Fritz Drury of the Rhode Island School of Design, Lecturer Peter Falkingham of Liverpool John Moores University, and Brown CS Professor David H. Laidlaw.
"Large-scale simulations of substrate flow," the researchers explain, "have recently been used to explore the relationship between track morphology and foot movement using data from modern birds and fossilized specimens found in the field. However, the spatial complexity of these unsteady flow datasets make it difficult to analyze them using off-the-shelf visualization tools." 
In response, Johannes and his co-authors conducted a two-year design study of developing VR flow visualization tools for the analysis of dinosaur track creation using the Scientific Sketching design methodology created by Brown CS PhD alum Daniel F. Keefe and his collaborators. They enlisted the aid of twenty-five art and computer science students from a VR design course who continually created visualizations in an iterative process guided by paleontologist collaborators through multiple critique sessions. This allowed them to explore a wide range of potential visualization methods and select the most promising methods for actual implementation, eventually resulting in visualizations that answered research questions and generated new insight into the simulation datasets.
To learn more, watch a video preview, read the abstract, or look at the poster.
Professor Dawn Song of UC Berkeley visited Brown CS earlier this month to deliver the thirty-seventh lecture ("AI and Security: Lessons, Challenges and Future Directions") in the Distinguished Lecture Series.
Described as an innovator and "serial entrepreneur" in Professor Roberto Tamassia's introduction, Song began by setting a historical backdrop of cyberattacks that are increasing in number, financial cost, and volume of users exposed. "And as AI becomes more and more capable," she explained, "the consequences of misuse by hackers will become more severe....Security can really help AI and AI needs security." She then laid out a structured response to challenges at the intersection of the two disciplines, offering examples from her own research into secure and privacy-preserving machine learning solutions.
"One of the fundamental weaknesses of systems is humans," she said, following the observation with an overview of a project that produced an AI-enabled chatbot to monitor correspondence between the user and a third party and identify phishing attempts. She also highlighted the vulnerability of deep learning systems to adversarial examples, demonstrating how attackers could modify traffic signs to confuse self-driving cars even without having any insight into the car's programming.
But far from pessimistic, Song made an effort to depict a future in which machine learning can be less vulnerable and more privacy-preserving. One of her examples, again drawn from her research, was an intelligent and user-controlled virtual assistant that acts as an intermediary when dealing with data-mining third parties such as Google or Facebook. "This will maximize user value," she said, "not corporate value." She ended with a look at her latest venture, Oasis Labs, which is working to create a privacy-first cloud computing platform on blockchain. 
We caught up with Roberto after the lecture, and he spoke highly of Song's ability to look at an issue from all sides: "Her talk eloquently brought together the vast opportunities created by AI and the major challenges in securing its deployment and use."
As the presentation drew to a close, queries from the audience came rapidly, and Song provided answers about secure hardware, the difference between AI and machine learning, and the effectiveness of chatbots against social engineers. The final question, about whether future Oasis Labs releases would work with existing social media platforms or create new ones, brought an answer that seemed to reflect one of the lecture's major themes. Faced with continuing challenges, Song said, she hopes her work will "offer meaningful alternatives" that will bring about both behavioral change and security improvements.
Launched in 2013, the Dennis M. Ritchie Doctoral Dissertation Award was created by the Association for Computing Machinery's Special Interest Group on Operating Systems (ACM SIGOPS) to recognize research in software systems and to encourage the creativity that Dennis Ritchie embodied. Only one winner is chosen annually, and this year, Brown CS PhD alum Jonathan Mace, now a tenure-track faculty member at the Max Planck Institute for Software Systems, received Honorable Mention for the award.
"Many tools for monitoring and enforcing distributed systems," Jonathan explains, "capture information about end-to-end executions by propagating in-band contexts." In his thesis ("A Universal Architecture for Cross-Cutting Tools in Distributed Systems"), he characterizes a broad class of such cross-cutting tools and extends these ideas to new applications in resource management and dynamic monitoring. Finally, he identifies underlying commonalities in this class of tools, and proposes an abstraction layering that simplifies their development, deployment, and reuse.
Jonathan's thesis is available here.
"We have been exposed to new technologies for centuries," says Professor John Savage of Brown CS, "and every time a new technology is introduced, there are dislocations, unanticipated consequences...in our current situation, we have rushed to computerize so many things without knowing the hazards we were creating that we now have to address these problems urgently." 
John goes on to explain his belief that the software being written today is more reliable and secure than earlier efforts, but much of the rapidly-growing Internet of Things is run on highly insecure platforms, requiring a great deal of replacing and rewriting. This observation and others are part of a recent, wide-ranging interview with Savage conducted by Llewellyn King of White House Chronicle, a television program featured nationwide on more than 200 stations that focuses on the pressing issues of the day.
A recording of the entire interview is available here.
Out of more than 700 faculty members, two are chosen each year for Brown University's Presidential Faculty Award, and this semester, President Christina Paxson selected Professor Michael Littman of the Department of Computer Science (Brown CS).
Established in 2013, the Presidential Faculty Award recognizes members of Brown’s distinguished faculty who are conducting especially important and innovative scholarship. Michael is the first Brown CS recipient of the award, and it comes only months after he received the Philip J. Bray Award for Excellence in Teaching in the Physical Sciences. Prior winners of the Presidential Faculty Award include Bonnie Honig (Professor of Modern Culture and Media and Professor of Political Science), Jill Pipher (Elisha Benjamin Andrews Professor of Mathematics and Founding Director of the Institute for Computational and Experimental Research in Mathematics), and David Berson (Sidney A. Fox and Dorothea Doctors Fox Professor of Ophthalmology and Visual Sciences).
Michael will receive a research stipend of $5,000 and give a Presidential Faculty Lecture to provide an opportunity for faculty members in other disciplines to learn about his work.
A list of previous winners is available here.
Professor Shriram Krishnamurthi of Brown CS and collaborators have won the annual SIGPLAN Software Award for contributions to the development of the programming language Racket. This award recognizes the development of a software system that has had a significant impact on programming language research and on education, and will be shared with Barzilay, Felleisen (Northeastern University), Findler (Northwestern University), Flatt (University of Utah), McCarthy (University of Massachusetts - Lowell, Brown CS PhD alum), and Tobin-Hochstadt (Indiana University).
In continuous development since 1995, Racket is one of the world’s leading environments for language-oriented programming, elevating a “language” to its own status as a software building block. As a testament to Racket’s rapid and widespread adoption, two of the newest Brown CS faculty members (Professors George Konidaris and Seny Kamara) both used DrScheme, an earlier version of the environment, as undergrads.
Shriram continues to be recognized for his contributions to the field of computer science, with this award coming on the heels of his recent SIGSOFT Influential Educator Award.
VISSOFT 2018, the Sixth IEEE Working Conference on Software Visualization, was held last week in Madrid, Spain, and Professor Steven P. Reiss of Brown CS took home the Most Influential Paper Award for research ("The Paradox of Software Visualization") from 2005. In his paper, Steven raises the question of why software visualization has yet to be successfully deployed in a mainstream development environment, and argues that most past and current work in the field is out of touch with the reality of software development and that new approaches and new ideas are needed.
Session chairs Wim De Pauw and Jonathan Maletic presented the award just before the close of the conference on September 25, and Steve gave a retrospective of his work, declaring that challenges still remain and advocating for embedding visualizations in existing tools and using them to address real problems.
Brown CS is glad to announce that applications are open for the Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, which provides $10,000 annually to support an undergraduate engaged in an intensive faculty-student summer research partnership with the Department of Computer Science.
Last year, the award went to Nathaniel Weir, who who used the funds to explore a deep learning approach to Natural Language Interfaces for Databases (NLIDBs).
To apply, no later than February 10, 2019, students should email Associate Professor (Research) and Vice Chair Tom Doeppner either: (A) a copy of their summer UTRA application or (B) a two-page description of their proposed research and a letter of support from the Brown CS faculty member that they intend to work with.
Professor Jennifer Rexford of Princeton University visited the CIT last week to address Brown CS and deliver the thirty-sixth lecture ("Hitting the Nail on the Head: Interdisciplinary Research in Computer Networking") in the Distinguished Lecture Series. Expanding on the metaphor of hammers (the honed skills of researchers in established disciplines) and nails (interesting and important problems), she took her audience through what she described as a career in rethinking the foundations of how we manage networks, through the entire "control loop" of measurement, analysis, and control.
Working chronologically, Rexford looked closely at three example projects: optimization theory to derive distributed network protocols, composition of network policies, and traffic monitoring in network hardware. In the first section, an offhand remark that "for some reason, it wasn't obvious at first that network protocols should be solving a well-defined problem" brought knowing laughter from the audience, and they listened carefully to comments on topics such as the new understanding of modularity's importance in networking and the need to favor the concrete instead of the abstract and design iteratively. 
At the end of the lecture, offering best practices and "hard-won" lessons learned, Rexford struck a balance between caution and enthusiasm. "Real problems are often interdisciplinary," she said, explaining that interdisciplinary work and its striving for impact can be intellectually exciting and enjoyable, but also dangerous for junior collaborators who haven't mastered their own hammer. Her advice? Start with your hammer: "It's easier for a programming languages person to learn part of networking than a networking person to learn programming languages." Join an emerging community, she recommended but be careful not to be too late, and seek out collaborators who are physically proximal, stay engaged, and have similar values.
We caught up with Professor Shriram Krishnamurthi after the lecture. He says, "Jen Rexford, or 'J-Rex' to her friends and admirers, is an awesome creature of prodigious powers. Whenever she arrives at a new networking problem area, weak ideas and poor foundations tremble before her might. Her oeuvre is characterized by beautiful and powerful results, but unlike a T-Rex, she also deserves credit for her open mind and welcoming nature, which has made the networking tent a lot larger than it might otherwise have been." 
Fittingly, the final note of the lecture was a positive one as Rexford praised networking's important, real-world challenges and intellectually rich space of problems: "The Internet is still in its infancy and we need to create systems worthy of the trust we place in them....There's a grand challenge across fields if we can reach across the divide."
SIGGRAPH, a special interest group for the Association for Computing Machinery, is one of the world’s largest communities of researchers, artists, developers, and filmmakers dedicated to the advancement of computer graphics and interactive techniques. Their Awards Committee has recently announced that Professor Andy van Dam of Brown University’s Department of Computer Science  (Brown CS) has been named to the ACM SIGGRAPH Academy, an honorary group of individuals known for contributions to the field of computer graphics as well as impact through research directives and innovations.
"It's a great honor to be in the first class of inductees into the SIGGRAPH Academy. When in 1967 I co-founded SICGRAPH, the precursor to SIGGRAPH, computer graphics wasn't even recognized as a specialty within the then new field of CS, and now SIGGRAPH is the dominant Special Interest Group in ACM, running the largest annual conference. And the Visual Computing Group in our department has six faculty and a senior lecturer, as many people as founded our department in 1979!", he says. Already recognized by the ACM as a pioneer in the field of graphics (previously winning the ACM Karl V. Karlstrom Outstanding Educator Award, and the ACM SIGGRAPH Steven A. Coons Award), Andy continues to innovate in the computer graphics realm.
The Department of Computer Science at Brown University seeks applicants for multiple faculty positions at the rank of lecturer, senior lecturer, or distinguished senior lecturer. The initial appointment would be for a 3-year period (renewable with potential for promotion and longer-term contracts). These positions are a part of a major expansion plan for the department as it plans to increase its roster by 50% over the next five years.
These positions involve teaching four undergraduate courses per year and advising undergraduate CS majors. At least some of the teaching would be in first- and second-year courses. Candidates will also teach some upper-level undergraduate courses, based on their expertise and department needs. The department seeks candidates who will contribute to its overall intellectual culture; lecturers are included in faculty meetings, advise undergraduate research projects, and participate in graduate research with the rest of the faculty. Lecturers with substantial research participation and supporting funds may be eligible for periodic course
release. The department values teaching and educational innovation, and welcomes candidates interested in formally researching computing education in the context of their teaching. Brown offers a vibrant community for both teaching and research, with 33 tenured and tenure- track faculty members, one lecturer, three research faculty and several adjunct and visiting faculty members (with additional hiring planned over the next few years). The department has a strong undergraduate culture, anchored by a mature, endowed program for undergraduate teaching assistants, as well as a long history of published undergraduate research. Research and graduate programs leverage disciplinary strengths in CS as well as Brown’s broader interdisciplinary culture. CS is a founding partner in multiple university-wide initiatives including Data Science, Computational and Molecular Biology, Cybersecurity, and Human-Centered Robotics.
The positions are expected to start in the fall of 2019. In selecting candidates, we will consider quality of teaching, evidence of effective teaching, and compatibility with the area needs and interests of the department, as well as potential for effective participation in department or university activities and committees. For all applicants, we will consider potential for impact beyond Brown (through teaching, research, significant system building, outreach, or other professional activities, as appropriate for the candidate).
Applicants must have a Ph.D. by the start of the position. Applicants must submit a CV, a teaching statement, and an additional statement describing either research or other significant professional activities beyond classroom instruction (as appropriate for the candidate’s background and interests). Candidates must also arrange for at least three letters of reference to be submitted through the application website.
To apply, please use Interfolio (https://apply.interfolio.com/54052). Initial review will begin on October 1, 2018, but applications will be considered until the position(s) are filled. Inquiries may be addressed to: teaching_faculty_search_2019@lists.cs.brown.edu.
What does the future hold at the intersection of machine learning and computational biology? A conversation with the newest member of the Brown CS and Center for Molecular Computational Biology (CCMB) faculty provides an opportunity to ask an expert.
"I can't tell you whether it'll be five years, ten years, or whenever," says Ritambhara Singh, who joins us as Assistant Professor next year, "but in my head I have a perfect picture of how they'll fit together. A patient will walk into a clinic, provide a blood sample, and based on computational analysis of it, we'll help her doctor reduce the number of possible diagnoses and decide on the best treatment. At the moment, we have genetic diseases and we have treatments, but we don't know enough about why these diseases occur. At some point, the link will be made. At least, that's the utopia I have in mind!"
Before we leave the topic of the future, Ritambhara mentions something else that she wants to see in the days ahead. "I'm a very big promoter of women in STEM fields," she says. "It's something that I want to work on at Brown – I think a focus on the next generation of women scientists is really important, and I'm excited about hopefully inspiring women."
Excitement was a part of Singh's experience with computers long before she wrote her first line of code. Compared to the United States, she explains, personal computers were introduced to India quite late, and her parents didn't buy one until she was in the sixth grade. An eleven-year-old Ritambhara went to uncommon lengths to pursue CS studies from the very beginning, signing up for a computer class that her parents required before making the purchase. But afterward? "It was amazing," she says. "I was the only one in my family who knew how to use it, so I had this privileged status." 
After high school, computer science was the clear choice for a major even though Ritambhara hadn't done a great deal of coding yet. "It was a bit of a struggle at first," she says, "but once I got the hang of it, it was addictive." The experience of suddenly looking up and finding herself eight hours into working on a project was a common one, and the four years went by quickly.
The time that followed at University of Virginia (a Master's and then a PhD) were full of career-defining choices. "I wasn't sure what to study," Ritambhara says, "but my father is a doctor and I've always been interested in biology, so I thought computational biology would be a good fit." Once again, she found herself putting in extra effort by taking supplementary biology courses while working with Professor Mazhar Adli, a biochemist.
Professor Yanjun Qi, a machine learning (ML) expert, arrived at UVA in 2013, and Ritambhara saw the chance to combine ML with computational biology. "I'm so happy that I decided to take the risk," she says. "Machine learning was such a good fit because one person cannot make sense of the large amounts of data in biology. There was a bit of a learning curve, but I really enjoyed it. If there's something that excites me in a topic, I enjoy the learning process."
The collaborations that were so formative for her research are a big part of what made Ritambhara choose Brown CS: "The setup at Brown is an amazing collaborative space. Biologists are excited about computer science, and vice-versa. Brown is unique, and there are so many bright minds and great opportunities. It was wonderful to see how excited people were about my research. Being in academia gives me the freedom to try new approaches and truly experiment and explore."
"Both the Center for Computational Molecular Biology and CS communities at Brown were extremely impressed with Ritambhara's research talk and our one-on-one interactions with her," says Sohini Ramachandran, Associate Professor of Ecology and Evolutionary Biology, Associate Professor of Computer Science, and Director of the Center for Computational Molecular Biology. "She fielded questions from colleagues in Biology and CS with aplomb, and the Computational Biology graduate students all remarked on the thoughtful interest she showed in their research. I think she will fit right into the ethos at Brown, where we see all community members as people we can learn from, and where interdisciplinary work is highly valued."
How does Ritambhara describe her work? "My research involves developing and applying machine learning algorithms and models to understand data in biology. We all have the basic building blocks of DNA, but our environment plays a huge part in whether a cell stays healthy or becomes diseased. To start making sense of all the data we gather, we need to experiment. Machine learning helps us design experiments more intelligently, looking through the data to see if our hypotheses were correct and hopefully discovering which features may have caused the gene expression that turns a normal cell into a cancerous one."
"One of the main ideas that I see in computational biology," she says, "is that with every problem we work on, we hope to eventually help people. In the beginning, computational biologists had such a long road ahead of them because they first had to understand what was happening genetically. Then, we spent a great deal of time gathering data. It's exciting to finally be acting on all that knowledge that we've gathered, using machine learning to help. The point isn't just collecting data, it's ultimately helping people." 
An unexamined life may or may not be worth living, but it's clear that Tim Nelson, who has just been promoted to Brown CS Assistant Professor (Research), isn't leading one. In an hour-long interview, his reflections on our field repeatedly evoke the challenges that almost all students face in their early days of CS. His conversation finds links and commonalities and equidistant points: between math and computer science, between formal methods and programmers like the one he once was, and especially between Tim and his students. And it reveals a love of helping people escape fixed mindsets, perhaps through a process of discovery similar to his own.
Asked about his earliest interest in computers, Tim starts with what he sees as one of the core challenges of the field. "One of our problems," he says, "is that some people who study computer science receive more support than others. My parents encouraged me and made a lot of sacrifices, and when I was young, I never considered that other people wouldn't have the same. Lacking that reinforcement from family and friends –or having it and being overconfident, without awareness of how it affects others– can cause talented students to leave any field."
Partially because of that support, which kept him motivated even after failing a first-year programming class, Tim graduated from community college (Diablo Valley College in Pleasant Hill, California) at an age when most kids were finishing high school. After a move to Massachusetts at age 18, he started looking for a software job.
He found one at a company that made fiduciary tax software for estates and trusts. It was a grind, he says, an environment without much testing infrastructure where a build would be finished only hours before it needed to be ready, then frantically put into production: "You don't need a computer science degree to program, it's true, but my lack of fundamentals hurt me. I was just hacking desperately at that point, trying to stay afloat. For example, I had to write a parser, but had never been taught to build one properly, and so it took me ages and ended up buggy. I'm still recovering from some of the toxic aspects of programmer culture. But it taught me a real respect for testing, validation, and verification."
Eventually laid off, Tim describes the year of unemployment that followed as one of the best things that ever happened to him, providing the opportunity to complete a Bachelor's degree in computer science at Worcester State University. Inspired by hearing Brown CS Professor Andy van Dam speak at a conference, Tim applied to Brown, but was rejected. ("I wasn't prepared," he explains.) He chose Worcester Polytechnic Institute instead, where he worked with Daniel J. Dougherty and Kathi Fisler, now Professor (Research) and Director of the Brown CS Undergraduate Program. After learning of Tim's work from Kathi, Brown CS Professor Shriram Krishnamurthi took notice, acting as an informal advisor. Eventually, the association led to postdoctoral work at Brown.
"Research at Brown," Tim notes, "puts students in what's often called a 'high-octane' environment. I prefer to instead say 'high-energy'. Collaboration is such an important part of research, and I love that I get to work with and learn from wonderful, smart people who add energy and inspiration to the process every day. The positive energy and collegiality are the real benefits of the environment here."
Tim's research is in an area called formal methods, which uses mathematical techniques to reason about systems. "Formal logic isn't just a philosophical topic. It has real applications in computing, for real-world professionals," Tim says. "For instance, some cloud-services providers have been using logic to help customers verify security goals for years now. And these customers aren't logicians: they're working programmers and sysadmins! It's an exciting time to be in the field and see academic ideas find industrial application." 
One of the most important things Shriram offered, Tim says, was the chance to be mentored while co-teaching CSCI 1950-Y Logic for Systems: "I was very lucky to land a postdoc at a place where I wasn't told to do research and shut up. Shriram built this entire opportunity for me, sitting in class with his laptop and taking notes while I taught. Taking the time to mentor a postdoc in teaching – who does that? I owe him a great deal."
"In designing the course," says Tim, "we work really hard to find a balance of accessibility and rigor, and we change a third or more of the class every year. My goal is to teach every student enough so that if they end up in a job that's FM-adjacent, they'll think, 'Oh, hey, I can do this!'"
It seems to be working. Brown CS alum Louisa Conwill writes, "Learning computer science and how to program taught me a completely new way of thinking and reasoning. However, Logic for Systems built on, challenged, and stretched my reasoning skills by taking them a step farther – instead of learning how to solve computer science problems and build computer systems, in Logic for Systems I learned how to analyze those very problems and systems." Despite the fact that it doesn't focus on a trendy research area or feature an instructor who's a legend in their field, it's one of the largest upper-level undergraduate courses at Brown CS. Tim says that he'll happily teach it as long as he's allowed to.
What's next in the world of formal methods? "There's a motto in programming: 'move fast and break things'. But of course we can't leave them broken! What this really means that we should fix things quickly," Tim argues. "Formal methods give chaotic innovators a safety net." He returns to something that he's mentioned earlier, the need to change perspective: "Formal methods lead programmers to fix bugs, often before the bugs make it into code. Formalism, assurance, and proof don't have to be at odds with mainstream hacker culture."
In the years ahead, Tim is eager to work simultaneously on teaching and research, enjoying how they feed from and give ideas to each other. He's also looking forward to teaching CSCI 00320 Software Engineering and continuing to develop his skills as an advisor, both in terms of Brown's curriculum but also in research as well. Tim has a history of involving undergraduates in his projects, and he cites several that went on to be published: work on programing languages, firewall verification, network monitoring, and software engineering. "Working with Brown students is a real blessing," he says. "It's not just about writing code I tell them to write – the best students engage with the research itself."
And what would he say to those young computer scientists that he'll be advising? The question brings Tim full circle, and he thinks for a long time. "It's about changing mindsets again. CS has a unique role in our changing economy, and we should be aware of how our field impacts jobs – both positively and negatively. But more importantly, as the nature of work changes, where do we derive meaning?I believe in the ennobling power of work, not in the clichéd sense of being forced to pull ourselves up by our bootstraps, but as a way to have an impact. How will we help people see that service is a calling? For me, teaching is how I have an influence and how I make things better."
"These are kids who would never have seen a real robotics lab otherwise," says Annalisa Marchessault, Pre-Engineering Instructor at Providence Career and Technical Academy (PCTA), one of New England's premier technical high schools. "Many of them don't even realize the magnitude of what Stefanie has done for them."
She's talking about Professor Stefanie Tellex of Brown University's Department of Computer Science (Brown CS), whose four-year outreach effort with the school's students has produced some inspiring results. Jose Toribio, one of the program's alums, is not only studying robotics but helping teach it to the next generation of students.
Now almost a decade old, Providence Career and Technical Academy allows students to choose a particular area of technology and then pursue their studies with a mix of internships and classwork. Computer science has always been a popular subject, and four years ago, Annalisa's predecessor approached Stefanie with a request that Tellex share her robotics work with PCTA students in the Engineering program.
The response was surprising: along with giving a talk, Tellex invited students to come and work with drones and robots in her Humans To Robots lab during the summer. "What makes Stefanie amazing is that she's so approachable," Annalisa says. "She reaches out, she wants everyone to learn, and nothing could have been better than giving our kids an opportunity to learn like this locally, working alongside Brown students." 
Jose Toribio was part of Stefanie's second cohort. For someone with a lifelong interest in computers, the Humans To Robots lab was filled with exciting possibilities. He says, "I liked how open the program was. If I came up with an idea, everyone was ready to think about how it could be achieved." Jose mentions that giving public demonstrations of drones he'd built with fellow students was one of the things he enjoyed most. (One of his demos was recently seen by Richard M. Locke, Brown's Provost.) "At one of them," he remembers, "I met someone from Google who was from Providence, someone just like us. It was really exciting to feel that connection to her." 
Maybe part of the excitement was because Jose was beginning to think about being a role model himself. After completing his second summer with Stefanie's lab, he ran the program for the five juniors and seniors who joined the following year, and accepted Annalisa's offer to serve as PCTA's Robotics Coach. This summer, he taught a high school course as part of Summer@Brown and mentored two high school students doing paid internships with Stefanie's lab.
"It's wonderful to see Jose's eagerness to share knowledge," Stefanie says. "He's the sort of person that when he first came into the lab, he immediately started showing another high school student how to solder. PCTA students have been a strong asset in hardware design and creating documentation for the course for the high school level.  I'm excited to continue working with them as our project grows!"
Now studying computer science at Rhode Island College, Jose says that it's a chance to revisit the important high school years that passed by so quickly: "It's incredible to see the next generation of students doing what I did. I love being able to share my experience and show them different ways to look at an experiment." When he graduates, he's hoping to move to a bigger city and continue his engineering work, particularly in the area of robotic vision. "I just want to thank Stefanie and Annalisa," Jose says. "Without them, I might be programming, but I'd never have had this chance to work up close with robots." 
Annalisa expects that there are big things in store for him. "Two of the most amazing things about this program," she says, "are Jose and Stefanie themselves. For kids in Jose's position, seeing how he wanted to go and learn, then come back and volunteer, is the coolest thing. He's someone like them, so they feel comfortable, and they see that everything he's done is attainable for them. And Stefanie doesn't have to do what she does, but she always makes the time. Thanks to her, these kids have seen possibilities for their lives that they'd never known about." 
Imagine that you're the Chief of Surgery for a major hospital. Dozens of patients are being operated on every day, using a variety of techniques. If you wanted to find out which surgery was most effective on patients with similar problems, you might start with their medical records, looking at things like post-surgery pain levels and how long each patient needed to recover. With the right analysis, you might see clear benefits to a particular approach, thus improving the lives of thousands of future patients.
There's only one problem: even though the medical records are electronic, most of the information you need doesn't come from a check box or the answer to a multiple-choice question. It's in the free text fields where your doctors have written notes that are unstandardized and idiosyncratic to say the least. You could use machine learning to extract that information, but you need labeled training data, or data that's been tagged with meaningful information to learn from. Especially in fields like medicine, crowdsourcing training data at this scale is impossible.
So you decide to call Stephen Bach (he's joining Brown CS as Assistant Professor this fall) and ask him about Snorkel, which he describes as a training data creation and management system for machine learning.
"In order to find the subtle patterns in a data set that humans don't see," he explains, "we need to label the data, but if humans are doing the labeling, it's expensive and challenging, so Snorkel uses weakly supervised machine learning to shrink the human effort needed. You can't ask someone to write a million labels, but one of our developers can write ten functions that describe the task and label the data programmatically. The results are often noisy and contradictory, but Snorkel models the labeling functions statistically, to learn which are better than others, and then applies them to train a high-quality end model."
Only two years old, Snorkel is already being used by dozens of organizations around the world, ranging from Alibaba Group and Intel to DARPA, Stanford Medicine, and the District Attorney of New York County. With all of these uses for the software, it's tempting to ask Stephen what's in store for the project, which data sets are next in line to give up their secrets. But let's start at the beginning.
"I was always interested in computers," Stephen says, "but even starting college, I didn't understand what computer science was about, how much breadth there was and how many new, open questions." He'd planned to be an economist, but couldn't fit an economics course in his first-semester schedule at Georgetown University, so he took a CS class to fulfill one of his requirements. And got hooked.
Spurred on by an initial interest in artificial intelligence, Stephen began doing undergraduate research, including multiple summer fellowships. He recommends it to anyone: "They really gave me a taste of how different research was from coursework, the whole idea of learning how to approach open questions." One of his early goals was using automation to reduce tedious, routine tasks that are a burden to researchers. And as a fascination with machine learning grew, he became interested in the idea of helping people make discoveries, providing tools for use by scientists in other domains.
Working toward his doctorate at the University of Maryland, Stephen became interested in one of AI's oldest debates: logical methods such as programming and first-order logic versus statistics, which asks computers to deal with the uncertainty and ambiguity of data. "Scientists have gone back and forth because of the trade-offs between the two," he explains. "We're great at symbolic reasoning but not at estimating uncertainty, and I wanted to help people by combining the advantages of both approaches."
Hearing that, it's easy to see how Snorkel, which Stephen co-created with colleagues at Stanford University while doing postdoctoral research, was a logical next step. Will his work with the project continue here at Brown CS? "Absolutely. I want to keep going, and I want to work with people in other fields, not just computer scientists. Brown's culture is such a great place for that. And I want to invest time in validating applications, making sure they're really useful. Students at Brown are going to have their own real-world applications for weakly supervised machine learning, and they're the kind of people I really want to work with."
This fall, Stephen will be teaching a seminar (CSCI 2952-C Learning with Limited Labeled Data), and in the spring, CSCI 1420 Machine Learning, which explores the theory and practice of statistical machine learning, focusing on computational methods for supervised and unsupervised data analysis. "I'm eager to hit the ground running," he says, "with my teaching and with working on some new problems shaped by what we learned on the Snorkel project. We've used machine reading to find facts in text, but now I’m interested in new ways to extend our techniques. Code-as-supervision in areas like video and computer vision, is a big, open challenge. I also want to look at managing the process of having models train other models for different tasks."
What he finds exciting about machine learning, Stephen says, is that it can have tremendous indirect effects: "Doctors making better surgical decisions is a great example of how we can all benefit from ML discoveries. Just managing the increasing complexity of our lives is going to become incredibly important, like smarter personal assistants that will manage our schedules and summarize all the information we don’t have to time to look at. The joy of machine learning is that we can help people live more pleasant and productive lives if we do it right."
Brown University's Department of Computer Science (Brown CS) has made it our mission to create and sustain a diverse and inclusive environment in which all students, faculty, and staff can thrive. Few things are more meaningful than to see our female students become known leaders and role models in the field, and Brown CS undergraduate Natalie Reed has just received significant recognition: she's been named a Women Techmakers Scholar. The award is part of Google's Women Techmakers Scholars Program, which aims at furthering Anita Borg’s vision of creating gender equality in the field of computer science.
2018 is the ninth year in which a Brown CS student has either received the scholarship (formerly known as the Google Anita Borg Memorial Scholarship) or been a finalist, and it's the seventh year in a row. Students previously recognized include Tiffany Chen (Scholar) in 2017; Julia Wu (Scholar) in 2016; Sharon Lo (Scholar) and Danaë Metaxa-Kakavouli (Scholar) in 2015; Molly Long (Scholar), Layla Oesper (Scholar), and Eden Weizman (Scholar) in 2014; Irina Calciu (Finalist) in 2013; Jessica Lu (Finalist) in 2012; Theresa Avitabile (Scholar) in 2010; and Lucia Ballard (Scholar) in 2005.
GWT Scholars receive recognition and support for their education as they become part of the next generation of diverse tech leaders. This includes a visit to Google's Scholars Retreat in Mountain View, California, filled with tech talks, networking opportunities, developmental activities and sessions, and social activities with the other Scholars.
The Very Large Databases (VLDB) Conference, one of the most prominent venues for research and development results in the field of database management, will be held in Rio de Janeiro this year, from August 27 to August 31. It's still a month away, but the Awards Committee has already announced that Adjunct Associate Professor (formerly Assistant Professor) Tim Kraska of Brown University's Department of Computer Science (Brown CS) has won the VLDB Early Career Research Contribution Award.
Already honored multiple times for his early career achievements (most recently, Brown's Early Career Research Achievement Award and the VMware Early Career Systems Research Award), this award recognizes Tim for advancing systems research on interactive data analytics.
Thanks to a grant from General Motors, Bootstrap (a K-12 CS curriculum used worldwide that's directed by Kathi Fisler, Shriram Krishnamurthi, and Emmanuel Schanzer of Brown CS) has begun training dozens of educators to teach the new Bootstrap:Data Science module. Working with longtime regional partners WeTeach_CS and the University of Texas, Bootstrap will deliver four-day workshops from July of 2018 to June of 2020 for teachers in the Austin Independent School District.
After being trained, the educators will be able to deliver the new module, which gives students the opportunity to form their own questions about the world around them, analyze data using multiple methods, and write a research paper about their findings. Topics covered include functions, looping and iteration, data visualization, linear regression, and more. Social studies, science, and business teachers can utilize the module to help students make inferences from data, math teachers can use it to introduce foundational concepts in statistics, and it's aligned to the Data standards in CS Principles.
In addition to funding the workshop and classroom materials for teachers and students alike, GM's support will allow Bootstrap to study the implementation of the module alongside their external evaluator in order to iterate and improve in subsequent years. 
PhD alum Justin Pombrio and Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS) have received the PLDI 2018 Distinguished Artifact Award. PLDI (the ACM SIGPLAN Conference on Programming Language Design and Implementation) is a leading forum in the field of programming languages and programming systems research, and the award recognizes the artifact ("SweetT: Resugaring Type Systems") accompanying a paper ("Inferring Type Rules for Syntactic Sugar") that Justin presented at the conference.
Pombrio and Krishnamurthi explain that type systems and syntactic sugar are both valuable, but are sometimes at odds, causing problems for programmers. Their research addresses these problems by presenting a process for automatically reconstructing type rules for the surface language using the type rules of the core language.  
How does it work? SweetT takes as input:
and produces a set of type rules for the sugars.
This is the second award that Justin and Shriram have received for their collaborations: two years ago, their work with desugaring and resugaring ("Hygienic Resugaring of Compositional Desugaring") was selected as one of ACM SIGPLAN ICFP's best papers.
The artifact ("SweetT: Resugaring Type Systems") is available here and the paper ("Inferring Type Rules for Syntactic Sugar") is available here.
Pedro Felzenszwalb, Professor of Engineering and Computer Science at Brown University, has just received the 2018 Longuet-Higgins Prize for fundamental contributions in computer vision. The prize recognizes work in Computer Vision and Pattern Recognition (CVPR) from ten years ago, Felzenszwalb’s 2008 paper ("A Discriminatively Trained, Multiscale, Deformable Part Model") with David McAllester and Deva Ramanan.
The prize is given annually by the Technical Committee on Pattern Analysis and Machine Intelligence (TCPAMI) at the Conference on Computer Vision and Pattern Recognition, and recognizes CVPR papers from ten years ago with significant impact on computer vision research. The prize is named after theoretical chemist and cognitive scientist H. Christopher Longuet-Higgins. Winners are decided by a committee appointed by the TCPAMI Awards Committee.
The prize was first awarded in 2005, and Felzenszwalb is among a select group of repeat winners. He previously won in 2010 for his 2000 paper ("Efficient Matching of Pictorial Structures") with Daniel P. Huttenlocher.
A new article from MIT Technology Review examines work from Professor Stefanie Tellex and PhD student David Whitney of Brown University's Department of Computer Science (Brown CS), who are using augmented reality as a means to help humans and robots work together better in real environments.
The full story and a video are available here.
Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS) has just received the SIGSOFT Influential Educator Award. Presented annually, it's given to an educator or educators who have made significant contributions to, and impact on, the field of software engineering through accomplishments as teachers, mentors, researchers (in education or learning), authors, and/or policy makers. In particular, the award committee noted Shriram's contributions to the advancement of the research and practice of software engineering.
The award is international recognition of continued strength in education at Brown CS, and it follows recent awards for three faculty members and one PhD student that highlight campus-wide recognition of the Department's teaching excellence. It comes less than halfway into a busy year in which Shriram has already given keynote addresses at Programming 2018 and elsewhere, written a cover story for Communications of the ACM, and been appointed to co-lead the CACM Research Highlights Editorial Board. With this win, he joins Brown alum Barbara Ryder and the late David Notkin, a Brown CS alum, who won the prestigious award in 2015 and 2012, respectively.
The full list of winners dating back to 2009 is available here.
Brown University's Alfred H. Joslin Joslin Awards recognize a small group of seniors who have contributed in a very significant way to the quality of student life at Brown, enhancing the learning environment for all students by providing their peers with services, programs, and other opportunities for involvement. This year, one of the winners was Brown CS student Chelse-Amoy Steele, who recently graduated with a joint concentration in Computer Science and Africana Studies.
Out of numerous achievements (President of the Undergraduate Council of Students, multiple teaching assistantships, spearheading Project Tampon, contributions to a free textbook initiative, and activism work with accountability and institutional bias), Chelse cites Mosaic+ as the creation she's most proud of during her time in Providence. The student-led initiative aims to create a receptive and equitable space for racially underrepresented minority students and faculty at Brown CS, and Chelse describes it as the best example of the focus on diversity and access to resources that's been the center of her student experience.
"The spirit we started with," she says, "had activism at its core. We wanted to be in community and work together with folks who looked like us, not just improve numbers but take proactive measures to secure a sense of belonging for underrepresented folks in the department. That meant that we needed to look at who was missing and not just include them compositionally, but make space for them in all facets of Brown CS. Our goal was to start a conversation, to really change the tide." 
Now that other students will be taking the Mosaic+ helm, Chelse says she's eager to see how Brown CS will continue to take a proactive stance and implement the Diversity and Inclusion Action Plan that she helped create: "We were students in the Department who felt forgotten. It took us boldly speaking up and acknowledging our experiences –and the inequities that created them– as consequential in order for change to happen. I'm proud of the work that we did and am proud of the students who will be leading the organization as we leave. I hope that when they bring forth new issues students are facing, they'll be heard and responded to, but more than anything, this experience has taught me that response is not enough. It is important to proactively see and understand the needs of folks who are often not thought of. I hope that Brown CS, folks in industry, research, and even at other institutions will continue to nurture this work – not solely in response to crises, but through conditioning themselves to be more critical and thoughtful in regards to the needs and experiences of underrepresented students." 
Modestly describing herself as a "do what needs to be done" person, Chelse is also looking forward to her new career at Microsoft, where she's accepted a position as a Product Manager in their Educational Products division, which produces software such as Immersive Reader and OneNote Class Notebook. There's little doubt she'll be putting to use the energy and creative powers that helped earn her Joslin Award.
"It's not a stretch to see certain needs – to know that there is an issue," she says. "Figuring out how to address it is the hard part. I'm proud of the creativity of the interventions that I've been a part of and hope that when I come back to visit Brown CS, the world the next generations of leaders of Mosaic+, the Diversity Advocates, WiCS, and the Mental Health Advocates create will continue to be one of excellence, rigorous criticism, and passionate activism!" 
One of the most widely-used texts on designing computer programs has just been republished. A second edition of How to Design Programs, which Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS) co-authored with Matthias Felleisen, Robert Bruce Findler, and Matthew Flatt, is now available in print and online. The book, originally published in 2001, has been used at numerous colleges and universities, ranging as widely as Canada, Chile, Germany, and Israel.
How to Design Programs is notable for its placement of computer science at the center of a liberal arts education. Its focus is on the design process in its entirety, walking the learner through analyzing problem statements, formulating goals, creating examples, outlining, finishing a program, and testing it. Instead of using an off-the-shelf programming language, the book offers the unique feature of a custom-designed programming environment, DrRacket, which supports new learners with carefully-designed feedback. As the reader's skills increase, more and more of the environment is revealed, and they arrive at the end of the book with a fully-fledged language applicable to a wide variety of tasks.
The result of a complete revision, the new edition introduces different design recipes for interactive programs with graphical interfaces and batch programs and provides many new hints to assist the learner. It also provides support for images as plain values, testing, event-driven programming, and distributed programming.
With cryptocurrency and cybersecurity issues continuing to make daily news (just yesterday, genealogy site MyHeritage announced a data breach of more than 92 million users), Professor John Savage of Brown University's Department of Computer Science (Brown CS) has been sharing his technical and policy expertise on the world stage. Last month, he served on the Socio-economic Aspects of Blockchain and Crypto panel of the ninth annual Asian Leadership Conference in Seoul, South Korea, which chose "Globalization in Crisis: Navigating the World with New Opportunities" as its theme.
Other panelists included Ben Banerjee (Co-founder and Board Member of Swiss Impact Investment Association (SIIA) and the Board of Councilors at East West Institute and Climate Leadership Coalition), Amir Dossal (President and CEO at Global Partnerships Forum / Former United Nations Chief Liaison for Partnerships), and Jason Hsu (Crypto Congressman and Legislator of Taiwan). Sponsored by Chosunilbo, one of the three largest South Korean daily newspapers, the Asian Leadership Conference draws global leaders, policymakers, and top academics from around the world, who come together to discuss and provide possible solutions for the pressing issues facing Asia today. This year's conference had approximately 3,000 attendees.
Associate Professor (Research) Tom Doeppner, Vice Chair of Brown University's Computer Science Department (Brown CS), has been chosen by Brown's graduating senior class to receive the Barrett Hazeltine Citation. The other recipient was Monica Linden, Senior Lecturer in Neuroscience. Named for a beloved professor emeritus, the annual award recognizes excellence in teaching, guidance, and support by two members of the faculty and/or administration. Writing to Tom, a member of the award committee says, "As the person who writes the text of the citations and spends a couple of days culling through student submitted nominations, I have to tell you how much your students appreciate you, and how much of an impact you have made on them."
Tom is the second Brown CS recipient of the award, following Professor Paul Valiant, who was honored in 2016. He was recognized at the President's Reception for the class, and he joins numerous notable prior recipients, including award-winning author James Morone, Barbara Tannenbaum of the Department of Theatre Arts and Performance Studies, and Michael Lysaght, founder and director emeritus of Brown University’s Center for Biomedical Engineering.
Brown University’s Department of Computer Science (Brown CS) is announcing one of its most significant milestones of the past four decades: the full funding of a $10M Undergraduate Teaching Assistant (UTA) Endowment. “We’re immensely grateful to the more than 500 donors who have helped us reach that ambitious goal, and to the group of alums who led the campaign,” says Ugur Çetintemel, Professor and Brown CS Department Chair. “Our UTA program, in place since 1965, has allowed literally thousands of undergraduates to contribute to all aspects of instruction and play a mentoring role that's vital to the quality of coursework, the educational experience of their peers, and their own personal growth.”
“I’d like to start,” Ugur says, “by thanking the campaign committee and Brown CS staff for their hard work and incredible dedication: John Bazik, Marc Brown, Karen Catlin, Lauren Clarke, Tom Doeppner, Aisha Ferrazares, Tashana Landray, Ed Lazowska, Adam Leventhal, Daniel Leventhal, Norm Meyrowitz, Jesse Polhemus, Will Poole, Brad Silverberg, Andy van Dam, and Jeff Vogel. This campaign was a success because of Norm’s tremendous leadership, and I’m grateful for Ed’s continuing contributions and Lauren’s many, many hours of hard work. I’d also like to thank President Christina Paxson and Provost Richard Locke, who helped launch this project and gave it the final push that it needed, and Brown’s Division of Advancement for their support.”
One of the signature elements of the campaign was its use of crowdfunding, including a website that enabled people to donate and watch a “scoreboard” that showed progress toward the goal in real time. Gifts ranged from $5 to $1,000,000, creating a mosaic of donors that includes alums and friends of all ages, numerous professions, and many different life experiences.
Some donors formed affinity groups, such as the Women of CS ‘83, ‘84, and ‘85, creating UTAships whose name reflects their graduation year, or ones that honor or memorialize someone special. Not only will their tributes last as long as Brown CS does, showing appreciation for such people as Tom Doeppner, Doreen Green, Marie Moses, Randy Pausch, and Andy van Dam, many of the UTAships are designated for women or underrepresented minorities, helping add to the diversity of the Brown CS community. In the end, more than 500 gifts have funded 4 Meta UTAships, close to 20 Head UTAships, and close to 200 UTAships each year.
“We’re thrilled that the UTA program will continue to thrive and benefit new generations of students,” remarks Professor Andy van Dam, who started the program 53 years ago, “and our UTAs feel the same,” Meta-TA Zach Kirschenbaum, who maintains he’ll always be a CS 15 Humor TA, says, “The UTA program means a lot more to me than grading and debugging code. It’s about the sense of community with your staff, the ownership you have over a course, and the feeling of reward you get when a student has that ‘aha’ moment – it’s what keeps me coming back semester after semester. I’m extremely grateful for having the opportunity to serve as a UTA and MTA, and I’m excited for others to have the same opportunity.”
The UTA program has always been one of the cornerstones of Brown CS and its undergraduate student experience. “It turned out to be a game changer,” says alum Karen Catlin. “Being a UTA made me feel like I belonged. I loved being part of the Brown CS community, hanging out after hours with other undergrads and grad students, getting to know faculty and staff, and contributing to the success of CS courses. And the sense of community was especially important to me, a newbie to coding. I thrived because of it.”
The UTA program has expanded as Brown CS has grown, but in recent years, due to soaring enrollment and new federal employment regulations, it’s become dramatically more expensive. At the moment, more than one in six students at Brown is a CS concentrator, requiring up to 50 UTAs in some classes and a total of over 250 per semester, with more than 300 expected in the fall. At the same time, Brown faces increased budget pressure because of its admirable need-blind, no-loan financial aid policy. This endowment helps ensure that this one-of-a-kind program will operate at its current level for years to come.
As the academic year drew to a close, Brown University's Department of Computer Science (Brown CS) held its fourth Undergraduate Computer Science Research Symposium, organized by Professors Jeff Huang and Stefanie Tellex and undergraduate students Marshall Lerner, Jina Yoon, and Alan Yu. The annual event features presentations of student research with the goal of showcasing cutting-edge work and encouraging other undergraduates to undertake research projects of their own.
A total of two thousand dollars in prizes was generously donated by sponsors Amazon Robotics and Bulger Partners.
The Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, given this year to Nathaniel Weir to support his work with Professor and Department Chair Ugur Çetintemel, recognizes strong achievement from young students and offers them the opportunity to partner with faculty and advance work that began in the Brown CS undergraduate research program.
Nathaniel's project, he explains, explores a deep learning approach to Natural Language Interfaces for Databases (NLIDBs). "We're building a user-friendly interface," he says, "for interaction with a relational database management system (RDBMS) without requiring prior knowledge of SQL or the specifics of database schemas. It should be able to respond to any natural language query of the database with the proper table of requested information. Our project attempts to solve a number of current challenges of pertinence to the database and natural language processing communities. These include database independence, vocabulary robustness, stateful conversationality, and the ability to respond to complicated or comprehensive requests that involve difficult database operations or multi-level nested queries."
"It feels amazing to be recognized for this kind of work," Nathaniel says. "Natural language understanding is one of the coolest new areas of deep learning, and I'm extremely grateful to be a part of the team pushing the initiative at Brown. Working with Carsten and Ugur has been really interesting – our weekly group meetings are always full of new ideas and debate, and I'm lucky even to be able to sit in on them, let alone contribute to them in a meaningful way." 
Currently, their system implements a novel sequence-to-sequence deep model to translate natural language utterances into SQL, with performance comparable to other state-of-the-art NLIDBs. The next steps will be to further extend its capabilities. Possible avenues include adding more query templates for more functionality, using off-the-shelf dependency parsers, automatically recognizing nested queries and translating them independently, and expanding automatic paraphrasing using Paraphrase Database (PPDB). 
"I'm incredibly happy to have the opportunity to continue working on this project," says Nathaniel, "I'm very excited by challenges that use the capabilities of artificial intelligence to guide the process of human research: building a system that automates the process of receiving insights from databases is a perfect example of such a project. It's also given me the chance to explore the behavior of deep learning models in a hands-on way. A lot of the first obstacles in the process of getting the system up and running have already been passed in the last year – now I'm really excited to start tackling more complicated tasks. It's great that we already have a system performing to the level that it does, but knowing that there are ways to make it even more capable is really exciting."
That sort of excitement is exactly what Peter Norvig is looking for. He sees this award as a "multiplier" that will amplify the value of his gift and extend it through time. "In the past," he says, "we had to build all our own tools, and we didn't have time to combine computer science with other fields. Now, there are so many opportunities to do so. I think it's a wise choice: you invest in things that you think will do good, and educating a student allows them to help add to the things that you're already trying to accomplish." 
Each year, Brown University's Dean of the Faculty recognizes Brown faculty members with five excellence awards. Three are reserved for colleagues in other disciplines, but one of the remaining two awards is open to all assistant professors and lecturers, and the other is available to faculty in all of the physical sciences. This year, the Department of Computer Science (Brown CS) won both of them.
Michael Littman won the Philip J. Bray Award for Excellence in Teaching in the Physical Sciences, which recognizes Brown faculty members for sustained and continued excellence in teaching. He's the fifth Brown CS faculty member to win the Award, and he joins recent winners Barbara Meier and Philip Klein. Paul Valiant won the Dean's Award for Excellence in Teaching for the physical or life sciences, which recognizes faculty who hold the rank of assistant professor or lecturer. 
The full list is available here.
"We're moving," says Professor Theophilus "Theo" A. Benson of Brown University's Department of Computer Science, "into a new generation of software-defined infrastructure that incorporates virtualized network functions, 5th-generation cellular networks and advanced wireless communication technologies, and edge computing. Software-defined networking, or SDN, is a key enabling technology facilitating that transition. It's going to play a crucial role, but SDNs are prone to software design and implementation errors that can cause network failures and other catastrophic consequences. The networking research community needs key techniques and tools to maintain highly available and resilient networks."
Earlier this month, Theo won a National Science Foundation (NSF) CAREER Award that will support creating those tools. CAREER Awards are given in support of outstanding junior faculty teacher-scholars who excel at research, education, and integration of the two within the context of an organizational mission, and Theo joins multiple previous Brown CS winners of the award, including (most recently) Stefanie Tellex, Jeff Huang, and Rodrigo Fonseca. 
Theo's research will address the needs of SDN by introducing a novel paradigm that advocates overcoming software "bugs" by transforming the bug-triggering inputs into "safe" inputs. To support this paradigm, the project will develop designs, abstractions and algorithms to identify, isolate, and transform the triggering inputs into safe inputs in a systematic and principled manner.
The project will examine several research thrusts to enable practical bug-tolerant networks, including:
After successfully defending his doctoral thesis last month, PhD candidate Justin Pombrio of Brown University's Department of Computer Science (Brown CS) has a new honor to look back on as he plans his next move. Out of approximately 400 Brown graduate students with instruction-related appointments, he was one of only four to receive a Presidential Award for Excellence in Teaching.
The award, which was given at a ceremony in Pembroke Hall on April 30, recognizes outstanding pedagogical achievement. Its criteria include teaching that influences, motivates, and inspires students to learn and fosters independent learning; development of curriculum and resources that promote student learning; and development of students as individual learners.
Since 2013, Justin has been the sole graduate teaching assistant for Professor Shriram Krishnamurthi's upper-level course on programming languages, CSCI 1730, which has had enrollments of up to nearly 80 students. Justin developed assignments for the class that varied from year to year, conducted some grading, and held frequent office hours. "I love them," he says. "Teaching a whole class is difficult because everyone's on a different page, but in hours, I can figure out what a student is thinking, and explain things from their perspective."
In addition to his official duties, Justin devoted a considerable amount of his own time to what may be his most noteworthy contribution to the class. The Mystery Language approach to teaching, which he developed with Shriram, was described in a paper published at the Summit on Advances in Programming Languages, has already been used at other universities and has shown the potential to dramatically change the teaching of the subject.
Each mystery language is one language (syntax) with multiple semantics that explore the design space for a feature. All in all, Justin implemented 11 languages, with a total of 35 semantics. As the course moves forward, each language builds incrementally on the previous one, and each assignment introduces new syntactic features, requiring students to explore how new ones interact with old ones. Given a set of variants of a mystery language, a student’s task is to apply adversarial thinking to figure out how the variants differ, and then explain and categorize the differences.
"They've been a lot of fun to develop," Justin says, "and students seem to enjoy them too. They help teach you how to explore a programming language, which isn't something that's otherwise taught."
Professor Stefanie Tellex of Brown University's Department of Computer Science (Brown CS) has just received an Early Career Research Achievement Award from Brown's Office of the Vice President for Research. Inaugurated last year, the Awards are given in three areas (Arts, Humanities, and Social Sciences; Physical Sciences; and Life Sciences and Public Health) to a member of the faculty at the Assistant Professor level, or who was promoted to Associate Professor in the previous academic year, in recognition of an exemplary portfolio of research achievement during their first years at Brown. Stefanie joins Brown CS Adjunct Professor Tim Kraska, who won last year's Award in the Physical Sciences category. The Award is accompanied by a research stipend of $5,000.
Stefanie was cited for her artificial intelligence research into new approaches for human-robot communication. Her research group, the Humans to Robots Lab, aims to empower every person with a collaborative robot partner. She has won many top-tier awards and fellowships, including an NSF CAREER Award, Sloan Fellowship, and a DARPA Young Faculty Award. She has published more than 18 technical papers, with many in the top-tier venues of her research area. One of them ("Asking for Help Using Inverse Semantics") received a Best Paper Award. She has also secured significant grant funding from federal and industry sources, and her research has been covered by sources that include NPR, the New Yorker, Wired, and IEEE Spectrum. Stefanie also leads efforts to build community in her field, founding the Northeast Robotics Colloquium that annually brings together roboticists of all varieties from across the northeastern United States.
In the photo above: Richard M. Locke, Stefanie, Huajian Gao, Rose McDermott, Rena Wing, Jill Pipher
École Polytechnique Fédérale de Lausanne (EPFL) has recently made "computational thinking" part of its curriculum for all students, and last month, they held the Teaching Computational Thinking Workshop to begin addressing questions about how the subject can and should be integrated into scientific and engineering programs.
As one of two keynote speakers (the other was Mark Guzdial of Georgia Institute of Technology), they invited Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS), who lectured on the idea of curriculum design as an engineering problem, and an extremely difficult one. "It requires substantial work," he writes, "on both technical and human-factors fronts; the audience is often unsophisticated and vulnerable; and if you screw up, you can do real damage to not only individuals but also the field and society."
The talk at EPFL (video available here) was the first of Shriram's two recent lectures in Europe. He also spoke at the Programming 2018 conference in Nice.
"People learn at different speeds and use different methods to reinforce what they are learning," writes Vinton G. Cerf in the latest issue of Communications of the ACM. Looking for pedagogy that breaks with tradition, the Internet pioneer finds inspiration in Bootstrap, a K-12 CS curriculum used worldwide that's directed by Kathi Fisler, Shriram Krishnamurthi, and Emmanuel Schanzer of Brown CS. "What makes Bootstrap...so memorable for me," he writes, "is that the team has focused heavily on accessibility."
Intrigued by a coding demonstration that includes an audible depiction of the programmer's current status in the program, Cerf is reminded that a need for accessibility parallels the necessity of an educational approach that adapts to the learner: "What seems more important about the work at Bootstrap...is the potential to provide students closer to STEM learning with tools that move at the same pace at which the students can move."
"Designed for this kind of adaptability and flexibility," Cerf writes, Bootstrap is an example of self-paced learning with varying reinforcement methods. And the need for this new approach, he thinks, is real: "This sort of adaptable and convenient online learning is sure to be a part of 21st-century careers."
This week, Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS) continues a busy season of domestic and international lectures by delivering a keynote address ("The Recurring Rainfall Problem") at the International Conference on the Art, Science, and Engineering of Programming (Programming 2018) conference in Nice, France. Now in its second year, Programming is focused on various programming topics, including the experience of programming. Shriram will be speaking on Wednesday, April 11, at 9 AM, and his talk includes the unusual feature of requiring the audience to write programs in real time.
The Rainfall problem, developed by Elliot Soloway, is a classic benchmark in computing education research, designed to study plan composition: how students decompose problems into tasks, solve them, and compose the solutions. "Over multiple decades, locations, and languages," Shriram explains, "students have done poorly at it. However, recent multi-institution studies of students using functional programming and How to Design Programs find very different outcomes. What do these results tell us?" His talk, based on his work with Brown CS Professor (Research) Kathi Fisler, will explore relationships between programming languages, program design, curricula, and how students perceive code structure.
Most recently, Shriram spoke at École Polytechnique Fédérale de Lausanne (video available here).
For decades, we've heard from our undergrads that the chance to perform cutting-edge research is one of the primary reasons why they chose Brown University's Department of Computer Science (Brown CS). Far from the clerical duties or errand-running that some have encountered elsewhere, we offer the chance to work with world-class faculty in a truly collaborative environment on problems of real worth. Just in the past six months, one of our undergrad researchers was chosen as one of 25 students from a pool of 6,432 for an Undergraduate Award and two others won CRA Outstanding Undergraduate Researcher Honorable Mentions.
During the summer, Brown's Undergraduate Teaching and Research Awards (UTRAs) provide students with a $3,750 stipend for their work with faculty. This year, Brown CS has received a record number of them, and they reflect both the growth of computer science at Brown and the breadth and depth of our research:
We also have a number of Interdisciplinary Team (I-Team) UTRAs as part of a program designed to increase research opportunities for a diverse range of first and second year students, encourage student intellectual growth through peer mentorship and group learning, and encourage interdisciplinary scholarship between and among Brown faculty: 
We're looking forward to a great summer, and we can't wait to share the stories of their results in the days ahead. 
Less than a year after Professor Theophilus (Theo) A. Benson's arrival at Brown University’s Computer Science Department (Brown CS), and less than two after Professor George Konidaris joined the Brown CS faculty, they've each received a Richard B. Salomon Faculty Research Award. A full list of awardees is available here. The award, given annually by Brown’s Office of the Vice-President for Research, was established to support excellence in scholarly work by providing funding for selected faculty research projects of exceptional merit with preference given to junior faculty who are in the process of building their research portfolio. They join multiple previous Brown CS winners, including Jeff Huang, Stefanie Tellex, Rodrigo Fonseca, Sherief Reda, and Ugur Cetintemel.
"Mobile devices," says Theo when asked to explain his research, "have become the primary mode of Internet access in both developed and developing countries. Yet, in developing regions, driven by their low cost, mobile devices are often equipped with small memory sizes and slow CPUs. For example, according to a recent study of mobile devices in Pakistan, 90% of devices were equipped with at most 1024MB of RAM, and 89% had 1GHz or slower processors. Despite the prevalence of such low-end devices in these markets, there are few systematic studies of the differences between web performance on low-end and high-end smartphones. Additionally, we lack principled techniques to analyze and improve performance on these low-end phones."
The goal of his proposal is to fill that vital gap:
George explains his research as follows: "Solar panel tracking improves the solar panel energy production by pointing them towards the sun throughout the day. Existing tracking algorithms compute the location of the sun in the sky via astronomical calculations, and move solar panels to match that angle. However, these calculations do not account for reflective and diffuse radiance or weather conditions that impact the efficiency of tracking algorithms. We reframe the problem of collecting energy from the sun as a contextual bandit problem, where the solar panel is controlled by a learning program attempting to maximize the 'reward' (i.e. energy) it collects. A small working group of graduate and undergraduate students at Brown has performed extensive simulated experiments in a variety of scenarios, indicating that in many locations on the Earth contextual bandit approaches outperform existing baselines. The group has also constructed a small, low-cost prototype of a single-axis solar tracking system, leading to preliminary results and publications accepted to RLDM, EnviroInfo, and IAAI conferences."
They propose three goals:
George and Theo will receive their awards at Brown's annual Celebration of Research on Thursday, April 19th, at the Faculty Club.
Research by Professor Theophilus "Theo" A. Benson of Brown University's Department of Computer Science (Brown CS) and his collaborators has just won one of the Internet Research Task Force (IRTF)'s six Applied Networking Research Prizes for 2018. It's also one of only two papers that will be presented at the IRTF Open Meeting during the IETF-1 conference in London next month. Out of 55 eligible nominations, the selection committee chose Theo's work on the grounds of scientific excellence and substance, timeliness, relevance, and potential impact on the Internet.
The research ("Performance Characterization of a Commercial Video Streaming Service"), which Theo conducted while at Duke University in 2016 with lead author Mojgan Ghasemi (Princeton University) and co-authors Partha Kanuparthy (Yahoo Research), Ahmed Mansy (Yahoo), and Jennifer Rexford (Princeton University), offers detailed analysis of a commercial video streaming service and the problems that impact its performance.
"Our work," Theo explains, "extends the community's understanding of well-accepted approaches to diagnosing video performance. In particular, while existing research focuses on diagnosing problems with information available at either the client, the server, or the ISPs packet, this extensive study demonstrated that a strong and prevalent class of performance problems can only be discovered if data from both clients and servers are collected and systematically analyzed. Building on these observations, we worked with the developers of Apache Traffic Server, a prominent web server that's being actively used by many Fortune 500 companies, including Yahoo, Comcast, Akamai, and LinkedIn, to introduce fixes and patches."
In the ongoing effort to bring diversity and inclusion to the field of computer science, it was one step backward and then at least two in a profoundly positive direction. When the world's largest gathering of security experts, RSA Conference, announced this year's speaker list last week, many attendees were stunned and disappointed to find that only one woman was included. That disappointment was quickly channeled into action, and within a few days, a group of computer scientists had put together OUR Security Advocates (OURSA), a competing conference with a diverse set of speakers, including Professor Seny Kamara of Brown University's Department of Computer Science (Brown CS).
OURSA will be held on April 17, 2018, in San Francisco, and include four topic sessions (Advocating for High-Risk Groups, Applied Security Engineering, Practical Privacy Protection, and Security Policy and Ethics for Emerging Tech), each with short talks from multiple experts followed by a moderated discussion.
"The speaker list from RSA caused a lot of frustration," says Seny, "because there are a lot of really accomplished women in security that they could have asked. It’s disappointing but I'm really impressed by the OURSA organizers, who put together a stellar lineup of people almost overnight. I'm very happy to be a part of it." 
OURSA has already received a great deal of positive media coverage, including CNET, USA Today, Gizmodo, and the BBC. 
The Kleiner Perkins Caufield and Byers (KPCB) Engineering Fellows Program is one of technology’s most prestigious fellowships, with only 51 students chosen in 2018 from a pool of almost 3,000. This year, undergraduate students Lauren Ho and Nina Polshakova were selected from Brown University's Department of Computer Science (Brown CS), and they join winners from previous years that include Michael Chang, Michael Markell, Noah Picard, and Hannah Tipperman.
Despite the program's title, it's open to outstanding students who are studying computer science, engineering, mathematics, physics, or other fields related to software development: Lauren and Nina are both computer science concentrators. They'll join other students from across the country in Silicon Valley for a work experience (participating companies span the industry and the alphabet from Airbnb to Zumper) that will be supplemented with events and programming led by CEOs and executives from KPCB portfolio companies and KPCB Partners.
Brown University's Department of Computer Science (Brown CS) is happy to announce that pending the anticipated approval of the Corporation of Brown, Amy Greenwald will be promoted to the rank of Full Professor and Seny Kamara will be promoted to Associate Professor with tenure, effective July 1, 2018.
Amy joined Brown CS on January 1, 2000. Her research has two goals: first, the effort to design and implement AI agents that interact effectively in multiagent environments where the other agents might be humans and might sometimes exhibit surprising behavior; second, the effort to understand, explain, and accurately predict the long-term and short-term dynamics of such interactions. Recently, she gave a keynote address at Microsoft's Faculty Summit; she also leads the Brown CS chapter of Google's igniteCS initiative, in which university student volunteers teach basic computer science literacy to K-12 students in local, underserved public schools.
Seny joined Brown CS in 2016 after working for Microsoft Research. His work is in applied cryptography and is driven by real-world problems from privacy, security and surveillance. His primary focus is on the design and analysis of encrypted algorithms, which are efficient algorithms that operate on encrypted data. He maintains interests in various aspects of theory and systems, including applied and theoretical cryptography, data structures and algorithms, cloud computing, data management, economics, technology policy and networking. Recently, he co-authored an NAS report on encryption and co-created Pixek, an app that offers searchable encryption for digital photos.
"Though my head is often in security, networking, formal methods, and HCI," says Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS), "my heart is in programming languages." He now gets to share his latest thoughts on the subject with a wide audience: he and six co-authors have made the cover of Communications of the ACM with an article ("A Programmable Programming Language") that evaluates the current state of the 20-year-old Racket project and looks ahead to what's next.
In the article, Shriram and his co-authors focus on the need of programmers to create and use embedded program-specific languages as part of software development. The Racket project, which began in 1995, started as an experiment to create a language for experimenting with pedagogic programming languages, but developed into a multi-lingual system with one guiding principle: Empower programmers to create new programming languages easily and to add them with a friction-free process to a code base.
Advances such as Racket's modular syntax system have been made, the authors explain, but areas such as security and dynamic checking have only been partially developed. In the end, they note, "Programming language research has stopped short of the ultimate goal, namely, to provide software developers with tools to formulate solutions in the languages of problem domains." More challenges remain to be solved, and so the Racket project goes on.
You can read the full article on the CACM website.
Just over a year ago, Professor Seny Kamara of Brown University's Department of Computer Science (Brown CS), along with 13 other experts from academia and industry, was appointed to a new committee created by the National Academy of Sciences (NAS). The committee ("Law Enforcement and Intelligence Access to Plaintext Information in an Era of Widespread Strong Encryption: Options and Tradeoffs") is part of the organization's Division on Engineering and Physical Sciences, and it's been working to examine the tradeoffs associated with mechanisms to provide authorized government agencies with access to the plaintext version of encrypted information.
Last week, the committee released their first report ("Decrypting the Encryption Debate: A Framework for Decision Makers"), with the goal of better informing the policy debate and future decision making on encryption. The report covers the entire spectrum of how encryption is used, including:
In an area where there's little consensus among government officials, privacy advocates, and industry and academic experts, it's an attempt to create the language and shape the outlines of a dialogue that will inform leading policymakers worldwide. "The committee hopes," they write, "that the common vocabulary and broad context provided by this report, as well as the analytical framework, will make future conversations easier, more productive, and more likely." 
"Originally, this class was intended to be a quiet graduate seminar with about ten people," says Professor Maurice Herlihy of Brown University's Department of Computer Science (Brown CS). He's talking about a class of his own design (CSCI 2952-A Blockchains and Cryptocurrencies) that's one of a very few courses dealing with the subject nationwide, particularly from a scientific perspective. It had to be capped at 60 students this semester. "I was impressed," he says, "by how eager students were to look past the sensationalism and approach this from the scientist's point of view, finding the substance underneath the hype."
A blockchain is a cryptographically-secured series of records usually managed by a peer-to-peer network that allows transactions between parties to be verified and permanently recorded. At present, Bitcoin is easily the most well-known. Bitcoin is a cryptocurrency, which is a type of digital currency that uses cryptography to secure transactions. Cryptocurrencies and the other forms that blockchains take are a natural fit for Maurice's expertise in distributed systems.
"In a certain sense, blockchains are the ultimate distributed systems," he explains. "They're a very productive approach, and I think it's fair to say that blockchain creators are helping reinvent distributed computing. But they don't always get it right."
"Because blockchains are a new area," Maurice says, "we're seeing many promises that can't be kept. People are being led, naively or sometimes fraudulently, to invest money or effort in projects that can't possibly work." What will work, he thinks, is a revolution of how trust operates in distributed systems, producing results not in the cryptocurrency realm but in areas such as financial trading transparency, anti-corruption efforts, or even verifying the origins of seafood.   
"I believe that's the future of blockchains, and my students have really enjoyed getting a new perspective and depth on them. Instead of being a get-rich-quick scheme, I think they'll be turned to purposes that are increasingly challenging, important, and technically sophisticated, and Brown's interdisciplinary strengths are a perfect fit to organize something lasting on the subject."
To see the slides (PPTX format) for Maurice's keynote ("Blockchains and the Future of Distributed Computing") from the 2017 ACM Symposium on Principles of Distributed Computing (PODC), click here.
Professor Elizabeth Mynatt of Georgia Institute of Technology visited the CIT last week to address Brown University's Department of Computer Science (Brown CS) and deliver the thirty-fifth lecture in the Distinguished Lecture Series. Using a retrospective of her research projects as a foundation, the talk ("Rethinking Ubiquitous Computing to Transform Healthcare") demonstrated how an integration of computing research, human-centered design, and health management theory can create promising approaches for promoting wellness, supporting behavior change, and delivering improved health outcomes.
"The unique capabilities of pervasive computing technologies have the potential to transform healthcare," she explained, "by shifting care from institutional to home settings, by helping individuals engage in their own care, by facilitating problem solving and decision making, and by creating a network of communication and collaboration channels that extends healthcare delivery to everyday settings."
From the first research project Mynatt mentioned (the Digital Family Portrait, which she cited as an example of the difficulty of visually conveying how a person is doing), questions were frequent and enthusiastic. When she spoke of her work inadvertently predicting the attention economy, the audience was ready with a light-hearted response: "So you're responsible, then!"
In each of her examples, Professor Mynatt returned to the idea that our health is personal, social, and negotiated, and that technological interventions often have unintended consequences. "We have to remember," she said, "that the social implications of our work are very hard to predict." But she spoke enthusiastically about the joy of helping create "new cycles of mastery" for diabetics, or the moment when the technological aspect of a solution became invisible, allowing trusted relationships to continue. Many of her research's best discoveries, she explained, came from starting not from a medical school mentality but a family and home mentality. 
Professor Jeff Huang, who was in attendance, appreciates that approach. "It was exciting," he said, "to hear from Professor Mynatt, who is thinking about accessible health data from a patient-centered perspective, rather than a traditional provider-centric solution. I heard from several students who were inspired by her ability to tell a compelling story around her research projects, sprinkled with witty sci-fi references."
You can view a recording of the lecture here.
The Computing Research Association (CRA) is a coalition of more than 200 organizations with the mission of enhancing innovation by joining with industry, government and academia to strengthen research and advanced education in computing. Every year, they recognize North American students who show phenomenal research potential with their Outstanding Undergraduate Researcher Award, and last year, Brown University's Department of Computer Science (Brown CS) made one of the strongest showings in the Honorable Mentions category. Out of forty-seven students who received Honorable Mentions, three of them are Brown CS students: Siddharth Karamcheti, Sorawee Porncharoenwase, and Eric Rosen.
Asked about his work, Siddharth says, "My primary areas of research are natural language processing and machine learning – I specifically focus on problems dealing with how we use language to interact with and understand the world around us. I have two big projects that I’ve worked on the last couple of years, under the supervision of Professor Charniak and Professor Tellex. The first is in human-robot interaction: how do we get robots to follow natural language instructions? The second big project is focused on question-answering: how do we develop models that can read and understand short stories, perform reasoning, and answer complex questions? Outside of natural language, I also work on problems on combining machine learning with automated program testing, or 'fuzzing'. The idea here is to use methods from reinforcement learning in conjunction with existing tools for program analysis and testing to find inputs that crash or unearth bugs in the programs under test, in a manner that is faster and more efficient than existing fuzzers." 
"My work (with Tim Nelson and Shriram Krishnamurthi)," says Sorawee, "improves model finders, which are programs producing a set of models that satisfy input constraints. Commonly used in software engineering to explore and verify systems' behavior, model finders however could inundate users with models that don't offer more useful information. We explored using syntax to evaluate how 'interesting' a model is, and investigated how to find a set of these interesting models. The result is a model finder that produces a more concise and useful set of models."
Eric wanted to share some links to four different blog posts about his research. The first three, he explains, are paper publications, and the fourth is an introductory research project from his first year at Brown: 
You can see the full list of Outstanding Undergraduate Researcher winners here. Congratulations, Siddharth, Sorawee, and Eric!
by Kevin Stacey (Science News Officer, Physical Sciences)
Researchers from Brown University and MIT have developed a method for helping robots plan for multi-step tasks by constructing abstract representations of the world around them. Their study, published in the Journal of Artificial Intelligence Research, is a step toward building robots that can think and act more like people.
Planning is a monumentally difficult thing for robots, largely because of how they perceive and interact with the world. A robot’s perception of the world consists of nothing more than the vast array of pixels collected by its cameras, and its ability to act is limited to setting the positions of the individual motors that control its joints and grippers. It lacks an innate understanding of how those pixels relate to what we might consider meaningful concepts in the world.
“That low-level interface with the world makes it really hard to do decide what to do,” said George Konidaris, an assistant professor of computer science at Brown and the lead author of the new study. “Imagine how hard it would be to plan something as simple as a trip to the grocery store if you had to think about each and every muscle you’d flex to get there, and imagine in advance and in detail the terabytes of visual data that would pass through your retinas along the way. You’d immediately get bogged down in the detail. People, of course, don’t plan that way. We’re able to introduce abstract concepts that throw away that huge mass of irrelevant detail and focus only on what is important.”
Even state-of-the-art robots aren’t capable of that kind of abstraction. When we see demonstrations of robots planning for and performing multistep tasks, “it’s almost always the case that a programmer has explicitly told the robot how to think about the world in order for it to make a plan,” Konidaris said. “But if we want robots that can act more autonomously, they’re going to need the ability to learn abstractions on their own.”
In computer science terms, these kinds of abstractions fall into two categories: “procedural abstractions” and “perceptual abstractions.” Procedural abstractions are programs made out of low-level movements composed into higher-level skills. An example would be bundling all the little movements needed to open a door — all the motor movements involved in reaching for the knob, turning it and pulling the door open — into a single “open the door” skill. Once such a skill is built, you don’t need to worry about how it works. All you need to know is when to run it. Roboticists — including Konidaris himself — have been studying how to make robots learn procedural abstractions for years, he says.
But according to Konidaris, there’s been less progress in perceptual abstraction, which has to do with helping a robot make sense of its pixelated surroundings. That’s the focus of this new research.  
“Our work shows that once a robot has high-level motor skills, it can automatically construct a compatible high-level symbolic representation of the world — one that is provably suitable for planning using those skills,” Konidaris said.
Learning abstract states of the world
For the study, the researchers introduced a robot named Anathema Device (or Ana, for short) to a room containing a cupboard, a cooler, a switch that controls a light inside the cupboard, and a bottle that could be left in either the cooler or the cupboard. They gave Ana a set of high-level motor skills for manipulating the objects in the room—opening and closing both the cooler and the cupboard, flipping the switch and picking up a bottle. Then they turned Ana loose to try out her motor skills in the room, recording the sensory data from her cameras and actuators before and after each skill execution. Those data were fed into the machine-learning algorithm developed by the team.
The researchers showed that Ana was able to learn a very abstract description of the environment that contained only what was necessary for her to be able perform a particular skill. For example, she learned that in order to open the cooler, she needed to be standing in front of it and not holding anything (because she needed both hands to open the lid). She also learned the proper configuration of pixels in her visual field associated with the cooler lid being closed, which is the only configuration in which it’s possible to open it.
She learned similar abstractions associated with her other skills. She learned, for example, that the light inside cupboard was so bright that it whited out her sensors. So in order to manipulate the bottle inside the cupboard, the light had to be off. She also learned that in order to turn the light off, the cupboard door needed to be closed, because the open door blocked her access to the switch. The resulting abstract representation distilled all that knowledge down from high-definition images to a text file, just 126 lines long.
“These were all the important abstract concepts about her surroundings,” Konidaris said. “Doors need to be closed before they can be opened. You can’t get the bottle out of the cupboard unless it’s open, and so on. And she was able to learn them just by executing her skills and seeing what happens.”
Planning in the abstract
Once Ana was armed with her learned abstract representation, the researchers asked her to do something that required some planning: take the bottle from the cooler and put it in the cupboard.
As they hoped she would, Ana navigated to the cooler and opened it to reveal the bottle. But she didn’t pick it up. Instead, she planned ahead. She realized that if she had the bottle in her gripper, then she wouldn’t be able to open the cupboard, because doing so requires both hands. So after she opened the cooler, she navigated to the cupboard. There she saw that the light switch in the “on” position, and she realized that opening the cupboard would block the switch, so she turned the switch off before opening the cupboard, returning to the cooler and retrieving the bottle, and finally placing it in the cupboard. In short, she planned ahead, identifying problems and fixing them before they could occur.
“We didn’t provide Ana with any of the abstract representations she needed to plan for the task,” Konidaris said. “She learned those abstractions on her own, and once she had them, planning was easy. She found that plan in only about four milliseconds.”
Konidaris says the research provides an important theoretical building block for applying artificial intelligence to robotics. “We believe that allowing our robots to plan and learn in the abstract rather than the concrete will be fundamental to building truly intelligent robots,” he said. “Many problems are often quite simple, if you think about them in the right way.”
Konidaris’ coauthors on the paper were Leslie Pack Kaelbling and Tomas Lozano-Perez from MIT. The research was supported by an award from the Defense Advanced Research Projects Agency and by MIT’s Intelligence Initiative. 
You can watch videos of both keynotes here: http://bit.ly/2wajwPu and http://bit.ly/2hrxAkx
You can read Bertram’s paper here: http://bit.ly/2fn7yyd
You can watch videos of ten SIRoS 2 talks here: http://bit.ly/2uz2Q2c
Security breach by security breach, data leak by data leak, digital privacy is becoming a household phrase, but for many people, putting a four-digit passcode on their smart phone is the extent of their security measures. Just this week, researchers from Brown University's Department of Computer Science (Brown CS) have helped users of all kinds significantly improve their privacy in the digital world by targeting one of its most popular components: cloud photo storage. Professor Seny Kamara, Postdoctoral Researcher Tarik Moataz, and Brown CS alum Martin Zhu have just released Pixek, a new app that uses structured encryption to provide a simple solution for keeping our photos private while in the cloud. 
"My sense is that photos are this special case," Seny says, "where people have to use the cloud because the sentimental value is too high to risk losing them and the storage costs are too large. And they give up privacy because of it."
Pixek protects users by offering encryption for the entire life cycle of a photo: when you take a photo on your smart device, the app immediately encrypts it, generating a key that only exists on your device, and then the photo is stored in encrypted form on Pixek's servers. Because the key stays only on your hardware, it remains encrypted and inaccessible to anyone else.   
So now you have a hundred, a thousand, ten thousand encrypted photos. But what's the use of encryption if you can't securely search them without giving up privacy once again? Pixek solves this problem by using a technique known as structured encryption, which allows search functions to be performed without the system understanding the work that's being done. 
Here's how it happens: Pixek automatically uses machine learning analysis to recognize objects and elements in your photos, then generates a series of tags that are encrypted alongside the photos. When you search for "cat" or any other tag, Pixek uses your unique key to encrypt the search and generates a unique token to unlock relevant photos. Without the token, it's impossible to replicate the same search, and because the token isn't stored on servers, you retain control.   
"People today know what end-to-end encryption is," Seny says. "They're starting to have an expectation that their apps are end-to-end encrypted. At some point people will expect that their photos will be end-to-end encrypted, too."
Pixek is only being distributed in its alpha version on Android at the moment, but a public beta will appear in the months ahead and an iOS version will follow. It's already received considerable attention worldwide, from an article in WIRED to a feature on CBC Radio and a BoingBoing post. You can also watch a video of Seny talking about building and deploying encrypted search systems at the Real World Crypto conference.
The Research Highlights section of Communications of the ACM (CACM-RH) is devoted to publishing the most important and interesting CS research of recent years. Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS), already a member of CACM-RH's Editorial Board, has become co-Chair of the group along with Azer Bestavros of Boston University.
Chosen for the depth of his expertise and breadth of his research areas, Shriram will lead the effort to identify and nominate papers for consideration, assess nominations that fall within his interests, help govern CACM-RH internally, communicate with authors, and facilitate the creation of the technical perspectives that accompany articles. 
CACM-RH is comprised of 26 thought leaders in the field from both academia and industry, a list of luminaries that runs from Abadi to Zeller. Former members of the Editorial Board include Professor Maurice Herlihy of Brown CS and Brown CS alum David Salesin.
This story was originally published in the December 2017 issue of Conduit, the annual Brown CS magazine.
It’s half past noon in CIT 368 on May 18, 2017. After fourteen years of Senior Lecturer Barbara Meier’s computer animation classes at Brown, after months of focused effort, we have something very new, a first: Toymaker, a seven-minute animated short.
Every seat is full, and much of the floor. The lights are dimmed. In darkness, there’s room for multiplicity. The student who started the whole thing is here with her team of thirteen. There’s the teacher and mentor who didn’t just advise but worked the long hours alongside them; here’s the composer who wrote the score and helped perform it. Classmates from RISD as well as Brown are gathered, friends and friends of friends, and the colleagues and experts who are maybe the only ones to fully understand the scope of what’s been accomplished.
But why subdivide the ineffable? The flickering screen makes us all spectators again. We’re drawn to the high spire, the telescope, to humankind’s mastery of art or science on the grandest scale, but also to the personal and small: the studio in all its clutter, the Wozniakian garage, or here, the string of fleeting moments that tell a much bigger story.
We’re ready to wonder again. Barb’s about to hit the ► button.
Take a moment to see it for yourself: https://vimeo.com/242488116
Not Knowing What The Recipe Would Make: Barbara Meier
Before story, history: Barb traces her interest in animation to when she was twelve or thirteen. Her parents disapprove of most daytime television, so she watches when they aren’t home, and is awed by the hand-drawn work that she sees in an animated film festival. At first, a career as an animator seems as unlikely as becoming an Olympic skater, but in the years that follow, Barb finds herself returning to the idea that animation is just drawing on paper. “It didn’t have to be this mysterious process,” she says. “I felt like it was an actual possibility.”
But there’s plenty of mystery to come along a zigzagging career path. Studying computer science as a Brown undergrad, Barb sees the potential for using computer graphics (CG) in animation and is drawn to auteur animation’s ability to realize the vision of a single artist. She credits Andy van Dam for encouraging her to apply to the Brown CS Master’s program, and postgraduate work follows at RISD, the School of the Museum of Fine Arts in Boston (their curriculum replicates the working life of an artist, an idea that Barb returns to later in her own teaching), and the Art Center College of Design in Pasadena.
Industry work catches Barb’s attention first, and she spends almost a decade in California, creating animated effects and digital matte paintings. It isn’t easy: long hours, constant pressure to work faster and lower costs, extremely narrow margins. “I knew it wasn’t going to sustain me forever,” she says. “I wanted kids, a life. It was a really great ride, but I thought I could take what I’d learned everywhere and bring my industry experience to students, even if I didn’t know what the recipe would make.” A book about making a career change had recommended teaching, and with more than a little hesitation (Barb remembers saying, “No, not that! That’s for people who are washed-up.”) she returns to Brown.
Barb agrees to take on CS195-9 (now CS 125) in mid-July of 2003. Beyond any doubt, it’s trial by fire. She remembers frantically picking software for the course in August, trying to prepare lectures with two small boys running around the house, finishing her notes an hour before the class begins. She says, “The one thing I think I did well was setting up a structure and creating assignments. Right away, I saw the students doing really good work, exceeding expectations on every task I gave them.” Graduates of the course often continued working with Barb independently, which sparked the need for a second course, CS 128 Intermediate Computer Animation. One trend continues year after year: students are coming to her with more interest in animation and better experience with it, so she continually moves content from the second course to the first.
CS 125 presents students with the new challenge of creating a short film at the end of the course. “But they’re really bare-bones,” Barb says, smiling. “It's an important project because it forces students to tackle all parts of the film, even those they've barely practiced. If they pause to polish one area, they'll never finish the rest. CS 128 addresses the desire to go back and do it right, to learn the artistic and technical skills at a deeper level. The price is that lavishing more time on every stage of production makes it impossible to finish a short film at the end. The ultimate dream is to flex the deeply-honed skills to finally make a high-quality short film.”
Occasionally, some students follow that dream.
Portraits From Life: Nellie Robinson
“One of the main things I got out of CS 125,” says Nellie Robinson, “was working as a team on our one-month short. I wanted to repeat the experience.” That was the start of Toymaker.
Since graduating in May (she has a dual degree from RISD), Nellie has been working at Ingenuity Studios in Los Angeles. As a kid, we find her working with crafts (turning tissue boxes into apartment buildings, building a tiny playground out of concert tickets) and drawing portraits from life: “I always enjoyed drawing, but I never thought about how it could be applied to other things.” She doesn’t take any CS courses in her first year at Brown and RISD, but learns Processing, a programming language built on Java for the art and design community. “I really liked looking at the output of code I’d written, the visual aspect.” she says. “Maybe I was drawn to computer animation because it’s squarely between the technical and the artistic.”
That summer, Nellie interns at Pixar, and CS 125 follows. It’s motivation enough for a big project of her own, so she goes to Barb in November of 2015 with a pitch for a new class. The reaction?
“Ugh, not again!” Meier laughs, pantomiming despair by putting a hand to her forehead and slowly lowering her head to her desk. In the past, students have asked her for a third class, to work on bigger projects: it was offered once, in 2011. “I was wearing too many hats, producer and director, and it was frustrating in many ways. Unintentionally, we set ourselves up to fail. When Nellie came to me, I gave her a choice, and when she picked a class instead of a group independent study project, I set very high bars: lots of monitoring throughout the process, at least ten people on the team, and having one person as the sole producer.”
And so it begins. “At the start,” Nellie says, “it felt entirely reasonable.”
Spring And Summer: Pre-Production
Take a look at Toymaker again, focusing on nothing but the textures: the dull plastic handle on the cheap pair of scissors that we all own, clumped and gummy glue, the glint of a paperclip. But there’s more. Look closer. See how paint feathers at its edges into woodgrain, or how age and reuse turn a soda bottle’s perfect transparence into translucence, or –quick, you only have seconds to admire it– the hundreds of crisscrossing scores that a utility knife makes in the slightly nubbly rubber of a tabletop-protecting mat.
In the spring of 2016, Robinson has her story, so she brings it to Megan Jerbic of RISD, who begins turning it into a script. Nellie starts working with other RISD friends (Mariel Rodriguez and Julie Kwon on character design, Michelle Zhuang and Yoo Jin Shin on set design), and soon it’s time for choosing color palettes and designing objects that will make up the Toymaker world. “The main feed for the visuals,” she says, “was the DIY mentality. I’m really interested in repurposing objects, turning trash into fantasy worlds.” As a possible aesthetic influence, Nellie cites the stop-motion work of Laika Entertainment: “Objects have flaws. There aren’t a lot of perfect perpendiculars, it’s kind of wonky – you can tell the materials of everything.”
Her original idea for the cast of characters is an old man and his daughter: “That was the first thing that came to mind, but the artists pointed out, and I agreed, that it was a trope that’s been done a lot, the old craftsman neglecting his kids. "Turning the toymaker into a working single mother is more nuanced and relevant. We're instantly aware of her work/life balance, and this places more importance on the difference between providing for her daughter and connecting with her daughter.”
It’s a notion that makes it easy to understand how Nellie’s mentor might have been drawn to the project. “I want to change how people look at the seemingly small issues of everyday life,” Barb writes in a recent Artist Statement, “but those that affect us and our relationships in some of the more profound ways.”
Summer: Much More Than A Community
Summer arrives, and Nellie has managed to gather enough people to meet Barb’s requirement.
“If I’d stopped to think about it,” says Kenji Endo (now Head TA for CS 125, he’s graduating in the spring of 2018), “maybe I wouldn’t have thought Toymaker was feasible. But it was so cool! Nellie has a great artistic eye and technical skill. And the people drew me in, the sense of community.”
It’s time to meet the team. Thanks to prior work with Barb, the students know the production pipeline of animation inside and out: Meier immediately assigns each phase of the project to one or two leaders and possibly a “worker bee” or two under their direction. Each leader has to define their job and their team’s goals. The poster for Toymaker lists everyone in alphabetical order: Luci Cooke, Dash Elhauge, Kenji Endo, Felege Gebru, Emma Herold, Simon Jones, Barbara Meier, Vivian Morgowicz, Ray Muñoz, Ben Nacar, Emily Reif, Nellie Robinson, Yoo Jin Shin.
Megan Jerbic, Julie Kwon, Mariel Rodriguez, and Michelle Zhuang also contributed to the script, storyboards, character design, prop design, and set design; Melanie Ambler and Irene Tang accompanied Ben Nacar on the Toymaker score.
“I think Nellie said it in a Facebook post,” says Barb. “The whole reason wasn’t to make this film but to make it with these people. The students were already a community from taking classes together, but they were much more than that at the end. The hard work came out of not wanting to let each other down.”
At the beginning, did it ever feel too ambitious, even impossible? Nellie thinks for a moment: “I’m not sure, but if it did, that feeling definitely increased as time went on. When something is pushed back a few days, it pushes back everything else. The whole project felt slightly more unreasonable each week.”
Fall: Modeling, Shading, Character Modeling, Rigging, Layout, Blocking
And the weeks are already going by. For a visual, picture the MS Lab, which serves as home base. No matter which hour of the day or night it may be, Nellie and the others have gotten used to seeing at least one member of the Toymaker team at work whenever they walk in.
The leaders have set up the assignments for their worker bees, but sometimes all hands are needed. Modeling is one of the biggest pieces of the puzzle, and it involves creating the geometry of everything in the set based on sketches, then applying textures, color, surface properties. Set dressers are putting the props into place, just like they’re moving into a new apartment: walls, doors, shelves, down to the last detail. The shading of an object depends on where it sits in the set and how it’s used. The spine of a book on a shelf that never gets read doesn’t take much time, but the shaders kick into high gear on certain objects that drive the story, that we see again and again.
Kenji says that he likes tackling technical challenges from an artistic angle. “We’re making everything in this world from scratch,” he says. “We’re responsible for time of day, the mood. The architecture of the world helps tell the story.”
The layout process turns the storyboard into shots: it could be a one-to-one ratio, or a shot might be made up of as many as six storyboard panels. Blocking, the positioning of characters, begins with all of them in T-pose, with their arms straight out. There’s not an actual camera to be found, of course, but the team is agonizing over close-ups and long shots like any Hollywood director.
Eventually, the storyboard is replaced with quickly-rendered blocking shots. “It’s really tough,” Barb says, “because what you have is crude and what you want is very high-level. It’s like editing a paper to adjust the arguments before the grammar is completely there.” Character modeling is another big technical challenge: it’s a lot more complicated to make a person than a coffee cup. They might be too tall, too old, too cute, and soon they’ll have to move, so their underlying geometry has to have integrity. The rigging process builds an armature inside the characters, giving the animators controls to grab onto.  
At various points in time, Nellie explains, many of the students are working on the project almost every day. On many nights and weekends, Barb works from her home in Barrington, her coffee intake slowly increasing; at other times, the students wrestle the file system and work remotely, separately. But every week starts the same way: a look at everyone’s work, two and a half hours of detailed critiques. “It was really fun to go in depth in our reviews every week, to iterate and improve over time,” says Kenji. “Everyone got to specialize in their favorite area, but we all learned more about each component of the project, even if we weren’t working on it.”
“I loved that part,” Nellie says. “I was surprised every week by what people were doing – you got to see a little bit of magic.”
The Music Drives Things: Ben Nacar
Brown CS alum Ben Nacar is one of the first people to join Nellie’s team: Barb emails him an invitation in May, 2016. (She knows him from his work on an advisee’s capstone project.) His contributions begin early, but we deliberately turn to him just now, in the middle of the action. Until we have a sense of the textures, the props, the production work that’s going on around him as he writes the score, it’s impossible to see how the music springs from the deepest core of the story.
Go back to Toymaker again and listen. A picture book open on a girl’s lap shows a fairground lit with the nearer stars of carnival lights, the far ones tiny and sunk in indigo sky. There’s only a piano playing in the background; the midway calliope is just your imagination. But did you catch that little flourish as Angela’s hand moves over the carousel? Later, just shy of one minute into the short, Ben has exactly seven seconds to capture all the subtleties of this mother looking back at this sleeping girl in this moment. Listen to what he does with it. 
“I was really drawn to the story,” he says. As a kid, Ben dabbles in filmmaking, telling stories with stuffed animals. “We moved the animals around with nylon threads that we hoped people wouldn’t be able to see, but they could anyway! I went back to those memories of just playing, imagining, reading Robert Louis Stevenson – trying to recapture that. The world of Toymaker is very vivid but fragile. Maria is doing work but her heart isn’t in it: even when they’re happily reading in bed, it’s a little sad. When the village comes to life, it’s a separate melody, more emotions.”
One of Ben’s early tasks is to create a mock soundtrack as a reference. He uses bits of Beethoven, the Firefly soundtrack, John Williams, but it’s “a bit of a chore” because it reminds him how far he’ll have to go for the results he wants. “I had to ask early on what the instrumentation would be,” he explains. “It’s more intimate than a regular Hollywood blockbuster – an orchestra would be overkill. I looked at eastern European music, which historically was often used in animation, but traditional classical is my idiom, so I went with piano, violin, and cello, letting myself be influenced by klezmer and other traditions rather than trying to duplicate a particular style.”  
Ben describes himself as a melody-centric composer, and as the team starts feeding him materials (only sketches at first), he begins drafting just a couple of lines: capturing character and mood, then extrapolating from there. Tara Fisler (daughter of Professors Kathi Fisler and Shriram Krishnamurthi) and team member Emily Reif star in a live action demo that gives him a sense of timing and flow, and within a week, he has a rough draft of the first half. The second follows a week later. Starting at the beginning of October, Ben’s there at every weekly meeting, and not as a silent partner: “I loved being involved in the process...I really felt like part of the whole.”
He has a practice session with his musicians in late autumn, and the final recording session at the end of January. “By then,” Ben says, “the timing was pinned down and the animators had to work from what I’d done. It’s the opposite of live-action films, but animators and composers work much more in parallel. At that point, Barb and the others and I agreed that we had to lock the timing so I could proceed. The script inspires the music, not the other way around, but the music drives things, not so much the plot as the emotional content of the film.”
With his score in the background, production work goes on. And as our conversation with Ben winds down, we ask him about being part of the Toymaker team. In retrospect, what was the entire experience of working on the short like? “Every time I go back to it,” he says, “It feels special. That’s a mark of success for me.”
Winter And Spring: Animation, Lighting, Shading, Sound
Technically, animation starts in the last week of November. The leads are assigning shots to animators, and each student’s work has to join with work from the student before them and the one after. If one of those two finishes first, and the merge doesn’t look good, the animator in the middle may be forced to redo their shot.
This is when we see real emotions come out of the characters: here are the corners of the mouth for Angela’s mad face, and here’s Maria thinking about something. What about those eyelids, are they too open? According to Barb, animation is different from any other part of the pipeline: people who struggled with modeling may turn out to be natural animators, and vice-versa.
Two-thirds done is Meier’s rough estimate for where things are at the end of the year. Only an approximate 20% of the work is creative at this point, and the rest is babysitting the rendering, cleaning up the little things. Thousands and thousands of renders. A broken render farm that can’t be fixed until next year’s software update. The rig for Maria’s hands isn’t right, and they have to work joint by joint. The students and Barb can recollect this phase pretty clearly: a long slog, they call it. The swamp.
January means completing 95% of the animation so they can move on to final lighting and rendering. Communication among the team members is constant. Barb says, “We texted our latest , but nobody wants to make decisions in a text thread, and it wouldn’t be a good idea anyway. When we got together, everyone had so much respect for the other people at the table, and you’d hear ‘good enough, good enough’ go around the room. For some decisions, we just let time run out, and that’s not always a bad way to do things. If nobody proposes something different to what’s currently on the table, that’s the way it goes.”
Lighting is being adjusted and finessed even as the final 5% of animation is winding down. At first, it’s the lighting for a room, a whole scene; later, it gets adjusted in minute detail, shot by shot. “Every technical detail is there to tell a better story,” Kenji says. “Animation tells a visual story, designed to create emotion, empathy, through these technical details. Throughout the project, Barb gave incredible feedback for ongoing improvement, and she had such dedication and belief that we could complete this."
Lighting tests continue until the clock runs out, but Barb remembers the end of animation as a milestone. Her family remembers it as the time period where they’d ask her what was for dinner and she’d think it was still lunchtime. “At this point in the project,” she says, “we had almost all the pieces, but it still takes hundreds of hours to go from 90% to 100%. Whether it gets done depends on the huge dedication of the students who continue to work on the project even after the official course is over.”
Late spring is for foley recording, sound effects, making sure the little details that add realism don’t overshadow the expressive score. Lighting and shading are still being tweaked all the way into May. Glass objects are still too bright; remembering that particular struggle, Barb puts her hand to her mouth and pretends to shout across a room: “Someone turn down that jar!”
And then one day, it’s done. (That day is May 17, the day before the screening.)
“Pretty crazy” is how Nellie describes the screening. “It was the first time a lot of us had seen it final-final, with music, sound effects, color correction.”
“I couldn’t stop thinking about the computer hours that had gotten crunched,” Kenji remembers. “We had only 20 computers working on the short, 10,000 frames and about an hour to render each frame – around 10,000 computer hours to just render it once. And we got up to version 5 or 6 of most shots. There were so many points when it seemed like we wouldn’t finish or it wouldn’t be up to the standard we were looking for. So I was really, really proud of it. It didn’t hit me until the screening that it was a Brown/RISD first.”
“It reminded me that I really like doing,” Barb says. “A lot of what faculty members do is help students do things, so it was wonderful to be in the zone and get stuff done. I went into animation in the first place because I wanted to make things.”
Animation A Bit Differently
With those numbers alone (10,000 frames, 10,000 hours of rendering), Toymaker astounds the layperson, but what about the expert? A week after its debut, Barb shows the short to eight alums at Commencement. Some of them have known her for her entire teaching career.
“When I saw the video file that Barb was opening, my jaw dropped,” says Mike Ravella. Now a Technical Director at Pixar, he came to Brown with no experience in art or computer graphics. “It was just incredible. You never see students even at the top animation schools do seven-minute shorts.”
“It was always competitive to get into Barb’s classes,” he remembers, “but these kids are insanely more qualified than we were. When one of my friends and I made our short, we tried to be conservative. We put a lot of love into it, and we were so excited to make something, but it still didn’t come out as well as we’d hoped. Toymaker is what I wanted to do, but that team of people didn’t exist that year. It does now, and the community that Barb has fostered is only on the rise. It’s self-sustaining: with more grads, students see more and more role models, so the career path looks viable to them.”
But the biggest challenge, Mike says, is animation’s barrier to entry: “And I want to see more kids get this opportunity, because Barb does such a great job breaking that barrier down! But she can only do so much teaching alone. When I heard that Toymaker was more about collaborating with friends and less about winning awards, it reminded me how Barb does animation a bit differently. She has the same focus on craft as other schools, but this is deeply personal for her. Animation is art to Barb, and anything from her students has a lot more heart, and it speaks to people better than something thrown together for a demo reel.”
The proof that it’s speaking to people occurs at that very same screening. Stunned by a plot twist, an alum’s five-year-old son voices his thoughts in a whisper heard throughout the room: “She’s breaking all the things the mom made!”
I Hope It Happens Again
The posters went up a few days before the screening.
Brown and RISD get credit at the top, the thirteen names are all the same size at the bottom, and just below that is a tiny postscript, a point of pride in technique and local habitation: “ARTISANALLY RENDERED IN PROVIDENCE, RI”. The title appears in gleaming white script, but the scene is subdued, even dim. Mother and daughter are sitting on the floor with a book, Angela stretching a little as kids do to make up for lack of height. Light is coming from stage left, but the sun is low. Near the ceiling the blue-green wall is black. The white mopboard has gone pink; soon it’ll be gray. Disrupting any sense of staginess or calculation in the pose or the scene, the animators have tucked one of Angela’s drawings behind her head, semi-obscured, throwing off the symmetry a little. All the months of work have culminated with a complete lack of pretense.  
“There’s a reason why we were able to do it,” Barb says. “I put a structure in place that made it possible to be successful, and we had the right group of students. After working ten or twelve hours in the lab, they’d go to a movie together. They’re friends who deeply cared about the project and each other.”
So, what’s next for Nellie? “I don’t know,” she says, but it has to involve computer graphics tools and collaboration. “You can’t do this work alone.” She pauses. “I’m glad this actually happened. I hope it happens again.”
“I hope so, too,” says Ben. “I hope Barb’s willing to put up with me again!”
Months later, three words recur when we ask Barb if there’s anything else to say: “I don’t know. I just re-rendered the video today, actually. I'd love to run the class again and make another short, but it's tricky because I have my other courses to teach. A project like this requires a lot of specific creativity and problem-solving to produce a quality film, compared to a typical two-week learning assignment in the intro course where mistakes are small and soon forgotten. It'd be great to have someone else come in and teach the intro course on a visiting basis now and then. But this was really a showpiece for what Brown CS can do. It was aspirational, but we had specific things in place to make it work. Based on what we learned, projects like this can only get better.”
And she’s been doing this for more than three decades. The best gardeners, Barb admits, are people in their sixties and seventies, because they’ve seen more growing conditions. A grin breaks out: “Yeah, I’d like to do something like this again.”
It’s the last week of summer and everyone’s coming back to campus. The screening was almost four months ago, but the posters are still hanging up in the CIT (mother and daughter leaning close in that late light) and new students will see them. Some won’t make the cut for CS 125 but will get in next year, some won’t get as far as they wanted in four years but make it to Pixar anyway, and some will have the idea and form the team and Barb will have to start drinking more coffee again.
Four months later and counting, nobody seems in any hurry to take them down.
Bloating, or adding unnecessary functionality to software that damages its performance or ease of use, probably started very soon after the first program was written. For most of us, the reaction is mostly frustration: we only want to make a playlist, or write a letter, and we're distracted by virtual assistants that were intended to be helpful, or we can't find the option we want because the menu has swelled with choices.  
But Professor Vasileios Kemerlis of Brown University's Department of Computer Science (Brown CS) sees a much bigger problem: "More code means a larger attack surface, and more vulnerabilities. It's no different than trying to protect a mansion against intruders instead of a tiny shed." His work with Georgios Portokalidis of Stevens Institute of Technology and Junfeng Yang of Columbia University has just won a shared $3,200,000 grant from the Office of Naval Research (ONR). "Our proposal," he explains, "uses new methods and tools to reduce the attack surface of deployed binary applications, and then we take advantage of the reduced surface to secure them by adding a breadth of targeted defenses."
Their project, ABIDES (Adaptive BInary DEbloating and Security), views attack surface as a multifaceted concept and aims to reduce it by:
To quantify the benefits of reducing the attack surface, Kemerlis and his colleagues will devise metrics that go beyond code size and consider qualitative aspects of the removed or disabled code. Once they reduce the attack surface and identify its various facets, protecting the resultant software becomes much simpler, so they intend to follow that insight by creating effective defenses for debloated software, extending their previous work on continuous code randomization and control-flow integrity (CFI).
"We're very excited about this," Vasileios says. "Beyond simply removing unused code and functionality, we're developing fine-grained, dynamic debloating techniques, and we think our work will truly improve the state of the art in attack surface reduction for binaries and the OS kernel."
It's still a hypothetical, but easy to imagine: high school students and their families, about to take on thousands of dollars in loans and nervous about how the money will be spent, turn to a national database that details projected costs and financial outcomes. A few keystrokes later, they've entered enormous amounts of sensitive information, from a social security number and a street address to someone's employment history and financial status. That data is enormously attractive to hackers, and who's protecting it?
To address this, Senators Wyden, Rubio, and Warner introduced new legislation (the "Student Right to Know Before You Go Act") that offers more transparency for the costs and outcomes associated with higher education. Just as importantly, it requires use of a technique known as secure multi-party computation (MPC), which protects sensitive information by allowing parties to jointly compute a function over their inputs while keeping those inputs private.
Professor Seny Kamara of Brown University’s Department of Computer Science (Brown CS), who served as a technical advisor on the bill, says, "This is significant because we believe this bill is the first of its kind in requiring a privacy-preserving system based on MPC. It's an important moment: the government at its highest level has begun to realize that whenever we collect sensitive data, there have to be privacy protections in place, and those protections need to use the best and most modern cryptographic  techniques."
Seny expects that the legislation will have a large impact not just on education but on technology and privacy. He says, "This shows that the benefits of big data can be obtained without compromising sensitive data. By integrating state-of-the-art privacy technologies, this bill illustrates what can be achieved when good policy and advanced technology combine."
The Associated Press describes it as "an interaction in which the human is needed" and researchers from Brown University's Department of Computer Science (Brown CS) and Humanity-Centered Robotics Initative (HCRI) have some very important and very human interactivity in mind for their new partnership with entertainment leader Hasbro. They're working on a robotic cat with the aim of having it serve as a companion to older people and assist them with simple tasks.
The research is known as ARIES (Affordable Robotic Intelligence for Elderly Support), and it aims to add artificial intelligence capabilities to a product that Hasbro has already had on the market for two years. Without overpromising (the cat will never prepare meals or even navigate a house), they hope to expand its capabilities to include a small set of tasks that can be challenging for older adults, such as remembering medications or finding lost objects. The ultimate goal is allowing seniors to maintain their quality of life and stay in their homes longer.
The Associated Press story has already been featured on newspapers, radio stations, and television worldwide, including ABC News, The Washington Post, and The Daily Mail. You can read it in full here.
Even as autonomous robots get better at doing things on their own, there will still be plenty of circumstances where humans might need to step in and take control. New software developed by Brown University computer scientists enables users to control robots remotely using virtual reality, which helps users to become immersed in a robot’s surroundings despite being miles away physically.
The software connects a robot’s arms and grippers as well as its onboard cameras and sensors to off-the-shelf virtual reality hardware via the internet. Using handheld controllers, users can control the position of the robot’s arms to perform intricate manipulation tasks just by moving their own arms. Users can step into the robot’s metal skin and get a first-person view of the environment, or can walk around the robot to survey the scene in the third person — whichever is easier for accomplishing the task at hand. The data transferred between the robot and the virtual reality unit is compact enough to be sent over the internet with minimal lag, making it possible for users to guide robots from great distances.
“We think this could be useful in any situation where we need some deft manipulation to be done, but where people shouldn’t be,” said David Whitney, a graduate student at Brown who co-led the development of the system. “Three examples we were thinking of specifically were in defusing bombs, working inside a damaged nuclear facility or operating the robotic arm on the International Space Station.”
Whitney co-led the work with Eric Rosen, an undergraduate student at Brown. Both work in Brown’s Humans to Robots lab, which is led by Stefanie Tellex, an assistant professor of computer science. A paper describing the system and evaluating its usability was presented this week at the International Symposium on Robotics Research in Chile.
Even highly sophisticated robots are often remotely controlled using some fairly unsophisticated means — often a keyboard or something like a video game controller and a two-dimensional monitor. That works fine, Whitney and Rosen say, for tasks like driving a wheeled robot around or flying a drone, but can be problematic for more complex tasks.
“For things like operating a robotic arm with lots of degrees of freedom, keyboards and game controllers just aren’t very intuitive,” Whitney said. And mapping a three-dimensional environment onto a two-dimensional screen could limit one’s perception of the space the robot inhabits.
Whitney and Rosen thought virtual reality might offer a more intuitive and immersive option. Their software links together a Baxter research robot with an HTC Vive, a virtual reality system that comes with hand controllers. The software uses the robot’s sensors to create a point-cloud model of the robot itself and its surroundings, which is transmitted to a remote computer connected to the Vive. Users can see that space in the headset and virtually walk around inside it. At the same time, users see live high-definition video from the robot’s wrist cameras for detailed views of manipulation tasks to be performed.
For their study, the researchers showed that they could create an immersive experience for users while keeping the data load small enough that it could be carried over the internet without a distracting lag. A user in Providence, R.I., for example, was able to perform a manipulation task — the stacking of plastic cups one inside the others — using a robot 41 miles away in Cambridge, Mass.
In additional studies, 18 novice users were able to complete the cup-stacking task 66 percent faster in virtual reality compared with a traditional keyboard-and-monitor interface. Users also reported enjoying the virtual interface more, and they found the manipulation tasks to be less demanding compared with keyboard and monitor.
Rosen thinks the increased speed in performing the task was due to the intuitiveness of the virtual reality interface.
“In VR, people can just move the robot like they move their bodies, and so they can do it without thinking about it,” Rosen said. “That lets people focus on the problem or task at hand without the increased cognitive load of trying to figure out how to move the robot.”
The researchers plan to continue developing the system. The first iteration focused on a fairly simple manipulation task with a robot that was stationary in the environment. They’d like to try more complex tasks and later combine manipulation with navigation. They’d also like to experiment with mixed autonomy, where the robot does some tasks on its own and the user takes over for other tasks.
The researchers have made the system freely available on the web. They hope other robotics researchers might give it a try and take it in new directions of their own.
In addition to Whitney, Rosen and Tellex, other authors on the paper were Elizabeth Phillips, a postdoctoral researcher with Brown’s Humanity Centered Robotics Initiative, and George Konidaris, as assistant professor of computer science. The work was funded in part by the Defense Advanced Research Projects Agency (DARPA) (W911NF-15-1-0503, YFA: D15AP00104, YFA: GR5245014 and D15AP00102) and NASA (GR5227035).
The Paris C. Kanellakis Memorial Lecture, a seventeen-year tradition at Brown University's Department of Computer Science (Brown CS), honors Paris Kanellakis, a distinguished computer scientist who was an esteemed and beloved member of the Brown CS community. Paris came to Brown in 1981 and became a full professor in 1990. His research area was theoretical computer science, with emphasis on the principles of database systems, logic in computer science, the principles of distributed computing, and combinatorial optimization. He died in an airplane crash on December 20, 1995, along with his wife, Maria Teresa Otoya, and their two young children, Alexandra and Stephanos Kanellakis.
Each year, Brown CS invites one of the field's thought leaders to address wide-ranging topics in honor of Paris. Last year, Donald Knuth of Stanford University returned to Brown to give a "history of clever ideas that arose around the world” as he traced the evolution of a combinatorial problem dating back to antiquity. Just this week, on December 11, 2017, Piotyr Indyk of the Massachusetts Institute of Technology delivered the seventeenth annual Paris C. Kanellakis Memorial Lecture.
Indyk is a Professor of Electrical Engineering and Computer Science at MIT. He joined MIT in 2000, after earning his PhD from Stanford University and his Magister degree from ​Uniwersytet Warszawski in 1995. His research interests lie in the design and analysis of efficient algorithms, and specific interests include high-dimensional computational geometry, sketching and streaming algorithms and sparse recovery. Indyk has received the Sloan Fellowship (2003), the Packard Fellowship (2003), and the Simons Investigator Award (2013). His work on Sparse Fourier Transform has been named to the Technology Review “TR10” in 2012, while his work on locality-sensitive hashing has received the 2012 ACM Kanellakis Theory and Practice Award.
Speaking in front of an enthusiastic crowd that filled CIT 368, Piotyr gave a lecture ("Below P vs. NP: Conditional Quadratic-Time Hardness for Big Data Problems") that worked outward from one of the key concepts of computational complexity, the theory of NP-hardness, to address some of the newest challenges presented by the era of Big Data. Quadratic-time algorithms, Indyk stated, can be inefficient when used on moderately-sized inputs, but lose their usefulness on problems that involve gigabytes or more of data. Using recent research in string processing and machine learning as evidence, he made the case that under a natural complexity-theoretic conjecture, there are no near-linear time algorithms for such problems, and he described how this framework has led to the development of new algorithms.
Professor James Tompkin of Brown CS, whose research is in graphics, vision, and interaction techniques, was one of the many attendees. Asked about Piotyr and his work, he says, "His and his colleagues work on analyzing complexity of empirical risk minimization is highly relevant to computer vision, as we often use supervised learning methods like SVM and neural networks. For instance, Indyk showed that quadratic time is required to compute the gradient of the empirical loss in neural nets."
Imagine a self-driving car zooming down the highway, or a robotic arm welding car parts in a factory. Suddenly, in less than a second, another car swerves in front of it, or a human trips and falls into the work area. The difference between a close shave and catastrophe will depend largely on one thing: the robot's reaction time. 
In 2016, Professor George Konidaris (then at Duke University) of Brown University's Department of Computer Science and Professor Dan Sorin of Duke University founded Realtime Robotics, focused on developing on a processor that would allow robots to perform motion-planning tasks at up to 10,000 times faster than previous speeds. Last month, the company secured $2 million in seed funding to further its growth. Key investors in the seed round include SPARX Group Ltd., Scrum Ventures, and Toyota AI Ventures, a venture capital subsidiary of Toyota Research Institute (TRI).
Realtime Robotics enables complex robotic motion planning tasks to be accomplished up to 10,000 times faster than previously possible using a proprietary special-purpose processor, allowing robotic systems to instantly react to their environments and compute how and where to move as their situation is changing. This groundbreaking ability to instantly plan motion in response to rapidly changing conditions overcomes one of the primary challenges preventing robots and autonomous vehicles from achieving their enormous potential.
“Realtime has set itself apart by providing a novel solution with far-ranging implications” says Tak Miyata, General Partner at Scrum Ventures.
Uses for Realtime's lightning-fast processor are wide-ranging: it enables robots with sophisticated arms to be utilized in dynamic environments, dramatically increasing the types of industrial tasks they can perform. It can also be used by autonomous vehicles to help them operate at normal speeds —like humans, but safer— instead of slowing to a crawl when there is uncertainty regarding other cars, bikes, or pedestrians.
"I'm very excited about the technology we're developing at Realtime," says George. "When I was young, programming video games was much harder, because an incredible amount of effort had to go into generating realistic graphics on quite slow hardware. GPUs changed all of that, and led to a massive burst of creativity and excitement in computing gaming. The processor Realtime is developing will do that for motion planning. At the moment it's slow and difficult to get right, so there are virtually no deployed applications. Once we're done, the processor will open up a whole new world of possibilities for robot automation."
Brown University's Department of Computer Science (Brown CS) is glad to announce that applications are open for the Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, which provides $10,000 annually to support an undergraduate engaged in an intensive faculty-student summer research partnership at Brown CS.
Last year, the award went to Sorawee Porncharoenwase, who worked with Postdoctoral Reserach Associate Tim Nelson to use logic to aid users in verifying correctness of a system. He was introduced to software verification and programming languages at Brown and continued his interest in them by taking Logic for Systems with Tim, then TAing the course twice. Over the summer, he worked on program synthesis combined with past work that Tim, Professor Shriram Krishnamurthi, and their PhD student, Natasha Danas, have done on presenting the output of formal methods tools. 
To apply, no later than February 10, 2018, students should email Associate Professor (Research) and Vice Chair Tom Doeppner either: (A) a copy of their summer UTRA application or (B) a two-page description of their proposed research and a letter of support from the Brown CS faculty member that they intend to work with.
One of the things we hear most often from our alums is that they keenly remember the pressures and uncertainties as they approached Brown graduation and weighed their next step in life.  They want to share the experience that followed with the next generation, and our students who are getting ready to graduate are just as eager to pick up a few pointers at a crucial moment in their careers.
The Life After Brown series features successful Brown CS alums sharing their perspective on the challenges and opportunities that await our graduates in the hope that current students can benefit. The inaugural lecture was given by Adam Leventhal, now at Transposit, a subsequent lecture was delivered by Mary Fernandez of MentorNet, and next week, the series returns with Peter Norvig, Director of Research at Google, whose talk will be held in CIT 368 from 4-5 PM on Friday, November 17.
Peter is a Fellow of the American Association for Artificial Intelligence and the Association for Computing Machinery. At Google Inc he was Director of Search Quality, responsible for the core web search algorithms from 2002-2005, and has been Director of Research from 2005 on. Previously he was the head of the Computational Sciences Division at NASA Ames Research Center, making him NASA's senior computer scientist. He received the NASA Exceptional Achievement Award in 2001. He has served as an assistant professor at the University of Southern California and a research faculty member at the University of California at Berkeley Computer Science Department, from which he received a Ph.D. in 1986 and the distinguished alumni award in 2006. He has over fifty publications in Computer Science, concentrating on Artificial Intelligence, Natural Language Processing and Software Engineering, including the books Artificial Intelligence: A Modern Approach (the leading textbook in the field), Paradigms of AI Programming: Case Studies in Common Lisp, Verbmobil: A Translation System for Face-to-Face Dialog, and Intelligent Help Systems for UNIX. He is also the author of the Gettysburg Powerpoint Presentation and the world's longest palindromic sentence.
Peter's lecture will be aimed primarily at undergraduates, but all are welcome to attend.
A group of academic researchers, led by cognitive and computer scientists from Brown University, is teaming up with a cross-functional team from global play and entertainment leader Hasbro to design a smart robotic companion capable of assisting older people with simple but sometimes challenging tasks of everyday living.
The project, dubbed ARIES (Affordable Robotic Intelligence for Elderly Support), will add artificial intelligence capabilities to Hasbro’s current Joy for All Companion Pets — animatronic dogs and cats designed to provide interactive companionship, comfort and joy for older adults. The research team’s goal is to develop additional capabilities for the ARIES companions to help older adults with simple tasks that could include help in finding lost objects, medication reminders or other tasks that sometimes become challenging, especially those who may have mild dementia.
The work is supported by a $1 million grant from the National Science Foundation and will be led by Brown’s Humanity-Centered Robotics Initiative (HCRI), a group of computer and social science researchers who explore the societal opportunities and challenges presented by robotics. The academic project team also includes researchers from Brown’s Warren Alpert Medical School, Butler and Bradley hospitals in Providence, R.I., and the University of Cincinnati.
“Hasbro did a great job developing a product that can provide comfort and joy for older people,” said Bertram Malle, a professor in Brown’s Department of Cognitive, Linguistic and Psychological Sciences, co-director of HCRI and the principal investigator on the grant. “What we want to do now is leverage our expertise in cognitive and computer science to add capabilities to this robotic pet. Neither of us could do this on our own, but together we have the expertise to potentially develop something truly beneficial.”
Over the next three years, the group plans to perform a variety of user studies to understand how ARIES might best assist older adults. Then they’ll work on developing and integrating a variety of artificial intelligence technologies that will meet the needs identified in the user studies. These could include sensor systems that allow the ARIES companion to identify and keep track of important objects around the house, such as keys or eyeglasses, help the person remember important tasks and events, and enhance safety.
The team will also study means of effective communication between the ARIES companion and users.
“The Joy for All Companion Pets currently make some realistic pet sounds and gestures,” Malle said. “We may want to expand those capacities and add intelligence to them, so the companions give meaningful clues — gestures, nudges, purrs — that help to guide users toward misplaced objects or let them know that it’s time to do something.”
The early user studies will play a key role in how the project unfolds, Malle says.
“There are some things — like locating objects and taking medications — that we know from the literature people find useful,” he said. “But in our first year we want to find out what other challenges people face that we don’t know about, and then see if we can develop technologies to address them.”
One critical factor the researchers will keep in mind is cost.
“The ‘A’ in ARIES stands for ‘affordable,’ and that's something we're taking very seriously,” said Michael Littman, a professor of computer science at Brown and co-principal investigator on the grant. “This is one of the important reasons Hasbro is a great industry partner for this project. The current Joy for All pets cost roughly $100 while similar robotic products can cost $5,000 to $6,000. We want the ARIES robot to be available to anyone who needs it.”
Ultimately, the team hopes to complete a prototype and test it with target users by the end of the three project years. The researchers stress that they don’t intend this to be a technology that can take the place of human caregivers. They hope instead that it can complement the work of caregivers and help in a small way to meet the challenge presented by an aging population.
“To us, this project really represents what we do at HCRI, which is to let societal needs drive technology development,” Malle said. “We know that caring for an aging population will be a tremendous challenge in the coming years, and we think technologies like ARIES could play a small but potentially important role in helping people meet that challenge.”
Ted Fischer, vice president for business development at Hasbro, says the company has been excited by the response to its Joy for All line. “Social isolation and loneliness are growing issues for older adults and our companion pets make people smile, Fischer said. “Hasbro’s expertise in play and engaging experiences in collaboration with leading scientists from Brown’s HCRI is a powerful combination to explore additional impactful uses for ARIES companions.”
The research team will include Peter Haas, associate director of HCRI; HCRI postdoctoral researchers Maartje de Graaf and Elizabeth Philips; Michael Armey from Butler Hospital and Brown’s Warren Alpert Medical School; Gary Epstein-Lubow from Butler, Brown and Hebrew Senior Life; Claudia Rébola, an industrial design professor at the University of Cincinnati; Ron Seifer of Bradley Hospital; and a multidisciplinary team from Hasbro including designers, engineers and marketing professionals.
The Defense Advanced Research Projects Agency (DARPA) has awarded four Brown University faculty members with Director’s Fellowships, which are given to the top performers among agency’s Young Faculty Award recipients.
David Borton, Amanda Jamieson, George Konidaris and Stefanie Tellex each won Young Faculty Awards in 2015, which provide up to $500,000 in research support over two years. The Director’s Fellowship provides a third year of funding of up to $500,000 to recipients who demonstrated “exceptional performance” over their first two years.
“The Young Faculty Award itself is highly competitive — these four faculty members were among only 24 awardees in DARPA’s class of 2015 — and the Director’s Fellows represent the best of the best,” said Jill Pipher, vice president for research at Brown. “Their innovative approaches in research may lead to transformative capabilities in immunology, robotics and neurotechnology.”
David Borton, assistant professor of engineering
Borton’s lab at Brown focuses on neurotechnologies that may one day restore movement in people who have been paralyzed by an injury or illness. His DARPA work aims to develop new lower-limb prosthetics that can do something no current artificial leg can do: provide sensory feedback to the nervous system. Having a sense of where our limbs are in space helps with balance, posture and fall-avoidance. Borton is developing the computational models necessary to recreate that sensory information, as well as a system that can write that information into the nervous system through mild electrical stimulation of the spine. During his third year of funding, Borton plans to expand the work to include stimulation in the brain, which could provide increased awareness of an artificial limb’s position in space.
Amanda Jamieson, assistant professor of molecular microbiology and immunology
The development of infection after injury or surgery is a serious problem for patients in civilian hospitals, but is especially challenging for warfighters given the nature of their injuries and the settings in which they are treated. While much medical research is focused on healing wounds and clearing infections, Jamieson’s research has a different but no less critical focus. She’s interested in ways to make people more resilient in the face of injury or infection—better able to weather the storm while healing takes place or an infection runs its course. For her DARPA project, Jamieson is looking at how lung infections influence the body’s ability to heal external wounds, in search of ways to make both skin and lung tissue more resilient to insult.
George Konidaris, assistant professor of computer science
Konidaris’ lab is working to develop intelligent, general-purpose robots. While the physical capabilities of robotic hardware have improved dramatically in recent years, the software side of robotics still lags a bit behind. Part of the problem, Konidaris says, is that robots still see the world essentially as sets of pixels. Getting a robot to translate those pixels into higher-level concepts — and then to associate concepts with appropriate actions — remains a challenge. Konidaris’ DARPA research aims to develop algorithms that help robots learn higher-level representations of the world — making the leap from pixels to abstract concepts — autonomously. Such algorithms could make robots much better at planning their actions in unstructured, real-world domains.
Stefanie Tellex, assistant professor of computer science
Tellex’s Humans to Robots lab is working to create robots that work collaboratively with people. Part of doing that requires developing better ways to communicate with robots. For her DARPA research, Tellex has been working on algorithms that enable robots to combine information from voice commands with information inferred from human gestures. People communicate in this way all the time. For example, when asking someone to hand them a tool, a person might say, "Could you hand me that wrench?" while pointing to the one they want. Tellex has shown that when robots combine speech and gesture they are able to perform fetching tasks more quickly and more accurately. In the third year of her DARPA award, she’ll continue to hone her algorithm using user studies and a mobile robot fetching objects in unfamiliar settings.
From a sea of 6,432 applicants from across the globe, two recent Brown University graduates landed honors among just 25 global winners cited for the top undergraduate research papers of 2017 as judged by the prestigious Undergraduate Awards.
Tushar Bhargava, a 2017 Brown CS graduate, earned top honors for the politics and international relations category with a paper titled “Partners in Crime: Telecommunication Companies and Intelligence Agencies.” Working with Timothy Edgar, a senior fellow at Brown’s Watson Institute for International and Public Affairs, Bhargava examined the cooperation between telecomm companies and government agencies to shed light on the current privacy concerns raised by civil liberties activists.
Bhargava, a computer science concentrator originally from New Delhi, India, said that while the award is a personal achievement, it’s also a reflection of how Brown’s liberal arts education stands out from its peers.
“It is a testament to the value of the Open Curriculum,” Bhargava said. “Though the paper was in the international relations category, it was multidisciplinary because I relied on skills I learned through coursework in English, history, computer science and social psychology.”
Bhargava now works in Seattle, Wash., as a software engineer at Microsoft.
In the category of literature, fellow Class of 2017 graduate Noah Fields bested the competition with a paper titled “Men Reading Jane Austen: Close Writing Across Gender Scripts,” which examined cross-gender identification in the literature of Jane Austen and was a rethinking of the marriage plot using the vantage point of today’s same-sex marriage laws.
“I was taken completely by surprise when I found out my paper actually won, because I think it's a pretty risqué essay,” Fields said. “It was really affirming and humbling to get this kind of recognition.”
Fields, who is originally from Thousand Oaks, Calif., is currently a musician and poet living in Chicago.
Oludurotimi Adetunji, associate dean of the College for undergraduate research at Brown, said competition for the Undergraduate Awards is fierce and showcases the best work by college students from across the world.
“The Undergraduate Award is broadly known as the ‘junior Nobel Prize’ because the chances of being selected are less than 0.4 percent,” Adetunji said.
Adetunji was impressed by this year’s Brown candidates, whose papers were tailored from coursework and projects stemming from Brown’s Undergraduate Teaching and Research Awards program.
“The awards are an indication of the kind of academic excellence that results from Brown’s Open Curriculum and highlight the vast opportunities for students to engage in research at Brown,” Adetunji added.
The Undergraduate Awards program recognizes both undergraduate research and original student work. This year, a group of independent and international panel judges narrowed down the 6,432 submissions to name a single winner for 25 distinct categories. All winners came from a broader pool of 150 “highly commended” recipients.
Bhargava and Fields weren’t the only strong performers from Brown. The work of 17 other Brown students was also honored as “highly commended,” more than doubling the number of recipients from the University compared to last year. Two of those students, Emily Schwartz and Issac Kim, were also recognized as Regional Winners.
All 19 current and now graduated Brown students have been invited to attend the Undergraduate Awards Global Summit in Dublin this November, and the work of all winners will be published in the Undergraduate Journal.
A delegation from Brown University's Department of Computer Science (Brown CS) has had a busy week at the Grace Hopper Celebration (GHC), an annual event that's become the world's largest gathering of women technologists. The conference was started in 1994 by Anita Borg and Telle Whitney as a forum to highlight the contributions of women in computing both in academia and industry, but its main aim is helping young women find a place in the field of CS.
Over the years, Brown CS participants have described the conference as a source of inspiration and resources as they try to close the gender gap, and this year was no different. Attendees have had a busy schedule: the week started with keynotes from speakers such as Melinda Gates and Fei-Fei Li, Professor and Director of Stanford University’s AI Lab and Chief Scientist at Google Cloud AI/ML, and has continued with workshops and talks on topics ranging from Community and Career to Artificial Intelligence and Data Science. This year, Brown CS alum Karen Catlin was an invited speaker, leading a special session about negotiating skills ("Learn to Negotiate And Stop Holding Yourself Back").
It's clearly been another successful GHC, and our community members have been delighted with the interest from women who want to learn more about studying CS at Brown and our work creating a diverse and inclusive environment in which all students, faculty, and staff can thrive. Reporting from the Brown CS booth, Lauren Clarke (Manager, Academic and IP Programs) says that visits from prospective students are at a record high. 
by Albert Dong '19 (Computer Science and Economics)
Registration is open now at www.startupatbrown.org!
When I got to Brown as a freshman, I knew I wanted to dive right into the startup scene. Since I can remember, I’ve always loved creating things and working on projects with teams. I had come to the first Hack@Brown as a senior in high school and it was a no-brainer to join the organizing team once I got to campus. I was also part of the first iteration of the Innovation Dojo, a semester-long workshop series for underclassmen by the Brown Entrepreneurship Program, and I immediately joined Brown EP as well.
Most of these initiatives were new, and I felt the entrepreneurship scene at Brown was catching sparks. I went to the fall career fair and there were a lot of big companies, but I felt it wasn’t as welcoming for freshmen, and I also didn’t see a clear way to apply to startups. I really wanted to feel the rush of working at a VC-backed startup, so through the hackNY Fellowship, I was fortunate to end up interning at a startup in NYC for the summer.
I felt Startup@Brown was a great way to expose more students to this thrilling world of startups and kick off the year by trying to start a fire.
Startup@Brown relies on a simple premise: connect students and startups. It’s a single idea, but the intention is threefold. First, demystify the process of founding a startup. Second, enable a great recruiting starting-point for both startups and students. And third, but not least, inspire, grow, and bring together the startup community on College Hill. We structured the conference around these goals.
Demystify The Process Of Founding A Startup
There’s been an explosion of interest on the startup world. In our first year, we only had spots for 250 students, but when we opened registration for a bit over 2 weeks, more than 500 students from Brown and RISD responded. In many of the responses and conversations, students said they had startup ideas but that they would like to learn more about the process of bringing them to reality.
We didn’t want Startup@Brown to be another 300-person lecture (apart from the keynote), so we took a more dynamic yet intimate approach, mirroring the idea of closeness and face-to-face contact in startups. We had workshops in which 20-30 students learned from startup leaders, from founders and CTOs of startups such as Teespring, Casper, and Figma, to venture capitalists such as Y Combinator, Techstars, and Andreessen Horowitz.  In total, we had 17 workshops ranging from “Building Your First Physical Product” to “Funding 101” to “User Experience”. Kevin Hale, partner at Y Combinator, one of the top startup incubators in the world, also held office hours for student startups.
Enable A Great Recruiting Starting Point
It’s hard for startups to access top talent the way big companies do. Startups have way less time and money to spend on recruiting than corporations. Big companies have full-time recruiters and startups have founders focusing on their product.
And for students, the application process to work at small startups is shrouded in mystery, if not nonexistent. In contrast to larger organizations, startups have no information about intern programs on their websites and only a few have visible job postings. Career fairs are usually filled with large companies and little to no startups.
Therefore, right after the opening keynote we had the Startup Fair, a startup-only career fair. Students had a chance to walk around, meet with startups and learn about their work, challenges, and career opportunities. Similar to a hackathon, students didn’t have to bring their resumes to each table — a student could choose to have her resume shared with all the companies before the event.
Inspire, Grow, And Bring Together The Startup Community On College Hill
The reason why some people go to Silicon Valley to start companies is because of the community. College Hill and Providence have an enormous potential for creative endeavors; it’s a very fertile ground that has cultivated many great entrepreneurs, startups, and social change, but we believe there’s always room for growth.
Many students said one of their favorite moments was meeting and interacting with like-minded people. Bringing close to 300 students and startup leaders together for a weekend in September made for a great start of the year.
Naturally, many of the founders, engineers, and designers were alums, and they brought plentiful experience and enthusiasm back to campus. The help of alums was one of the main catalysts for Startup@Brown: we were thrilled to see over 20 alums from the different startups back on campus.
We sent a survey after the event where we got some feedback from attendees. 79% of the students are now more inclined to start their own startup, including six students saying they were starting one after Startup@Brown, and 97% would consider working with at least one of the startups that came. Feedback from the startups included that it was their “favorite career-fair” and that “the focus on startups was a huge win because every student was very engaging”.
Out of another survey we sent in January, 59% of respondents interviewed with at least one Startup@Brown startup, and 37% got offers for a summer internship, full-time job, or non-summer internship.
The close interactions created new friendships and now a stronger startup community vibrates through College Hill. There has been a snowball effect: out of conversations with Y Combinator, a new Brown CS startup class, csciStartup started being offered in Spring 2016, taught by Prof. John Jannotti, supported by Y Combinator, and with guest lectures from some of Startup@Brown’s speakers.
Startup@Brown was hosted by Brown University’s Department of Computer Science and organized in collaboration with the student-clubs Hack@Brown and Brown Entrepreneurship Program. We owe special thanks to alums: Startup@Brown wouldn’t have been possible without you.
We learned a lot from the first iteration and are now onto our third! Registration closes on Sept. 22nd. Find out more at www.startupatbrown.org.
Brown CS grad students are invited to a launch event ("Empowering Collaboration / Expanding Networks") by Brown University's Data Science Initiative (DSI). It includes lunch, roundtables with data science-related research groups and institutes, a reception, and a keynote by Andrew Moore, Dean of Computer Science at Carnegie Mason University. 
Brown University Executive Master in Cybersecurity (EMCS) Academic Director Timothy Edgar has just published Beyond Snowden: Privacy, Mass Surveillance and the Struggle to Reform the NSA. Described as “a must read” by Siobhan Gorman, Wall Street Journal former intelligence reporter, the book is already being hailed as a landmark in our legal understanding of cybersecurity in today’s digitized borderless world.
In Beyond Snowden, Edgar —whose career in government coincided with Snowden’s tenure— grapples with many of the issues that consumed the former NSA contractor now in exile in Moscow. His book takes us on a journey through America’s surveillance state to find the answer to this central question: What should we do about mass surveillance?
Tim is a longtime civil liberties activist who worked inside the intelligence community for six years during the Bush and Obama administrations. He believes that the NSA’s programs are a profound threat to the privacy of everyone in the world. At the same time, he argues that mass surveillance programs can be made consistent with democratic values — if we make the hard choices needed to bring transparency, accountability, privacy, and human rights protections into complex programs of intelligence collection.
Although the NSA and other agencies already comply with rules intended to prevent them from spying on Americans, Edgar argues that these rules are inadequate for this century and outlines practical modern reforms.
To hear more from Tim on Beyond Snowden:
If we posit a not-so-distant future where robots are ubiquitous, it stands to reason that we need writers to ground portrayals of them in reality. We say good-bye to crazed androids from pulp magazines who inexplicably try to kidnap beautiful women, and we're equally skeptical of Hollywood depictions of an AI apocalypse that still prevail today. 
And first impressions matter, which is why Professor Stefanie Tellex of Brown University's Department of Computer Science (Brown CS) has served as an advisor for a just-released children's picture book (My Friend Robot!) from Barefoot Books, written by Kate "Sunny Scribens" DePalma and illustrated by Hui Skipp. It's part of Barefoot's series of STEM titles for kids, and the public got an early glimpse of the book at July's Robotics: Science and Systems conference at MIT.
"My young son was a big motivation," Stefanie explains. "Having a kid has made me even more interested in robotics outreach to the general public, and teachers often ask me where to go for more information, what to do next."
Senior Editor at Barefoot Books, Kate explains that her husband, Dr. Nick DePalma, a scientist who specializes in human-robot interaction, recommended Stefanie as a partner. "I needed help," she says, "with making sure the text and artwork were reasonably scientifically accurate. Dr. Tellex reviewed both while they were in process to make sure they were 'accurate' despite depicting a somewhat fantastical humanoid robot." 
"I'd been a fan of Barefoot Books forever," says Stefanie, "so I knew that I'd love to help. I did some brainstorming about what robots are used for: space exploration, healthcare, manufacturing, and then I wrote some text to describe them. I also sent along some references and visual examples for things like Voyager 1, a Roomba, and a surgical robot."
The collaboration, Kate remembers, quickly grew from there. "Dr. Tellex was instrumental in helping create the educational endmatter that closes the book and building the content for two of the spreads from scratch. Writing the endmatter was a huge challenge, because it needs to stay relevant and not be outdated in a few years. This book emphasizes the human element of robotics — how we can work with robots and have different careers in robotics. We want to get young children interested in STEM/STEAM/STREAM fields!"
"Just seeing the proofs, the beautiful art, was so cool," Stefanie enthuses, clearly inspired by the collaboration. "Imagine kids realizing that Voyager 1, the manmade object that's traveled farthest from Earth, is a robot! 60 years after it was launched, it's still working. Its story is still unfolding for us today, right now."
"Stefanie was a joy to work with and totally critical to the process of creating the book," says Kate. "She helped us bring a whole new dimension of information and accuracy to it. And we have her to thank for the fun, computerless programming activity we included! We were thrilled to credit her prominently on the copyright page so readers, caregivers, and educators know how much care and thought went into the development of the book." 
It's clear that Stefanie feels the same way. "On top of publishing diverse and inclusive books, Barefoot is known for getting their facts straight, and that's important to me. It was great to interact with someone who knows kids, what they need and how they learn. It's exciting to me that there's this wide open space for us to educate the public. Parents really need things like this."
The images above are © 2017 by Barefoot Books and used with permission.
"Humans make hundreds of routine decisions daily," explains Professor Amy Greenwald of Brown University's Department of Computer Science (Brown CS). "More often than not, the impact of our decisions depends on the decisions of others. As AI progresses, we're offloading more and more of these decisions to artificial agents. My research is aimed at building AI agents that make effective decisions in environments inhabited by both human and artificial agents."
On July 18, Amy gave a keynote address  ("The Interplay of Agent and Market Design") at Microsoft's Faculty Summit 2017 ("The Age of AI"). The event, now in its eighteenth year, brings together thought leaders and researchers from a broad range of disciplines, including computer science, the social sciences, human design and interactions, and policy. This year's goal was to highlight some of the key challenges posed by artificial intelligence and identify the next generation of approaches, techniques, and tools that will be needed to develop AI to solve the world’s most pressing challenges.
In her talk, Amy explained how current efforts in building AI agents have become increasingly relevant to economic domains, mostly in the service of perfecting market designs. She explained a mathematical connection between auctions and contests, and then observed that productivity can be maximized by rewarding not only the very best submissions, but rather by promising to reward any one of the top (for example) quarter of the submissions. She also discussed the importance of inverse reinforcement learning to market design, where the goal is to extract underlying motivations from agent behaviors. She concluded by touching on AI agent design in applications, ranging from renewable energy markets to online ad exchanges to wireless spectrum auctions.
You can watch a video recording of Amy's keynote here.
Less than one year after launching its Data Science Initiative (DSI), Brown University has been awarded a $1.5 million grant by the National Science Foundation to establish a new research institute aimed at developing mathematical and computational tools for data-driven discovery.
Brown’s award is one of 12 nationwide Transdisciplinary Research in Principles of Data Science (TRIPODS) grants announced by the NSF on Aug. 24, 2017. The grants represent a $17.7 million commitment to developing “state-of-the-art mathematical and statistical tools, better data mining and machine learning approaches, enhanced visualization capabilities and more,” according to an agency statement.
“Brown’s Data Science Initiative was formed with the mission of bringing together people from pure and applied mathematics, statistics, computer science and elsewhere to develop new tools for applying data to complex problems,” said Jeffrey Brock, professor of mathematics and director of the DSI. “It’s gratifying that our approach to data science is so closely aligned with NSF’s vision. This award plants a flag for Brown as a place that excels in this kind of collaborative research in data science.”
The three-year grant will support intensive research activity and workshops bringing top researchers from higher education and industry to campus to work with experts on the Brown faculty. The work of Brown’s institute will focus on ways to improve the predictive power of mathematical and computational models of complex systems in the real world.
Brock noted that work in each of the institute’s three research themes will be geared toward real-world application in a wide variety of domains, from better understanding gene-environment interactions, to understanding outcomes of public policy decisions, to better ways of recognizing injury or disease from CT scans and MRIs. Over the course of the grant, those three themes will guide the development of:
The grant will also support intensive two-week immersion sessions for graduate students and faculty to learn about theoretical developments in one of the TRIPODS focus areas. These sessions, as well as end-of-semester workshops, will be hosted at ICERM, Brown’s NSF-funded mathematics institute. The grant also supports disseminated research results to the broader public through BrownX, the University’s online learning initiative.
“Brown’s DSI is motivated by the idea that the application of data to real-world problems should drive our theoretical and foundational work,” Brock said. “That will also motivate the work we do as a TRIPODS institute.”
Brock will serve as the grant’s principal investigator and TRIPODS institute director. Stuart Geman, professor of applied mathematics; Joseph Hogan, chair of biostatistics; Bjorn Sandstede, professor of applied mathematics; and Eli Upfal, professor of computer science will serve the grant’s co-investigators. The grant leaders will work with a broad network of researchers across campus who will contribute to TRIPODS research.
The $1.5 million award is the first phase of TRIPODS funding. Phase I institutes are invited to submit proposals to become larger Phase II institutes after three years.
“The pace of change in the data science field is extremely rapid, and we think that the collaborative approach to research that we’ve established with DSI and now with TRIPODS is the right way to keep pace, Brock said. “We’re looking forward to getting started.”
The ACM Symposium on Principles of Distributed Computing (PODC) is perhaps the most prominent international forum on the theory, design, analysis, implementation, and application of distributed systems and networks. Its common goal is to improve understanding of the principles underlying distributed computing.
Each year's conference gathers the field's thought leaders, and prominent among them this year was Professor Maurice Herlihy of Brown University's Department of Computer Science (Brown CS), who delivered a keynote address ("Blockchains and the Future of Distributed Computing"). After winning the crowd over with a humorous review of recent headlines in the cryptocurrency world, Maurice quickly turned serious, situating blockchain technology squarely in the world of distributed computing and looking at applications that he believes will have a pervasive effect on how we live. 
The ACM has kindly hosted Maurice's slide presentation here (PPTX format).
From a deluge of job openings to new university programs, Data Science has become a hot topic. But if it’s so important, why wait until a student enters university to introduce it?
Children are natural data scientists! They argue about who was the greatest quarterback, the most successful singer, which chain has the best pizza. These questions quickly shift to data: did athlete X win more trophies than athlete Y, are Grammy nominations or albums-sold a better indicator of talent, and so on. As they mature, they want to know whether a law is racist, or whether the outcomes of going to a particular college justify the extra student loans. In a world that’s data-rich, what these students need isn’t data. It’s the ability to ask questions and make meaning from that data.
Bootstrap, founded and co-led by three current Brown CS faculty/staff members, is one of the only groups in the CS Education field that builds our own curriculum, software, and programming tools. This gives us a unique opportunity to fill that gap, with a programming language that makes operations on tabular data (literally, spreadsheets) accessible without the overhead of teaching loops. We’ve leveraged our world-class language development team to bring rigorous Data Science to an introductory computing module. And since our unit of data storage is a spreadsheet, there’s a smooth on-ramp for teachers who are comfortable with Microsoft Excel and Google Sheets.
You can learn more about Bootstrap in the expandable sections below:
Professor Eli Upfal of Brown University's Department of Computer Science (Brown CS) and Michael Mitzenmacher of Harvard University have just released a significantly larger second edition of their widely-used textbook, Probability and Computing: Randomization and Probabilistic Techniques in Algorithms and Data Analysis. It's already receiving high praise from experts in the field: Stanford University's Donald E. Knuth says, "This textbook provides a rigorous yet accessible introduction to fundamental concepts that need to be widely known. The new chapters in this second edition, about sample size and power laws, make it especially valuable for today's applications." Richard Karp of University of California, Berkeley, explains that his favorite course that he's taught at Berkeley is one based on Probability and Computing: "Students appreciate the clarity and crispness of the arguments and the relevance of the material to the study of algorithms." 
Readers of the book need only an elementary background in discrete mathematics, and the new material covers such topics as normal distributions, sample complexity, VC dimension, Rademacher complexity, power laws and related distributions, cuckoo hashing, and the Lovasz Local Lemma. A full list of contents includes:
Eli explains that the book's new subtitle (it changes from "Randomizing Algorithms and Probabilistic Analysis" to "Randomization and Probabilistic Techniques in Algorithms and Data Analysis") is significant. "The new material is mostly related to the theory of machine learning and data analysis," he says, "following their growing importance in CS. We want students to learn the best modern techniques and applications, so we provide many new exercises and examples, including programming-related ones that provide training in solving these kinds of problems."
The image above is © 2017 by Cambridge University Press.
It is with great sadness that we share the news that Peter Wegner, Professor Emeritus of Computer Science, passed away this morning following a brief illness. Peter came to Brown almost fifty years ago, in 1969, and we remember him with great respect not just as educator, theorist, historian, and researcher, but as one of the founding members of Brown CS.
Born in St. Petersburg to Austrian parents, Peter was present in Vienna for the terrifying days of the Anschluss and Kristallnacht, and escaped the Holocaust on the special trains known as Kindertransports. Graduating from Regent Street Polytechnic in London, he studied mathematics at Imperial College of London University, organizing the University of London Philosophical Study Group, which sponsored lectures by such luminaries as C.E.M. Joad, J.B.S. Haldane, and Karl Popper. This interest in philosophy continued throughout Peter’s life, and philosophical analysis was a frequent component of his scientific work.
Later, at Cambridge University, Peter completed a graduate program in Numerical Analysis and Automatic Computing, also working with Professor Maurice Wilkes on the EDSAC computer, now widely seen as the first practical general-purpose stored-program electronic computer. It was during his stay at Cambridge that Peter met his future wife, the late Judith Romney, and received a Post-Graduate Diploma in Numerical Analysis and Automatic Computing at a time when there were no Master’s or doctoral programs in computer science anywhere. He is believed to be one of the world’s first two or three CS postgraduates.
After working at the Prudential Insurance Company, Pennsylvania State University, MIT (Wegner worked with Fernando Corbato on the Multics project, for which Corbato would later receive a Turing award), the London School of Economics, and Cornell University, Peter came to Brown in 1969, accepting a position with tenure. During his time at Brown CS, he supervised six doctoral students and taught numerous courses, particularly in the areas of programming languages, software engineering, and theoretical computer science.
Peter was the author of several books, including Programming Languages, Information Structures, and Machine Organization, one of the significant texts of the discipline’s early history, concerned with the very nature of computing. He continued writing until his death, publishing Interactive Computation: the New Paradigm (with Dina Goldin and Scott A. Smolka) in 2008.
He was also the editor of the ACM Curriculum Committee’s Curriculum 1968, which provided recommendations for CS academic programs. It was an effort more than a half-decade in the making, and its effects on computer science education are still being felt today. Peter’s close involvement with the organization continued for decades: he served on multiple committees, led its Publications division, and was later named a Fellow of the ACM and given a Distinguished Service Award for “focusing the field’s intellectual energy” through his commitment to research and publishing. Four years later, he received the Austrian Cross of Honor for Science and Art, 1st Class. For more than a decade, he was also one of two editors of the Brown University Faculty Bulletin, where he wrote on Russian literature and continued his love of philosophy with essays on Popper and Russell.
“I have always been in a position,” Peter Wegner once said, “of trying to understand new ideas when they entered my life.” Interviewed just a year before his death, he urged computer scientists to focus on the opportunities of the field: “Sometimes we work too hard trying to do things we can’t do and neglect the things we can. In computer science we work with possibilities and hope we’ll someday be able to solve them.”  
Brown CS sends our condolences to the entire Wegner family. Peter was a founder and a friend, and we have benefited for decades from his eagerness to contribute, his participation through questioning, and his hope of unifying bodies of knowledge. He will be greatly missed and long remembered.
You can also read an autobiography of Peter’s days before coming to Brown and a CACM memorial for Peter.
The continued expansion of Brown University's Department of Computer Science (Brown CS) has brought an important milestone: an Associate Chair will now assist Department Chair Ugur Cetintemel with important duties, and Professor John "Spike" Hughes will fill the role. Professor Tom Doeppner continues in his role as Vice-Chair.
"Things don't always scale up nicely," John explains. "Doubling in size as a department actually means four or eight or sixteen times as many things to do. Ugur has done a really wonderful job, with Tom's huge help, but the workload has gotten to be too much." He's looking forward to being a "sanity check" and sounding board, helping with tasks and giving feedback on ideas that are important but don't require the attention of the Executive Committee.
As one example of an early task, Spike explains that he'll be helping allocate faculty members to the various Brown CS committees in late summer.  "It's mostly mechanical," he says, but no doubt it's still a considerable balancing act, and a careful weighing of preferences and aptitudes and workloads. A bigger effort will be to work with the faculty to conduct a self-study of Brown CS, looking at strengths, weaknesses, obstacles, and opportunities. It's the groundwork for a detailed review that Brown conducts approximately every five years to guide its academic departments and help them make improvements.
What does this change of roles mean for his career? "It might end it!" he jokes, but quickly turns serious. "My whole career has been a little peculiar. I was trained to be a mathematician, so nobody taught me how to teach, or to run a research group. Likewise, I haven't been trained in administrative work, but I've seen enough of what happens here to know what goes on, and I have a pretty strong sense of fairness. I'll be relying on that to be my guide when tricky things come up." 
If there's one thing John wants to do, he says, it's simplify, looking at every aspect of the items that cross his desk and asking if they're all necessary. "If I find that we've been asking what a particular committee has done for the past five years, it might be a good choice to get rid of it and talk about the same issues over coffee."
Few are likely to object, and faculty, students, and staff have already started sharing their enthusiasm for Spike's energy and skill. "I'm truly grateful to John for his willingness to serve in this demanding role," says Ugur. "I've had the privilege to work with him closely as part of our Executive Committee, and have always been informed and inspired by his exceptional wisdom and intellect. At a time when CS has become a central discipline for all and our department has become the largest at Brown, getting Spike's help in this leadership role will let us better address the needs of our increasingly bigger and more diverse community."
When John Savage came to Brown a half-century ago, computing was thought of as a service, not a discipline, and the idea of a Department of Computer Science at Brown was highly controversial.  Few could have predicted the infrastructure necessary today, or the challenges we face in the areas of cryptography, cybereconomics, big data, and many others. One of the founders of Brown CS, John's interests have changed as our field has changed, leading him to contribute to such areas as computational complexity, scientific computation, computational nanotechnology, and cybersecurity policy and technology.  
On Friday, May 26, 2017, we held a special event to celebrate how John Savage and Brown CS have evolved together. Speakers that included Brown University President Christina Paxson, Senator Jack Reed, and several of John's students and colleagues gave research talks and shared personal reminiscences.
Video recordings of all talks are available here.
Professor Shriram Krishnamurthi of Brown University's Department of Computer Science (Brown CS) has just accepted a four-year commitment to serve on the Editorial Board of the Research Highlights section of Communications of the ACM (CACM-RH), which is devoted to publishing the most important and interesting CS research of recent years.
Chosen for the depth of his expertise and breadth of his research areas, Shriram will identify and nominate papers for consideration, assess nominations that fall within his interests, contribute to CACM-RH internal governance, communicate with authors, and facilitate the creation of the technical perspectives that accompany articles. 
Shriram will serve alongside 26 thought leaders in the field from both academia and industry, a list of luminaries that runs from Abadi to Zeller. Former members of the Editorial Board include Professor Maurice Herlihy of Brown CS and Brown CS alum David Salesin.
"I'm delighted, of course," says PhD candidate Esha Ghosh of Brown University's Department of Computer Science (Brown CS). "There are so many reasons, but this is special to me because it focuses on underrepresented groups. It reinforces the confidence of young women that they belong in STEM fields."
It's a significant moment: Esha is among the inaugural recipients of the Microsoft Research Dissertation Grant, which targets members of underrepresented groups in computing who are past their fourth year in a PhD program at a US or Canadian university. "Awards were made based on the technical merit and potential for impact of the proposed dissertation research," explains Meredith Ringel Morris, Principal Researcher and Chair of the Microsoft Research Dissertation Grant Program. "We had the difficult task of awarding only a few grants from the nearly 200 applications we received, so this was a very selective and competitive process."
Esha's work focuses on cloud computing and security: in a world where increasingly organizations and and individuals outsource data storage and application processing to the cloud, she is developing innovative solutions that allow users to benefit from cloud computing to get quick, verifiable, and tamper-proof answers that don't reveal unnecessary information to the cloud provider. Esha is advised by Professor Roberto Tamassia and expects to complete the requirements for her PhD by the summer of 2018.
The grant will fund her research (the working title is "Efficient, Secure, Privacy-Preserving Cloud Storage and Computation") in the amount of $20,000. Among other things, Esha expects to use it to hire interns to help complete the implementation of her prototype system. She and the other recipients will be visiting Microsoft Research’s headquarters in Redmond, Washington for a two-day career workshop on November 13 and 14, where they'll have the opportunity to present their dissertation work and receive feedback and career advice from a panel of Microsoft’s research scientists. "I'm also excited about the chance to present my work and get feedback," says Esha. "It's a great mentoring opportunity."
Brown University's Department of Computer Science (Brown CS) has just added another new faculty member: Ellie Pavlick is joining us as Assistant Professor in the summer of 2018. “I’m still interested in economics,” she says (Ellie has a BA in that field and a BM in saxophone performance), “but I felt like it was much messier.” Maybe so, but after sitting down to talk for an hour, it’s clear that Pavlick in no way finds messiness to be a source of discomfort. An expert in crowdsourcing and paraphrasing, studying language in all its complexity has allowed her to use what she calls a problem-driven approach, combining sprawling interests in a world where no two words mean the same thing.
If you think that description sounds like a good fit for Brown University, you’re not alone. “I love the broad interests at Brown,” says Ellie. “Understanding language is a deep, difficult interdisciplinary problem. Academia lets you play that long game. And getting to work with students means less uniformity. Each student has their own interests, so they help you get involved with a lot of different things.”
Pavlick’s own involvement with computer science came surprisingly late. “I’ve always been interested in multiple things,” she says. “I find it hard to feel fulfilled with just one area. I think if I’d really had a chance to explore CS in high school, I would’ve gone into it sooner because it provides this common skill set that can be applied to so many areas.” During undergraduate studies at Johns Hopkins, using LaTeX and MATLAB sparked Ellie’s interest in the field. Soon after, participation in Women in CS events helped her find an advisor and start new research projects. After graduation, when one of her professors transferred to University of Pennsylvania, she followed, giving up plans for a Master’s in favor of a PhD.
Natural language processing and crowdsourcing appealed to Pavlick from the beginning: “People speak and understand human language, not programming languages, which is what computers quote-unquote ‘speak’. I find it really exciting: our language is very complex, and it’s not designed for talking to computers. It’s full of not just commands but implications and feelings. Can we get computers to unpack all of that implicit meaning?”
Driven by my own interests, I pull Ellie into a side discussion of writerly craft and whether any two words can mean exactly the same thing. Does she see herself as a writer? “Definitely. I like working with language because it’s an incredible mix of structure and freedom. I used to think I’d end up as more of an artsy person...even when I write up research, which I guess is technical writing, I really enjoy it and probably overthink it. People can be very imprecise with language, but we can also be very particular about how we say what we say. That has gotten less attention from an AI perspective. People write and speak with the expectation that the reader or listener is a human, so we use a lot of hints and cues to indicate our opinions and impressions and these are completely lost on computers.”
And that person-to-computer communication, ultimately at a seamless level, is coming, Ellie thinks, but the timeframe is tough to estimate. “We’ve made incredible progress in AI on things that look like language understanding,” she says, “but the models are shallow. As a field, we’ve just started a conversation about modeling intents and goals, but computers haven’t even begun to understand how human language expresses differing personalities and similar but distinct goals.”
The way to get there, Pavlick says, is to make computer science even more interdisciplinary than it already is: “We can’t just keep cranking away at what we’re doing. Natural language processing is a young field, and it’s hard to predict what kind of knowledge is needed next. People coming into CS with different interests and perspectives provides the raw ingredients we need for progress, for the next advance.”
And having a different methodology is equally important. “One question I get a lot,” Ellie says, “is if deep learning will solve all of our problems. ‘Isn’t language solved?’ As computer scientists, we sometimes fall into this love affair with a certain method. We find an exciting model and start looking for things to apply it to, and then want to report on what’s going well. What I like about Brown is the problem-driven approach, which drives you to collaborate and forces you to question and to choose the best methods from a large, interdisciplinary set of possible methods. Research should be based around the things we don’t know how to do.”
In a recent letter to the Brown University community, President Christina H. Paxson announced the appointment of sixteen faculty members to named chairs. Professor Stefanie Tellex of the Department of Computer Science (Brown CS) was among them: she becomes the Joukowsky Family Assistant Professor of Computer Science, alongside colleagues from the Departments of Physics, Anthropology, Environmental Studies, and others.
"I'm really honored by the recognition from the University," Stefanie says, "and I thank the Joukowsky family for their generosity. This is not just a nod to me, but to all the students and postdocs working beside me in the lab, as well as to all the wonderful support staff here at Brown. Their hard work and creativity are the reason for our success."
Stefanie has been a Brown CS faculty member since 2013, and her research aims to create robots that collaborate with people to meet their needs, so that human-robot collaboration approaches the ease of human-human collaboration. To create collaborative robots, her Humans To Robots Laboratory focuses on three key challenges: 1) perceiving the world using the robot’s sensors; 2) communicating with people to understand their needs and how to meet them; and 3) acting to change the world in ways that meet people’s needs.
She's the third named chair for Brown CS in the past two years, following Eugene Charniak (University Professor of Computer Science), Maurice Herlihy (An Wang Professor of Computer Science), Sorin Istrail (Julie Nguyen Brown Professor of Computational and Mathematical Science), Franco Preparata (An Wang Professor of Computer Science), Eli Upfal (Rush C. Hawkins Profesor of Computer Science), and Andy van Dam, (Thomas J. Watson Jr. University Professor of Technology and Education).
"We're very happy to see this recognition," says Department Chair Ugur Cetintemel. "Stefanie has been pursuing an ambitious research agenda with success and it's wonderful to see Brown acknowledging her achievements after just four years at Brown CS."
Brown University's Department of Computer Science (Brown CS) has made it our mission to create and sustain a diverse and inclusive environment in which all students, faculty, and staff can thrive. Few things are more meaningful than to see our female-identifying students become known leaders and role models in the field, and Brown CS undergraduate Tiffany Chen has just received significant recognition: she's been named a Women Techmakers Scholar. The award is part of Google's Women Techmakers Scholars Program, which aims at furthering Anita Borg’s vision of creating gender equality in the field of computer science.  
Last year, Brown CS student Julia Wu was a recipient of the scholarship (formerly known as the Google Anita Borg Memorial Scholarship), and Sharon Lo and Danaë Metaxa-Kakavouli were 2015 recipients. Other students winning Google scholarships over the past two years include Daniel Milstein, Eli White, and Luis Aguirre.
When John Savage came to Brown a half-century ago, computing was thought of as a service, not a discipline, and the idea of a Department of Computer Science at Brown was highly controversial. Few could have predicted the infrastructure necessary today, or the challenges we face in the areas of cryptography, cybereconomics, big data, and many others.
One of the founders of Brown CS, John's interests have changed as our field has changed, leading him to contribute to such areas as computational complexity, scientific computation, computational nanotechnology, and cybersecurity policy and technology. In place of our annual reunion, we hope you’re planning to join us in Room 130 of 85 Waterman Street from 2-6 PM on Friday, May 26, for a celebration of how John Savage and Brown CS evolved together, including research talks as well as personal reminiscences.  A reception will follow at 115 Waterman Street on the third floor of the CIT.
In preparation for all the fun, let’s take a moment to go back in time and take a look at 50 years of co-evolution.
Starting Out
“I’ve had an interesting career,” says John Savage. He’s sitting in his office on the fifth floor of the CIT, looking out onto rooftops in January sunlight. “I don’t enjoy talking about myself, but I like discussing my work. I hope it provides motivation for others to reach out, to experiment.”
The impetus of that career, which has ranged as widely as any in the field, began in Lynn, Massachusetts, where John was the oldest of six children, growing up in a French-Canadian neighborhood and attending schools where some of the instruction was in French. (He still speaks the language.) “Somehow,” he says mysteriously, he acquired a BB gun in his preteen years. “But I was told it was dangerous and urged to get rid of it.”
John set aside his hopes of becoming a sharpshooter, abandoning the gun in favor of a crystal radio. “I was absolutely fascinated by it,” he says. In his teens, studying at the now-defunct Saint Jean Baptiste High School, a French-Canadian parochial school, he became a ham radio operator, upgrading to a 400 MHz receiver/transmitter and working on his Morse code skills until he could transmit 17 words per minute. Like many other enthusiasts, he enjoyed communicating with peers around the world. “But unlike them,” John says, “I was more interested in the technology.”
Inspired to pursue electrical engineering in college, John knew that he needed physics, so he took a night class at the public high school. He soon discovered that the physics teacher, an out-of-his-element basketball coach, was confused by course material about levers that he was supposed to be explaining to students. He sent out his college applications, and the acceptances started coming in: Tufts, then Northeastern. But at MIT, his first choice, John was waitlisted. This meant a visit to the Dean of Admissions, the intimidatingly-named B. Alden Thresher. “He sat at this massive desk in the middle of a big room,” John remembers. “His secretary pulled my folder, and then he looked at my letters of recommendation. ‘Nothing special!’ he said.”
Antone (“Tony”) Medeiros, Professor Emeritus of Medicine at Brown, met John in 1976, and they biked around the East Side every morning together for years. He adds an interesting detail to the story: “When the dean told John that ham radio enthusiasts didn’t often do well at MIT, John had an answer, which was that he was less interested in radio techniques and more interested in the ideas behind the technology. That says something.”
MIT Years
Whatever the reason, Thresher relented. It was an early success with something that John was to do again and again in his future career, making a case for something against difficult odds and delivering a convincing argument. MIT was a financial challenge as much as an intellectual one: John commuted from home, paying tuition first with a $400 scholarship from his church and earnings from a paper route, then working in the refectory, then with a $100 loan from his father, a self-made man whose parents had died young. Eventually, a Massachusetts Bay Scholarship eliminated the need for John to work, which was fortunate. “Going in,” he says, “I’d only had a half-course of trigonometry, and some of my classmates had taken calculus. At the end of the first semester, my grades were exactly the mean for my class!”
As an undergraduate John joined the Cooperative Program at MIT and spent semesters with 25,000 engineers at Bell Labs, working at locations in New Jersey and Massachusetts. Before each assignment he requested that he be allowed to work with antennas, a subject that fascinated him. He was taken to Holmdel and told that they needed to calibrate a massive antenna by pointing it at radio stars, a job known as “bore sighting”. Deciding that this was not the kind of antenna work he envisioned doing, he took an assignment in signal processing. However, the people who did work with the Holmdel Horn Antenna noticed an inexplicable hissing sound. When it was investigated it provided evidence for the Big Bang and led to the Nobel Prize in Physics for Arno Penzias and Robert Woodrow Wilson for the discovery of cosmic microwave background radiation. John had narrowly missed a big opportunity.
John earned his ScB from MIT in 1961, then stayed on to earn his ScM a year later and a PhD in 1965. A return to Bell Labs followed, where one of the projects he worked on was a digital data scrambler that altered scrambled a bitstream so that it would be unintelligible without the appropriate descrambler. He also met his wife, the descendant of multiple Brown alums, in Cambridge (John had borrowed a friend’s ID to obtain access to a Harvard event). Her father played a key role in John’s choice of both career and home.
“I thought I’d get an industry job, but my father-in-law recommended Brown. Providence was very different then: there wasn’t any Interstate 95, and we wondered where we’d live. I thought we’d be here about three years,” John laughs.
A Combined Approach
John’s industry experience proved to be extremely useful for future academic work. His PhD thesis had been on sequential decoding, and at Bell Labs, he noticed the huge disparity between the size of encoders and decoders. It was the beginning of a long-term interest in how the idea of complexity was relevant to computing in general, which John pursued across multiple research projects, including a 1972 paper (“Computational Work and Time on Finite Machines”) for the Journal of the Association for Computing Machinery.
Barrett Hazeltine, Professor of Engineering Emeritus and Adjunct Professor of Engineering, has known John from his earliest days at Brown. “It’s hard to explain this now that everyone understands the value of computer science,” he notes, “but we deal with gigabits now, and John was a real pioneer of complexity and minimization in the days when you had to build every gate by hand. There really wasn’t anyone else doing CS with his combined approach of engineering, math, and theory.”
In the years to follow, Savage’s interests grew to encompass applied theory of computation, which includes space-time tradeoffs in serial computation and area-time tradeoffs in the very large scale integrated (VLSI) model of computation, as well as silicon compilers (John notes that colleague Steve Reiss wrote code for one of the earliest of these) and the parallel algorithms used with them.
“Computational complexity is still interesting to me,” John says. “One of my early computer science results was reminiscent of the Heisenberg Uncertainty Principle. I demonstrated that the product of storage space and computation time on a random access machine could not both be made arbitrarily small simultaneously. The circuit complexity of the problem being computed puts a lower limit on the product.”
A New Framework
John’s imagined three years at Brown became six, and when his first sabbatical arrived, he brought his wife and two small children to the Netherlands. While there, an important long-distance conversation with colleagues Andy van Dam and Peter Wegner was starting. A movement had slowly begun: universities nationwide were establishing Departments of Computer Science, primarily for graduate studies.
At the time, some faculty members of Brown’s Divisions of Applied Math and Engineering considered themselves computer scientists and were teaching what were effectively CS courses. Capitalizing on those early efforts, Savage, van Dam, and Wegner wanted to create a new framework for the study of CS at Brown. An initial attempt to obtain permission for a Department of Computer Science failed, but with the aid of Maurice Glicksman (first Dean of the Graduate School and later Provost), the three created a cross-Division Program in Computer Science in 1975.
“After we proved through being a Program that we were a survivable entity,” Andy remembers, “we asked again about becoming a Department, and that's when the antibodies came flooding out. There was fear and even anger that we were proposing to remove valued parts of both Applied Math and Engineering, and Maurice had to make assurances that new positions would be made available to the Divisions. Throughout the whole effort, John and I had complementary views that CS was worthy of becoming a unit of some kind, and we each represented part of the entire discipline. He and I worked together to advance the ball, so I know how much effort he put into it. He was definitely skilled at building the argumentation and delivering it effectively.”
Four years after the Program was established, John assembled a committee that included such luminaries as Dick Karp (University of California, Berkeley), Peter Elias (MIT), Juris Hartmanis (Cornell), and Alan Newell (Carnegie Mellon), and the trio’s years of effort finally paid off with the creation of the Department of Computer Science in 1979. “John saw beyond that era’s conceptions of what computer science was,” says Tony Medeiros. “And then he ran with it!” But even the inaugural symposium, which John organized, wasn’t without its challenges: a student, perhaps overawed by the honor of carrying slides for the world-renowned Donald Knuth, who gave the keynote address, promptly spilled the entire carousel on the floor.
“Our department has worked together so well for decades,” John says, “and to some degree, I take it as a measure of success of our launch. We established practices that are still followed today, and we balanced theory and practice at a time when our competitors were only doing one or the other. One of the first things that our new faculty members say is that our faculty gets along. By no means is that true everywhere!”
In The Chair
The 1980s were a time of rapid growth for Brown CS. Early in the decade, grants from the National Science Foundation and Exxon funded the first electronic workstation classroom and the first large group of networked Apollo workstations anywhere. John followed Andy as Department Chair, and by his second year in the role, new workstations were needed. John led the effort of creating a request for proposal to all vendors, which were eventually narrowed down to Sun, DEC, and NeXT. The goal was what was called a “3M” machine: one million bytes of memory, cycles per second, and pixels on a screen. A highly contentious “bake-off” (Steve Jobs of NeXT was in his argumentative prime) followed, with Tom Doeppner making the case for Sun, Steve Reiss for DEC, and Andy van Dam for NeXT. Sun was the winner, resulting in a multi-decade relationship.
“There was always a lot of back and forth when I was Chair,” John remembers. “Between students and faculty, among faculty members, with the administration. I started weekly faculty meetings over lunch to help us solve problems collaboratively, and they continue today.”
Andy notes, “As Chair, John maintained his strong interest in growing the department. He stayed the course, which for that particular time period was much more challenging than it sounds, given that we were still a very small department trying to compete for faculty, students, and research grants with much better-known and much larger CS departments. Absolutely, he was a great Chair, always active, always helping to build in a variety of ways.”
Another of John’s major contributions was the Industry Partners Program (IPP), which he founded in 1989 in conjunction with Brown’s Development Office and Roy Bonner of IBM. The program creates closer connections between Brown CS and industry, and member companies are encouraged to recruit students, participate in the selection of topics for IPP symposia, and advise on the employment and research needs of corporations. Income from the program has proved crucial to the department’s growth, supporting everything from equipment to faculty searches to distinguished lectures.
The most visible reminder of John’s days as Department Chair is the Thomas J. Watson, Senior Center for Information Technology (the CIT). Andy explains that the building was finished on John’s watch, and he oversaw its execution: “You could count on John. If he said he’d do something, he got it done, on time, and with high quality.” Professor Stan Zdonik adds that John’s work as Department Chair was one of his biggest contributions to Brown CS. “He took the job extremely seriously and did great things,” says Stan. “He was part of a small group of people who deserve huge credit for basically getting the CIT built. That took an incredible amount of work with the university, getting people to agree.”
An Awful Lot To Learn
As the 1980s turned into the 1990s, John’s research interests turned toward scientific computing, working with a student named Jose Castellanos who later joined the IBM Blue Gene team. In addition to their work on implementing a finite element method in distributed computing and mesh adaptation to 2-space and 3-space tetrahedral to simulate the Navier-Stokes equations used to model fluid moving around obstacles, again with an eye toward distributed processing.
It was more evidence for the polymathy that’s been a hallmark of his career. “When I met John in 1976,” says Tony Medeiros, “personal computing was in its early days, and I was a bit of a techie, which meant stepping outside my field, but I was nothing compared to John, who was full of wonderful ideas in my area of interest, microbial antibiotic resistance, and everything else. You couldn’t bring up a topic that he wasn’t intrigued about.”
“Ideas excite me,” John says when asked to explain his wide-ranging interests, telling the story of how he took charge of his college education after being poorly prepared in high school, and was motivated to delve into computational nanotechnology by reading through a rising stack of nanotechnology papers on his desk, plunging in even when he didn’t fully understand them. “I can’t really help being curious about things, and I have other interests that I haven’t fully explored, like cryptography. There’s still an awful lot to learn.”
John’s work with nanotechnology began in 2001, with a one-year starter grant. That led to a large four-year NSF grant that he shared with Charles Lieber of Harvard and André DeHon of Caltech. He says, “When I discovered nanotechnology, I thought it would have the impact that the VLSI revolution did in the 1970s. It hasn’t turned out that way yet, but it might eventually.” Meanwhile, he was learning by attacking the problem in several different directions: giving lots of talks and developing an introductory course on nanotechnology that included quantum computing, nanowire-based technology, and synthetic computing. His research continued until 2011, and he earned his last patent in the area in 2016. Most recently, John’s interest in applied theory of computation has led to publications on input/output complexity for multi-core chips with shared memory.
According to Barrett Hazeltine, the pleasure that John gets out of new and varied research finds synergy with his interpersonal skill: “When you talk to John, you see that he’s enjoying what he’s doing, and he expects people to do the same. ‘Wouldn’t it be fun if’ is how he starts a conversation. Maybe because of his many interests, he gets people to approach their research in another way, and he’s gracious, he makes you think you’re the most important person in the room. He’s a very strong colleague who’s had the courage to change the university for the better in many ways.”
Service Near And Far
Much of this change has occurred through John’s decades of service to Brown, often during times of protest (in response to the effect that the proposed 1975-1976 budget would have on minorities, students occupied University Hall) and strained relations between the faculty and administration. Savage has been Chair, Vice Chair, and Past Chair of the Faculty, Chair of the Task Force on Faculty Governance, Chair of the Nominations Committee, Chair of the Search Committee for Vice President for Public Affairs and University Relations, President of the Faculty Club Board of Managers, and a chair or member of numerous other committees. Among many other achievements, he formed the Academic Priorities Committee, made significant changes to policies concerning budgets and tenure, and created a massive Handbook for the Task Force on Faculty Governance. In recognition of all these efforts, John received the President’s Award for Excellence in Faculty Governance in 2009.
“A lot of people see service as a chore,” Barrett adds, “but John is both a leader and a doer. He answered the question of how faculty can usefully contribute to how Brown runs. He’s one of the people responsible for all the benefit we get from being in a community where the best ideas can come forth and be made known.”
And although it might not be service in the traditional sense, Tony Medeiros points out the Savage family’s incredible allegiance to Brown: “I don’t know if anyone else has said it, so I want to mention that John may have set some kind of a record: all four of his children went to Brown and married Brown alums. He’s a great father and family man, a wonderful friend.”
John’s service has also taken him far beyond the Van Wickle Gates. His contributions to professional societies have been many, including being chosen as a member of the NSF Review Panel on Emerging Technologies and the Program Committee for the IEEE/ACM International Symposium On Nanoscale Architectures. But it’s John’s work in cybersecurity and Internet governance that’s had a truly international scale: he’s served as a Jefferson Science Fellow for the US Department of State, a panelist for the Global Futures Forum in Singapore, an honored guest of the government of Vietnam, and a member of the Scientific and Technical Intelligence Committee.
Judith Strotz, Director of the Office of Cyber Affairs at the US Department of State, worked extensively with John when he was asked to help policymakers in her newly-formed Office better understand science and follow rapidly-emerging trends. “To me,” she says, “the main thing is that he trained us all. It doesn’t sound momentous, but it really is. It’s really hard for non-scientists to address all these issues, and we made huge strides thanks to his knowledge, persistence, and patience. He’s a true expert and a great colleague who helped us sort through problems and find a way forward. I miss working with him.”
John is also a Professorial Fellow for the EastWest Institute, a non-profit organization with the goal of reducing conflict worldwide. Bruce McConnell is the organization’s Global Vice-President, and one of Savage’s frequent collaborators, including a recent paper on Internet governance. He says, “John’s influence and impact are immeasurable…He’s hardworking, and when he says he’ll do something, he gets it done. But then he goes beyond that by bringing his enthusiasm, curiosity, and creativity. He always opens up options.”
“It’s been great fun,” John says of his time in Washington, remembering days when he had the maximum level of security clearance, unable to bring any technology into his office and required to store his top-secret disk in a safe every night. Always the educator, he makes an interesting point about autodidacticism: “90 to 95 percent of the information used in intelligence analysis is open source. A deep understanding of a region or country is something that can be taught, and avid readers who devote themselves to an area can teach themselves.”
Great Teaching
When asked about John’s contributions, Judith Strotz begins with his instructional skill, saying, “John is really humble. He taught our team of policy makers in simple terms at first, but without ever being condescending. He showed respect and understanding of where everyone was at in order to help them learn. To me, that’s great teaching.”
Over the years, John has taught more than 15 courses at Brown, covering topics from computational complexity to cybersecurity. This spring, he’s teaching CSCI 1800 Cybersecurity and International Relations for the seventh time, bringing the subject material to life with a series of world-class speakers, including an expert in cyber economics and the National Intelligence Officer for Cyber Issues.
He’s also fielded multiple winning teams of students at the CYFY, CyberSeed, and Cyber 9/12 competitions. “Teaching is how I get firsthand experience,” he says, citing a class (CSCI 1951-E Computer Systems Security: Principles and Practice) that functions as an enriched version of a course developed by Professor Roberto Tamassia. “The best way to fill gaps in my knowledge is to study something, then teach it.”
Co-Evolution
The theme of co-evolution was John’s carefully-considered choice for the upcoming celebration in May. It reflects not just his multifaceted career and the growth of our department but the evolving face of computer science and the continuing challenge of developing the technology needed for a changing world.
“Start by thinking about how difficult it is to build an operating system,” John says. “Very roughly, you’re looking at 80 million lines of code that have to be designed, written, tested, documented. If someone can write 20 lines of code per day, at 250 working days per year, you’re looking at 16,000 person-years of work. It’s an enormous effort.”
At the same time as John and his colleagues were crafting policy and procedure for an evolving Brown CS, they were creating an equivalent framework for computer science itself: “Over the years, we’ve had to build the entire infrastructure to support our own discipline: we had to invent computer graphics so we could have graphical user interfaces, we had to create operating systems that scheduled jobs correctly, we had to define protocols for sending packets so the Internet can work. If this had been commonly understood in the 1970s, we would’ve been given a department the first time we asked!”
And yet there’s no sign of cynicism in the remark. Bruce McConnell sees John’s positivity as an unmistakable part of his secret to success: “He always sees another angle and brings in ideas from other domains. It’s intellect plus optimism. For John, the glass is always more than half full!”
The next set of challenges, John explains, demands rigor. As scientists, responsibility has been placed directly in front of us. “Most people can learn to write software for simple tasks,” he says, “but the difficult problems need scientists. There are new challenges in artificial intelligence, cybersecurity, nanotechnology, data science. There are very specific problems, like the role of social media in promoting fake news or the ethical questions of self-driving cars, which demonstrate that we’re in an era where the technology we invented has unintended consequences. Computer science has become a critically important field. These are technical challenges that demand expertise and will play out on a social, economic, and geopolitical scale. We need to play a central role.”
That role is also an interdisciplinary and intersectional one: “Computer science will become important to more and more people. I’d love to see more women in the field and more historically underrepresented groups: we need different perspectives because of the impact that we’re having on international relations, engineering, and other areas. I want to see people be tolerant and generous with each other -- we’re all human beings. A hundred thousand years of thought have gone into our survival, and we need to cooperate and pass on knowledge. My message to the next generation of students is to stay positive: when you encounter problems, get the best advice, work them out, keep plowing through, and don’t give up.”
John Savage’s track record shows a more than ample willingness to take up that gauntlet with energy and purpose. “He has this quality,” says Andy van Dam, “of positivity and non-superficiality, of digging in, of having a grounded position and willingness to keep pushing at something.”
“Whatever the challenge is,” says Tony Medeiros, “he always has ideas. It’s trivial to say that John is incredibly smart, but his enthusiasm has always taken him everywhere, and it still does.”
Could anything be more valuable to a young startup than expert-led coaching sessions on how to identify opportunities and design business models, or the chance to pitch ideas to industry leaders? Probably not much, but a check for $100,000 might be a close contender.
NYU Stern's W. R. Berkley Innovation Labs $300K Entrepreneurs Challenge, held at New York University's Stern School of Business, offers all those things. Last week, PhD Candidate Thomas Dickerson of Brown University's Department of Computer Science (Brown CS) returned from the competition with some valuable experience and one of those comically-oversized checks that are almost too big for one person to carry. After months of workshops, coaching sessions, and deliverables, his startup, Geopipe, was selected for its potential for great impact, challenging assumed boundaries, and inspiring a sense of what's possible. It won the top prize of $100,000 in the Challenge's Technology Venture Competition.
Co-founded last year with Dr. Christopher Mitchell, a New York University alum, Geopipe builds algorithms to turn 2D and 3D data into highly detailed 3D virtual models. Their system ingests and analyzes data, including satellite photos, maps, laser scans, and much more. It then combines machine learning with a distributed systems approach to rapidly correlate data sources, understand structure, and produce complete 3D models at many different scales. One of Geopipe's strengths is that it offers more semantic modeling than competing solutions with massive amounts of content while coupling models to real-world data without a great deal of manual effort. It puts models in the hands of customers, who can use them in their own software suites with automatic customization options at a consistently high level of visual quality.
"This is an extremely important moment for Geopipe," says Thomas, "and we'll put the money to good use. We have a pretty extensive R&D road-map for the next 12 months, and we're looking to balance pushing that with getting hands-on feedback through pilot programs with customers in the architecture market."
The image above is ©NYU Photo Bureau: Hollenshead and used with permission. For more information about this story, click the link that follows to contact Brown CS Communication Outreach Specialist Jesse C. Polhemus.
In addition to advancing the field at a local and national level, Brown CS community members have a global impact on education and research. Click the links that follow for more content on Brown CS around the world and Franco Preparata.
Collège de France has invited An Wang Professor of Computer Science, Emeritus Franco Preparata of Brown University to be the keynote speaker at the forthcoming colloquium "Computational Geometry and Topology in the Sciences". The event will be held in Paris on June 6, 2017, and Franco will address the topic of "Computational Geometry: the Early Days" in his remarks.
From a 16th-century automaton and a metal swan that wowed Mark Twain to present day wonders that care for the elderly and help autistic children develop empathy, a new exhibit ("Robots") at London's Science Museum is one of the most comprehensive of its kind ever assembled. The Humans to Robots Laboratory, led by Professor Stefanie Tellex of Brown University's Department of Computer Science (Brown CS), contributed a demonstration of their Baxter robot, which they've used to do pioneering work in robotic grasping, robot-to-robot communication, and human-robot interaction. The exhibit will run until September 3, and it's already received extensive coverage from the Telegraph, the Guardian, the New Scientist, and Nature.
Last semester, for the first time, Computer Science became the most commonly-chosen major at Brown University. The total number of students majoring in CS was 321, exceeding the second most popular major by more than a sixth. This semester, the margin is even greater: 440 students are majoring in CS, a quarter more than the next-highest selection. As another way to look at these numbers, more than one sixth of Brown students with declared majors picked computer science.
What is computational thinking, and how should it be taught? Almost three decades after the term was coined, the answer is still being worked out. How do we help students reframe research to formulate questions to a data set instead of a human? How do we teach broad use of computation to students who won't become computer scientists?
Last month, the New York Times explored these questions with a selection of students, academics, and even developers of children's media. They took particular interest in a course (CSCI 0030 Introduction to Computation for the Humanities and Social Sciences) that was created ten years ago at Brown by Professors Tom Doeppner, John Hughes, Shriram Krishnamurthi, and Steve Reiss to help students learn this "new mode of thinking". Featured in the article and interviewed in depth, Shriram and Stephen Brawner, a PhD candidate currently teaching the course, share their experience with the challenges of helping students learn not just to program but to use programming to solve problems.
The full article is available here.
In the era of big data, there’s plenty of software on the market that helps people to explore and visualize datasets in search of patterns and new discoveries. But how can users tell if the patterns they’re seeing are real or if they simply appear in the data by random chance?
The answer is that they can’t — not unless they apply appropriate statistical tests to make sure their findings are valid, a feature that currently available commercial data exploration tools do not provide. But with a new $3.1 million grant from the Defense Advanced Research Projects Agency, Brown University computer scientists are aiming to develop a software package that brings new statistical rigor to interactive data exploration.
“The goal is to build a user-friendly system than can easily explore data and produce useful visualizations, but also continuously controls for the statistical validity of the results,” said Eli Upfal, professor of computer science at Brown and the project’s principal investigator. 
The grant brings together a team of Brown professors, postdocs and students to tackle different aspects of the project. Tim Kraska, an assistant professor, and Carsten Binnig, adjunct associate professor, are machine learning and database experts who will work mainly on the data management side of the project. Computer graphics pioneer Andy van Dam will work on the user interface and visualizations. Upfal, an expert in computational theory, will work mainly on the statistical side of the project.
Statistical Safeguards
Statisticians and scientists routinely use a suite of tests to measure whether or not a result is statistically significant. But the statistical issues in the big data world go well beyond basic significance tests. Modern data exploration tools make it easy to poke and prod a dataset in myriad ways with a few mouse clicks. That can create an issue known to statisticians as the “multiple comparisons problem,” and it’s one of the things Upfal and his colleagues hope to address. 
The problem is essentially this: The more questions you ask of a dataset, the more likely you are to stumble upon something that looks like a genuine correlation, but is actually just a random fluctuation in the data. Without proper statistical correction, it can lead to false discoveries.
“To some extent it’s our fault here in computer science that we have made analysis of data so easy,” Upfal said. “If I give you a huge database and let you simply push a button to ask question after question, you’re eventually reach something that’s there purely by chance.”
There are statistical techniques for dealing with the problem, but none of them are easily implemented in a real-time data exploration setting. So Upfal and his colleagues will need to develop an appropriate technique on their own.
“We’ll be building some new theory about how to evaluate a sequence of data queries,” Upfal said. “And we’ll need it to be computationally efficient. Our system is interactive, so we need everything to compute right away.”
Better Data Science
The researchers envision a system that continually monitors the questions people ask in the process of exploring data and warns them when they’re on shaky statistical ground. By doing so, the system will help users — especially those without statistics training — to avoid making false discoveries.
And as the use of data exploration expands into new domains, statistical safeguards like these become ever more important. Companies like Netflix and Google for years have combed huge datasets looking for correlations that help them suggest movies or target advertising. In those settings, false correlations lead to a few bad recommendations or mis-targeted ads.
“That’s not such a big deal,” Upfal said. “But when we’re applying these techniques to medicine, for example, we need to be a bit more careful.”
The project will be part of Brown’s recently launched Data Science Initiative, which is broadly aimed at developing these kinds of novel approaches to dealing with data.  
“Ultimately we want to promote data science and see it be successful,” Upfal said. “We hope this project will be a step toward that.”
Three things that we take for granted when collaborating with other humans, perceiving, acting, and communicating, are some of the areas that are most difficult for today's robots. Assistant Professor Stefanie Tellex of Brown University's Department of Computer Science (Brown CS) has decided to tackle them all at once, and she's just won a National Science Foundation (NSF) CAREER Award to support her research.
CAREER Awards are given in support of outstanding junior faculty teacher-scholars who excel at research, education, and integration of the two within the context of an organizational mission, and Stefanie joins multiple previous Brown CS winners of the award, including (most recently) Jeff Huang, Rodrigo Fonseca, and Tim Kraska. 
"We're unifying perception, action, and communication," Stefanie says, "by using object-oriented multimodal decision-theoretic models, and the goal is to create robots that collaborate with us to meet our needs. This project enables a robot to acquire models for detecting, localizing, and manipulating physical objects, plan in very large spaces to find appropriate actions involving those objects, and communicate with people to learn how to use objects to meet their needs. We have to have this integrated model for a collaborative robot assistant because it needs to be able to robustly interpret natural language instructions, actively collect new information to complete its tasks, and ask for help when things go wrong." 
Stefanie's research will take place in two settings: assisting the disabled and elderly through object delivery in the home, and collaborative assembly in a factory. Along the way, the project aims to set a record by creating the largest-ever dataset of object instances by distributing data collection across everyone with a Baxter robot. Data collection will be carried out by high school students in an internship program that's been made possible in part by the Northeast Robotics Colloquium (NERC), which Stefanie co-founded, introducing hundreds of students from diverse backgrounds to the larger academic and industrial robotics community in the northeast.
Only a few months after the founding of Brown University's Data Science Initiative (DSI), the Department of Computer Science (Brown CS) is eager to share three innovative projects that represent the recent fruits of long-standing efforts to advance the state of the art in data science research:
Upfal, Binnig, Kraska, And Van Dam Win A $3.1M DARPA Grant For Quality-Aware Interactive Curation Of Models
Professor Eli Upfal (PI) and Professors Carsten Binnig, Tim Kraska, and Andy van Dam (co-PIs) have just won a Defense Advanced Research Projects Agency (DARPA) grant of more than 3.1 million dollars, and domain experts of all kinds stand to benefit.
In the past, researchers have attempted to make large scale data analysis more accessible, but existing machine learning (ML) tools are still far from allowing non-technical users to explore data interactively. They require a deep technical understanding and do not address the risk factors in iterative model development. Upfal and his collaborators have responded by developing the first system for Quality-aware Interactive Curation of Models (QuIC-M). The new project will enable domain experts to build models themselves without the need to involve a data scientist, and to do so in a risk-aware manner, with the software continuously monitoring the user’s interactions to automatically warn them about potential false discoveries, and if possible, suggest solutions or even automatically correct common mistakes.
Beginning with a selection of algorithms from a model family and hyper-parameter tuning techniques implemented in MLBase, the researchers have integrated these techniques into their interactive human-in-the-loop data exploration and model building suite, Vizdom/IDEA. Vizdom is a novel pen-and-touch interface that allows domain experts to curate ML workflows in an intuitive, fluid way, while IDEA is the back-end for Vizdom that enables the interactive curation and evaluation of models on large data sets.
The result will be interactive and quality-assured data exploration that continuously monitors users and warns them about potential wrong conclusions or models. Upfal and his colleagues expect that domain experts who take an introductory class in statistics and ML but are not data scientists will be able to use QuIC-M to build models an order of magnitude faster than an ML expert, with quality that's only slightly (and acceptably) inferior to the expert solution.
Tim Kraska Wins A Sloan Award For Democratizing Data Exploration And Analysis
Picture yourself in a meeting, looking up at a conference room wall. Not far in the future, Professor Tim Kraska expects it to be equipped with an interactive whiteboard that will enable a broad range of users to work together in a single meeting to visualize, transform, and analyze complex data in real time. Between our present and this future is a complete rethinking of the full analytics stack, from its user interface to its smallest components, and incorporating pertinent algorithms.
Tim has just been named an Alfred P. Sloan Research Fellow in one of the oldest and most competitive fellowship programs in the country. He's the eighth Brown CS faculty member to receive the honor, which Brown CS has now received for four years in a row. The fellowships, which take the form of a $50,000 grant used over a two-year period, honor and promote the science of outstanding researchers early in their academic careers who show outstanding promise for fundamental contributions to new knowledge.
Tim's work that will be funded by the fellowship aims to democratize data science by enabling a broader range of users to unfold the potential of their data through the development of a new generation of algorithms and systems for interactive and sustainable data-driven discovery. It includes three major components: Tupleware (a parallel high-performance UDF processing system designed for “normal” users, not the world's Googles and Microsofts), Vizdom, and new techniques to control the multi-hypothesis error.
At SIGMOD, Zhao, De Stefani, Zgraggen, Binnig, Upfal, And Kraska Will Present On Controlling False Discoveries In Interactive Data Exploration
Have you ever been skeptical of recent news items claiming that the secret to winning a Nobel Prize is eating more chocolate and that drinking a glass of wine is as good as spending an hour at the gym? Data-driven stories like these are prone to inaccuracy due to false inferences, and with the rise of interactive data exploration tools, the likelihood of error is significantly increased.
But after analyzing the think-aloud protocols of various user studies with more than 50 participants, researchers at Brown CS have an answer. At SIGMOD '17, a leading international forum for database researchers, PhD candidates Zheguang Zhao, Lorenzo De Stefani, and Emanuel Zgraggen, and Professors Carsten Binnig, Eli Upfal, and Tim Kraska will present the first end-to-end system, QUDE (Quantifying Uncertainty in Data Exploration), to automatically control the risk of false discovery for visual, interactive data exploration.
Their work offers a user interface and an initial set of meaningful default hypotheses to control the ratio of false discoveries without interrupting the exploration process, provides a superior and more modern criterion of controlling the false discovery rate, and demonstrates how the system controls false discovery for experts and novice users alike using generated and real-world data.
Looking Forward
"Data science has been a Brown CS strength for a long time," says Department Chair Ugur Cetintemel, "and the latest work of my colleagues in this area is rigorous, transformative, and will be extremely far-reaching because it addresses real pain points and pitfalls commonly experienced by data scientists. We're excited about the real impact ahead and all the collaborations this line of work will enable across the methods and applications of data science."
Jeff Brock, Director of the DSI, is also eager: “The DSI is extremely fortunate to count among its senior leadership and founding architects these path-breaking researchers in computer science and data science. These breakthrough research projects, each funded through an extremely competitive process, represent the driving philosophy for the DSI: that data and its tools should be accessible to the domain specialist,  distributable in an equitable and transparent manner, and interactive, in a way that maintains the virtuous cycle from model, to inference, to refinement at the hands of the human expert. The DSI is proud to support their work and count these innovative efforts among its first guiding projects."
At the USENIX Symposium on Networked Systems Design and Implementation (NSDI) this week in Boston, Massachusetts, Professor Rodrigo Fonseca of Brown University's Department of Computer Science (Brown CS) and collaborators from University of California San Diego and University of California Berkeley accepted an award for the most influential paper among those presented a decade ago at the annual conference. Rodrigo and co-author George Porter accepted the 2017 NSDI Test of Time Award for their paper (“X-Trace: A Pervasive Network Tracing Framework”) along with their co-author and former advisor, Professor Ion Stoica, at a luncheon on March 26 to honor a paper published at NSDI 2007. George is now a professor at the University of Calfornia San Diego, where he is a co-director of the university's Center for Networked Systems (CNS).
Rodrigo and George were at University of California Berkeley when they wrote the original paper, which was also co-authored with Professors Randy H. Katz and Scott Shenker of the same institution. “We wrote X-Trace while we were PhD students,” recalls George. “It was really an honor to work with my colleagues on this project, which formed the basis of Rodrigo’s and my PhD dissertations.”
Modern Internet systems often combine different applications, span different administrative domains, and function in the context of network mechanisms (tunnels, VPNs, overlays, and so on). In their 2007 paper, Rodrigo and his collaborators argued that “diagnosing these complex systems is a daunting challenge”. He says, “Many diagnostic tools existed at the time, but none existed for reconstructing a comprehensive view of service behavior.”
X-Trace was not the first tracing framework, but it was influential given that it was effectively the first framework for end-to-end tracing to focus on generality and pervasiveness. “It was based on the observation that an increasing number of systems would be built from heterogeneous components, built and operated by different people,” Rodrigo explains. “In contrast, existing tracing frameworks required a specific language, or were targeted to a particular system.”
The researchers implemented X-Trace in protocols and software systems, and in their prize-winning paper, they set out to explain three different use scenarios: domain name system (DNS) resolution; a three-tiered photo-hosting website; and a service accessed through an overlay network.
The image at left represents an HTTP request going through CoralCDN, a distributed content distribution network. The request is tried in parallel in four different web caches (the different colored paths) before succeeding at the last one.
Hari Balakrishnan, who co-chaired NSDI in 2007, broke the news of the Test of Time Award to the recipients. “We’re very pleased to share that your X-Trace paper from NSDI 2007 has been selected for an NSDI Test of Time Award,” he wrote. “The award honors a paper published ten years earlier at NSDI with retrospectively the most impact on research or practice.”
The X-Trace paper has proved to be prescient in both research and practice. “Today, many Internet-scale backend systems are built using a ‘microservices’ approach, with hundreds of loosely connected components tied together to offer larger services,” noted George. “Debugging these systems effectively requires what X-Trace provided: the ability to correlate events in one component to events in other arbitrary components, even if they were many steps far removed from the first.”
The rapid adoption of tracing began with Google’s introduction of Dapper in 2010, which offered a similar primitive to X-Trace. Twitter’s Zipkin and Cloudera’s HTrace were open-source implementations of Dapper. Another current competitor in the market, called Traceview, also has X-Trace in its DNA after a series of startups and acquisitions dating back to 2010.
“By 2015, many companies such as Netflix, Baidu, Uber, Facebook, and Etsy were deploying internal trace solutions very similar to our ideas presented in the X-Trace paper,” observes Rodrigo. “And the interest persists in a rather recent initiative called OpenTracing, which is trying to standardize end-to-end tracing.” In 2017, the excitement surrounding tracing continues unabated. For example, earlier this year, Amazon released X-Ray, which offers distributed tracing for Amazon Web Services, and another company, Datadog, also released a new end-to-end tracing product.
The NSDI award is not his first for his work on tracing: he co-authored a paper on "pivot tracing" that received a Best Paper award at the 2015 Symposium on Operating Systems Principles. That same year, Rodrigo won an NSF CAREER Award for his work on "causal tracing" to elucidate understanding of the performance of distributed systems. (Causal tracing covers a wide variety of tracing systems and frameworks, including X-Trace itself, as well as Dapper, Zipkin, HTrace, and many others.)
“It’s becoming increasingly difficult to understand how a system behaves, and, especially, how and why it fails,” he says. “Causal tracing is a technique that captures the causality of events across all components, layers, and machines, and it eases the task of understanding complex distributed systems.”
Besides his work on tracing, Rodrigo is more broadly interested in distributed systems and computer networking: "Ultimately, I'm always trying to design systems that are more useful, efficient, and understandable to developers, operators, and end users."
Brown CS gratefully acknowledges Doug Ramsey and University of California San Diego for the research and content creation that originated this news item. For more information, please click the link that follows to contact Brown CS Communication Outreach Specialist Jesse C. Polhemus.
"Nationwide, the concept of 'CS for all' has become ubiquitous," says Brown CS Professor Shriram Krishnamurthi, co-director of Bootstrap. "But not all students will become computer scientists. We know already that opportunities to use data will be part of their lives ahead. To be successful, they need to seize those opportunities and use data well, and we want to help them do that." Bootstrap is a series of research-based K-12 curricula now used globally, and they've just launched a data science program with the goal of helping students learn to use data effectively.
"We wanted to provide a foundation for data science and statistical thinking," says former Brown CS undergraduate and current Master's student Sam Dooman, who co-taught the program at Providence, Rhode Island's Central High School. "In Bootstrap Data Science, students write programs to ask and answer authentic questions with data.  By exposing students to these concepts in different contexts, we hope to provide more equitable computer science and statistics education."
Like other Bootstrap curricula, the data science program, which uses the Pyret programming language, integrates into existing classes, providing equitable access for school systems in which money isn't available to hire computer science specialists. Co-instructing with teachers Jennifer Geller and Tom Hoffman, Sam worked with a class of Social Studies students. For an inner-city school, the topic was extremely relevant: using Rhode Island Public School data to find correlations between school performance and demographics. 
"The students wrote programs to answer questions that real sociologists ask," Sam says, "and the students were really motivated. Most had never programmed before, and they were all able to complete the course and answer questions about the dataset. There was one moment when a student who had no prior experience with programming helped a friend to debug his code, and we hadn't even taught debugging yet, which was really cool. They started the program afraid that they'd break the computers, and almost immediately overcame that fear. It was incredible to see how fast they got a handle on Pyret, getting error messages at the start and then working through them."
The Bootstrap team is now in editing mode, evaluating the success of Sam's work and perfecting the curriculum, adding more exercises and scaffolding worksheets, and making some improvements to Pyret. This summer, the program will begin again with a workshop in Colorado to prepare another cohort of teachers.
"Data science now drives many aspects of our lives," says Sam. "Whether it's with SQL tables or Excel spreadsheets, working with data is part of almost every profession. Teaching statistics is a very difficult task, and we've seen tremendous societal problems that could have been prevented with a better understanding of data science. We need students to be able to decompose statistical problems, look for structure, apply patterns, and answer questions with confidence."
And the new data science curriculum has been designed with exactly that in mind, providing an entry point with no prerequisites needed. "Our program is extremely accessible," Shriram explains. "It could be any kid's first exposure to computer science and/or statistical methods. For years, Bootstrap has been focused on teaching foundational skills, creating equity, and integrating into existing classrooms, and now we're tackling a longstanding problem of teaching statistics. It's a really comprehensive solution." 
The Kleiner Perkins Caufield and Byers (KPCB) Engineering Fellows Program is one of technology’s most prestigious fellowships, with only 54 students chosen in 2017 from a pool of 2,000. This year, participation from Brown University's Department of Computer Science (Brown CS) set a new record for the number of wins in a year: undergraduate students Michael Markell, Noah Picard, and Hannah Tipperman were all selected as fellows.
Despite the program's title, it's open to outstanding students who are studying computer science, engineering, mathematics, physics, or other fields related to software development. Hannah and Michael are CS concentrators; Noah concentrates in both CS and mathematics. They'll join other students from across the country in Silicon Valley for a work experience (participating companies span the industry and the alphabet from Airbnb to Zumper) that will be supplemented with events and programming led by CEOs and executives from KPCB portfolio companies and KPCB Partners.
The Randy F. Pausch '82 Computer Science Undergraduate Summer Research Award, given this year to Sorawee Porncharoenwase, recognizes strong achievement from young students and offers them the opportunity to partner with faculty and advance work that began in the Brown CS undergraduate research program.
"My research project with [Postdoctoral Research Associate] Tim Nelson," Sorawee explains, "is about using logic to aid users in verifying correctness of a system. I was introduced to software verification and programming languages at Brown, and I've been interested in them ever since. After taking Logic for Systems with Tim in my first year, I continued my interest by TAing the course last year and this year." 
This summer, he'll be working on program synthesis combined with past work that Tim, Professor Shriram Krishnamurthi, and their PhD student, Natasha Danas, have done on presenting the output of formal methods tools. Sorawee says, "I'm very excited for this summer. The project is an opportunity to work with Tim again. Moreover, now we'll be developing a tool for other people to use! I'm very grateful for the award, and I'd also like to thank Tim and Shriram for all of their support."
Tool-building is exactly what Peter Norvig is looking for. He sees this award as a "multiplier" that will amplify the value of his gift and extend it through time. "In the past," he says, "we had to build all our own tools, and we didn't have time to combine computer science with other fields. Now, there are so many opportunities to do so. I think it's a wise choice: you invest in things that you think will do good, and educating a student allows them to help add to the things that you're already trying to accomplish." 
"There's been great success at democratizing the creation of complex visualization with tools like D3, Vega, ggplot, and Tableau," says PhD candidate Connor Gramazio of Brown University's Department of Computer Science (Brown CS). "But at the same time, just because we can create something doesn't necessarily mean that we should – and right now, those types of design judgements still require a lot of expertise that many visualization creators don't necessarily have. And even if people do have the design expertise, regularly figuring out usable design can take prohibitively long."
Luckily, he's offering a solution: three open-source projects that aim to reduce barriers for effective visualiztion design. Connor will be presenting them on April 25 as part of an invited talk at OpenVis Conf, a two-day, single-track conference centered around the practice of visualizing data on the web.
Connor says, "This is a huge area of research, and we've only just scraped the surface, but I've tried to make a dent in color design. The problem is that visualization design is fairly specialized in that it typically requires a contextual balance of discriminability and preference in order to be useful. This balance is particularly important for color, and although there are a huge amount of tools out there for making color palettes, the majority don't take it under consideration."
His first project has been used by about 14,000 unique users since its launch in August, and seen considerable activity on GitHub as well:
The other two projects are both D3 modules, Connor's implementations of open-sourced color science publications developed by other researchers:
"Being selected as a speaker at OpenVis Conf was an indescribably huge honor," Connor says. "The two keynotes, and many of the other speakers, were some of the people who originally inspired me to get into visualization research way back in 2010, and it's still a little crazy to think that I'm speaking alongside them at the same venue. It's a really, really fantastic way to capstone my PhD at Brown."
Only five years ago, Professor Shriram Krishnamurthi of Brown University's Computer Science Department (Brown CS) was the inaugural winner of SIGPLAN's Robin Milner Young Researcher Award. But age comes fast. A needle skips on a record somewhere, the hair of the youthful researcher is turned white, and the mantle of Experience is clapped onto Shriram's shoulders.
The Università della Svizzera Italiana, the primary university for the Italian part of Switzerland, will be awarding him an honorary doctorate in May. Shriram is sensitive to the short duration in which he's gone from a Young Researcher to Elder Statesman. Nevertheless, recognizing his transition, Shriram says, "I will do my best to act as crotchety as possible to honor this passage."
Working with colleagues from the University of Texas at El Paso and the University of Kansas, Caroline Klivans (a Senior Lecturer in applied mathematics and computer science at Brown University and Associate Director of the Institute for Computational and Experimental Research in Mathematics) has achieved a rare feat: disproving a conjecture first put forth in 1979.
In mathematics, a simplicial complex is a set composed of points, line segments, triangles, and their n-dimensional counterparts. In 1979, Richard Stanley made the Partitionability Conjecture: that all simplicial complexes that met a certain algebraic condition also met a particular combinatorial condition. Until the counterexample provided by Caroline and colleagues, the conjecture was widely perceived in the community to be true. Questions of how various algebraic, geometric, topological and combinatorial properties interact have now been reopened.
Their research is featured in the Notices of the American Mathematical Society and is available here. 
Brown University's Department of Computer Science (Brown CS) is happy to announce that pending the anticipated approval of the Corporation of Brown, Caroline Klivans will be promoted to Senior Lecturer, effective July 1, 2017.
A graduate of the Massachusetts Institute of Technology, Caroline joined Brown in 2011 after teaching and conducting research at The Mathematical Sciences Research Institute, Cornell University and  the University of Chicago. A lecturer in both Applied Mathematics and Computer Science, she is also Associate Director of the Institute for Computational and Experimental Research in Mathematics (ICERM). Caroline's teaching includes topics such as discrete structures, probability, and statistical inference, and her research focuses on algebraic, geometric, and topological combinatorics.
Brown University's Department of Computer Science (Brown CS) is proud to note that with Assistant Professor Tim Kraska's recent win, our faculty have earned a total of eight Sloan Research Fellowships. This is the fourth year in a row of Brown CS receiving the honor:
The image above is © 2017 by Dylan Gattey and used with permission. For more information about this news item, please click the link that follows to contact Brown CS Communication Outreach Specialist Jesse C. Polhemus.
Picture yourself in a meeting with colleagues, looking up at a conference room wall. Not far in the future, Assistant Professor Tim Kraska of Brown University's Department of Computer Science (Brown CS) expects it to be equipped with an interactive whiteboard that will enable domain experts and data scientists to work together in a single meeting to visualize, transform, and analyze complex data in real time. Among many other benefits, the process would remove the need for multiple days of back-and-forth interactions. Between our present and this future is a complete rethinking of the full analytics stack, from its user interface to its smallest components, and incorporating pertinent algorithms. 
Tim has just been named an Alfred P. Sloan Research Fellow in one of the oldest and most competitive fellowship programs in the country. He's the eighth faculty member to receive the honor, which Brown CS has now received for four years in a row. The fellowships, which take the form of a $50,000 grant used over a two-year period, honor and promote the science of outstanding researchers early in their academic careers who show outstanding promise for fundamental contributions to new knowledge.
"Very few people," Tim says, "possess a strong domain expertise and a deep understanding of machine learning, data management, visualization, and many other related fields. My research aims to democratize data science by enabling a broader range of users to unfold the potential of their data through the development of a new generation of algorithms and systems for interactive and sustainable data-driven discovery."
Tim's research that the Sloan fellowship will help fund has three major components: 
Lecturer in Engineering and Adjunct Lecturer in Computer Science Ian Gonsher and Assistant Professors Jeff Huang and Stefanie Tellex of Brown University’s Department of Computer Science (Brown CS) have just received Seed Awards from Brown’s Office of the Vice President for Research (OVPR) to help them compete more successfully for large-scale, interdisciplinary, multi-investigator grants. They join numerous previous Brown CS recipients of OVPR Seed Awards, including (most recently) Ugur Cetintemel, Sorin Istrail, Tim Kraska, and Michael Littman.
Ian Gonsher And Stefanie Tellex
One of the interesting aspects of Ian and Stefanie's research is that it sprang from a class project for 1951C Designing Humanity Centered Robots in the fall of 2014. Student input has been ongoing, and they expect it to continue, hopefully inspiring an increasing number of Brown and RISD students to pursue research opportunities. Motivated by some ominous demographic trends (over the next decade, the retiree population is predicted to increase by 33%, while the labor force caring for them is expected to shrink), the researchers propose ubiquitous, minimally invasive, networked robotics as a solution to help older adults age in place.
The stepping stone for the project funded by the OVPR Seed Award is Tablebot, a prototype "situated robot" that's integrated into the built environment and engages the user with movement, telepresence, and artificial intelligence. Building upon this model, the Walkerbot project will create a unique hybrid of telepresence robot and powered walker, designed to be ergonomic but also collect biometric data in real time. This combination will help users not only navigate their environment better but benefit from decreased isolation and loneliness and even allow medical professionals to remotely conduct diagnostics and respond to emergencies.
Jeff Huang
Jeff's research focuses on the problem of poor sleep, which plagues 60 million Americans. His team will conduct a large-scale study running over 10 years, combining computational and clinical techniques into an automated system that makes actionable recommendations, called SleepCoacher. Their solution uses mobile phones, which are placed on the user's bed and monitor whether recommendations are effective, continuously changing them to slowly improve sleep over an extended period of time.
The SleepCoacher project combines several innovations, from using Bayesian statistical analysis to giving probabilistic interpretations of data to letting users conduct small-scale experiments on their own sleep, helping SleepCoacher learn and adjust its recommendations in a rapid feedback cycle. PhD student Nediyana Daskalova, who has had a core role in this project, noted, "With such distinct differences between individuals' sleep patterns, it takes more than just a couple of nights to help someone improve their sleep, so we need a more refined system to study this issue." The prototype app will soon be on both Android and iOS and the recommendation engine is already compatible with Sleep as Android, a commercial app with more than 10 million installs and 1.5 million active users.
Theophilus "Theo" A. Benson is joining Brown CS as Assistant Professor in September, and if you see him out on a run, training for his next Tough Mudder, consider joining him for at least a block or two. He’s always eager to discuss recent IoT security challenges, cloud outages, and performance issues.
Theo’s introduction to CS dates back to the era when cloud computing was only possible in science fiction: “I remember there not being a lot of color and complexity on the computer screen. Back then, it was so much simpler to find out how things worked.” His interest in the field continued through high school, but an early job at a startup, doing testing and development, systems engineering, and release engineering, provided additional momentum and a definite direction for Theo’s career. “One day,” he remembers, “I got paged because the sysadmin was out sick and the server had gone down. I understood the release process, so I thought I’d just reboot and things would be fine. The problem was that some of the process had been documented, but part hadn’t, and a lot had to be inferred. Not quite as easy as I’d thought!”
“And that got me thinking about the broad space of networking,” he says, “and how to take humans out of the loop when they’re going to cause errors or slow things down. The problem is, we don’t want to be taken out of it!”
In graduate school at University of Wisconsin, Theo focused his attention on the intersection of networks and security, using the tool of configuration management, a process that analyzes the requirements, design, and performance of a network to improve the consistency of its performance. “Configuration management is a huge challenge due to competing economic interests,” he says. “There can be a lot of animosity when sysadmins think that we’re taking away their jobs, but we’re really just trying to minimize the time they spend on certain tasks.” He gives the analogy of going to the doctor: patients describe their symptoms but don’t diagnose their own illnesses.
Despite industry experience that also includes AT&T and Microsoft, academia has always felt like home: “There’s a lot more freedom to attack problems. In the corporate world, when a new buzzword arrives, everyone switches over because it’s the next big thing. I like the wide scope of interests at Brown CS. Instead of being in a silo of networking experts, I’m surrounded by different perspectives.”
When we ask Theo about the outreach efforts listed on his web site (among other things, his research group has allowed high school students to collaborate with them during the summer), his answer is refreshingly free of platitudes. “That experience was very valuable to the students,” he says, “but I don’t feel like I’m being helpful enough. As long as we’re interacting with students at their level, it helps, but getting outreach right isn’t easy, and I’m still trying to figure things out. A lot has been given to me, so I feel like I need to do more.”
He’ll have a chance in Providence, where he’s looking forward to being physically closer to both family and colleagues in the area. Part of the Brown CS appeal, he explains, is that he’ll be working alongside experts in software-defined infrastructure, programming languages, and the Internet of Things, not just networking. “We’ll amplify each other,” Theo says. “And I’ve heard great things about the undergrads and how they take a huge amount of initiative in jumping in with research. I want to see what we can do together.”
Some of the research possibilities include the area of software-defined networking, which he sees as going through a major paradigm shift. “There’s going to be a whole new level of automation,” he says. “Instead of transplanting network protocols designed for Google to developing countries, where there are huge infrastructure discrepancies, we’ll have algorithms that can learn about local conditions and adjust, networks that are able to learn by themselves.”
“It requires us,” explains Theo, “to question assumptions about what works and what doesn’t.” It’s part of what appeals to him about the work of Ion Stoica, whom he cites as one of his heroes. “I always looked up to his work with Spark, which really impacted the way big data interact with networks.”  In the end, he says, meaningful computer science speaks for itself far better than becoming a household name. “When you have that much impact, that’s what people remember. They may not know your name, but if it works, it works!”
Brown University's Department of Computer Science (Brown CS) is happy to announce that Rodrigo Fonseca has been promoted to Associate Professor with tenure, effective July 1, 2017 (pending the anticipated approval of the Corporation of Brown).
Rodrigo joined Brown CS in 2009 after doing postdoctoral work at Yahoo! Research and receiving his PhD from the University of California at Berkeley. His work revolves around distributed systems, networking, and operating systems. Broadly, he's interested in understanding the behavior of systems with many components for enabling new functionality and making sure they work as they should. In particular, he's interested in how to build, operate, and diagnose large scale Internet systems, and in the networks connecting them.
Recently, Rodrigo received a Best Paper Award at SOSP and won a NSF CAREER Award for his work on understanding the  performance of distributed systems through causal tracing. His work has been funded and recognized by the National Science Foundation, Google, Intel, Microsoft, and Facebook.
One way to think of Kathi Fisler’s career, she says, is that it’s been a constantly shifting balance between formal systems (and how they work) and people (and how they learn). For years an Adjunct Professor at Brown CS, she’s just been appointed Professor (Research) and Associate Director of the Undergraduate Program. It’s a momentous shift toward the latter half of that balance, letting her focus on CS education for both K-12 students and Brown ones. “This is deeply personal to me,” she says. “There’s an opportunity, a challenge, and it’s just a fun playground for me to work on connecting people with computer science.”
The main reason why the challenge of CS education has a personal dimension for Kathi originates in her first year at Williams College. Only intending to focus on mathematics and Asian studies (Chinese in particular), she was required as a math major to take CS 1. “I wasn’t very good at it, and it drove me nuts, instructing a hunk of metal and having it be totally my fault when something didn’t work.” Fisler gestures toward an empty spot on her desk as if the offending computer were right in front of her. “I didn’t fail, but I didn’t do well, either.”
Determined to prove that she could conquer programming, Kathi signed up for CS 2. By the mid-term, she was failing. But thanks to many office hours spent with a supportive faculty, she kept going: by two-thirds of the way through the year, her grades had soared, and early in CS 3, she realized that she enjoyed CS more than math. “This is one reason why I love CS education, particularly introductory courses. I deeply understand when it doesn’t work, and I know how it feels to power through the learning curve that many students face. I was the only newbie, the only woman across 3 consecutive years of CS majors. Nobody would have been surprised if I'd walked away from computer science, but thankfully I was too stubborn to do that and my brain soon caught up.”
Traveling to University of Indiana at Bloomington for her Master’s and PhD, Kathi envisioned studies in computational linguistics, but was waylaid by a hardware design course. A new research lab was starting up, and she jumped at the chance to do work that touched on math, computer science, and even philosophy: examining how people reason with diagrams, particularly in hardware design. “Traditionally,” she says, “mathematical proofs were largely written in text form, and diagrams weren’t considered rigorous. Why is that? Think about a civil engineer, who uses math and modeling instead of sending cars over an unproven bridge to see whether it collapses. Why can’t we use diagrams to reason about how a system will work?”
This line of inquiry led to research in verification techniques, and later to access control policies and their security implications. “Every problem I studied,” says Kathi, “kept coming back to the question of how humans understand a question involving computational systems.” She gives the example of Facebook privacy settings, which many people believe they understand fully, yet have resulted in numerous privacy leaks when reality didn’t match someone’s conception of how things worked.
“When you have people trying to use systems successfully,” she says, “people are the difficult part. I really want to understand what’s going on for them cognitively, psychologically, how they learn.” More than twenty years ago, that was part of the motivation for work that led to Bootstrap, a family of curricula that integrate computer science with other disciplines in K-12. Initially a “side hobby” for Fisler and her long-time collaborator, Professor Shriram Krishnamurthi, it’s now used by roughly 15,000 students per year across 17 states and five countries.
CS education has been in the news a lot lately, but what has Kathi seen in her years of research that the layperson may be unaware of? “It’s taken years to realize how much we don’t understand, how many concepts are packed into a simple programming task. The literature is overflowing with evidence that many people struggle to learn CS, and now we understand that we can’t accept the idea that some people just won’t be good at programming as an answer. Computer science is a form of literacy for everyone now: there are economic and equity issues. If we preach CS for all, we need to accept that everyone isn’t going to be a computer scientist, and think about what we teach and how we teach it.”
For years, Kathi has divided her time between being a Professor at Worcester Polytechnic Institute (WPI) and an Adjunct Professor at Brown. Recently, her juggling act (Bootstrap at Brown, research and teaching at WPI) has become unbalanced by Bootstrap’s rapid growth, which made the opportunity to consolidate all three efforts hugely appealing. Brown offered not only the perfect job description but the chance to form an entirely in-house research group that has few rivals.  
“This lets me focus,” Kathi says. “I love advising and working with students for all the reasons that I’ve mentioned, and this lets me participate more fully with Bootstrap. Together with Shriram and the rest of the group, I have a CS education team with decades of experience, deep programming language expertise, and everyone under one roof. Anywhere in the world, that’s incredibly hard to find.” Giving up the commute to Worcester may also give Kathi more time for her hobbies, which include playing music, singing, running (she notes a theme of rhythm among all three), jigsaw puzzling, and cooking vegetarian food.
But not too much spare time. There are big challenges ahead, she explains: “We need to find out how to provide broad CS education before we lose another generation. We need to enable teachers who don’t see themselves as computer scientists to introduce students to computing. This new role is my best chance yet to do the deeply personal work of taking everything that I’ve learned about systems, about learning, and to help the people who are facing one of the greatest educational tasks in history. I can work with a struggling college student at the same time that I’m looking out for an atypical learner in kindergarten or high school, and that’s so important to me.”
by Bethany Hung
Brown’s annual hackathon takes place this weekend, February 4-5! Over 500 students from around the world will gather on College Hill to learn, play, and collaborate for Hack@Brown 2017. We’ve got everyone from high schoolers to graduate students, from Canada to South Korea. All in all, they represent 119 schools.
In past years, hackers have created a diverse compendium of products. First-time hackers mixed social good with Google APIs to produce SNAPy at Hack@Brown 2016, a service that allows anyone with an SMS-enabled mobile (not just a smartphone!) to request a list of grocery stores that accept food stamps. On the more humorous side of things, another 2016 team wrote Chrome extension How Many Pens. With this installed, users can opt to replace dollar amounts with their equivalent of choice – say, that shirt isn’t $20 but rather 134 BIC pens.
Beyond the projects, we’re honored to host a diverse field of speakers and mentors. Mary Fernandez, CEO of MentorNet and activist for underrepresented minorities in STEM, will be our keynote speaker this year. Sponsors include Microsoft, Google, Bracket, pMD, Polymer, a16z, Two Sigma, Facebook, Oculus, Adobe, Qualtrics, and Cimpress. The judging team is comprised of representatives from these companies and Brown faculty, ensuring both industrial and academic perspectives.
We’re excited to see what participants come up with this year, and everyone is invited to view the results. A demo fair and judging panel will take place on Sunday in Sayles Hall – make sure to stop by to check out all the great work!
Students who know about Daniel Ritchie’s arrival (he’s joining Brown CS as Assistant Professor in July) have already shared their excitement about his work in AI systems to enable creativity, generative models for graphics and design, and probabilistic programming. But given Brown’s reputation for interdisciplinary study, they may be equally enthused about the need for creativity that runs alongside his love of research and experimentation.
“As a kid I thought I’d be a novelist,” he says, “or work in music or film, the arts. Computer science started out as a hobby for me, and I didn’t know where it’d lead. I didn’t get serious about CS until college, and it wasn’t until my internship at Pixar that I realized I could bring the two interests together.”
As an undergraduate at University of California, Berkeley, Daniel studied computer graphics, working in physical simulations and developing an interest in applications that could be used in film production. Branching out into artificial intelligence and machine learning while in grad school at Stanford University, he experimented with automatic generation of common 3D environments such as bedrooms and kitchens.
“I like bringing things to life and sharing them with people,” he says. “The creativity of it always brings me the most joy.” Looking to find some joy for our own playlists and Netflix queues, we press Daniel for recent favorites across various media, and he speaks with feeling about a band called Cloud Cult (“deeply personal, lyrics about loss and fear of the unknown”), an independent film called Short Term 12, and the works of Patrick Rothfuss, citing the author’s world-building talent and ability to choose words with care.
Is computer science creative? “It absolutely is. No idea comes out of a vacuum, but new insights are required in order to make progress. Research isn’t that dissimilar to design, or the artistic process: experiment, prototype, iterate, then check direction.”
Broadly speaking, Daniel explains, his own research is about using artificial intelligence and machine learning to create content for graphics. “A lot of machine learning,” he says, “is concerned with coming up with explanations for data that we have about the world: categories, structure, how it was created.” Probabilistic programming, which repeatedly generates models, draws inferences from them, and then generates new ones, is a useful tool. Ritchie gives the example of using an algorithm to generate buildings, then automatically eliminating the ones that aren’t stable: “It’s reasoning backward from the goals that you want to satisfy.”
“That was where machine learning really clicked for me,” he explains. “The results are often surprising, which provides a lot of potential for inspiration.” It’s also democratic: “We’re augmenting what the average person is capable of, automating away tedious details, guiding them back to good design.”
With his PhD from Stanford now complete, Daniel is looking forward to his arrival at Brown and continuing his work with tools that will have a broad user base. “I want to get more people well-versed in artificial intelligence and machine learning methods, especially people who don’t become AI/ML specialists....It’s really satisfying to work with students who are just starting out in their careers, who are ready for new ideas. I like how Brown CS is a community, not a collection of research labs. That’s rare and special.”
And what’s next for his research? “There’s been an explosion of deep learning technology,” Daniel says. “I’m really interested in combining it with probabilistic programming. I'm also interested in exploring more applications for creative AI. For example, AI-assisted content creation tools in VR, or even applications beyond graphics, such as music composition.”
Despite rapid technological advances, Daniel has little fear that humans are at risk of obsolescence: “Creative AI systems are powered by some combination of carefully coded design principles and learning from examples. But people had to come up with those design principles and create those examples in the first place! So these systems depend on people; in one way or another, they capture the best of what people know about how to create. When it comes to creativity, I’m a firm believer in augmenting human capabilities, rather than trying to automate them away. Creativity provides joy for people — why would you want to automate that away?”
Commencement is a busy time, so if you want to attend, please click here to RSVP no later than April 15.
When John Savage came to Brown a half-century ago, computing was thought of as a service, not a discipline, and the idea of a Department of Computer Science at Brown was highly controversial.  Few could have predicted the infrastructure necessary today, or the challenges we face in the areas of cryptography, cybereconomics, big data, and many others.
One of the founders of Brown CS, John's interests have changed as our field has changed, leading him to contribute to such areas as computational complexity, scientific computation, computational nanotechnology, and cybersecurity policy and technology.  Join us at Room 130 of 85 Waterman Street from 2-6 PM on Friday, May 26, for a celebration of how John Savage and Brown CS evolved together, including research talks as well as personal reminiscences.  A reception will follow at 115 Waterman Street on the third floor of the CIT.  (Please note that this event will take the place of the annual Brown CS Reunion.)
Impostor syndrome, a term coined by Clance and Imes in 1978, can be described as a lack of belief in one's accomplishments and an ongoing fear that any success to date is illusory and will be discovered as false. Increasingly, it's become a topic of discussion in the field of computer science: a post about impostor syndrome to the internal Brown CS Facebook group (note that you must have a brown.edu email address linked to your Facebook profile to view this group) has resulted in more than 130 comments by faculty, students, and alums.
It appears that women and members of historically underrepresented groups are particularly prone to the syndrome, and alum Sarah Sachs explains in a recent Washington Post article that even though she graduated from Brown, won a prize for her undergraduate thesis, and earned a much-coveted job at Google, she still doubted herself on a daily basis. That changed last year, during a chance encounter with Michelle Obama: attempting to thank the First Lady for being a role model, Sarah was stunned to find Ms. Obama thanking her for her mentorship and inspiration.
"I’ve decided to take a new approach," Sarah writes. "I’m going to accept that I can need to receive support but still be able to give support to those who follow me in our field....There are other people like me looking around to see if they are alone. We aren’t. We belong."
While some privacy advocates are warily eyeing our smart refrigerators and Internet-enabled toaster ovens, Professor Shriram Krishnamurthi and PhD candidate Hannah Quay-de la Vallee of Brown University's Department of Computer Science (Brown CS) have turned their attention to an area that's been largely overlooked: app stores, which power our phones and tablets as well as our smart homes.
"Most app stores," they write for Fast Company, "completely fail to help users protect their privacy." Their op-ed points out the need for preserving privacy in this "beating heart of smartphone ecosystems, where most users go to search, discover, and install apps", evaluates the current configurations at Google and Apple, and recommends simple solutions for improved privacy design thinking.
The full piece is available here.
Among many other things, the visit was timely: a post about Donald Knuth’s multi-volume opus, The Art of Computer Programming, was trending on Slashdot just last week.
Earlier this month, on Thursday, December 1, 2016, and Friday, December 2, Brown University’s Department of Computer Science (Brown CS) hosted Knuth, Professor Emeritus at Stanford University. Widely regarded as an artistic genius and perhaps the most gifted programmer of all time, he delivered the 16th Paris C. Kanellakis Memorial Lecture and a John von Neumann Lecture.
“This is the most mind-boggling thing,” Knuth said at the start of the first lecture, proceeding immediately to boggle the minds of a record-size audience with insights that varied from the specific to the far-reaching and from the offhand to the profound. Below, we sketch just the outline of his visit in text, photographs, and video.
Organ Music
The mornings of both days were occupied by visits to two prominent local pipe organs, one at Brown’s Sayles Hall and the other at the Cathedral of Saints Peter and Paul. Knuth played a selection of Bach, seasonal favorites, and part of his own Fantasia Apocalyptica, which he described with enthusiasm and a vocabulary more reminiscent of an undergraduate than an octogenarian: “So far, I’m psyched about it!”
Explaining the mechanics of a pipe organ to his listeners, he made the computer science analogy of columns and rows, saying that there are n notes and m tonalities to choose from, and thus the total number of sounds that the organist can possibly make is at most 2 to the power m-plus-n. However, the number of pipes is m times n; therefore, if a computer were able to turn each individual pipe on or off independently, the organ would be able to produce many, many more sounds: 2 to the power m-times-n!
For example, at Sayles Hall both m and n are less than 70, so fewer than 2 to the power of 140 sounds are possible. But that organ has 3355 pipes, so it’s capable of 2 to the power of 3355 different sounds. Thus, Knuth mused, for every sound the organist can play, the instrument is actually able to make more than 64,000,000,...,000 (imagine 957 other zeroes in the expression) others. “Most of the sounds this instrument can make haven’t been heard yet,” he said. “We don’t know if the unheard ones are beautiful or not. Are we missing something important?”
Hamiltonian Paths And Satisfiability
Both lectures were spectacularly well-attended, setting Brown CS records. Introduced at one point by Professor Sorin Istrail as one of “two real-life superheroes, von Neumann and Donald Knuth”, and wearing the same brightly-colored shirt from when he spoke at the opening of the CIT in 1988, Knuth seemed to relish speaking to what he described as a “pretty geeky” audience.
The first lecture, which Knuth framed as a “history of clever ideas that arose around the world”, traced the evolution of a problem that dates back to antiquity: finding a path that encounters all points of a network without retracing its steps. Knuth’s sense of aesthetics, curiosity, and his love of minutiae and the absurd were on full display (“look at the ingenious wordplay….nonsense things like this are easy to learn”) as he took his audience from Hellenic icosahedrons to a knight’s tour of the chessboard to the sandals of Lord Rama, mentioning with offhanded modesty feats such as teaching himself enough Sanskrit to find errors in Wikipedia entries devoted to obscure treatises half a millennium old.
In the second lecture, Knuth returned the audience to his multi-decade project, The Art of Computer Programming. (“It makes a wonderful Christmas present,” he said to considerable laughter.) His subject was the concept of satisfiability, in which a formula is declared satisfiable if one can find and prove a model that makes the formula true. Again, the approach was in part historical, tracking inflection points such as the “unseen breakthrough” of lazy data structures in 1982, but Knuth also made an argument for the importance of what he described as a “million-dollar” problem. (The number refers to the value of the award given by the Clay Mathematical Institute for solutions to their Millennium Prize Problems, some of the most famous unsolved questions in mathematics.) “I didn’t anticipate,” he said, “that I was going to have 300 pages on it, but eventually I realized that satisfiability is a basic technique that deserves to be much better known, part of every programmer’s toolkit.”
The occasional digressions were equally interesting. Knuth explained that creating a graph of character interactions in Anna Karenina increased his appreciation of Tolstoy, and that he obtained “maximum insights per hour” through a technique of taking a random walk through the Bible as well as through student papers that he needed to grade. “Some people thought I graded at random,” he joked. “That wasn’t exactly true.”
Knuth And Wegner 
At the end of the first lecture, Sorin offered a champagne toast to celebrate a remarkable collaboration between Knuth and Brown CS Professor Emeritus Peter Wegner. Almost fifty years ago, Knuth explained, a “hot topic in those days” was the hope to “define the precise meaning of programs in a formal way, noting that all of the existing proposals for programming language definition at the time were too uncomplicated and unintuitive to be fruitful”.
During a visit to Wegner’s home in 1967, Peter suggested that information could be conveyed down a parse tree as well as upward from the bottom. This seemed preposterous at the time, Knuth explained, sharing the memory of how he was so agitated by the idea that he suddenly realized he was yelling instead of speaking in a normal tone. But the suggestion lingered in his mind because he understood that Peter was absolutely right. This suggestion soon led to the invention of attributed grammars. “Much of the technology of modern compilers,” write Shasha and Lazere in Out of Their Minds: the Lives and Discoveries of 15 Great Computer Scientists, “dates from these insights.”
Questions Answered
After each lecture, when students asked a variety of difficult, far-reaching, and even unanswerable questions, Knuth shone. The witticisms were present again as he answered a query as to whether a supreme deity plays dice with the universe (“God could perfectly well play dice, if that leads to a good algorithm”), but weight and seriousness returned as he urged attendees to lengthen their attention spans. “Computer science shouldn’t automatically lead directly to Wall Street,” he said; computer scientists should “advance civilization” instead.
Not even the most broad and fundamental challenges were out of bounds. When asked about what’s widely considered the most important question in computer science theory, Knuth took the unpopular position that P=NP, giving just enough detail in his response to preserve and even augment, rather than destroy, the mystery of one of the most famous problems of our discipline.
Some of Knuth’s most powerful moments were when his remarks took on the fullest possible global and cultural scope: “The notions that we now think of as computational ideas are part of world culture going way back.” Each time, whether the topic was look-ahead solvers, random sampling of the Bible, or the “thrill” of feeling like he was talking to a fellow scholar from the 13th century, segments of the talks devoted to theory and comprehensive detail were illuminated by the personal example of Knuth’s intellectual restlessness and eagerness, his unabashed love of experimentation. Even as he lamented that he was unable to persuade any of his PhD students to interest themselves in the history of computer science, he urged his audience to do so, and thereby pursue the life of ideas: “We need to see what other people have thought and how they approached problems, a combination of breadth and depth….good computer science, like good mathematics, has a long half-life.”
Photos
For a full gallery of photos from Professor Knuth’s visit, click here.
Video
Click any link below to watch a video from the two-day event:
Barely a month after his arrival at Brown University's Department of Computer Science (Brown CS), Assistant Professor George Konidaris has received an Air Force Office of Scientific Research (AFOSR) Young Investigator Research award.
The objective of the Young Investigator Research program is to foster creative basic research in science and engineering, enhance early career development of outstanding young investigators, and increase opportunities for the young investigators to recognize the Air Force mission and the related challenges in science and engineering. George's proposal ("Constructing Abstraction Hierarchies for Robust, Real-Time Control") was one of only 56 selected chosen from more than 230 proposals in areas as diverse as aerothermodynamics, theoretical chemistry, and quantum electronic solids.
"I'm interested in robot decision-making," George says. "Robots sense the world via a constant stream of noisy, high-dimensional sensations, and can only ultimately act by emitting low-level motor control signals, but decision-making at that level of detail is just unnecessarily hard. Imagine trying to figure out where to go for lunch when you can only think about where you must place each footstep, and exactly what high-resolution images you'll see along the way.
"A core theme in my research is trying to understand how to do high-level reasoning in a low-level world; this proposal addresses one specific aspect of that, which is trying to build hierarchies that allow the robot to reason about the world at just the right level -- as abstractly as possible."
Ultimately, George hopes to develop principled algorithms for autonomously learning abstraction hierarchies, and to show that such hierarchies can very rapidly generate flexible, robust, generally-capable goal-oriented behavior. "We need to get robots acting intelligently in a wide range of environments, beyond those their designers can foresee or test."
Assistant Professor Stefanie Tellex of Brown University's Department of Computer Science (Brown CS) has just returned from the largest conference devoted to natural language processing (NLP), where she gave an invited talk. SIGDAT (the Association for Computational Linguistics' special interest group on linguistic data and corpus-based approaches to NLP)'s Conference on Empirical Methods in Natural Language Processing (EMNLP) 2016 was held in Austin, Texas, from November 1-5, 2016.
One of only three invited speakers, Stefanie spoke on Thursday, November 3. In her talk ("Learning Models of Language, Action and Perception for Human-Robot Collaboration"), she described robots as "force multipliers" that can assist an astronaut with a repair on the International Space station, a UAV taking flight over our cities, or an autonomous vehicle driving through our streets.
"The aim of my research program," she explains, "is to create autonomous robots that collaborate with people to meet their needs by learning decision-theoretic models for communication, action, and perception....I am creating new methods for learning how to plan in very large, uncertain state-action spaces by using hierarchical abstraction. Perception for collaboration requires the robot to detect, localize, and manipulate the objects in its environment that are most important to its human collaborator. I am creating new methods for autonomously acquiring perceptual models in situ so the robot can perceive the objects most relevant to the human's goals. My unified decision-theoretic framework supports data-driven training and robust, feedback-driven human-robot collaboration."
Now in his fiftieth year at Brown University, Professor John Savage of the Department of Computer Science (Brown CS) continues not only to consult on cybersecurity at the international level but serve as ambassador and architect for countries that are joining the global effort to improve the safety of cyberspace.
Most recently, John has returned from a week-long trip to Vietnam, where he met with political leaders, technology and cybersecurity experts, academics, and students to address what Vietnam sees as a top priority: building the necessary cyber resilience to protect their scientific and technological development. His agenda was extremely varied, ranging from discussions of an innovation center being built in the coastal city of Nha Trang to a keynote address at the Conference on Solutions for Practice on Global Citizenship Education in Cyber Civil Defense in Vietnam, held at Dalat University.
John traveled to Vietnam as a representative of the Boston Global Forum, a non-governmental organization founded in December, 2012, by Governor Michael Dukakis; Mr. Tuan Nguyen, Member of the Harvard Business School Global Advisory Board; Professor Thomas Patterson of the Harvard Kennedy School, and Professor John Quelch of the Harvard Business School. Boston Global Forum is dedicated to solving global issues of peace, and security, especially in cybersecurity.
Some highlights of John’s trips are detailed in the photos below, and you can click here to see a video clip (in Vietnamese) of his meeting with President of Vietnam Tran Dai Quang.
John is welcomed to Hanoi by Vinh Vu of Vietnam Report, a Vietnamese news agency
On the red carpet at the Presidential Palace of Vietnam, from right to left: John, Nguyen Anh Tuan of Boston Global Forum, Vinh Vu of Vietnam Report, and Nguyen Thi Lan Anh of VietNamNet
John meets with Vietnamese President Tran Dai Quang
John delivers his keynote lecture at Dalat University
John meets with faculty, students, and the president of Dalat University
John visits the Po Nagar temple in Nha Trang with Mr. Nguyen Tuong and Mr. Ta Hong Viet
The Paris C. Kanellakis Memorial Lecture honors a distinguished computer scientist who was an esteemed and beloved member of our faculty. This year, on December 1 and 2, Brown CS welcomes one of the greatest minds of our discipline, widely regarded as an artistic genius, Renaissance man, and perhaps the most gifted programmer of all time, Donald Knuth. We're extremely happy to return Professor Knuth to a city that he remembers fondly (he holds an honorary doctorate from Brown, and his daughter is an alum), where he will deliver two lectures, continue a conversation with Professor Emeritus Peter Wegner that shaped the course of CS more than 50 years ago, and even take a few moments to enjoy himself by playing two prominent local pipe organs.
"Knuth Days At Brown" Schedule 
4 PM on Thursday, December 1, in CIT 368, the Paris C. Kanellakis Memorial Lecture: "Hamiltonian Paths in Antiquity"
About 1850, William Rowan Hamilton invented the Icosian Game, which involved finding a path that encounters all points of a network without retracing its steps. Variants of his game have turned out to be important in many modern computer applications. The speaker will give evidence that people have been interested in such questions since at least Graeco-Roman times. Furthermore, ingenious Sanskrit and Arabic documents from the ninth century, and continuing through medieval times, also reveal that this is perhaps the oldest nontrivial combinatorial problem in the history of civilization.
This lecture is hosted by Sorin Istrail and a reception will follow.
4 PM on Friday, December 2, in CIT 368, a John von Neumann Lecture: "The Art of Computer Programming: Satisfiability and Combinatorics"
This lecture is hosted by Sorin Istrail and Eli Upfal and a Sweat Box Session featuring rigorous questioning from graduate students and other attendees will follow.
Less than a semester after his arrival at Brown University's Department of Computer Science (Brown CS), Associate Professor Seny Kamara continues to distinguish himself as a thought leader in the areas of cryptography and security. Recently chosen as a Michael Dukakis Leadership Fellow by the Boston Global Forum, he's now been appointed to a new committee created by the National Academy of Sciences.
The committee ("Law Enforcement and Intelligence Access to Plaintext Information in an Era of Widespread Strong Encryption: Options and Tradeoffs") is part of the organization's Division on Engineering and Physical Sciences, and it will examine the tradeoffs associated with mechanisms to provide authorized government agencies with access to the plaintext version of encrypted information. 
Their work will describe the context in which decisions about such mechanisms would be made and identify and characterize possible mechanisms and alternative means of obtaining information sought by the government for law enforcement or intelligence investigations. It will seek to find ways to measure or otherwise characterize risks so that they could be weighed against the potential law enforcement or intelligence benefits. However, it will not seek to answer the question of whether access mechanisms should be required but rather will provide an authoritative analysis of options and tradeoffs.
Seny joins a select group of 13 other experts from academia and industry, including Google, Intel, Microsoft, Massachusetts Institute of Technology, and Stanford University.
Brown University has launched a Data Science Initiative to catalyze new research programs to address some of the world’s most complex challenges and provide students with innovative educational opportunities relating to “big data”. The initiative builds on established strengths in mathematical and computational sciences and a long history of data-related research across its core academic departments.
“From deciphering disease and improving the delivery of health care, to modeling climate change and evaluating public policies, Brown faculty are already on the cutting edge of the big data revolution,” said Brown President Christina Paxson. “The Data Science Initiative will build on that tradition and unearth new methods for using big data to solve big problems.”
Despite recent advances, growth in the volume and complexity of data continues to outpace the development of new techniques needed to translate these data into cutting edge research. At the same time, the application of big data to new questions and disciplines requires novel approaches.
In its initial stages, the Data Science Initiative will include a new one-year Master’s degree in data science, expanded undergraduate course offerings, and the addition of ten new faculty members and researchers whose research and teaching will focus on fundamental methods of data science and their application to a variety of research questions.
The Data Science Initiative aligns with Brown’s commitment, as articulated in the University’s Building on Distinction strategic plan, to taking an integrative approach to developing solutions to complex challenges — an approach that bridges and unites multiple academic areas of research and study. Brown’s departments of mathematics, applied mathematics, computer science and biostatistics will serve as the initiative’s hub, but a key focus will be to create a campus-wide community in data science, engaging students and faculty in life and physical sciences, social sciences and the humanities.
Ultimately, the initiative aims to ensure that scholars across Brown’s disciplines become fluent with data in a way that encourages them to integrate data science into their teaching and research in novel and creative ways.
“Different types of data —genome sequences, data from social networks and medical records, to name just a few— are giving rise to entirely new frameworks and theories on how to extract meaning from data,” said Jeffrey Brock, chair of the Mathematics Department and director of the initiative. “We want to explore fundamentally new techniques and methods for eliciting new knowledge from data.”
Innovative Research
In addition to more traditional research projects in the life, physical and social sciences, scholars from the Data Science Initiative will work with Brown’s Cogut Center for the Humanities to seek new connections across the cultural divide between the sciences and humanities and ways of using data in new scholarly contexts.
Partnerships with Brown’s Watson Institute for International and Public Affairs and the Center for the Study of Race and Ethnicity in America will investigate the societal and cultural impacts of data, including questions related to data access, privacy, security, equity and justice.
“As the use of big data expands in commerce, public policy and in our everyday lives, it presents new challenges that cut across disciplinary boundaries,” said Brown Provost Richard M. Locke. “Brown’s Open Curriculum and collaborative research ethos put us in a unique position to help chart the future of the data-enabled society.”
Each new research program arising from the initiative will build upon a history and tradition of data-related research in the initiative’s core departments.
In the mid 1970s, a distinguished group of Brown faculty formed the Pattern Theory Group in Applied Mathematics. That team’s work in the early stages of image processing, computer vision, the theory of artificial neural networks and other areas established foundational data manipulation techniques widely used today.
Computer scientists at Brown are developing new algorithms and machine learning techniques for automated analysis of large datasets that may include text, audio, video and other types of information. Scholars are also creating new types of systems for manually searching, manipulating and visualizing data. Roboticists are using crowdsourcing and other big data techniques to increase the capabilities of robotic technologies.
The Department of Mathematics has research strengths in topology, geometry and graph theory, areas of pure mathematics that have found new application in data science. These techniques use the “shape” of datasets to identify clusters indicative of, for example, hubs in a social network or subtypes of a particular disease. These strengths complement those in harmonic analysis and cryptography, already central areas of data-related expertise.
Biostatistics faculty have leveraged data to create better screening protocols for lung cancer and public health strategies for preventing HIV spread. Researchers help to process genomic data to look for the mutations that drive cancer, as well as leveraging various datasets for precise and personalized treatment of individual patients.
“Building outward from these core departments, Brown will engage with the foundational questions of the data revolution, becoming a lighthouse for methodological innovation in data science,” Brock said.
Educational Opportunities
The management consulting firm McKinsey & Company estimates that by 2018, the U.S. will have a shortage of 1.5 million managers capable of using data analysis to make informed decisions. There will be an additional shortage of as many as 190,000 employees with deep data skills necessary to develop complex analyses and communicate findings through visual media.
To prepare students for the data-enabled economy, faculty in the Data Science Initiative will partner with departments across campus to create data science course sequences to promote data fluency in students studying in a variety of disciplines. New faculty added through the initiative will expand the course options already available at Brown. Current course offerings include two introductory courses —“Data Fluency for All” in Computer Science and “What’s the Big Deal with Data Science” in Applied Math— both designed to introduce the field to students without much experience with data science techniques.
The master’s program, which began recruiting its initial cohort this month, will offer a deeper dive into the methods applied by data scientists. In addition to a core curriculum focusing on foundational mathematical and computational techniques, an elective class will let students explore particular applications of their choice. A capstone project will help students apply what they’ve learned to real-world questions and problems.
“The program aims to provide students with the deep data fluency necessary for leadership in data-centric careers,” said Carsten Binnig, adjunct professor of computer science and director of the master’s program. “Courses will provide a fundamental understanding of the tools of data science that students can apply in a huge variety of careers, whether in business, health care delivery, academic research or something else.”
In both the educational and research mission of the Data Science Initiative, Brock said that collaboration across the disciplines will drive the initiative as an overarching theme.
“You never know when a technique applied to a data problem will be useful in another,” Brock said. “An approach used by one of our physicists sifting through the data produced by the Large Hadron Collider could be helpful to someone who is looking at data on how people behave in an economic market.
“We want the Data Science Initiative to be a place where those connections are made.”
The Department of Computer Science at Brown University (Brown CS) seeks applicants under a focused cluster hiring plan for multiple anticipated tenure-track faculty positions in the area of machine learning, broadly construed (including, but not limited to machine learning for speech, vision, robotics and text; machine learning theory; systems and languages for machine learning; deep learning; unsupervised learning; relational learning; reinforcement learning; optimization; and data mining).
Brown is launching a university-wide Data Science Initiative being led by the Computer Science, Applied Math, Biostatistics and Math departments, who are closely collaborating to create a rich, multi-disciplinary research and education environment around data science and engineering. The Data Science Initiative identified machine learning as a priority area. Successful candidates are expected to actively participate in these ongoing efforts.
The positions are expected to start on September 1, 2017. In selecting candidates, we will consider quality of research, teaching, and compatibility with the needs and interests of the department. The position is open for all ranks with preference given to junior candidates. The department is committed to building a diverse faculty and strongly encourages women, underrepresented minorities and those who can contribute, through their research, teaching or service, to the excellence, diversity and inclusivity of our academic community. We strongly encourage the candidates to report any relevant experience and plans in their teaching statements.
The department has 27 tenured and tenure-track faculty members as well as several adjunct, research, and visiting faculty members. Department members frequently take advantage of Brown's interdisciplinary culture via collaborations with numerous other departments and centers, including Africana Studies, Applied Mathematics, Art, Biology, Brain Science, Cognitive Linguistic and Psychological Sciences, Computational Molecular Biology, Computer Vision, Economics, Engineering, Mathematics, Medicine, the Physical Sciences, and Psychology.
Brown University is located in Providence, RI, close to Narragansett Bay, an hour from Boston and about three hours from New York City. Providence has been consistently rated as among the Northeast's most livable cities and is home to diverse intellectual, artistic, and business communities.
Junior applicants must have completed all requirements for the doctoral degree by the start of the position. The initial appointment as assistant professor is for four years and is renewable. Applicants for a junior position must submit three letters of reference, and senior candidates should submit five names of references whom the committee may contact. To apply, please use Interfolio. Applications will be considered until the position(s) are filled but we strongly encourage the candidates to submit complete applications (including reference letters) by December 16, 2016 for full consideration.
Inquiries may be addressed to: faculty_search_2017@lists.cs.brown.edu.
Brown University is pleased to welcome the first student cohort to begin its newly launched Executive Master in Cybersecurity (EMCS) program. The 16-month EMCS program, with classes beginning this week, builds on the university’s interdisciplinary excellence in cybersecurity, highly accomplished faculty and students, and practical, hands-on experience to produce visionary and confident leaders who understand the technology, human, and policy issues that form a successful cybersecurity strategy and can lead transformative change within their organizations.
The class of 2018 is a diverse group of 28 accomplished professionals averaging 17 years of experience in a variety of sectors and professional roles. From a current U.S. Marine to marketing professionals, practicing attorneys, security officers, consultants and investors, the selectivity and diversity of the student cohort are key benefits of the program. Students are selected based on what they will contribute to the peer learning community and their readiness to benefit from the program.
"With unique education and experience, every student is an important contributor to the cohort," said Alan Usas, EMCS Program Director. "Not only do the students selected act as valuable resources for peers, but they become a trusted member of the lifelong professional networks that forms. We have a very strong group entering the program and we are prepared to graduate visionary and strategic leaders to meet the global, technical, human and policy challenges of cybersecurity."
According to the recently published report, "The State of Cyber Security Professional Careers," conducted by independent research firm, Enterprise Strategy Group (ESG), 46 percent of organizations claim to have a problematic shortage of cyber security skills and nearly two-thirds (65%) of cyber security professionals struggle to define their career paths. Operating on the central tenet that Strategy is the Best Security, the comprehensive and timely curriculum of the EMCS program prepares students with the conceptual framework and readiness to respond to challenges in cybersecurity, including:
For the first time in the university's history, Brown opened the ceremonial Van Wickle gates to EMCS students during the Oct. 16 convocation. Opened only three times a year for convocation, mid-year graduation and commencement, the opening of the Van Wickle gates for the EMCS students ushers in a new tradition for the university and underscores the importance of the Executive Master portfolio of programs, and its students, to the Brown community.
According to Jonathan Powell, who has just finished leading the new Mosaic+ Transition Program, which helps underrepresented minority (URM) students prepare for studying computer science at Brown University, his experience as a new CS student was shared by many of his peers.
“I was interested in the program,” he says, “because I heard a lot of second-year URM students talking about the struggles that they’d had. The academics were part of it, but even the small things, like finding where labs are and setting up your account, they’re all barriers. CS has a fundamentally different learning curve, and it starts very early. For many students, their experience in high school helped build fundamental skills in math, writing, and other areas that they need for college courses. But for computer science, very few schools have a curriculum that helps build that foundation. That’s what we’re trying to do.”  
In the spring of 2015, Mosaic+ began working on what they call a “two-tier solution centered around the social and academic issues” faced by URM students: a Transition Program that would take place just before their arrival at Brown, and a Mentoring Program that would continue afterward. According to Mosaic+ co-coordinators Nifemi Madarikan and Chelse Steele, the student organization “seeks to equip these students with the resources, skills, and support structure that will allow them to thrive at Brown and become the next generation of computer scientists from the moment they step onto campus”.
A great deal of research followed. With assistance from Department Chair Ugur Cetintemel, Vice-Chair Tom Doeppner, and Professor Shriram Krishnamurthi, Mosaic+ studied programs at University of Michigan, Stanford University, and other schools to find models of engagement and useful best practices. They also partnered with Brown’s New Scientist Catalyst summer program, which helps incoming URM students in science, technology, engineering, and math, but doesn’t focus on CS.
“It was really Chelse and Nifemi’s brainchild,” says Laura Dobler, the Brown CS Financial and Outreach Coordinator, who works extensively with Mosaic+ on diversity-related initiatives. “The Transition Program is a comprehensive solution for so many different types of problems. For URM students, it’s not just about helping them past the learning curve of doing research, it’s about creating a community, leveling the playing field, providing mentors and resources, and doing everything we can to make their experience more typical.”
Thirteen students arrived in August as the program’s first cohort. Their mornings were spent on the Transition Program’s technical curriculum, designed to introduce them to functional and object-oriented programming concepts, build technical skill, and expose them to various CS applications. It included lectures, tech talks and demos, labs, and problem sets, culminating in the creation of a unique app.
“We wanted students to create something they could really use,” Jonathan explains, “to learn about databases and JavaScript but have a product at the end and an ‘I made this’ moment.” Many volunteers contributed to the effort, with Shriram, Elbert Wang, and Abdul Raziq Tabish playing a key role by developing the curriculum and the online instructional component. Jewel Brown and Ebube Chuba both gave talks on student groups, academic technology, and interdisciplinary possibilities with computer science. Purvi Goel created a Pokémon Go-inspired augmented reality tour of the CIT, and Joanna Simwinga documented the entire program, taking pictures and providing support.
Afternoons provided a bit of a break in the form of the non-technical curriculum, which prepared students for other aspects of academic life, such as Brown CS culture and campus resources. Field trip destinations included the Boston Museum of Science, the Google headquarters in Boston, and New Hampshire’s Mount Monadnock. Jonathan credits Laura as well as Lynsey Ford of the Science Center for the “administrative acrobatics” of many emails sent, phone calls made, and releases and waivers tracked down.
“It was really a success,” he says. “Even as we were doing it, we were thinking of things to do differently next year, like making sure that we have all the faculty members who teach intro courses join us as speakers, but I’m proud that we not only covered things like programming and coursework but also gave students the full set of skills they need to do well.”
“This is Mosaic+ really giving back to the next generation,” Laura adds. “It’s a wonderful thing to be a part of.” In the future, Brown CS is hoping to build on the success of this program by extending it or creating similar programs for students from other historically underrepresented groups (HUGs) as well.
“I’m grateful to everyone who jumped in with their spare time and engaged with our participants,” says Jonathan. “The students told me again and again how much they valued hearing about our experiences and seeing a community designed just for them start sprouting up around them. Now, with the Mentoring Program getting started, we’re going to take that skill-building and support to a whole new level.”
Tech Fair 2016 is here! Brown CS's Industry Partner Program, in conjunction with the CareerLAB, is hosting our third annual event on September 28-29, 2016, from 11 AM to 3 PM each day in Sayles Hall.
Brown CS has a long history of matching our students with industry leaders, startup companies, and everything in between, so we're really excited about continuing that great run. The Tech Fair is a highly-anticipated opportunity for students at all degree levels who are looking for jobs and internships in the tech sector to connect with employers looking to hire them. Companies of every size and description will be on hand to answer questions, collect resumes, and provide insight into their recruiting process: if you're a student, click here to see the list of participants on 9/28 and here for 9/29.
If you're looking to work in the tech sector or even thinking about it, drop by. See you at the Tech Fair!
Boston Global Forum has honored Associate Professor Seny Kamara of Brown University's Department of Computer Science (Brown CS) as an emerging thought leader and also given him a shared platform to continue his work on a global level by awarding him a Michael Dukakis Leadership Fellowship. Boston Global Forum is an independent organization founded to bring together experts from around the globe to participate in open public forums to discuss and illuminate the most critical issues affecting the world at large. Their principal mission is to provide an interactive and collaborative world forum for identifying and developing action-based solutions to our most profound problems.
The Fellowship Program aims to recognize rising stars and future world leaders in different fields, and to enrich their leadership capability. The program is highly selective, and Seny, selected for his potential for leadership and his commitment to global peace contribution, was chosen as one of just four fellows this year.
Seny's work focuses on designing and analyzing cryptographic algorithms, protocols and systems; often motivated by privacy issues in cloud computing, surveillance and databases. He joins Dinesh Bharadia, a researcher in networks and wireless communication; Ryan Maness, an expert in international security; and Mario Macilau, a photographer and educator. 
The image at left shows students at Brown Data Science's inaugural DataFest, aimed at promoting the learning of data science.
Extracting meaning and value from increasingly complex and voluminous data requires a distinct set of skills, methods, and tools that form the emerging discipline of Data Science. Brown’s Data Science Initiative (DSI) is an interdisciplinary collaboration between four core foundational departments (Applied Mathematics, Biostatistics, Computer Science, and Mathematics) and domain disciplines that aims to catalyze data-enabled science and scholarship across the campus, creating new opportunities for innovation in both its methods and application.
The DSI is happy to announce the creation of a new Master's program that will draw on Brown’s strengths in Data Science and offer students from a wide range of disciplinary backgrounds a rigorous, distinctive, and attractive education for building a career in Data Science and/or Big Data management.
The program's main goal is to provide a fundamental understanding of the methods and algorithms of Data Science. This understanding will be achieved through a study of relevant topics in mathematics, statistics, and computer science, including machine learning, data mining, security and privacy, visualization, and data management. Additionally, a Data and Society course will prepare students for ethical and societal challenges and considerations surrounding data science, and final “capstone” projects will provide experience in important, frontline Data Science problems in a variety of fields.
Brown University's Department of Computer Science (Brown CS) is happy to announce that Sherief Reda of the School of Engineering has just been given a secondary joint appointment to Brown CS. He joins R. Iris Bahar, Pedro Felzenszwalb, and Ravindra Pendse, who have similar joint status. His collaboration and theirs further strengthen the interdisciplinary approach that the university and both the School of Engineering and Brown CS are known for, offering students numerous new educational and research possibilities. 
Sherief, now Associate Professor of Engineering and Computer Science, joined the Computer Engineering group at Brown in 2006 after receiving his Ph.D. in computer science and engineering from University of California, San Diego. He received his B.Sc. (with Honors) and M.Sc. in electrical and computer engineering from Ain Shams University, Cairo, Egypt in 1998 and 2000. His research interests are in the area of computer engineering, with focus on energy-efficient computing systems, design automation and test of integrated circuits, and reconfigurable computing. Prof. Reda has over 60 refereed conference and journal papers (Google scholar profile). He served as a member of technical program committees for many IEEE/ACM conferences including DAC, ICCAD, ASPDAC, DATE, ICCD, GLSVLSI, and SLIP. Professor Reda received a number of awards and acknowledgments, including a best paper award in DATE 2002, a hot article in Operations Research letters in 2004, a first place award in ISPD VLSI placement contest in 2005, best paper nominations in ICCAD 2005, ASPDAC 2008, Brown's Salomon award in 2008, a NSF CAREER award, a best paper award in ISLPED 2010, and a best paper nomination in ICCAD 2015. His research is funded by NSF, DoD, DARPA, AMD, Intel and Qualcomm. He is a senior member of IEEE.
For many of us, being handed a C++ compiler in high school would have been a difficult and inauspicious introduction to computer science. Not so for George Konidaris, who joins Brown CS as assistant professor this autumn: as a teenager, growing up in South Africa, he had mostly used computers for games, but the compiler looked interesting. “It was magical,” he says of teaching himself to program. “It was a profound change for me, a mini-intellectual revolution. The world shifted half a degree on its axis. That’s really where I learned how to think.”
A love of research has spanned George’s entire career, and as a teenager, the tools for seeking knowledge were close at hand. An older sister, having graduated with a joint degree in computer science and economics, no longer needed her textbooks. “I plundered them,” says George. “It was fascinating, and I became very serious about learning everything about computers that I could.”
“Starting out as an undergraduate at the University of Witwatersrand in Johannesburg,” he explains, “I was drawn to artificial intelligence.” To Konidaris, it had the biggest, most interesting open questions.
George’s social circle shared his interest in research. “Five of my friends from university ended up getting their PhDs overseas. We hung out every day, and we talked each other into the idea of studying abroad.” He won a scholarship to do his Master’s degree in the UK, and chose the University of Edinburgh for their strong programs in artificial intelligence but also robotics, which had been of increasing interest throughout his undergraduate years. After Edinburgh, his travels took him to the University of Massachusetts Amherst, where he earned his PhD, leaving him with four degrees from universities on three different continents.
What was the appeal of robotics? Konidaris sums up his thoughts with a simple statement that turns unexpectedly introspective. (It may also explain the ease with which he puts his arm around one of his own robots and smiles for a photograph.) “If the brain is a computer, and I think it is, then humans are robots.”  
Asked to explain his unbroken interest in research, George says that it’s been interesting, fun, and turned out to be something he was “reasonably good” at. “When you look at the history of computer science, it’s very inspiring. Its researchers are brilliant, just amazing people. And, in the space of a generation, they changed the world in a profound way.”  
Konidaris describes his own work as an attempt to design intelligent robots. “Robots have the fundamental problem of making plans and generating action when they’re doomed to interface with the world at the level of the pixel and the motor. That’s their reality. But to achieve more complex behavior, to do something like having a robot get to the right gate at an airport to catch a plane, we need them to be able to make abstractions, to plan using higher-level concepts. At the moment, it’s more like they have to carefully plan every individual footstep and then struggle with things like recognizing what the right gate looks like when the scene has changed even a little bit. I’m interested in helping them gain the ability to reason and learn at the right level for solving the problem at hand.”
He’s in good company. George says that he’s always wanted to work with faculty members like Amy Greenwald, Michael Littman, and Stefanie Tellex. “It was just too tempting! Brown CS is a great fit for me. It was a combination of things: a department that’s really good at what it does, an outstanding place that’s really friendly.” Konidaris is eager to meet students as well. “A career in computer science can be a lifetime commitment,” he says. “I hope people are looking ahead, not at just the next big thing but the long run. I think it’s important for all of us to look back at the end of our contributions and see something that’s been worthy of an intellectual lifetime.”
“We have responsibilities as scientists, and a need for rigor,” says James Tompkin, who joins Brown CS as Assistant Professor this summer. But, watching a video overview of his recent research, the areas of inquiry sometimes surprise. Sophisticated editing of image and video content, new interaction and display devices from light fields, and...animal-shaped glockenspiels? “There’s also space for more fun,” laughs James, “and for the understanding that people are human.”
This understanding is represented in James’s interest in interaction, a thread that runs through almost a decade of research. “I create graphics, vision, and interaction techniques to improve our understanding of the connections within media,” he explains, “to further our ability to edit and explore the visual world.” The interest in interaction may have early origins: as a child, growing up in the London suburbs, he favored Fighting Fantasy non-linear gamebooks, tabletop and computer strategy games like X-COM, and “played tremendously with LEGO”. His father, the headmaster of a local high school, would bring home a succession of clunky, barely-portable computers, “...and then I would break them,” James grins, “as part of the ‘learning process’.”
Combined interests in computation and music spiralled outward from those early days of tinkering, leading to an undergraduate thesis at King’s College London in novel interaction methods for composing music. Later, at University College London, Tompkin’s work with visual computing and his association with architects at the interdisciplinary Bartlett Centre for Advanced Spatial Analysis only heightened his interest in not just the interactive but the tangible. “I like interfaces that exist in the real world,” he says, “and if you want more natural interfaces, you have to build them. A lot of my work is pixel-pushing, but sometimes you want to go into a machine shop and build something.”
After work at the Intel Visual Computing Institute, the Max-Planck-Institute for Informatics, and recent postdoctoral research at the Harvard Paulson School of Engineering and Applied Sciences, James found himself drawn to the “exceptional” students of Brown and the faculty’s rapport with them. “I think Brown is different. There’s a focus on teaching and really a will to support students. The ratio of TAs to undergraduate students is impressive, and I’ve worked with a Brown CS PhD, so I have a healthy respect for them too.” James is also enthusiastic about the proximity to the Rhode Island School of Design. “I have so much appreciation for the work of designers,” Tompkin says. “Computer scientists, we’re so in the box! It helps to look outside.”
So, what is he excited about doing here at Brown? James pauses for a second. “Computer graphics is in a very interesting position at the moment. In many ways, we’ve already solved the grand challenge of realistic image synthesis. Think about that: how many fields can say, ‘We did it all in about fifty years. Sorry!’ That amazes me.” But the amazement quickly gives way to a maker’s eagerness: “In visual computing, I see a mismatch between how we see and interact with the world and the tools that we use to capture it. Video is most similar to how we actually see. It’s a powerful medium: extremely fun, highly creative, good for storytelling, but it’s still very difficult to use. My research tries to remove some of these barriers to self-expression. I’m so pleased that as a community, we’re developing cheap and good ways for people to use cameras to make rich models of the world around them. This lets us give powerful new tools to novices and professionals, and expand what is possible with media, especially interactive media.”
As the credits roll on his research video and an orchestral score wafts in, Tompkin casually tosses out one more marvel: an antique view camera with a tablet housed inside, letting the viewer explore the Hitchcock classic Rear Window in real time as the voyeur, panning and zooming and changing perspective. “Most of the things people want to do are related to the real world,” James notes, “firmly grounded in reality. That’s why I try to make camera-captured media easier to create, analyse, and explore.” But like everything else on his resume, this looks highly creative. And extremely fun.
James will be teaching CSCI2951-I: Computer Vision for Graphics and Interaction in the fall.
2016 continues to be a significant year for Assistant Professor Stefanie Tellex of Brown University's Department of Computer Science (Brown CS), who has just won a NASA Early Career Faculty Award for a recent proposal ("Human-Robot Collaboration on Complex Tasks"). Earlier this year, she received a Sloan Fellowship, was listed in MIT Technology Review's 2016 Breakthrough Technologies, and was named one of four "Women Who Changed Science In 2015" by Wired UK. The Early Career Faculty Award recognizes outstanding faculty researchers early in their careers, challenging them to examine the theoretical feasibility of ideas and approaches that are critical to making science, space travel, and exploration more effective, affordable, and sustainable.
Stefanie's prior research has uniquely prepared her for the task of contributing to the advancement of space exploration. She first developed probabilistic models that enable a robot to infer actions in the physical world that correspond to natural language descriptions, then extended this work to enable robots to ask natural language questions that clarify ambiguous commands and eventually to ask for help. 
"A robotic collaborator," she explains, "must be an integrated system which is able to perceive the environment and perform a diverse set of actions to accomplish different tasks. We've seen fully autonomous systems that can assemble furniture, delivery library books, and even cook, and our approach focuses on planning in very large state spaces like these, but integrated into a communication and learning framework. Existing approaches suffer from problems in slow inference because they don't use state representations and conditional independence assumptions that exploit problem structure."
Tellex's solution is to design efficient state representations and factored inference algorithms that lead to efficient reasoning and inference. This factored, hierarchical structure leads to information-gathering behavior, such as asking questions, in a framework that supports efficient incorporation of multimodal observations from low-level sensors, human language and gesture, and background knowledge. The aim of her proposal is to test the hypothesis that a system can increase speed and accuracy at inferring human intentions and increase the number of robots a single astronaut can supervise by inferring a person’s mental state from their language utterances and actively asking questions when confused. (For example, a robot could ask its operator if she meant the Philips screwdriver or the flat-head screwdriver.) The end goal is to achieve seamless human-robot cooperation on complex tasks, approaching the ease and accuracy of human-human collaboration.
"Robots that can use fluid language at multiple levels of abstraction," Stefanie says, "can flexibly respond to a person’s requests. The ultimate impact is a world where robots actively interpret a person’s instructions, asking questions when they're confused, and asking for help when they encounter a problem."
Congressional Quarterly Roll Call recently interviewed Professor John Savage of Brown University's Department of Computer Science (Brown CS) to document a unique moment in history. "The May 26-27 meeting of the Group of Seven in Ise-Shima, Japan," writes Paul Merrion, "produced the G7's first-ever stand-alone agreement on cybersecurity, data protection and internet governance."
To give a bit of history, the Boston Global Forum (BGF), chaired by former governor Michael Dukakis, was founded to bring together thought leaders and experts from around the globe to participate in open public forums to discuss and illuminate the most critical issues affecting the world at large. In February, their CEO, Tuan Nguyen, asked John to address BGF and prepare an agenda for the G7 Summit, working with other individuals affiliated with BGF to develop his presentation into a formal proposal.
The G7 agreement ("The G7 Ise-Shima Leaders' Declaration"), which draws on the work of Savage and his colleagues, makes the landmark statement that cyberspace is under the rule of national law, and advocates for responsible state behavior during peacetime and the development of confidence-building measures to increase security. "It's very significant," John says. "It's progress, it's recognition that nations need to help one another."
The full article, located here, is only available to subscribers, but a summary of the Declaration is available here.
A letter to the Brown University community from President Christina Paxson on May 27, 2016, announced that Maurice Herlihy and Eli Upfal of the Department of Computer Science (Brown CS) have joined colleagues from across the University in being appointed to named chairs. Maurice is the An Wang Professor of Computer Science, and Eli is the Rush C. Hawkins Professor of Computer Science. The appointment honors the two men for their achievements and years of service and serves as another milestone for Brown CS, which now has a total of six named chairs.
The past two years have been important ones for Maurice: on July 15, 2014, the Association for Computing Machinery (ACM) honored him with a 60th Birthday Celebration in Paris at the ACM Symposium on Principles of Distributed Computing. Just last year, he was inducted into the American Academy of Arts and Sciences, joining leading thinkers in science, public affairs, business, arts and humanities. Most recently, he received the ACM SIGOPS (Special Interest Group on Operating Systems) Hall of Fame Award, recognizing the most influential Operating Systems papers from at least ten years in the past.
"It's an honor," he says, "to be appointed to the Wang chair, named for one of New England's tech pioneers. I'm grateful to the department and the university for their support." 
Eli has been similarly busy. Less than a year ago, he won a Brown Institute for Brain Science Innovation Award for his research into better understanding the activity of specific neural networks in the brain and how they change dynamically during learning. His research continues to span a variety of areas: last year, he published ten papers on topics ranging from hard-core theory of computing to computational biology, parallel computing, and knowledge discovery. Most recently, his research group had a remarkable two full papers and one poster paper accepted at the Conference on Knowledge Discovery and Data Mining (KDD 2016).
"Any success I've had here," Eli says, "was mainly due to the amazing students and post-docs that I have worked with at Brown. My background was mostly in math and statistics, and it was the collaboration with colleagues at Brown and elsewhere that made this knowledge relevant to CS."
"Both Maurice and Eli," says Department Chair Ugur Cetintemel, "have been doing innovative work that pushes the boundaries of computer science, making an impact within and outside of our discipline. We're pleased that they are being acknowledged with these prestigious named chairs."
Notable this year for the size of the graduating class (168 undergrads, breaking last year's record by 28%) and every year for sentiment, well-deserved pride, and a sense of fun, Brown University's Department of Computer Science (Brown CS) celebrated Commencement 2016 this past weekend.
Festivities began on Friday with the Hooding Ceremony, where friends and family members applauded as graduating Master's students received Brown CS sweatshirts and graduating PhD students were "hooded" by their advisors, who placed hooded sweatshirts directly over their heads, often to the sound of laughter.
Saturday was the 37th Anniversary Brown CS Reunion, and as always, the third floor of the CIT overflowed as the entire Brown CS community of students, alums, faculty, staff, family, and friends rejoined each other to celebrate. This year, Brown CS recognized two alums, Sridhar Ramaswamy and Norm Meyrowitz, celebrating their achievements and dedication to the Department with a special ceremony and champagne toast.
The capstone of the weekend took place on Sunday, when a record number of students walked down the aisle of the First Unitarian Church to receive their diplomas.
Congratulations to all our graduates! Photos of all three events (Commencement photos are still being processed) are available at https://flic.kr/s/aHskAjGMd5.
Brown University's Department of Computer Science (Brown CS) is proud to announce that alum Sridhar Ramaswamy's achievements in the field have earned him the Horace Mann Medal. Dean Peter M. Weber will present the award, given annually to a Brown University Graduate School alum who has made significant contributions to their field, at the Graduate School Doctoral Ceremony on May 29, 2016. Sridhar joins Brown CS alum Ingrid Carlbom, who received the Distinguished Graduate School Alumni Award (later replaced by the Horace Mann Medal) in 2000.
Part of a group of senior executives who report directly to CEO Sundar Pichai, Ramaswamy is currently the Senior Vice President of Advertising and Commerce at Google, where he oversees the design, innovation, and engineering of the company's advertising and commerce products. As leader of the engineering teams that helped define the vision and direction of AdWords, which built Google's multi-billion-dollar advertising business, he now directs the company's efforts in search advertising, display and video advertising, Google Shopping, Android Pay, and many other global platforms.
Prior to joining Google, Sridhar worked for Bellcore and Bell Labs, helping create a revolutionary system called AQUA (Approximate QUery Answering) that was aimed at providing provable guarantees for fast answers on massive datasets. His research provided the first efficient algorithm for mining outliers in large datasets and is considered a pioneering contribution to the field. Since then, his numerous innovations include the creation of online auction models with highly precise ad delivery, the development of a scientific framework and platform for understanding how website layouts and user interface components impact online monetization, and the establishment of ads quality systems.
Sridhar received his PhD in Computer Science in 1995 but continues to contribute to Brown CS, serving in the inaugural cohort of the Department's Advisory Board and delivering the first lecture in the IT Leaders Lecture Series. Interviewed prior to the event, he spoke of his love for solving the unsolvable: "What excites me is that computing itself is very young. It’s only begun to touch numerous aspects of our life. I can point kids to so many different areas and tell them that there are thousands of possible futures there, just waiting!”
"This is tremendous news," says Brown CS Department Chair Ugur Cetintemel. "We're proud of Sridhar's remarkable accomplishments and their daily impact on so many people worldwide. He's unique: our students who hope to lead and innovate in both technology and business couldn't ask for a better role model."  
Less than a year after earning a Sheridan Junior Faculty Teaching Fellowship, Paul Valiant of Brown University's Computer Science Department (Brown CS) has been chosen by the graduating senior class to receive the Barrett Hazeltine Citation for his efforts teaching the wildly successful Design and Analysis of Algorithms course each fall. Named for a beloved professor emeritus, the award recognizes excellence in teaching, guidance, and support by two members of the faculty and/or administration. Paul will be recognized at the President's Reception for the class on Monday, May 23, and he joins numerous notable prior recipients, including award-winning author James Morone, Barbara Tannenbaum of the Department of Theatre Arts and Performance Studies, and Michael Lysaght, founder and director emeritus of Brown University’s Center for Biomedical Engineering.
Text
Text1
Key-Alternating Ciphers and Key-Length Extension: Exact Bounds and Multi-user Security
Abstract

The best existing bounds on the concrete security of key-alternating ciphers (Chen and Steinberger, EUROCRYPT ’14) are only asymptotically tight, and the quantitative gap with the best existing attacks remains numerically substantial for concrete parameters. Here, we prove exact bounds on the security of key-alternating ciphers and extend them to XOR cascades, the most efficient construction for key-length extension. Our bounds essentially match, for any possible query regime, the advantage achieved by the best existing attack.

Our treatment also extends to the multi-user regime. We show that the multi-user security of key-alternating ciphers and XOR cascades is very close to the single-user case, i.e., given enough rounds, it does not substantially decrease as the number of users increases. On the way, we also provide the first explicit treatment of multi-user security for key-length extension, which is particularly relevant given the significant security loss of block ciphers (even if ideal) in the multi-user setting.

The common denominator behind our results are new techniques for information-theoretic indistinguishability proofs that both extend and refine existing proof techniques like the H-coefficient method.
Counter-in-Tweak: Authenticated Encryption Modes for Tweakable Block Ciphers
AbstractWe propose the Synthetic Counter-in-Tweak (\mathsf {SCT}𝖲𝖢𝖳\mathsf {SCT}) mode, which turns a tweakable block cipher into a nonce-based authenticated encryption scheme (with associated data). The \mathsf {SCT}𝖲𝖢𝖳\mathsf {SCT} mode combines in a SIV-like manner a Wegman-Carter MAC inspired from \mathsf {PMAC}𝖯𝖬𝖠𝖢\mathsf {PMAC} for the authentication part and a new counter-like mode for the encryption part, with the unusual property that the counter is applied on the tweak input of the underlying tweakable block cipher rather than on the plaintext input. Unlike many previous authenticated encryption modes, \mathsf {SCT}𝖲𝖢𝖳\mathsf {SCT} enjoys provable security beyond the birthday bound (and even up to roughly 2^n2^n tweakable block cipher calls, where n is the block length, when the tweak length is sufficiently large) in the nonce-respecting scenario where nonces are never repeated. In addition, \mathsf {SCT}\mathsf {SCT} ensures security up to the birthday bound even when nonces are reused, in the strong nonce-misuse resistance sense (MRAE) of Rogaway and Shrimpton (EUROCRYPT 2006). To the best of our knowledge, this is the first authenticated encryption mode that provides at the same time close-to-optimal security in the nonce-respecting scenario and birthday-bound security for the nonce-misuse scenario. While two passes are necessary to achieve MRAE-security, our mode enjoys a number of desirable features: it is simple, parallelizable, it requires the encryption direction only, it is particularly efficient for small messages compared to other nonce-misuse resistant schemes (no precomputation is required) and it allows incremental update of associated data.
XPX: Generalized Tweakable Even-Mansour with Improved Security Guarantees
AbstractWe present \mathrm {XPX}\mathrm {XPX}, a tweakable blockcipher based on a single permutation PP. On input of a tweak (t_{11},t_{12},t_{21},t_{22})\in \mathcal {T}(t_{11},t_{12},t_{21},t_{22})\in \mathcal {T} and a message m, it outputs ciphertext c=P(m\oplus \varDelta _1)\oplus \varDelta _2c=P(m\oplus \varDelta _1)\oplus \varDelta _2, where \varDelta _1=t_{11}k\oplus t_{12}P(k)\varDelta _1=t_{11}k\oplus t_{12}P(k) and \varDelta _2=t_{21}k\oplus t_{22}P(k)\varDelta _2=t_{21}k\oplus t_{22}P(k). Here, the tweak space \mathcal {T}\mathcal {T} is required to satisfy a certain set of trivial conditions (such as (0,0,0,0)\not \in \mathcal {T}(0,0,0,0)\not \in \mathcal {T}). We prove that \mathrm {XPX}\mathrm {XPX} with any such tweak space is a strong tweakable pseudorandom permutation. Next, we consider the security of \mathrm {XPX}\mathrm {XPX} under related-key attacks, where the adversary can freely select a key-deriving function upon every evaluation. We prove that \mathrm {XPX}\mathrm {XPX} achieves various levels of related-key security, depending on the set of key-deriving functions and the properties of \mathcal {T}\mathcal {T}. For instance, if t_{12}, t_{22}\ne 0t_{12}, t_{22}\ne 0 and (t_{21}, t_{22})\ne (0,1)(t_{21}, t_{22})\ne (0,1) for all tweaks, \mathrm {XPX}\mathrm {XPX} is XOR-related-key secure. \mathrm {XPX}\mathrm {XPX} generalizes Even-Mansour (\mathrm {EM}\mathrm {EM}), but also Rogaway’s \mathrm {XEX}\mathrm {XEX} based on \mathrm {EM}\mathrm {EM}, and various other tweakable blockciphers. As such, \mathrm {XPX}\mathrm {XPX} finds a wide range of applications. We show how our results on \mathrm {XPX}\mathrm {XPX} directly imply related-key security of the authenticated encryption schemes Prøst-\mathrm {COPA}\mathrm {COPA} and \mathrm {Minalpher}\mathrm {Minalpher}, and how a straightforward adjustment to the MAC function \mathrm {Chaskey}\mathrm {Chaskey} and to keyed Sponges makes them provably related-key secure.
Indifferentiability of 8-Round Feistel Networks
AbstractWe prove that a balanced 8-round Feistel network is indifferentiable from a random permutation, improving on previous 10-round results by Dachman-Soled et al. and Dai et al. Our simulator achieves security O(q^8/2^n)O(q^8/2^n), similarly to the security of Dai et al. For further comparison, Dachman-Soled et al. achieve security O(q^{12}/2^n)O(q^{12}/2^n), while the original 14-round simulator of Holenstein et al. achieves security O(q^{10}/2^n)O(q^{10}/2^n).
EWCDM: An Efficient, Beyond-Birthday Secure, Nonce-Misuse Resistant MAC
AbstractWe propose a nonce-based MAC construction called EWCDM (Encrypted Wegman-Carter with Davies-Meyer), based on an almost xor-universal hash function and a block cipher, with the following properties: (i) it is simple and efficient, requiring only two calls to the block cipher, one of which can be carried out in parallel to the hash function computation; (ii) it is provably secure beyond the birthday bound when nonces are not reused; (iii) it provably retains security up to the birthday bound in case of nonce misuse. Our construction is a simple modification of the Encrypted Wegman-Carter construction, which is known to achieve only (i) and (iii) when based on a block cipher. Underlying our new construction is a new PRP-to-PRF conversion method coined Encrypted Davies-Meyer, which turns a pair of secret random permutations into a function which is provably indistinguishable from a perfectly random function up to at least 2^{2n/3}2^{2n/3} queries, where n is the bit-length of the domain of the permutations.
A Subfield Lattice Attack on Overstretched NTRU Assumptions
AbstractThe subfield attack exploits the presence of a subfield to solve overstretched versions of the NTRU assumption: norming the public key h down to a subfield may lead to an easier lattice problem and any sufficiently good solution may be lifted to a short vector in the full NTRU-lattice. This approach was originally sketched in a paper of Gentry and Szydlo at Eurocrypt’02 and there also attributed to Jonsson, Nguyen and Stern. However, because it does not apply for small moduli and hence NTRUEncrypt, it seems to have been forgotten. In this work, we resurrect this approach, fill some gaps, analyze and generalize it to any subfields and apply it to more recent schemes. We show that for significantly larger moduli — a case we call overstretched — the subfield attack is applicable and asymptotically outperforms other known attacks.This directly affects the asymptotic security of the bootstrappable homomorphic encryption schemes LTV and YASHE which rely on a mildly overstretched NTRU assumption: the subfield lattice attack runs in sub-exponential time 2^{O(\lambda /\log ^{1/3}\lambda )}2𝑂(𝜆/log1/3𝜆)2^{O(\lambda /\log ^{1/3}\lambda )} invalidating the security claim of 2^{\varTheta (\lambda )}2𝛩(𝜆)2^{\varTheta (\lambda )}. The effect is more dramatic on GGH-like Multilinear Maps: this attack can run in polynomial time without encodings of zero nor the zero-testing parameter, yet requiring an additional quantum step to recover the secret parameters exactly.We also report on practical experiments. Running LLL in dimension 512 we obtain vectors that would have otherwise required running BKZ with block-size 130 in dimension 8192. Finally, we discuss concrete aspects of this attack, the condition on the modulus q to guarantee full immunity, discuss countermeasures and propose open questions.
A Practical Cryptanalysis of the Algebraic Eraser
Abstract

We present a novel cryptanalysis of the Algebraic Eraser primitive. This key agreement scheme, based on techniques from permutation groups, matrix groups and braid groups, is proposed as an underlying technology for ISO/IEC 29167-20, which is intended for authentication of RFID tags. SecureRF, the company owning the trademark Algebraic Eraser, markets it as suitable in general for lightweight environments such as RFID tags and other IoT applications. Our attack is practical on standard hardware: for parameter sizes corresponding to claimed 128-bit security, our implementation recovers the shared key using less than 8 CPU hours, and less than 64 MB of memory.
Lattice-Based Fully Dynamic Multi-key FHE with Short Ciphertexts
Abstract

We present a multi-key fully homomorphic encryption scheme that supports an unbounded number of homomorphic operations for an unbounded number of parties. Namely, it allows to perform arbitrarily many computational steps on inputs encrypted by an a-priori unbounded (polynomial) number of parties. Inputs from new parties can be introduced into the computation dynamically, so the final set of parties needs not be known ahead of time. Furthermore, the length of the ciphertexts, as well as the space complexity of an atomic homomorphic operation, grow only linearly with the current number of parties.

Prior works either supported only an a-priori bounded number of parties (López-Alt, Tromer and Vaikuntanthan, STOC ’12), or only supported single-hop evaluation where all inputs need to be known before the computation starts (Clear and McGoldrick, Crypto ’15, Mukherjee and Wichs, Eurocrypt ’16). In all aforementioned works, the ciphertext length grew at least quadratically with the number of parties.

Technically, our starting point is the LWE-based approach of previous works. Our result is achieved via a careful use of Gentry’s bootstrapping technique, tailored to the specific scheme. Our hardness assumption is that the scheme of Mukherjee and Wichs is circular secure (and thus bootstrappable). A leveled scheme can be achieved under standard LWE.
Cryptography with Auxiliary Input and Trapdoor from Constant-Noise LPN
AbstractDodis, Kalai and Lovett (STOC 2009) initiated the study of the Learning Parity with Noise (LPN) problem with (static) exponentially hard-to-invert auxiliary input. In particular, they showed that under a new assumption (called Learning Subspace with Noise) the above is quasi-polynomially hard in the high (polynomially close to uniform) noise regime.Inspired by the “sampling from subspace” technique by Yu (eprint 2009/467) and Goldwasser et al. (ITCS 2010), we show that standard LPN can work in a mode (reducible to itself) where the constant-noise LPN (by sampling its matrix from a random subspace) is robust against sub-exponentially hard-to-invert auxiliary input with comparable security to the underlying LPN. Plugging this into the framework of [DKL09], we obtain the same applications as considered in [DKL09] (i.e., CPA/CCA secure symmetric encryption schemes, average-case obfuscators, reusable and robust extractors) with resilience to a more general class of leakages, improved efficiency and better security under standard assumptions.As a main contribution, under constant-noise LPN with certain sub-exponential hardness (i.e., 2^{\omega (n^{1/2})}2^{\omega (n^{1/2})} for secret size n) we obtain a variant of the LPN with security on poly-logarithmic entropy sources, which in turn implies CPA/CCA secure public-key encryption (PKE) schemes and oblivious transfer (OT) protocols. Prior to this, basing PKE and OT on constant-noise LPN had been an open problem since Alekhnovich’s work (FOCS 2003).
The Multi-user Security of Authenticated Encryption: AES-GCM in TLS 1.3
Abstract

We initiate the study of multi-user (mu) security of authenticated encryption (AE) schemes as a way to rigorously formulate, and answer, questions about the “randomized nonce” mechanism proposed for the use of the AE scheme GCM in TLS 1.3. We (1) Give definitions of mu ind (indistinguishability) and mu kr (key recovery) security for AE (2) Characterize the intent of nonce randomization as being improved mu security as a defense against mass surveillance (3) Cast the method as a (new) AE scheme RGCM (4) Analyze and compare the mu security of both GCM and RGCM in the model where the underlying block cipher is ideal, showing that the mu security of the latter is indeed superior in many practical contexts to that of the former, and (5) Propose an alternative AE scheme XGCM having the same efficiency as RGCM but better mu security and a more simple and modular design.
A Modular Treatment of Cryptographic APIs: The Symmetric-Key Case
Abstract

Application Programming Interfaces (APIs) to cryptographic tokens like smartcards and Hardware Security Modules (HSMs) provide users with commands to manage and use cryptographic keys stored on trusted hardware. Their design is mainly guided by industrial standards with only informal security promises.

In this paper we propose cryptographic models for the security of such APIs. The key feature of our approach is that it enables modular analysis. Specifically, we show that a secure cryptographic API can be obtained by combining a secure API for key-management together with secure implementations of, for instance, encryption or message authentication. Our models are the first to provide such compositional guarantees while considering realistic adversaries that can adaptively corrupt keys stored on tokens. We also provide a proof of concept instantiation (from a deterministic authenticated-encryption scheme) of the key-management portion of cryptographic API.
Encryption Switching Protocols
Abstract

We formally define the primitive of encryption switching protocol (ESP), allowing to switch between two encryption schemes. Intuitively, this two-party protocol converts given ciphertexts from one scheme into ciphertexts of the same messages under the other scheme, for any polynomial number of switches, in any direction. Although ESP is a special kind of two-party computation protocol, it turns out that ESP implies general two-party computation (2-PC) under natural conditions. In particular, our new paradigm is tailored to the evaluation of functions over rings. Indeed, assuming the compatibility of two additively and multiplicatively homomorphic encryption schemes, switching ciphertexts makes it possible to efficiently reconcile the two internal laws. Since no such pair of public-key encryption schemes appeared in the literature, except for the non-interactive case of fully homomorphic encryption which still remains prohibitive in practice, we build the first multiplicatively homomorphic ElGamal-like encryption scheme over \((\mathbb {Z}_n,\times )\) as a complement to the Paillier encryption scheme over \((\mathbb {Z}_n,+)\), where n is a strong RSA modulus. Eventually, we also instantiate secure ESPs between the two schemes, in front of malicious adversaries. This enhancement relies on a new technique called refreshable twin ciphertext pool, which we show being of independent interest. We additionally prove this is enough to argue the security of our general 2-PC protocol against malicious adversaries.
Message Transmission with Reverse Firewalls—Secure Communication on Corrupted Machines
Abstract

Suppose Alice wishes to send a message to Bob privately over an untrusted channel. Cryptographers have developed a whole suite of tools to accomplish this task, with a wide variety of notions of security, setup assumptions, and running times. However, almost all prior work on this topic made a seemingly innocent assumption: that Alice has access to a trusted computer with a proper implementation of the protocol. The Snowden revelations show us that, in fact, powerful adversaries can and will corrupt users’ machines in order to compromise their security. And, (presumably) accidental vulnerabilities are regularly found in popular cryptographic software, showing that users cannot even trust implementations that were created honestly. This leads to the following (seemingly absurd) question: “Can Alice securely send a message to Bob even if she cannot trust her own computer?!”

Bellare, Paterson, and Rogaway recently studied this question. They show a strong impossibility result that in particular rules out even semantically secure public-key encryption in their model. However, Mironov and Stephens-Davidowitz recently introduced a new framework for solving such problems: reverse firewalls. A secure reverse firewall is a third party that “sits between Alice and the outside world” and modifies her sent and received messages so that even if the her machine has been corrupted, Alice’s security is still guaranteed. We show how to use reverse firewalls to sidestep the impossibility result of Bellare et al., and we achieve strong security guarantees in this extreme setting.

Indeed, we find a rich structure of solutions that vary in efficiency, security, and setup assumptions, in close analogy with message transmission in the classical setting. Our strongest and most important result shows a protocol that achieves interactive, concurrent CCA-secure message transmission with a reverse firewall—i.e., CCA-secure message transmission on a possibly compromised machine! Surprisingly, this protocol is quite efficient and simple, requiring only four rounds and a small constant number of public-key operations for each party. It could easily be used in practice. Behind this result is a technical composition theorem that shows how key agreement with a sufficiently secure reverse firewall can be used to construct a message-transmission protocol with its own secure reverse firewall.
Big-Key Symmetric Encryption: Resisting Key Exfiltration
Abstract

This paper aims to move research in the bounded retrieval model (BRM) from theory to practice by considering symmetric (rather than public-key) encryption, giving efficient schemes, and providing security analyses with sharp, concrete bounds. The threat addressed is malware that aims to exfiltrate a user’s key. Our schemes aim to thwart this by using an enormously long key, yet paying for this almost exclusively in storage cost, not speed. Our main result is a general-purpose lemma, the subkey prediction lemma, that gives a very good bound on an adversary’s ability to guess a (modest length) subkey of a big-key, the subkey consisting of the bits of the big-key found at random, specified locations, after the adversary has exfiltrated partial information about the big-key (e.g., half as many bits as the big-key is long). We then use this to design a new kind of key encapsulation mechanism, and, finally, a symmetric encryption scheme. Both are in the random-oracle model. We also give a less efficient standard-model scheme that is based on universal computational extractors (UCE). Finally, we define and achieve hedged BRM symmetric encryption, which provides authenticity in the absence of leakage.
Backdoors in Pseudorandom Number Generators: Possibility and Impossibility Results
Abstract

Inspired by the Dual EC DBRG incident, Dodis et al. (Eurocrypt 2015) initiated the formal study of backdoored PRGs, showing that backdoored PRGs are equivalent to public key encryption schemes, giving constructions for backdoored PRGs (BPRGs), and showing how BPRGs can be “immunised” by careful post-processing of their outputs. In this paper, we continue the foundational line of work initiated by Dodis et al., providing both positive and negative results.

We first revisit the backdoored PRG setting of Dodis et al., showing that PRGs can be more strongly backdoored than was previously envisaged. Specifically, we give efficient constructions of BPRGs for which, given a single generator output, Big Brother can recover the initial state and, therefore, all outputs of the BPRG. Moreover, our constructions are forward-secure in the traditional sense for a PRG, resolving an open question of Dodis et al. in the negative.

We then turn to the question of the effectiveness of backdoors in robust PRNGs with input (c.f. Dodis et al., ACM-CCS 2013): generators in which the state can be regularly refreshed using an entropy source, and in which, provided sufficient entropy has been made available since the last refresh, the outputs will appear pseudorandom. The presence of a refresh procedure might suggest that Big Brother could be defeated, since he would not be able to predict the values of the PRNG state backwards or forwards through the high-entropy refreshes. Unfortunately, we show that this intuition is not correct: we are also able to construct robust PRNGs with input that are backdoored in a backwards sense. Namely, given a single output, Big Brother is able to rewind through a number of refresh operations to earlier “phases”, and recover all the generator’s outputs in those earlier phases.

Finally, and ending on a positive note, we give an impossibility result: we provide a bound on the number of previous phases that Big Brother can compromise as a function of the state-size of the generator: smaller states provide more limited backdooring opportunities for Big Brother.
A \(2^{70}\) Attack on the Full MISTY1
Abstract

MISTY1 is a block cipher designed by Matsui in 1997. It is widely deployed in Japan, and is recognized internationally as a European NESSIE-recommended cipher and an ISO standard. After almost 20 years of unsuccessful cryptanalytic attempts, a first attack on the full MISTY1 was presented at CRYPTO 2015 by Yosuke Todo. The attack, using a new technique called division property, requires almost the full codebook and has time complexity of \(2^{107.3}\) encryptions.

In this paper we present a new attack on the full MISTY1. It is based on Todo’s division property, along with a variety of refined key-recovery techniques. Our attack requires almost the full codebook (like Todo’s attack), but allows to retrieve 49 bits of the secret key in time complexity of only \(2^{64}\) encryptions, and the full key in time complexity of \(2^{69.5}\) encryptions.

While our attack is clearly impractical due to its large data complexity, it shows that MISTY1 provides security of only \(2^{70}\) — significantly less than what was considered before.
Cryptanalysis of the FLIP Family of Stream Ciphers
Abstract

At Eurocrypt 2016, Méaux et al. proposed FLIP, a new family of stream ciphers intended for use in Fully Homomorphic Encryption systems. Unlike its competitors which either have a low initial noise that grows at each successive encryption, or a high constant noise, the FLIP family of ciphers achieves a low constant noise thanks to a new construction called filter permutator.

In this paper, we present an attack on the early version of FLIP that exploits the structure of the filter function and the constant internal state of the cipher. Applying this attack to the two instantiations proposed by Méaux et al. allows for a key recovery in \(2^{54}\) basic operations (resp. \(2^{68}\)), compared to the claimed security of \(2^{80}\) (resp. \(2^{128}\)).
The Magic of ELFs
Abstract

We introduce the notion of an Extremely Lossy Function (ELF). An ELF is a family of functions with an image size that is tunable anywhere from injective to having a polynomial-sized image. Moreover, for any efficient adversary, for a sufficiently large polynomial r (necessarily chosen to be larger than the running time of the adversary), the adversary cannot distinguish the injective case from the case of image size r.

We develop a handful of techniques for using ELFs, and show that such extreme lossiness is useful for instantiating random oracles in several settings. In particular, we show how to use ELFs to build secure point function obfuscation with auxiliary input, as well as polynomially-many hardcore bits for any one-way function. Such applications were previously known from strong knowledge assumptions — for example polynomially-many hardcore bits were only know from differing inputs obfuscation, a notion whose plausibility has been seriously challenged. We also use ELFs to build a simple hash function with output intractability, a new notion we define that may be useful for generating common reference strings.

Next, we give a construction of ELFs relying on the exponential hardness of the decisional Diffie-Hellman problem, which is plausible in pairing-based groups. Combining with the applications above, our work gives several practical constructions relying on qualitatively different — and arguably better — assumptions than prior works.
Breaking the Circuit Size Barrier for Secure Computation Under DDH
Abstract

Under the Decisional Diffie-Hellman (DDH) assumption, we present a 2-out-of-2 secret sharing scheme that supports a compact evaluation of branching programs on the shares. More concretely, there is an evaluation algorithm \(\mathsf{Eval}\) with a single bit of output, such that if an input \(w\in \{0,1\}^n\) is shared into \((w^0,w^1)\), then for any deterministic branching program P of size S we have that \(\mathsf{Eval}(P,w^0)\oplus \mathsf{Eval}(P,w^1)=P(w)\) except with at most \(\delta \) failure probability. The running time of the sharing algorithm is polynomial in n and the security parameter \(\lambda \), and that of \(\mathsf{Eval}\) is polynomial in \(S,\lambda \), and \(1/\delta \). This applies as a special case to boolean formulas of size S or boolean circuits of depth \(\log S\). We also present a public-key variant that enables homomorphic computation on inputs contributed by multiple clients.

The above result implies the following DDH-based applications:

A secure 2-party computation protocol for evaluating any branching program or formula of size S, where the communication complexity is linear in the input size and only the running time grows with S.

A secure 2-party computation protocol for evaluating layered boolean circuits of size S with communication complexity \(O(S/\log S)\).

A 2-party function secret sharing scheme, as defined by Boyle et al. (Eurocrypt 2015), for general branching programs (with inverse polynomial error probability).

A 1-round 2-server private information retrieval scheme supporting general searches expressed by branching programs.

Prior to our work, similar results could only be achieved using fully homomorphic encryption. We hope that our approach will lead to more practical alternatives to known fully homomorphic encryption schemes in the context of low-communication secure computation.
Extended Tower Number Field Sieve: A New Complexity for the Medium Prime Case
Abstract

We introduce a new variant of the number field sieve algorithm for discrete logarithms in \(\mathbb {F}_{p^n}\) called exTNFS. The most important modification is done in the polynomial selection step, which determines the cost of the whole algorithm: if one knows how to select good polynomials to tackle discrete logarithms in \(\mathbb {F}_{p^\kappa }\), exTNFS allows to use this method when tackling \(\mathbb {F}_{p^{\eta \kappa }}\) whenever \(\gcd (\eta ,\kappa )=1\). This simple fact has consequences on the asymptotic complexity of NFS in the medium prime case, where the complexity is reduced from \(L_Q(1/3,\root 3 \of {96/9})\) to \(L_Q(1/3,\root 3 \of {48/9})\), \(Q=p^n\), respectively from \(L_Q(1/3,2.15)\) to \(L_Q(1/3,1.71)\) if multiple number fields are used. On the practical side, exTNFS can be used when \(n=6\) and \(n=12\) and this requires to updating the keysizes used for the associated pairing-based cryptosystems.
Efficient Algorithms for Supersingular Isogeny Diffie-Hellman
Abstract

We propose a new suite of algorithms that significantly improve the performance of supersingular isogeny Diffie-Hellman (SIDH) key exchange. Subsequently, we present a full-fledged implementation of SIDH that is geared towards the 128-bit quantum and 192-bit classical security levels. Our library is the first constant-time SIDH implementation and is up to 2.9 times faster than the previous best (non-constant-time) SIDH software. The high speeds in this paper are driven by compact, inversion-free point and isogeny arithmetic and fast SIDH-tailored field arithmetic: on an Intel Haswell processor, generating ephemeral public keys takes 46 million cycles for Alice and 52 million cycles for Bob, while computing the shared secret takes 44 million and 50 million cycles, respectively. The size of public keys is only 564 bytes, which is significantly smaller than most of the popular post-quantum key exchange alternatives. Ultimately, the size and speed of our software illustrates the strong potential of SIDH as a post-quantum key exchange candidate and we hope that these results encourage a wider cryptanalytic effort.
New Insights on AES-Like SPN Ciphers
Abstract

It has been proved in Eurocrypt 2016 by Sun et al. that if the details of the S-boxes are not exploited, an impossible differential and a zero-correlation linear hull can extend over at most 4 rounds of the AES. This paper concentrates on distinguishing properties of AES-like SPN ciphers by investigating the details of both the underlying S-boxes and the MDS matrices, and illustrates some new insights on the security of these schemes. Firstly, we construct several types of 5-round zero-correlation linear hulls for AES-like ciphers that adopt identical S-boxes to construct the round function and that have two identical elements in a column of the inverse of their MDS matrices. We then use these linear hulls to construct 5-round integrals provided that the difference of two sub-key bytes is known. Furthermore, we prove that we can always distinguish 5 rounds of such ciphers from random permutations even when the difference of the sub-keys is unknown. Secondly, the constraints for the S-boxes and special property of the MDS matrices can be removed if the cipher is used as a building block of the Miyaguchi-Preneel hash function. As an example, we construct two types of 5-round distinguishers for the hash function Whirlpool. Finally, we show that, in the chosen-ciphertext mode, there exist some nontrivial distinguishers for 5-round AES. To the best of our knowledge, this is the longest distinguisher for the round-reduced AES in the secret-key setting. Since the 5-round distinguisher for the AES can only be constructed in the chosen-ciphertext mode, the security margin for the round-reduced AES under the chosen-plaintext attack may be different from that under the chosen-ciphertext attack.
Lightweight Multiplication in \(GF(2^n)\) with Applications to MDS Matrices
Abstract

In this paper we consider the fundamental question of optimizing finite field multiplications with one fixed element. Surprisingly, this question did not receive much attention previously. We investigate which field representation, that is which choice of basis, allows for an optimal implementation. Here, the efficiency of the multiplication is measured in terms of the number of XOR operations needed to implement the multiplication. While our results are potentially of larger interest, we focus on a particular application in the second part of our paper. Here we construct new MDS matrices which outperform or are on par with all previous results when focusing on a round-based hardware implementation.
Another View of the Division Property
Abstract

A new distinguishing property against block ciphers, called the division property, was introduced by Todo at Eurocrypt 2015. Our work gives a new approach to it by the introduction of the notion of parity sets. First of all, this new notion permits us to formulate and characterize in a simple way the division property of any order. At a second step, we are interested in the way of building distinguishers on a block cipher by considering some further properties of parity sets, generalising the division property. We detail in particular this approach for substitution-permutation networks. To illustrate our method, we provide low-data distinguishers against reduced-round Present. These distinguishers reach a much higher number of rounds than generic distinguishers based on the division property and demonstrate, amongst others, how the distinguishers can be improved when the properties of the linear and the Sbox layers are taken into account. At last, this work provides an analysis of the resistance of Sboxes against this type of attacks, demonstrates links with the algebraic normal form of an Sbox as well as its inverse Sbox and exhibit design criteria for Sboxes to resist such attacks.
Faster Fully Homomorphic Encryption: Bootstrapping in Less Than 0.1 Seconds
Abstract

In this paper, we revisit fully homomorphic encryption (FHE) based on GSW and its ring variants. We notice that the internal product of GSW can be replaced by a simpler external product between a GSW and an LWE ciphertext.

We show that the bootstrapping scheme FHEW of Ducas and Micciancio [11] can be expressed only in terms of this external product. As a result, we obtain a speed up from less than 1 s to less than 0.1 s. We also reduce the 1 GB bootstrapping key size to 24 MB, preserving the same security levels, and we improve the noise propagation overhead by replacing exact decomposition algorithms with approximate ones.

Moreover, our external product allows to explain the unique asymmetry in the noise propagation of GSW samples and makes it possible to evaluate deterministic automata homomorphically as in [13] in an efficient way with a noise overhead only linear in the length of the tested word.

Finally, we provide an alternative practical analysis of LWE based scheme, which directly relates the security parameter to the error rate of LWE and the entropy of the LWE secret key.
A General Polynomial Selection Method and New Asymptotic Complexities for the Tower Number Field Sieve Algorithm
Abstract

In a recent work, Kim and Barbulescu had extended the tower number field sieve algorithm to obtain improved asymptotic complexities in the medium prime case for the discrete logarithm problem on \(\mathbb {F}_{p^{n}}\) where n is not a prime power. Their method does not work when n is a composite prime power. For this case, we obtain new asymptotic complexities, e.g., \(L_{p^{n}}(1/3,(64/9)^{1/3})\) (resp. \(L_{p^{n}}(1/3,1.88)\) for the multiple number field variation) when n is composite and a power of 2; the previously best known complexity for this case is \(L_{p^{n}}(1/3,(96/9)^{1/3})\) (resp. \(L_{p^{n}}(1/3,2.12)\)). These complexities may have consequences to the selection of key sizes for pairing based cryptography. The new complexities are achieved through a general polynomial selection method. This method, which we call Algorithm-\(\mathcal {C}\), extends a previous polynomial selection method proposed at Eurocrypt 2016 to the tower number field case. As special cases, it is possible to obtain the generalised Joux-Lercier and the Conjugation method of polynomial selection proposed at Eurocrypt 2015 and the extension of these methods to the tower number field scenario by Kim and Barbulescu. A thorough analysis of the new algorithm is carried out in both concrete and asymptotic terms.
On the Security of Supersingular Isogeny Cryptosystems
Abstract

We study cryptosystems based on supersingular isogenies. This is an active area of research in post-quantum cryptography. Our first contribution is to give a very powerful active attack on the supersingular isogeny encryption scheme. This attack can only be prevented by using a (relatively expensive) countermeasure. Our second contribution is to show that the security of all schemes of this type depends on the difficulty of computing the endomorphism ring of a supersingular elliptic curve. This result gives significant insight into the difficulty of the isogeny problem that underlies the security of these schemes. Our third contribution is to give a reduction that uses partial knowledge of shared keys to determine an entire shared key. This can be used to retrieve the secret key, given information leaked from a side-channel attack on the key exchange protocol. A corollary of this work is the first bit security result for the supersingular isogeny key exchange: Computing any component of the j-invariant is as hard as computing the whole j-invariant.

Our paper therefore provides an improved understanding of the security of these cryptosystems. We stress that our work does not imply that these systems are insecure, or that they should not be used. However, it highlights that implementations of these schemes will need to take account of the risks associated with various active and side-channel attacks.
Simpira v2: A Family of Efficient Permutations Using the AES Round Function
AbstractThis paper introduces Simpira, a family of cryptographic permutations that supports inputs of 128 \times b128 \times b bits, where b is a positive integer. Its design goal is to achieve high throughput on virtually all modern 64-bit processors, that nowadays already have native instructions for AES. To achieve this goal, Simpira uses only one building block: the AES round function. For b=1b=1, Simpira corresponds to 12-round AES with fixed round keys, whereas for b\ge 2b\ge 2, Simpira is a Generalized Feistel Structure (GFS) with an F-function that consists of two rounds of AES. We claim that there are no structural distinguishers for Simpira with a complexity below 2^{128}2^{128}, and analyze its security against a variety of attacks in this setting. The throughput of Simpira is close to the theoretical optimum, namely, the number of AES rounds in the construction. For example, on the Intel Skylake processor, Simpira has throughput below 1 cycle per byte for b \le 4b \le 4 and b=6b=6. For larger permutations, where moving data in memory has a more pronounced effect, Simpira with b=32b=32 (512 byte inputs) evaluates 732 AES rounds, and performs at 824 cycles (1.61 cycles per byte), which is less than 13\,\%13\,\% off the theoretical optimum. If the data is stored in interleaved buffers, this overhead is reduced to less than 1\,\%1\,\%. The Simpira family offers an efficient solution when processing wide blocks, larger than 128 bits, is desired.
Towards Practical Whitebox Cryptography: Optimizing Efficiency and Space Hardness
Abstract

Whitebox cryptography aims to provide security for cryptographic algorithms in an untrusted environment where the adversary has full access to their implementation. Typical security goals for whitebox cryptography include key extraction security and decomposition security: Indeed, it should be infeasible to recover the secret key from the implementation and it should be hard to decompose the implementation by finding a more compact representation without recovering the secret key, which mitigates code lifting.

Whereas all published whitebox implementations for standard cryptographic algorithms such as DES or AES are prone to practical key extraction attacks, there have been two dedicated design approaches for whitebox block ciphers: ASASA by Birykov et al. at ASIACRYPT’14 and SPACE by Bogdanov and Isobe at CCS’15. While ASASA suffers from decomposition attacks, SPACE reduces the security against key extraction and decomposition attacks in the white box to the security of a standard block cipher such as AES in the standard blackbox setting. However, due to the security-prioritized design strategy, SPACE imposes a sometimes prohibitive performance overhead in the real world as it needs many AES calls to encrypt a single block.

In this paper, we address the issue by designing a family of dedicated whitebox block ciphers SPNbox and a family of underlying small block ciphers with software efficiency and constant-time execution in mind. While still relying on the standard blackbox block cipher security for the resistance against key extraction and decomposition, SPNbox attains speed-ups of up to 6.5 times in the black box and up to 18 times in the white box on Intel Skylake and ARMv8 CPUs, compared to SPACE. The designs allow for constant-time implementations in the blackbox setting and meet the practical requirements to whitebox cryptography in real-world applications such as DRM or mobile payments. Moreover, we formalize resistance towards decomposition in form of weak and strong space hardness at various security levels. We obtain bounds on space hardness in all those adversarial models.

Thus, for the first time, SPNbox provides a practical whitebox block cipher that features well-understood key extraction security, rigorous analysis towards decomposition security, demonstrated real-world efficiency on various platforms and constant-time implementations. This paves the way to enhancing susceptible real-world applications with whitebox cryptography.
Efficient and Provable White-Box Primitives
Abstract

In recent years there have been several attempts to build white-box block ciphers whose implementations aim to be incompressible. This includes the weak white-box ASASA construction by Bouillaguet, Biryukov and Khovratovich from Asiacrypt 2014, and the recent space-hard construction by Bogdanov and Isobe from CCS 2015. In this article we propose the first constructions aiming at the same goal while offering provable security guarantees. Moreover we propose concrete instantiations of our constructions, which prove to be quite efficient and competitive with prior work. Thus provable security comes with a surprisingly low overhead.
MiMC: Efficient Encryption and Cryptographic Hashing with Minimal Multiplicative Complexity
AbstractWe explore cryptographic primitives with low multiplicative complexity. This is motivated by recent progress in practical applications of secure multi-party computation (MPC), fully homomorphic encryption (FHE), and zero-knowledge proofs (ZK) where primitives from symmetric cryptography are needed and where linear computations are, compared to non-linear operations, essentially “free”. Starting with the cipher design strategy “LowMC” from Eurocrypt 2015, a number of bit-oriented proposals have been put forward, focusing on applications where the multiplicative depth of the circuit describing the cipher is the most important optimization goal.Surprisingly, albeit many MPC/FHE/ZK-protocols natively support operations in \text {GF}({p})\text {GF}({p}) for large p, very few primitives, even considering all of symmetric cryptography, natively work in such fields. To that end, our proposal for both block ciphers and cryptographic hash functions is to reconsider and simplify the round function of the Knudsen-Nyberg cipher from 1995. The mapping F(x) := x^3F(x) := x^3 is used as the main component there and is also the main component of our family of proposals called “MiMC”. We study various attack vectors for this construction and give a new attack vector that outperforms others in relevant settings.Due to its very low number of multiplications, the design lends itself well to a large class of applications, especially when the depth does not matter but the total number of multiplications in the circuit dominates all aspects of the implementation. With a number of rounds which we deem secure based on our security analysis, we report on significant performance improvements in a representative use-case involving SNARKs.
Balloon Hashing: A Memory-Hard Function Providing Provable Protection Against Sequential Attacks
Abstract

We present the Balloon password-hashing algorithm. This is the first practical cryptographic hash function that: (i) has proven memory-hardness properties in the random-oracle model, (ii) uses a password-independent access pattern, and (iii) meets—and often exceeds—the performance of the best heuristically secure password-hashing algorithms. Memory-hard functions require a large amount of working space to evaluate efficiently and, when used for password hashing, they dramatically increase the cost of offline dictionary attacks. In this work, we leverage a previously unstudied property of a certain class of graphs (“random sandwich graphs”) to analyze the memory-hardness of the Balloon algorithm. The techniques we develop are general: we also use them to give a proof of security of the scrypt and Argon2i password-hashing functions, in the random-oracle model. Our security analysis uses a sequential model of computation, which essentially captures attacks that run on single-core machines. Recent work shows how to use massively parallel special-purpose machines (e.g., with hundreds of cores) to attack memory-hard functions, including Balloon. We discuss these important attacks, which are outside of our adversary model, and propose practical defenses against them. To motivate the need for security proofs in the area of password hashing, we demonstrate and implement a practical attack against Argon2i that successfully evaluates the function with less space than was previously claimed possible. Finally, we use experimental results to compare the performance of the Balloon hashing algorithm to other memory-hard functions.
Linear Structures: Applications to Cryptanalysis of Round-Reduced Keccak
Abstract

In this paper, we analyze the security of round-reduced versions of the Keccak hash function family. Based on the work pioneered by Aumasson and Meier, and Dinur et al., we formalize and develop a technique named linear structure, which allows linearization of the underlying permutation of Keccak for up to 3 rounds with large number of variable spaces. As a direct application, it extends the best zero-sum distinguishers by 2 rounds without increasing the complexities. We also apply linear structures to preimage attacks against Keccak. By carefully studying the properties of the underlying Sbox, we show bilinear structures and find ways to convert the information on the output bits to linear functions on input bits. These findings, combined with linear structures, lead us to preimage attacks against up to 4-round Keccak with reduced complexities. An interesting feature of such preimage attacks is low complexities for small variants. As extreme examples, we can now find preimages of 3-round SHAKE128 with complexity 1, as well as the first practical solutions to two 3-round instances of Keccak challenge. Both zero-sum distinguishers and preimage attacks are verified by implementations. It is noted that the attacks here are still far from threatening the security of the full 24-round Keccak.
When Are Fuzzy Extractors Possible?
Abstract

Fuzzy extractors (Dodis et al., Eurocrypt 2004) convert repeated noisy readings of a high-entropy secret into the same uniformly distributed key. A minimum condition for the security of the key is the hardness of guessing a value that is similar to the secret, because the fuzzy extractor converts such a guess to the key.

We define fuzzy min-entropy to quantify this property of a noisy source of secrets. Fuzzy min-entropy measures the success of the adversary when provided with only the functionality of the fuzzy extractor, that is, the ideal security possible from a noisy distribution. High fuzzy min-entropy is necessary for the existence of a fuzzy extractor.

We ask: is high fuzzy min-entropy a sufficient condition for key extraction from noisy sources? If only computational security is required, recent progress on program obfuscation gives evidence that fuzzy min-entropy is indeed sufficient. In contrast, information-theoretic fuzzy extractors are not known for many practically relevant sources of high fuzzy min-entropy.

In this paper, we show that fuzzy min-entropy is sufficient for information theoretically secure fuzzy extraction. For every source distribution W for which security is possible we give a secure fuzzy extractor.

Our construction relies on the fuzzy extractor knowing the precise distribution of the source W. A more ambitious goal is to design a single extractor that works for all possible sources. Our second main result is that this more ambitious goal is impossible: we give a family of sources with high fuzzy min-entropy for which no single fuzzy extractor is secure. We show three flavors of this impossibility result: for standard fuzzy extractors, for fuzzy extractors that are allowed to sometimes be wrong, and for secure sketches, which are the main ingredient of most fuzzy extractor constructions.
More Powerful and Reliable Second-Level Statistical Randomness Tests for NIST SP 800-22
Abstract

Random number generators (RNGs) are essential for cryptographic systems, and statistical tests are usually employed to assess the randomness of their outputs. As the most commonly used statistical test suite, the NIST SP 800-22 suite includes 15 test items, each of which contains two-level tests. For the test items based on the binomial distribution, we find that their second-level tests are flawed due to the inconsistency between the assessed distribution and the assumed one. That is, the sequence that passes the test could still have statistical flaws in the assessed aspect. For this reason, we propose Q-value as the metric for these second-level tests to replace the original P-value without any extra modification, and the first-level tests are kept unchanged. We provide the correctness proof of the proposed Q-value based second-level tests. We perform the theoretical analysis to demonstrate that the modification improves not only the detectability, but also the reliability. That is, the tested sequence that dissatisfies the randomness hypothesis has a higher probability to be rejected by the improved test, and the sequence that satisfies the hypothesis has a higher probability to pass it. The experimental results on several deterministic RNGs indicate that, the Q-value based method is able to detect some statistical flaws that the original SP 800-22 suite cannot realize under the same test parameters.
Trick or Tweak: On the (In)security of OTR’s Tweaks
Abstract

Tweakable blockcipher (TBC) is a powerful tool to design authenticated encryption schemes as illustrated by Minematsu’s Offset Two Rounds (OTR) construction. It considers an additional input, called tweak, to a standard blockcipher which adds some variability to this primitive. More specifically, each tweak is expected to define a different, independent pseudo-random permutation.

In this work we focus on OTR’s way to instantiate a TBC and show that it does not achieve independence for a large amount of parameters. We indeed describe collisions between the input masks derived from the tweaks and explain how they result in practical attacks against this scheme, breaking privacy, authenticity, or both, using a single encryption query, with advantage at least 1/4.

We stress however that our results do not invalidate the OTR construction as a whole but simply prove that the TBC’s input masks should be designed differently.
Universal Forgery and Key Recovery Attacks on ELmD Authenticated Encryption Algorithm
Abstract

In this paper, we provide a security analysis of ELmD: a block cipher based Encrypt-Linear-mix-Decrypt authentication mode. As being one of the second-round CAESAR candidate, it is claimed to provide misuse resistant against forgeries and security against block-wise adaptive adversaries as well as 128-bit security against key recovery attacks. We scrutinize ElmD in such a way that we provide universal forgery attacks as well as key recovery attacks. First, based on the collision attacks on similar structures such as Marble, AEZ, and COPA, we present universal forgery attacks. Second, by exploiting the structure of ELmD, we acquire ability to query to the block cipher used in ELmD. Finally, for one of the proposed versions of ELmD, we mount key recovery attacks reducing the effective key strength by more than 60 bits.
Statistical Fault Attacks on Nonce-Based Authenticated Encryption Schemes
Abstract

Since the first demonstration of fault attacks by Boneh et al. on RSA, a multitude of fault attack techniques on various cryptosystems have been proposed. Most of these techniques, like Differential Fault Analysis, Safe Error Attacks, and Collision Fault Analysis, have the requirement to process two inputs that are either identical or related, in order to generate pairs of correct/faulty ciphertexts. However, when targeting authenticated encryption schemes, this is in practice usually precluded by the unique nonce required by most of these schemes.

In this work, we present the first practical fault attacks on several nonce-based authenticated encryption modes for AES. This includes attacks on the ISO/IEC standards GCM, CCM, EAX, and OCB, as well as several second-round candidates of the ongoing CAESAR competition. All attacks are based on the Statistical Fault Attacks by Fuhr et al., which use a biased fault model and just operate on collections of faulty ciphertexts. Hereby, we put effort in reducing the assumptions made regarding the capabilities of an attacker as much as possible. In the attacks, we only assume that we are able to influence some byte (or a larger structure) of the internal AES state before the last application of MixColumns, so that the value of this byte is afterwards non-uniformly distributed.

In order to show the practical relevance of Statistical Fault Attacks and for evaluating our assumptions on the capabilities of an attacker, we perform several fault-injection experiments targeting real hardware. For instance, laser fault injections targeting an AES co-processor of a smartcard microcontroller, which is used to implement modes like GCM or CCM, show that 4 bytes (resp. all 16 bytes) of the last round key can be revealed with a small number of faulty ciphertexts.
Authenticated Encryption with Variable Stretch
Abstract

In conventional authenticated-encryption (AE) schemes, the ciphertext expansion, a.k.a. stretch or tag length, is a constant or a parameter of the scheme that must be fixed per key. However, using variable-length tags per key can be desirable in practice or may occur as a result of a misuse. The RAE definition by Hoang, Krovetz, and Rogaway (Eurocrypt 2015), aiming at the best-possible AE security, supports variable stretch among other strong features, but achieving the RAE goal incurs a particular inefficiency: neither encryption nor decryption can be online. The problem of enhancing the well-established nonce-based AE (nAE) model and the standard schemes thereof to support variable tag lengths per key, without sacrificing any desirable functional and efficiency properties such as online encryption, has recently regained interest as evidenced by extensive discussion threads on the CFRG forum and the CAESAR competition. Yet there is a lack of formal definition for this goal. First, we show that several recently proposed heuristic measures trying to augment the known schemes by inserting the tag length into the nonce and/or associated data fail to deliver any meaningful security in this setting. Second, we provide a formal definition for the notion of nonce-based variable-stretch AE (nvAE) as a natural extension to the traditional nAE model. Then, we proceed by showing a second modular approach to formalizing the goal by combining the nAE notion and a new property we call key-equivalent separation by stretch (kess). It is proved that (after a mild adjustment to the syntax) any nAE scheme which additionally fulfills the kess property will achieve the nvAE goal. Finally, we show that the nvAE goal is efficiently and provably achievable; for instance, by simple tweaks to off-the-shelf schemes such as OCB.
Salvaging Weak Security Bounds for Blockcipher-Based Constructions
Abstract

The concrete security bounds for some blockcipher-based constructions sometimes become worrisome or even vacuous; for example, when a light-weight blockcipher is used, when large amounts of data are processed, or when a large number of connections need to be kept secure. Rotating keys helps, but introduces a “hybrid factor” m equal to the number of keys used. In such instances, analysis in the ideal-cipher model (ICM) can give a sharper picture of security, but this heuristic is called into question when cryptanalysis of the real-world blockcipher reveals weak keys, related-key attacks, etc.

To address both concerns, we introduce a new analysis model, the ideal-cipher model under key-oblivious access (ICM-KOA). Like the ICM, the ICM-KOA can give sharp security bounds when standard-model bounds do not. Unlike the ICM, results in the ICM-KOA are less brittle to current and future cryptanalytic results on the blockcipher used to instantiate the ideal cipher. Also, results in the ICM-KOA immediately imply results in the ICM and the standard model, giving multiple viewpoints on a construction with a single effort. The ICM-KOA provides a conceptual bridge between ideal ciphers and tweakable blockciphers (TBC): blockcipher-based constructions secure in the ICM-KOA have TBC-based analogs that are secure under standard-model TBC security assumptions. Finally, the ICM-KOA provides a natural framework for analyzing blockcipher key-update strategies that use the blockcipher to derive the new key. This is done, for example, in the NIST CTR-DRBG and in the hardware RNG that ships on Intel chips.
How to Build Fully Secure Tweakable Blockciphers from Classical Blockciphers
AbstractThis paper focuses on building a tweakable blockcipher from a classical blockcipher whose input and output wires all have a size of n bits. The main goal is to achieve full 2^n2^n security. Such a tweakable blockcipher was proposed by Mennink at FSE’15, and it is also the only tweakable blockcipher so far that claimed full 2^n2^n security to our best knowledge. However, we find a key-recovery attack on Mennink’s proposal (in the proceeding version) with a complexity of about 2^{n/2}2^{n/2} adversarial queries. The attack well demonstrates that Mennink’s proposal has at most 2^{n/2}2^{n/2} security, and therefore invalidates its security claim. In this paper, we study a construction of tweakable blockciphers denoted as \widetilde{\mathbb {E}}[s]\widetilde{\mathbb {E}}[s] that is built on s invocations of a blockcipher and additional simple XOR operations. As proven in previous work, at least two invocations of blockcipher with linear mixing are necessary to possibly bypass the birthday-bound barrier of 2^{n/2}2^{n/2} security, we carry out an investigation on the instances of \widetilde{\mathbb {E}}[s]\widetilde{\mathbb {E}}[s] with s \ge 2s \ge 2, and find 32 highly efficient tweakable blockciphers \widetilde{E1}\widetilde{E1}, \widetilde{E2}\widetilde{E2}, \ldots \ldots , \widetilde{E32}\widetilde{E32} that achieve 2^n2^n provable security. Each of these tweakable blockciphers uses two invocations of a blockcipher, one of which uses a tweak-dependent key generated by XORing the tweak to the key (or to a secret subkey derived from the key). We point out the provable security of these tweakable blockciphers is obtained in the ideal blockcipher model due to the usage of the tweak-dependent key.
Design Strategies for ARX with Provable Bounds: Sparx and LAX
Abstract

We present, for the first time, a general strategy for designing ARX symmetric-key primitives with provable resistance against single-trail differential and linear cryptanalysis. The latter has been a long standing open problem in the area of ARX design. The wide-trail design strategy (WTS), that is at the basis of many S-box based ciphers, including the AES, is not suitable for ARX designs due to the lack of S-boxes in the latter. In this paper we address the mentioned limitation by proposing the long trail design strategy (LTS) – a dual of the WTS that is applicable (but not limited) to ARX constructions. In contrast to the WTS, that prescribes the use of small and efficient S-boxes at the expense of heavy linear layers with strong mixing properties, the LTS advocates the use of large (ARX-based) S-Boxes together with sparse linear layers. With the help of the so-called long-trail argument, a designer can bound the maximum differential and linear probabilities for any number of rounds of a cipher built according to the LTS.

To illustrate the effectiveness of the new strategy, we propose Sparx – a family of ARX-based block ciphers designed according to the LTS. Sparx has 32-bit ARX-based S-boxes and has provable bounds against differential and linear cryptanalysis. In addition, Sparx is very efficient on a number of embedded platforms. Its optimized software implementation ranks in the top 6 of the most software-efficient ciphers along with Simon, Speck, Chaskey, LEA and RECTANGLE.

As a second contribution we propose another strategy for designing ARX ciphers with provable properties, that is completely independent of the LTS. It is motivated by a challenge proposed earlier by Wallén and uses the differential properties of modular addition to minimize the maximum differential probability across multiple rounds of a cipher. A new primitive, called LAX, is designed following those principles. LAX partly solves the Wallén challenge.
Side-Channel Analysis Protection and Low-Latency in Action
Abstract

During the last years, the industry sector showed particular interest in solutions which allow to encrypt and decrypt data within one clock cycle. Known as low-latency cryptography, such ciphers are desirable for pervasive applications with real-time security requirements. On the other hand, pervasive applications are very likely in control of the end user, and may operate in a hostile environment. Hence, in such scenarios it is necessary to provide security against side-channel analysis (SCA) attacks while still keeping the low-latency feature.

Since the single-clock-cycle concept requires an implementation in a fully-unrolled fashion, the application of masking schemes – as the most widely studied countermeasure – is not straightforward. The contribution of this work is to present and discuss about the difficulties and challenges that hardware engineers face when integrating SCA countermeasures into low-latency constructions. In addition to several design architectures, practical evaluations, and discussions about the problems and potential solutions with respect to the case study PRINCE (also compared with Midori), the final message of this paper is a couple of suggestions for future low-latency designs to – hopefully – ease the integration of SCA countermeasures.
Characterisation and Estimation of the Key Rank Distribution in the Context of Side Channel Evaluations
Abstract

Quantifying the side channel security of implementations has been a significant research question for several years in academia but also among real world side channel practitioners. As part of security evaluations, efficient key rank estimation algorithms were devised, which in contrast to analyses based on subkey recovery, give a holistic picture of the security level after a side channel attack. However, it has been observed that outcomes of rank estimations show a huge spread in precisely the range of key ranks where enumeration could lead to key recovery. These observations raise the question whether this is because of insufficient rank estimation procedures, or, if this is an inherent property of the key rank. Furthermore, if this was inherent, how could key rank outcomes be translated into practically meaningful figures, suitable to analysing the risk that real world side channel attacks pose? This paper is a direct response to these questions. We experimentally identify the key rank distribution and show that it is independent of different distinguishers and signal-to-noise ratios. Then we offer a theoretical explanation for the observed key rank distribution and determine how many samples thereof are required for a robust estimation of some key parameters. We discuss how this can be naturally integrated into real world side channel evaluation practices. We conclude our research by connecting non-parametric order statistics, in particular percentiles, in a practically meaningful way with business goals.
Taylor Expansion of Maximum Likelihood Attacks for Masked and Shuffled Implementations
Abstract

The maximum likelihood side-channel distinguisher of a template attack scenario is expanded into lower degree attacks according to the increasing powers of the signal-to-noise ratio (SNR). By exploiting this decomposition we show that it is possible to build highly multivariate attacks which remain efficient when the likelihood cannot be computed in practice due to its computational complexity. The shuffled table recomputation is used as an illustration to derive a new attack which outperforms the ones presented by Bruneau et al. at CHES 2015, and so across the full range of SNRs. This attack combines two attack degrees and is able to exploit high dimensional leakage which explains its efficiency.
Unknown-Input Attacks in the Parallel Setting: Improving the Security of the CHES 2012 Leakage-Resilient PRF
Abstract

In this work we present a leakage-resilient PRF which makes use of parallel block cipher implementations with unknown-inputs. To the best of our knowledge this is the first work to study and exploit unknown-inputs as a form of key-dependent algorithmic noise. It turns out that such noise renders the problem of side-channel key recovery intractable under very little and easily satisfiable assumptions. That is, the construction stays secure even in a noise-free setting and independent of the number of traces and the used power model. The contributions of this paper are as follows. First, we present a PRF construction which offers attractive security properties, even when instantiated with the AES. Second, we study the effect of unknown-input attacks in parallel implementations. We put forward their intractability and explain it by studying the inevitable model errors obtained when building templates in such a scenario. Third, we compare the security of our construction to the CHES 2012 one and show that it is superior in many ways. That is, a standard block cipher can be used, the security holds for all intermediate variables and it can even partially tolerate local EM attacks and some typical implementation mistakes or hardware insufficiencies. Finally, we discuss the performance of a standard-cell implementation.
A New Algorithm for the Unbalanced Meet-in-the-Middle Problem
Abstract

A collision search for a pair of n-bit unbalanced functions (one is R times more expensive than the other) is an instance of the meet-in-the-middle problem, solved with the familiar standard algorithm that follows the tradeoff \(TM=N\), where T and M are time and memory complexities and \(N=2^n\). By combining two ideas, unbalanced interleaving and van Oorschot-Wiener parallel collision search, we construct an alternative algorithm that follows \(T^2 M = R^2 N\), where \(M\le R\). Among others, the algorithm solves the well-known open problem: how to reduce the memory of unbalanced collision search.
Applying MILP Method to Searching Integral Distinguishers Based on Division Property for 6 Lightweight Block Ciphers
Abstract

Division property is a generalized integral property proposed by Todo at EUROCRYPT 2015, and very recently, Todo et al. proposed bit-based division property and applied to SIMON32 at FSE 2016. However, this technique can only be applied to block ciphers with block size no larger than 32 due to its high time and memory complexity. In this paper, we extend Mixed Integer Linear Programming (MILP) method, which is used to search differential characteristics and linear trails of block ciphers, to search integral distinguishers of block ciphers based on division property with block size larger than 32.

Firstly, we study how to model division property propagations of three basic operations (copy, bitwise AND, XOR) and an Sbox operation by linear inequalities, based on which we are able to construct a linear inequality system which can accurately describe the division property propagations of a block cipher given an initial division property. Secondly, by choosing an appropriate objective function, we convert a search algorithm under Todo’s framework into an MILP problem, and we use this MILP problem appropriately to search integral distinguishers. As an application of our technique, we have searched integral distinguishers for SIMON, SIMECK, PRESENT, RECTANGLE, LBlock and TWINE. Our results show that we can find 14-, 16-, 18-, 22- and 26-round integral distinguishers for SIMON32, 48, 64, 96 and 128 respectively. Moreover, for two SP-network lightweight block ciphers PRESENT and RECTANGLE, we found 9-round integral distinguishers for both ciphers which are two more rounds than the best integral distinguishers in the literature [22, 29]. For LBlock and TWINE, our results are consistent with the best known ones with respect to the longest distinguishers.
Reverse Cycle Walking and Its Applications
Abstract

We study the problem of constructing a block-cipher on a “possibly-strange” set \(\mathcal{S}\) using a block-cipher on a larger set \(\mathcal{T}\). Such constructions are useful in format-preserving encryption, where for example the set \(\mathcal{S}\) might contain “valid 9-digit social security numbers” while \(\mathcal{T}\) might be the set of 30-bit strings. Previous work has solved this problem using a technique called cycle walking, first formally analyzed by Black and Rogaway. Assuming the size of \(\mathcal{S}\) is a constant fraction of the size of \(\mathcal{T}\), cycle walking allows one to encipher a point \(x \in \mathcal{S}\) by applying the block-cipher on \(\mathcal{T}\) a small expected number of times and O(N) times in the worst case, where \(N = |\mathcal{T}|\), without any degradation in security. We introduce an alternative to cycle walking that we call reverse cycle walking, which lowers the worst-case number of times we must apply the block-cipher on \(\mathcal{T}\) from O(N) to \(O(\log N)\). Additionally, when the underlying block-cipher on \(\mathcal{T}\) is secure against \(q = (1-\epsilon )N\) adversarial queries, we show that applying reverse cycle walking gives us a cipher on \(\mathcal{S}\) secure even if the adversary is allowed to query all of the domain points. Such fully secure ciphers have been the the target of numerous recent papers.
Optimization of \(\mathsf {LPN}\) Solving Algorithms
Abstract

In this article we focus on constructing an algorithm that automatizes the generation of \(\mathsf {LPN}\) solving algorithms from the considered parameters. When searching for an algorithm to solve an \(\mathsf {LPN}\) instance, we make use of the existing techniques and optimize their use. We formalize an \(\mathsf {LPN}\) algorithm as a path in a graph G and our algorithm is searching for the optimal paths in this graph. Our results bring improvements over the existing work, i.e. we improve the results of the covering code from ASIACRYPT’14 and EUROCRYPT’16. Furthermore, we propose concrete practical codes and a method to find good codes.
The Kernel Matrix Diffie-Hellman Assumption
AbstractWe put forward a new family of computational assumptions, the Kernel Matrix Diffie-Hellman Assumption. Given some matrix \mathbf {{A}}\mathbf {{A}} sampled from some distribution \mathcal {D}\mathcal {D}, the kernel assumption says that it is hard to find “in the exponent” a nonzero vector in the kernel of \mathbf {{A}}^\top \mathbf {{A}}^\top . This family is a natural computational analogue of the Matrix Decisional Diffie-Hellman Assumption (MDDH), proposed by Escala et al. As such it allows to extend the advantages of their algebraic framework to computational assumptions.The k-Decisional Linear Assumption is an example of a family of decisional assumptions of strictly increasing hardness when k grows. We show that for any such family of MDDH assumptions, the corresponding Kernel assumptions are also strictly increasingly weaker. This requires ruling out the existence of some black-box reductions between flexible problems (i.e., computational problems with a non unique solution).
Cryptographic Applications of Capacity Theory: On the Optimality of Coppersmith’s Method for Univariate Polynomials
Abstract

We draw a new connection between Coppersmith’s method for finding small solutions to polynomial congruences modulo integers and the capacity theory of adelic subsets of algebraic curves. Coppersmith’s method uses lattice basis reduction to construct an auxiliary polynomial that vanishes at the desired solutions. Capacity theory provides a toolkit for proving when polynomials with certain boundedness properties do or do not exist. Using capacity theory, we prove that Coppersmith’s bound for univariate polynomials is optimal in the sense that there are no auxiliary polynomials of the type he used that would allow finding roots of size \(N^{1/d+\epsilon }\) for any monic degree-d polynomial modulo N. Our results rule out the existence of polynomials of any degree and do not rely on lattice algorithms, thus eliminating the possibility of improvements for special cases or even superpolynomial-time improvements to Coppersmith’s bound. We extend this result to constructions of auxiliary polynomials using binomial polynomials, and rule out the existence of any auxiliary polynomial of this form that would find solutions of size \(N^{1/d+\epsilon }\) unless N has a very small prime factor.
A Key Recovery Attack on MDPC with CCA Security Using Decoding Errors
Abstract

Algorithms for secure encryption in a post-quantum world are currently receiving a lot of attention in the research community, including several larger projects and a standardization effort from NIST. One of the most promising algorithms is the code-based scheme called QC-MDPC, which has excellent performance and a small public key size. In this work we present a very efficient key recovery attack on the QC-MDPC scheme using the fact that decryption uses an iterative decoding step and this can fail with some small probability. We identify a dependence between the secret key and the failure in decoding. This can be used to build what we refer to as a distance spectrum for the secret key, which is the set of all distances between any two ones in the secret key. In a reconstruction step we then determine the secret key from the distance spectrum. The attack has been implemented and tested on a proposed instance of QC-MDPC for 80 bit security. It successfully recovers the secret key in minutes.

A slightly modified version of the attack can be applied on proposed versions of the QC-MDPC scheme that provides IND-CCA security. The attack is a bit more complex in this case, but still very much below the security level. The reason why we can break schemes with proved CCA security is that the model for these proofs typically does not include the decoding error possibility.
A Tale of Two Shares: Why Two-Share Threshold Implementation Seems Worthwhile—and Why It Is Not
Abstract

This work explores the possibilities for practical Threshold Implementation (TI) with only two shares in order for a smaller design that needs less randomness but is still first-order leakage resistant. We present the first two-share Threshold Implementations of two lightweight block ciphers—Simon and Present. The implementation results show that two-share TI improves the compactness but usually further reduces the throughput when compared with first-order resistant three-share schemes. Our leakage analysis shows that two-share TI can retain perfect first-order resistance. However, the analysis also exposes a strong second-order leakage. All results are backed up by simulation as well as analysis of actual implementations.
Cryptographic Reverse Firewall via Malleable Smooth Projective Hash Functions
Abstract

Motivated by the revelations of Edward Snowden, post-Snowden cryptography has become a prominent research direction in recent years. In Eurocrypt 2015, Mironov and Stephens-Davidowitz proposed a novel concept named cryptographic reverse firewall (\(\mathsf {CRF}\)) which can resist exfiltration of secret information from an arbitrarily compromised machine. In this work, we continue this line of research and present generic \(\mathsf {CRF}\) constructions for several widely used cryptographic protocols based on a new notion named malleable smooth projective hash function. Our contributions can be summarized as follows.

We introduce the notion of malleable smooth projective hash function, which is an extension of the smooth projective hash function (\(\mathsf {SPHF}\)) introduced by Cramer and Shoup (Eurocrypt’02) with the new properties of key malleability and element rerandomizability. We demonstrate the feasibility of our new notion using graded rings proposed by Benhamouda et al. (Crypto’13), and present an instantiation from the k-linear assumption.

We show how to generically construct \(\mathsf {CRF}\)s via malleable \(\mathsf {SPHF}\)s in a modular way for some widely used cryptographic protocols. Specifically, we propose generic constructions of \(\mathsf {CRF}\)s for the unkeyed message-transmission protocol and the oblivious signature-based envelope (\({\mathsf {OSBE}}\)) protocol of Blazy, Pointcheval and Vergnaud (TCC’12). We also present a new malleable \(\mathsf {SPHF}\) from the linear encryption of valid signatures for instantiating the \({\mathsf {OSBE}}\) protocol with \(\mathsf {CRF}\)s.

We further study the two-pass oblivious transfer (\(\mathsf {OT}\)) protocol and show that the malleable \(\mathsf {SPHF}\) does not suffice for its \(\mathsf {CRF}\) constructions. We then develop a new \(\mathsf {OT}\) framework from graded rings and show how to construct OT- \(\mathsf {CRF}\)s by modifying the malleable \(\mathsf {SPHF}\) framework. This new framework encompasses the \(\mathsf {DDH}\)-based OT- \(\mathsf {CRF}\) constructions proposed by Mironov and Stephens-Davidowitz (Eurocrypt’15), and yields a new construction under the k-linear assumption.
Efficient Public-Key Cryptography with Bounded Leakage and Tamper Resilience
Abstract

We revisit the question of constructing public-key encryption and signature schemes with security in the presence of bounded leakage and tampering memory attacks. For signatures we obtain the first construction in the standard model; for public-key encryption we obtain the first construction free of pairing (avoiding non-interactive zero-knowledge proofs). Our constructions are based on generic building blocks, and, as we show, also admit efficient instantiations under fairly standard number-theoretic assumptions.

The model of bounded tamper resistance was recently put forward by Damgård et al. (Asiacrypt 2013) as an attractive path to achieve security against arbitrary memory tampering attacks without making hardware assumptions (such as the existence of a protected self-destruct or key-update mechanism), the only restriction being on the number of allowed tampering attempts (which is a parameter of the scheme). This allows to circumvent known impossibility results for unrestricted tampering (Gennaro et al., TCC 2010), while still being able to capture realistic tampering attacks.
Public-Key Cryptosystems Resilient to Continuous Tampering and Leakage of Arbitrary Functions
Abstract

We present the first chosen-ciphertext secure public-key encryption schemes resilient to continuous tampering of arbitrary (efficiently computable) functions. Since it is impossible to realize such a scheme without a self-destruction or key-updating mechanism, our proposals allow for either of them. As in the previous works resilient to this type of tampering attacks, our schemes also tolerate bounded or continuous memory leakage attacks at the same time. Unlike the previous results, our schemes have efficient instantiations, without relying on zero-knowledge proofs. We also prove that there is no secure digital signature scheme resilient to arbitrary tampering functions against a stronger variant of continuous tampering attacks, even if it has a self-destruction mechanism.
Improved Security Proofs in Lattice-Based Cryptography: Using the Rényi Divergence Rather Than the Statistical Distance
Abstract

The Rényi divergence is a measure of closeness of two probability distributions. We show that it can often be used as an alternative to the statistical distance in security proofs for lattice-based cryptography. Using the Rényi divergence is particularly suited for security proofs of primitives in which the attacker is required to solve a search problem (e.g., forging a signature). We show that it may also be used in the case of distinguishing problems (e.g., semantic security of encryption schemes), when they enjoy a public sampleability property. The techniques lead to security proofs for schemes with smaller parameters, and sometimes to simpler security proofs than the existing ones.
Multi-input Functional Encryption for Unbounded Arity Functions
Abstract

The notion of multi-input functional encryption (MI-FE) was recently introduced by Goldwasser et al. [EUROCRYPT’14] as a means to non-interactively compute aggregate information on the joint private data of multiple users. A fundamental limitation of their work, however, is that the total number of users (which corresponds to the arity of the functions supported by the MI-FE scheme) must be a priori bounded and fixed at the system setup time.

In this work, we overcome this limitation by introducing the notion of unbounded input MI-FE that supports the computation of functions with unbounded arity. We construct such an MI-FE scheme with indistinguishability security in the selective model based on the existence of public-coin differing-inputs obfuscation for turing machines and collision-resistant hash functions.

Our result enables several new exciting applications, including a new paradigm of on-the-fly secure multiparty computation where new users can join the system dynamically.
Multi-party Key Exchange for Unbounded Parties from Indistinguishability Obfuscation
Abstract

Existing protocols for non-interactive multi-party key exchange either (1) support a bounded number of users, (2) require a trusted setup, or (3) rely on knowledge-type assumptions.

We construct the first non-interactive key exchange protocols which support an unbounded number of parties and have a security proof that does not rely on knowledge assumptions. Our non-interactive key-exchange protocol does not require a trusted setup and extends easily to the identity-based setting. Our protocols suffer only a polynomial loss to the underlying hardness assumptions.
Adaptively Secure Puncturable Pseudorandom Functions in the Standard Model
AbstractWe study the adaptive security of constrained PRFs in the standard model. We initiate our exploration with puncturable PRFs. A puncturable PRF family is a special class of constrained PRFs, where the constrained key is associated with an element x'x' in the input domain. The key allows evaluation at all points x\ne x'x\ne x'.We show how to build puncturable PRFs with adaptive security proofs in the standard model that involve only polynomial loss to the underlying assumptions. Prior work had either super-polynomial loss or applied the random oracle heuristic. Our construction uses indistinguishability obfuscation and DDH-hard algebraic groups of composite order.More generally, one can consider a t-puncturable PRF: PRFs that can be punctured at any set of inputs S, provided the size of S is less than a fixed polynomial. We additionally show how to transform any (single) puncturable PRF family to a t-puncturable PRF family, using indistinguishability obfuscation.
Multilinear and Aggregate Pseudorandom Functions: New Constructions and Improved Security
Abstract

Since its introduction, pseudorandom functions (PRFs) have become one of the main building blocks of cryptographic protocols. In this work, we revisit two recent extensions of standard PRFs, namely multilinear and aggregate PRFs, and provide several new results for these primitives. In the case of aggregate PRFs, one of our main results is a proof of security for the Naor-Reingold PRF with respect to read-once boolean aggregate queries under the standard Decision Diffie-Hellman problem, which was an open problem. In the case of multilinear PRFs, one of our main contributions is the construction of new multilinear PRFs achieving indistinguishability from random symmetric and skew-symmetric multilinear functions, which was also left as an open problem. In order to achieve these results, our main technical tool is a simple and natural generalization of the recent linear independent polynomial framework for PRFs proposed by Abdalla, Benhamouda, and Passelègue in Crypto 2015, that can handle larger classes of PRF constructions. In addition to simplifying and unifying proofs for multilinear and aggregate PRFs, our new framework also yields new constructions which are secure under weaker assumptions, such as the decisional k-linear assumption.
New Realizations of Somewhere Statistically Binding Hashing and Positional Accumulators
Abstract

A somewhere statistically binding (SSB) hash, introduced by Hubáček and Wichs (ITCS ’15), can be used to hash a long string x to a short digest \(y = H_{\mathsf {hk}}(x)\) using a public hashing-key \(\mathsf {hk}\). Furthermore, there is a way to set up the hash key \(\mathsf {hk}\) to make it statistically binding on some arbitrary hidden position i, meaning that: (1) the digest y completely determines the i’th bit (or symbol) of x so that all pre-images of y have the same value in the i’th position, (2) it is computationally infeasible to distinguish the position i on which \(\mathsf {hk}\) is statistically binding from any other position \(i'\). Lastly, the hash should have a local opening property analogous to Merkle-Tree hashing, meaning that given x and \(y = H_{\mathsf {hk}}(x)\) it should be possible to create a short proof \(\pi \) that certifies the value of the i’th bit (or symbol) of x without having to provide the entire input x. A similar primitive called a positional accumulator, introduced by Koppula, Lewko and Waters (STOC ’15) further supports dynamic updates of the hashed value. These tools, which are interesting in their own right, also serve as one of the main technical components in several recent works building advanced applications from indistinguishability obfuscation (iO).

The prior constructions of SSB hashing and positional accumulators required fully homomorphic encryption (FHE) and iO respectively. In this work, we give new constructions of these tools based on well studied number-theoretic assumptions such as DDH, Phi-Hiding and DCR, as well as a general construction from lossy/injective functions.
Computing Individual Discrete Logarithms Faster in \({{\mathrm{GF}}}(p^n)\) with the NFS-DL Algorithm
Abstract

The Number Field Sieve (NFS) algorithm is the best known method to compute discrete logarithms (DL) in finite fields \({{\mathbb F}}_{p^n}\), with p medium to large and \(n \ge 1\) small. This algorithm comprises four steps: polynomial selection, relation collection, linear algebra and finally, individual logarithm computation. The first step outputs two polynomials defining two number fields, and a map from the polynomial ring over the integers modulo each of these polynomials to \({{\mathbb F}}_{p^n}\). After the relation collection and linear algebra phases, the (virtual) logarithm of a subset of elements in each number field is known. Given the target element in \({{\mathbb F}}_{p^n}\), the fourth step computes a preimage in one number field. If one can write the target preimage as a product of elements of known (virtual) logarithm, then one can deduce the discrete logarithm of the target.

As recently shown by the Logjam attack, this final step can be critical when it can be computed very quickly. But we realized that computing an individual DL is much slower in medium- and large-characteristic non-prime fields \({{\mathbb F}}_{p^n}\) with \(n \ge 3\), compared to prime fields and quadratic fields \({{\mathbb F}}_{p^2}\). We optimize the first part of individual DL: the booting step, by reducing dramatically the size of the preimage norm. Its smoothness probability is higher, hence the running-time of the booting step is much improved. Our method is very efficient for small extension fields with \(2 \le n \le 6\) and applies to any \(n > 1\), in medium and large characteristic.
Multiple Discrete Logarithm Problems with Auxiliary Inputs
Abstract

Let g be an element of prime order p in an abelian group and let \(\alpha _1, \dots , \alpha _L \in {\mathbb Z}_p\) for a positive integer L. First, we show that, if \(g, g^{\alpha _i}\), and \(g^{\alpha _i^d}\) (\(i=1, \dots , L\)) are given for \(d \mid p-1\), all the discrete logarithms \(\alpha _i\)’s can be computed probabilistically in \(\widetilde{O}( \sqrt{L \cdot p/d} + \sqrt{L \cdot d} )\) group exponentiations with O(L) storage under the condition that \(L \ll \min \{ (p/d)^{1/4}, d^{1/4}\}\).

Let \(f \in \mathbb {F}_p[x]\) be a polynomial of degree d and let \(\rho _f\) be the number of rational points over \(\mathbb {F}_p\) on the curve determined by \(f(x) - f(y) = 0\). Second, if \(g, g^{\alpha _i}, g^{\alpha _i^2}, \dots , g^{\alpha _i^d}\) are given for any \(d \ge 1\), then we propose an algorithm that solves all \(\alpha _i\)’s in \(\widetilde{O}( \max \{ \sqrt{L \cdot p^2 / \rho _f } , L \cdot d \} )\) group exponentiations with \(\widetilde{O}(\sqrt{ L \cdot p^2 / \rho _f })\) storage. In particular, we have explicit choices for a polynomial f when \(d \mid p \pm 1\), that yield a running time of \(\widetilde{O}( \sqrt{ L \cdot p/d } )\) whenever \(L \le \frac{p}{c \cdot d^3}\) for some constant c.
Solving Linear Equations Modulo Unknown Divisors: Revisited
Abstract

We revisit the problem of finding small solutions to a collection of linear equations modulo an unknown divisor p for a known composite integer N. In CaLC 2001, Howgrave-Graham introduced an efficient algorithm for solving univariate linear equations; since then, two forms of multivariate generalizations have been considered in the context of cryptanalysis: modular multivariate linear equations by Herrmann and May (Asiacrypt’08) and simultaneous modular univariate linear equations by Cohn and Heninger (ANTS’12). Their algorithms have many important applications in cryptanalysis, such as factoring with known bits problem, fault attacks on RSA signatures, analysis of approximate GCD problem, etc.

In this paper, by introducing multiple parameters, we propose several generalizations of the above equations. The motivation behind these extensions is that some attacks on RSA variants can be reduced to solving these generalized equations, and previous algorithms do not apply. We present new approaches to solve them, and compared with previous methods, our new algorithms are more flexible and especially suitable for some cases. Applying our algorithms, we obtain the best analytical/experimental results for some attacks on RSA and its variants, specifically,

We improve May’s results (PKC’04) on small secret exponent attack on RSA variant with moduli \(N = p^rq\) (\(r\ge 2\)).

We experimentally improve Boneh et al.’s algorithm (Crypto’98) on factoring \(N=p^rq\) (\(r\ge 2\)) with known bits problem.

We significantly improve Jochemsz-May’ attack (Asiacrypt’06) on Common Prime RSA.

We extend Nitaj’s result (Africacrypt’12) on weak encryption exponents of RSA and CRT-RSA.
Four\(\mathbb {Q}\): Four-Dimensional Decompositions on a \(\mathbb {Q}\)-curve over the Mersenne Prime
Abstract

We introduce Four\(\mathbb {Q}\), a high-security, high-performance elliptic curve that targets the 128-bit security level. At the highest arithmetic level, cryptographic scalar multiplications on Four\(\mathbb {Q}\) can use a four-dimensional Gallant-Lambert-Vanstone decomposition to minimize the total number of elliptic curve group operations. At the group arithmetic level, Four\(\mathbb {Q}\) admits the use of extended twisted Edwards coordinates and can therefore exploit the fastest known elliptic curve addition formulas over large prime characteristic fields. Finally, at the finite field level, arithmetic is performed modulo the extremely fast Mersenne prime \(p=2^{127}-1\). We show that this powerful combination facilitates scalar multiplications that are significantly faster than all prior works. On Intel’s Haswell, Ivy Bridge and Sandy Bridge architectures, our software computes a variable-base scalar multiplication in 59,000, 71,000 cycles and 74,000 cycles, respectively; and, on the same platforms, our software computes a Diffie-Hellman shared secret in 92,000, 110,000 cycles and 116,000 cycles, respectively.
Efficient Fully Structure-Preserving Signatures for Large Messages
Abstract

We construct both randomizable and strongly existentially unforgeable structure-preserving signatures for messages consisting of many group elements. To sign a message consisting of \(N=mn\) group elements we have a verification key size of m group elements and signatures contain \(n+2\) elements. Verification of a signature requires evaluating \(n+1\) pairing product equations.

We also investigate the case of fully structure-preserving signatures where it is required that the secret signing key consists of group elements only. We show a variant of our signature scheme allowing the signer to pick part of the verification key at the time of signing is still secure. This gives us both randomizable and strongly existentially unforgeable fully structure-preserving signatures. In the fully structure preserving scheme the verification key is a single group element, signatures contain \(m+n+1\) group elements and verification requires evaluating \(n+1\) pairing product equations.
A Provably Secure Group Signature Scheme from Code-Based Assumptions
Abstract

We solve an open question in code-based cryptography by introducing the first provably secure group signature scheme from code-based assumptions. Specifically, the scheme satisfies the CPA-anonymity and traceability requirements in the random oracle model, assuming the hardness of the McEliece problem, the Learning Parity with Noise problem, and a variant of the Syndrome Decoding problem. Our construction produces smaller key and signature sizes than the existing post-quantum group signature schemes from lattices, as long as the cardinality of the underlying group does not exceed the population of the Netherlands (\({\approx }2^{24}\) users). The feasibility of the scheme is supported by implementation results. Additionally, the techniques introduced in this work might be of independent interest: a new verifiable encryption protocol for the randomized McEliece encryption and a new approach to design formal security reductions from the Syndrome Decoding problem.
Type 2 Structure-Preserving Signature Schemes Revisited
Abstract

At CRYPTO 2014, Abe et al. presented generic-signer structure-preserving signature schemes using Type 2 pairings. According to the authors, the proposed constructions are optimal with only two group elements in each signature and just one verification equation. The schemes beat the known lower bounds in the Type 3 setting and thereby establish that the Type 2 setting permits construction of cryptographic schemes with unique properties not achievable in Type 3.

In this paper we undertake a concrete analysis of the Abe et al. claims. By properly accounting for the actual structure of the underlying groups and subgroup membership testing of group elements in signatures, we show that the schemes are not as efficient as claimed. We present natural Type 3 analogues of the Type 2 schemes, and show that the Type 3 schemes are superior to their Type 2 counterparts in every aspect. We also formally establish that in the concrete mathematical structure of asymmetric pairing, all Type 2 structure-preserving signature schemes can be converted to the Type 3 setting without any penalty in security or efficiency, and show that the converse is false. Furthermore, we prove that the Type 2 setting does not allow one to circumvent the known lower bound result for the Type 3 setting. Our analysis puts the optimality claims for Type 2 structure-preserving signature in a concrete perspective and indicates an incompleteness in the definition of a generic bilinear group in the Type 2 setting.
Design Principles for HFEv- Based Multivariate Signature Schemes
Abstract

The Hidden Field Equations (HFE) Cryptosystem as proposed by Patarin is one of the best known and most studied multivariate schemes. While the security of the basic scheme appeared to be very weak, the HFEv- variant seems to be a good candidate for digital signature schemes on the basis of multivariate polynomials. However, the currently existing scheme of this type, the QUARTZ signature scheme, is hardly used in practice because of its poor efficiency. In this paper we analyze recent results from Ding and Yang about the degree of regularity of HFEv- systems and derive from them design principles for signature schemes of the HFEv- type. Based on these results we propose the new HFEv- based signature scheme Gui, which is more than 100 times faster than QUARTZ and therefore highly comparable with classical signature schemes such as RSA and ECDSA.
Oblivious Network RAM and Leveraging Parallelism to Achieve Obliviousness
Abstract

Oblivious RAM (ORAM) is a cryptographic primitive that allows a trusted CPU to securely access untrusted memory, such that the access patterns reveal nothing about sensitive data. ORAM is known to have broad applications in secure processor design and secure multi-party computation for big data. Unfortunately, due to a logarithmic lower bound by Goldreich and Ostrovsky (Journal of the ACM, ’96), ORAM is bound to incur a moderate cost in practice. In particular, with the latest developments in ORAM constructions, we are quickly approaching this limit, and the room for performance improvement is small.

In this paper, we consider new models of computation in which the cost of obliviousness can be fundamentally reduced in comparison with the standard ORAM model. We propose the Oblivious Network RAM model of computation, where a CPU communicates with multiple memory banks, such that the adversary observes only which bank the CPU is communicating with, but not the address offset within each memory bank. In other words, obliviousness within each bank comes for free—either because the architecture prevents a malicious party from observing the address accessed within a bank, or because another solution is used to obfuscate memory accesses within each bank—and hence we only need to obfuscate communication patterns between the CPU and the memory banks. We present new constructions for obliviously simulating general or parallel programs in the Network RAM model. We describe applications of our new model in secure processor design and in distributed storage applications with a network adversary.
Three-Party ORAM for Secure Computation
Abstract

An Oblivious RAM (ORAM) protocol [13] allows a client to retrieve \({\mathrm {N}}\)-th element of a data array \({\mathsf {D}}\) stored by the server s.t. the server learns no information about \({\mathrm {N}}\). A related notion is that of an ORAM for Secure Computation (SC-ORAM) [17], which is a protocol that securely implements a RAM functionality, i.e.  given a secret-sharing of both \({\mathsf {D}}\) and \({\mathrm {N}}\), it computes a secret-sharing of \({\mathsf {D}}[{\mathrm {N}}]\). SC-ORAM can be used as a subprotocol for implementing the RAM functionality for secure computation of RAM programs [7, 14, 17]. It can also implement a public database service which hides each client’s access pattern even if a threshold of servers colludes with any number of clients.

Most previous works used two-party secure computation to implement each step of an ORAM client algorithm, but since secure computation of many functions becomes easier in the three-party honest-majority setting than in the two-party setting, it is natural to ask if the cost of an SC-ORAM scheme can be reduced if one was willing to use three servers instead of two and assumed an honest majority. We show a 3-party SC-ORAM scheme which is based on a variant of the Binary Tree Client-Server ORAM of Shi et al. [20]. However, whereas previous SC-ORAM implementations used general 2PC or MPC techniques like Yao’s garbled circuits, e.g. [14, 22], homomorphic encryption [11], or the SPDZ protocol for arithmetic circuits [15], our techniques are custom-made for the three-party setting, giving rise to a protocol which is secure against honest-but-curious faults using bandwidth and CPU costs which are comparable to those of the underlying Client-Server ORAM.
On Cut-and-Choose Oblivious Transfer and Its Variants
Abstract

Motivated by the recent progress in improving efficiency of secure computation, we study cut-and-choose oblivious transfer—a basic building block of state-of-the-art constant round two-party secure computation protocols that was introduced by Lindell and Pinkas (TCC 2011). In particular, we study the question of realizing cut-and-choose oblivious transfer and its variants in the OT-hybrid model. Towards this, we provide new definitions of cut-and-choose oblivious transfer (and its variants) that suffice for its application in cut-and-choose techniques for garbled circuit based two-party protocols. Furthermore, our definitions conceptually simplify previous definitions including those proposed by Lindell (Crypto 2013), Huang et al., (Crypto 2014), and Lindell and Riva (Crypto 2014). Our main result is an efficient realization (under our new definitions) of cut-and-choose OT and its variants with small concrete communication overhead in an OT-hybrid model. Among other things this implies that we can base cut-and-choose OT and its variants under a variety of assumptions, including those that are believed to be resilient to quantum attacks. By contrast, previous constructions of cut-and-choose OT and its variants relied on DDH and could not take advantage of OT extension. Also, our new definitions lead us to more efficient constructions for multistage cut-and-choose OT—a variant proposed by Huang et al. (Crypto 2014) that is useful in the multiple execution setting.
An Asymptotically Optimal Method for Converting Bit Encryption to Multi-Bit Encryption
Abstract

Myers and Shelat (FOCS 2009) showed how to convert a chosen ciphertext secure (CCA secure) PKE scheme that can encrypt only 1-bit plaintexts into a CCA secure scheme that can encrypt arbitrarily long plaintexts (via the notion of key encapsulation mechanism (KEM) and hybrid encryption), and subsequent works improved efficiency and simplicity. In terms of efficiency, the best known construction of a CCA secure KEM from a CCA secure 1-bit PKE scheme, has the public key size \(\varOmega (k) \cdot |pk|\) and the ciphertext size \(\varOmega (k^2) \cdot |c|\), where k is a security parameter, and |pk| and |c| denote the public key size and the ciphertext size of the underlying 1-bit scheme, respectively.

In this paper, we show a new CCA secure KEM based on a CCA secure 1-bit PKE scheme which achieves the public key size \(2 \cdot |pk|\) and the ciphertext size \((2k + o(k)) \cdot |c|\). These sizes are asymptotically optimal in the sense that they are the same as those of the simplest “bitwise-encrypt” construction (seen as a KEM by encrypting a k-bit random session-key) that works for the chosen plaintext attack and non-adaptive chosen ciphertext attack settings. We achieve our main result by developing several new techniques and results on the “double-layered” construction (which builds a KEM from an inner PKE/KEM and an outer PKE scheme) by Myers and Shelat and on the notion of detectable PKE/KEM by Hohenberger, Lewko, and Waters (EUROCRYPT 2012).
Selective Opening Security for Receivers
Abstract

In a selective opening (SO) attack an adversary breaks into a subset of honestly created ciphertexts and tries to learn information on the plaintexts of some untouched (but potentially related) ciphertexts. Contrary to intuition, standard security notions do not always imply security against this type of adversary, making SO security an important standalone goal. In this paper we study receiver security, where the attacker is allowed to obtain the decryption keys corresponding to some of the ciphertexts.

First we study the relation between two existing security definitions, one based on simulation and the other based on indistinguishability, and show that the former is strictly stronger. We continue with feasibility results for both notions which we show can be achieved from (variants of) non-committing encryption schemes. In particular, we show that indistinguishability-based SO security can be achieved from a tweaked variant of non-committing encryption which, in turn, can be instantiated from a variety of basic, well-established, assumptions. We conclude our study by showing that SO security is however strictly weaker than all variants of non-committing encryption that we consider, leaving potentially more efficient constructions as an interesting open problem.
Function-Hiding Inner Product Encryption
Abstract

We extend the reach of functional encryption schemes that are provably secure under simple assumptions against unbounded collusion to include function-hiding inner product schemes. Our scheme is a private key functional encryption scheme, where ciphertexts correspond to vectors \(\vec {x}\), secret keys correspond to vectors \(\vec {y}\), and a decryptor learns \(\langle \vec {x}, \vec {y} \rangle \). Our scheme employs asymmetric bilinear maps and relies only on the SXDH assumption to satisfy a natural indistinguishability-based security notion where arbitrarily many key and ciphertext vectors can be simultaneously changed as long as the key-ciphertext dot product relationships are all preserved.
Idealizing Identity-Based Encryption
Abstract

We formalize the standard application of identity-based encryption (IBE), namely non-interactive secure communication, as realizing an ideal system which we call delivery controlled channel (DCC). This system allows users to be registered (by a central authority) for an identity and to send messages securely to other users only known by their identity.

Quite surprisingly, we show that existing security definitions for IBE are not sufficient to realize DCC. In fact, it is impossible to do so in the standard model. We show, however, how to adjust any IBE scheme that satisfies the standard security definition IND-ID-CPA to achieve this goal in the random oracle model.

We also show that the impossibility result can be avoided in the standard model by considering a weaker ideal system that requires all users to be registered in an initial phase before any messages are sent. To achieve this, a weaker security notion, which we introduce and call IND-ID1-CPA, is actually sufficient. This justifies our new security definition and might open the door for more efficient schemes. We further investigate which ideal systems can be realized with schemes satisfying the standard notion and variants of selective security.

As a contribution of independent interest, we show how to model features of an ideal system that are potentially available to dishonest parties but not guaranteed, and which such features arise when using IBE.
A Framework for Identity-Based Encryption with Almost Tight Security
Abstract
We show a framework for constructing identity-based encryption (IBE) schemes that are (almost) tightly secure in the multi-challenge and multi-instance setting. In particular, we formalize a new notion called broadcast encoding, analogously to encoding notions by Attrapadung (Eurocrypt 2014) and Wee (TCC 2014). We then show that it can be converted into such an IBE. By instantiating the framework using several encoding schemes (new or known ones), we obtain the following:

We obtain (almost) tightly secure IBE in the multi-challenge, multi-instance setting, both in composite and prime-order groups. The latter resolves the open problem posed by Hofheinz et al. (PKC 2015).

We obtain the first (almost) tightly secure IBE with sub-linear size public parameters (master public keys). In particular, we can set the size of the public parameters to constant at the cost of longer ciphertexts and private keys. This gives a partial solution to the open problem posed by Chen and Wee (Crypto 2013).

By applying (a variant of) the Canetti-Halevi-Katz transformation to our schemes, we obtain several CCA-secure PKE schemes with tight security in the multi-challenge, multi-instance setting. One of our schemes achieves very small ciphertext overhead, consisting of less than 12 group elements. This significantly improves the state-of-the-art construction by Libert et al. (in ePrint Archive) which requires 47 group elements. Furthermore, by modifying one of our IBE schemes obtained above, we can make it anonymous. This gives the first anonymous IBE whose security is almost tightly shown in the multi-challenge setting.
Riding on Asymmetry: Efficient ABE for Branching Programs
Abstract

In an Attribute-Based Encryption (ABE) scheme the ciphertext encrypting a message \(\mu \), is associated with a public attribute vector \(\mathbf {{x}}\) and a secret key \( \mathsf {sk}_P\) is associated with a predicate P. The decryption returns \(\mu \) if and only if \(P(\mathbf {{x}}) = 1\). ABE provides efficient and simple mechanism for data sharing supporting fine-grained access control. Moreover, it is used as a critical component in constructions of succinct functional encryption, reusable garbled circuits, token-based obfuscation and more.

In this work, we describe a new efficient ABE scheme for a family of branching programs with short secret keys and from a mild assumption. In particular, in our construction the size of the secret key for a branching program P is \(|P| + \mathrm{poly}(\lambda )\), where \(\lambda \) is the security parameter. Our construction is secure assuming the standard Learning With Errors (LWE) problem with approximation factors \(n^{\omega (1)}\). Previous constructions relied on \(n^{O(\log n)}\) approximation factors of LWE (resulting in less efficient parameters instantiation) or had large secret keys of size \(|P| \times \mathrm{poly}(\lambda )\). We rely on techniques developed by Boneh et al. (EUROCRYPT’14) and Brakerski et al. (ITCS’14) in the context of ABE for circuits and fully-homomorphic encryption.
Conversions Among Several Classes of Predicate Encryption and Applications to ABE with Various Compactness Tradeoffs
Abstract
Predicate encryption is an advanced form of public-key encryption that yields high flexibility in terms of access control. In the literature, many predicate encryption schemes have been proposed such as fuzzy-IBE, KP-ABE, CP-ABE, (doubly) spatial encryption (DSE), and ABE for arithmetic span programs. In this paper, we study relations among them and show that some of them are in fact equivalent by giving conversions among them. More specifically, our main contributions are as follows:

We show that monotonic, small universe KP-ABE (CP-ABE) with bounds on the size of attribute sets and span programs (or linear secret sharing matrix) can be converted into DSE. Furthermore, we show that DSE implies non-monotonic CP-ABE (and KP-ABE) with the same bounds on parameters. This implies that monotonic/non-monotonic KP/CP-ABE (with the bounds) and DSE are all equivalent in the sense that one implies another.

We also show that if we start from KP-ABE without bounds on the size of span programs (but bounds on the size of attribute sets), we can obtain ABE for arithmetic span programs. The other direction is also shown: ABE for arithmetic span programs can be converted into KP-ABE. These results imply, somewhat surprisingly, KP-ABE without bounds on span program sizes is in fact equivalent to ABE for arithmetic span programs, which was thought to be more expressive or at least incomparable.

By applying these conversions to existing schemes, we obtain many non-trivial consequences. We obtain the first non-monotonic, large universe CP-ABE (that supports span programs) with constant-size ciphertexts, the first KP-ABE with constant-size private keys, the first (adaptively-secure, multi-use) ABE for arithmetic span programs with constant-size ciphertexts, and more. We also obtain the first attribute-based signature scheme that supports non-monotone span programs and achieves constant-size signatures via our techniques.
QA-NIZK Arguments in Asymmetric Groups: New Tools and New Constructions
Abstract

A sequence of recent works have constructed constant-size quasi-adaptive (QA) NIZK arguments of membership in linear subspaces of \(\hat{\mathbb {G}}^m\), where \(\hat{\mathbb {G}}\) is a group equipped with a bilinear map \(e:\hat{\mathbb {G}} \times \check{\mathbb {H}} \rightarrow \mathbb {T}\). Although applicable to any bilinear group, these techniques are less useful in the asymmetric case. For example, Jutla and Roy (Crypto 2014) show how to do QA aggregation of Groth-Sahai proofs, but the types of equations which can be aggregated are more restricted in the asymmetric setting. Furthermore, there are natural statements which cannot be expressed as membership in linear subspaces, for example the satisfiability of quadratic equations.

In this paper we develop specific techniques for asymmetric groups. We introduce a new computational assumption, under which we can recover all the aggregation results of Groth-Sahai proofs known in the symmetric setting. We adapt the arguments of membership in linear spaces of \(\hat{\mathbb {G}}^m\) to linear subspaces of \(\hat{\mathbb {G}}^{m} \times \check{\mathbb {H}}^{n}\). In particular, we give a constant-size argument that two sets of Groth-Sahai commitments, defined over different groups \(\hat{\mathbb {G}},\check{\mathbb {H}}\), open to the same scalars in \(\mathbb {Z}_q\), a useful tool to prove satisfiability of quadratic equations in \(\mathbb {Z}_q\). We then use one of the arguments for subspaces in \(\hat{\mathbb {G}}^{m} \times \check{\mathbb {H}}^{n}\) and develop new techniques to give constant-size QA-NIZK proofs that a commitment opens to a bit-string. To the best of our knowledge, these are the first constant-size proofs for quadratic equations in \(\mathbb {Z}_q\) under standard and falsifiable assumptions. As a result, we obtain improved threshold Groth-Sahai proofs for pairing product equations, ring signatures, proofs of membership in a list, and various types of signature schemes.
Dual-System Simulation-Soundness with Applications to UC-PAKE and More
Abstract

We introduce a novel concept of dual-system simulation-sound non-interactive zero-knowledge (NIZK) proofs. Dual-system NIZK proof system can be seen as a two-tier proof system. As opposed to the usual notion of zero-knowledge proofs, dual-system defines an intermediate partial-simulation world, where the proof simulator may have access to additional auxiliary information about the word, for example a membership bit, and simulation of proofs is only guaranteed if the membership bit is correct. Further, dual-system NIZK proofs allow a quasi-adaptive setting where the CRS can be generated based on language parameters. This allows for the further possibility that the partial-world CRS simulator may have access to additional trapdoors related to the language parameters. We show that for important hard languages like the Diffie-Hellman language, such dual-system proof systems can be given which allow unbounded partial simulation soundness, and which further allow transition between partial simulation world and single-theorem full simulation world even when proofs are sought on non-members. The construction is surprisingly simple, involving only two additional group elements for general linear-subspace languages in asymmetric bilinear pairing groups.

As a direct application we give a short keyed-homomorphic CCA-secure encryption scheme. The ciphertext in this scheme consists of only six group elements (under the SXDH assumption) and the security reduction is tight. An earlier scheme of Libert et al. based on their efficient unbounded simulation-sound QA-NIZK proofs only provided a loose security reduction, and further had ciphertexts almost twice as long as ours.

We also show a single-round universally-composable password authenticated key-exchange (UC-PAKE) protocol which is secure under adaptive corruption in the erasure model. The single message flow only requires four group elements under the SXDH assumption.

This is the shortest known UC-PAKE even without considering adaptive corruption. The latest published scheme which considered adaptive corruption, by Abdalla et al [ABB+13], required non-constant (more than 10 times the bit-size of the password) number of group elements.
Secret Sharing and Statistical Zero Knowledge
AbstractWe show a general connection between various types of statistical zero-knowledge (SZK) proof systems and (unconditionally secure) secret sharing schemes. Viewed through the SZK lens, we obtain several new results on secret-sharing:  Characterizations: We obtain an almost-characterization of access structures for which there are secret-sharing schemes with an efficient sharing algorithm (but not necessarily efficient reconstruction). In particular, we show that for every language \(L \in {{\mathbf {SZK}}_{\mathbf {L}}}\) (the class of languages that have statistical zero knowledge proofs with log-space verifiers and simulators), a (monotonized) access structure associated with L has such a secret-sharing scheme. Conversely, we show that such secret-sharing schemes can only exist for languages in \({\mathbf {SZK}}\).   Constructions: We show new constructions of secret-sharing schemes with both efficient sharing and efficient reconstruction for access structures associated with languages that are in \({\mathbf {P}}\), but are not known to be in \({\mathbf {NC}}\), namely Bounded-Degree Graph Isomorphism and constant-dimensional lattice problems. In particular, this gives us the first combinatorial access structure that is conjectured to be outside \({\mathbf {NC}}\) but has an efficient secret-sharing scheme. Previous such constructions (Beimel and Ishai; CCC 2001) were algebraic and number-theoretic in nature.   Limitations: We also show that universally-efficient secret-sharing schemes, where the complexity of computing the shares is a polynomial independent of the complexity of deciding the access structure, cannot exist for all (monotone languages in) \(\mathbf {P}\), unless there is a polynomial q such that \({\mathbf {P}} \subseteq {\mathbf {DSPACE}}(q(n))\). 
Compactly Hiding Linear Spans
Abstract

Quasi-adaptive non-interactive zero-knowledge (QA-NIZK) proofs is a recent paradigm, suggested by Jutla and Roy (Asiacrypt ’13), which is motivated by the Groth-Sahai seminal techniques for efficient non-interactive zero-knowledge (NIZK) proofs. In this paradigm, the common reference string may depend on specific language parameters, a fact that allows much shorter proofs in important cases. It even makes certain standard model applications competitive with the Fiat-Shamir heuristic in the Random Oracle idealization. Such QA-NIZK proofs were recently optimized to constant size by Jutla and Roy (Crypto ’14) and Libert et al. (Eurocrypt ’14) for the important case of proving that a vector of group elements belongs to a linear subspace. While the QA-NIZK arguments of Libert et al. provide unbounded simulation-soundness and constant proof length, their simulation-soundness is only loosely related to the underlying assumption (with a gap proportional to the number of adversarial queries) and it is unknown how to alleviate this limitation without sacrificing efficiency. In this paper, we deal with the question of whether we can simultaneously optimize the proof size and the tightness of security reductions, allowing for important applications with tight security (which are typically quite lengthy) to be of shorter size. We resolve this question by designing a novel simulation-sound QA-NIZK argument showing that a vector \(\varvec{v} \in \mathbb {G}^n\) belongs to a subspace of rank \(t <n\) using a constant number of group elements. Unlike previous short QA-NIZK proofs of such statements, the unbounded simulation-soundness of our system is nearly tightly related (i.e., the reduction only loses a factor proportional to the security parameter) to the standard Decision Linear assumption. To show simulation-soundness in the constrained context of tight reductions, we explicitly point at a technique—which may be of independent interest—of hiding the linear span of a vector defined by a signature (which is part of an OR proof). As an application, we design a public-key cryptosystem with almost tight CCA2-security in the multi-challenge, multi-user setting with improved length (asymptotically optimal for long messages). We also adapt our scheme to provide CCA security in the key-dependent message scenario (KDM-CCA2) with ciphertext length reduced by \(75 \,\%\) when compared to the best known tightly secure KDM-CCA2 system so far.
A Unified Approach to MPC with Preprocessing Using OT
Abstract

SPDZ, TinyOT and MiniMAC are a family of MPC protocols based on secret sharing with MACs, where a preprocessing stage produces multiplication triples in a finite field. This work describes new protocols for generating multiplication triples in fields of characteristic two using OT extensions. Before this work, TinyOT, which works on binary circuits, was the only protocol in this family using OT extensions. Previous SPDZ protocols for triples in large finite fields require somewhat homomorphic encryption, which leads to very inefficient runtimes in practice, while no dedicated preprocessing protocol for MiniMAC (which operates on vectors of small field elements) was previously known. Since actively secure OT extensions can be performed very efficiently using only symmetric primitives, it is highly desirable to base MPC protocols on these rather than expensive public key primitives. We analyze the practical efficiency of our protocols, showing that they should all perform favorably compared with previous works; we estimate our protocol for SPDZ triples in \(\mathbb {F}_{2^{40}}\) will perform around 2 orders of magnitude faster than the best known previous protocol.
Secure Computation from Millionaire
Abstract

The standard method for designing a secure computation protocol for function f first transforms f into either a circuit or a RAM program and then applies a generic secure computation protocol that either handles boolean gates or translates the RAM program into oblivious RAM instructions.

In this paper, we show a large class of functions for which a different iterative approach to secure computation results in more efficient protocols. The first such examples of this technique was presented by Aggarwal, Mishra, and Pinkas (J. of Cryptology, 2010) for computing the median; later, Brickell and Shmatikov (Asiacrypt 2005) showed a similar technique for shortest path problems.

We generalize the technique in both of those works and show that it applies to a large class of problems including certain matroid optimizations, sub-modular optimization, convex hulls, and other scheduling problems. The crux of our technique is to securely reduce these problems to secure comparison operations and to employ the idea of gradually releasing part of the output. We then identify conditions under which both of these techniques for protocol design are compatible with achieving simulation-based security in the honest-but-curious and covert adversary models. In special cases such as median, we also show how to achieve malicious security.
Garbling Scheme for Formulas with Constant Size of Garbled Gates
Abstract

We provide a garbling scheme which creates garbled circuits of a very small constant size (four bits per gate) for circuits with fan-out one (formulas). For arbitrary fan-out, we additionally need only two ciphertexts per additional connection of each gate output wire. We make use of a trapdoor permutation for which we define a generalized notion of correlation robustness. We show that our notion is implied by PRIV-security, a notion for deterministic (searchable) encryption. We prove our scheme secure in the programmable random oracle model.
Card-Based Cryptographic Protocols Using a Minimal Number of Cards
Abstract

Secure multiparty computation can be done with a deck of playing cards. For example, den Boer (EUROCRYPT ’89) devised his famous “five-card trick”, which is a secure two-party AND protocol using five cards. However, the output of the protocol is revealed in the process and it is therefore not suitable for general circuits with hidden intermediate results. To overcome this limitation, protocols in committed format, i.e., with concealed output, have been introduced, among them the six-card AND protocol of (Mizuki and Sone, FAW 2009). In their paper, the authors ask whether six cards are minimal for committed format AND protocols.

We give a comprehensive answer to this problem: there is a four-card AND protocol with a runtime that is finite in expectation (i.e., a Las Vegas protocol), but no protocol with finite runtime. Moreover, we show that five cards are sufficient for finite runtime. In other words, improving on (Mizuki, Kumamoto and Sone, ASIACRYPT 2012) “The Five-Card Trick can be done with four cards”, our results can be stated as “The Five-Card Trick can be done in committed format” and furthermore it “can be done with four cards in Las Vegas committed format”.

By devising a Las Vegas protocol for any k-ary boolean function using 2k cards, we address the open question posed by (Nishida et al., TAMC 2015) on whether 2k+6 cards are necessary for computing any k-ary boolean function. For this we use the shuffle abstraction as introduced in the computational model of card-based protocols in (Mizuki and Shizuya, Int. J. Inf. Secur., 2014). We augment this result by a discussion on implementing such general shuffle operations.
Field1_text
Text
FaceForensics++: Learning to Detect Manipulated Facial Images

The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on Deep-Fakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain-specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.
DeepVCP: An End-to-End Deep Neural Network for Point Cloud Registration

We present DeepVCP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the interference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable registration accuracy and runtime efficiency to the state-of-the-art geometry-based methods, but with higher robustness to inaccurate initial poses. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method make it attractive to the substantial applications relying on the point cloud registration task.
Shape Reconstruction Using Differentiable Projections and Deep Priors

We investigate the problem of reconstructing shapes from noisy and incomplete projections in the presence of viewpoint uncertainities. The problem is cast as an optimization over the shape given measurements obtained by a projection operator and a prior. We present differentiable projection operators for a number of reconstruction problems which when combined with the deep image prior or shape prior allows efficient inference through gradient descent. We apply our method on a variety of reconstruction problems, such as tomographic reconstruction from a few samples, visual hull reconstruction incorporating view uncertainties, and 3D shape reconstruction from noisy depth maps. Experimental results show that our approach is effective for such shape reconstruction problems, without requiring any task-specific training.
Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization

Long-term visual localization is the problem of estimating the camera pose of a given query image in a scene whose appearance changes over time. It is an important problem in practice that is, for example, encountered in autonomous driving. In order to gain robustness to such changes, long-term localization approaches often use segmantic segmentations as an invariant scene representation, as the semantic meaning of each scene part should not be affected by seasonal and other changes. However, these representations are typically not very discriminative due to the very limited number of available classes. In this paper, we propose a novel neural network, the Fine-Grained Segmentation Network (FGSN), that can be used to provide image segmentations with a larger number of labels and can be trained in a self-supervised fashion. In addition, we show how FGSNs can be trained to output consistent labels across seasonal changes. We show through extensive experiments that integrating the fine-grained segmentations produced by our FGSNs into existing localization algorithms leads to substantial improvements in localization performance.
SANet: Scene Agnostic Network for Camera Localization

This paper presents a scene agnostic neural architecture for camera localization, where model parameters and scenes are independent from each other.Despite recent advancement in learning based methods, most approaches require training for each scene one by one, not applicable for online applications such as SLAM and robotic navigation, where a model must be built on-the-fly.Our approach learns to build a hierarchical scene representation and predicts a dense scene coordinate map of a query RGB image on-the-fly given an arbitrary scene. The 6D camera pose of the query image can be estimated with the predicted scene coordinate map. Additionally, the dense prediction can be used for other online robotic and AR applications such as obstacle avoidance. We demonstrate the effectiveness and efficiency of our method on both indoor and outdoor benchmarks, achieving state-of-the-art performance. 
Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning

We show that denoising of 3D point clouds can be learned unsupervised, directly from noisy 3D point cloud data only. This is achieved by extending recent ideas from learning of unsupervised image denoisers to unstructured 3D point clouds. Unsupervised image denoisers operate under the assumption that a noisy pixel observation is a random realization of a distribution around a clean pixel value, which allows appropriate learning on this distribution to eventually converge to the correct value. Regrettably, this assumption is not valid for unstructured points: 3D point clouds are subject to total noise, i.e. deviations in all coordinates, with no reliable pixel grid. Thus, an observation can be the realization of an entire manifold of clean 3D points, which makes the quality of a naive extension of unsupervised image denoisers to 3D point clouds unfortunately only little better than mean filtering. To overcome this, and to enable effective and unsupervised 3D point cloud denoising, we introduce a spatial prior term, that steers converges to the unique closest out of the many possible modes on the manifold. Our results demonstrate unsupervised denoising performance similar to that of supervised learning with clean data when given enough training examples - whereby we do not need any pairs of noisy and clean training data.
Hierarchical Self-Attention Network for Action Localization in Videos

This paper presents a novel Hierarchical Self-Attention Network (HISAN) to generate spatial-temporal tubes for action localization in videos. The essence of HISAN is to combine the two-stream convolutional neural network (CNN) with hierarchical bidirectional self-attention mechanism, which comprises of two levels of bidirectional self-attention to efficaciously capture both of the long-term temporal dependency information and spatial context information to render more precise action localization. Also, a sequence rescoring (SR) algorithm is employed to resolve the dilemma of inconsistent detection scores incurred by occlusion or background clutter. Moreover, a new fusion scheme is invoked, which integrates not only the appearance and motion information from the two-stream network, but also the motion saliency to mitigate the effect of camera motion. Simulations reveal that the new approach achieves competitive performance as the state-of-the-art works in terms of action localization and recognition accuracy on the widespread UCF101-24 and J-HMDB datasets.
Goal-Driven Sequential Data Abstraction

Automatic data abstraction is an important capability for both benchmarking machine intelligence and supporting summarization applications. In the former one asks whether a machine can `understand' enough about the meaning of input data to produce a meaningful but more compact abstraction. In the latter this capability is exploited for saving space or human time by summarizing the essence of input data. In this paper we study a general reinforcement learning based framework for learning to abstract sequential data in a goal-driven way. The ability to define different abstraction goals uniquely allows different aspects of the input data to be preserved according to the ultimate purpose of the abstraction. Our reinforcement learning objective does not require human-defined examples of ideal abstraction. Importantly our model processes the input sequence holistically without being constrained by the original input order. Our framework is also domain agnostic -- we demonstrate applications to sketch, video and text data and achieve promising results in all domains. 
Jointly Aligning Millions of Images With Deep Penalised Reconstruction Congealing

Extrapolating fine-grained pixel-level correspondences in a fully unsupervised manner from a large set of misaligned images can benefit several computer vision and graphics problems, e.g. co-segmentation, super-resolution, image edit propagation, structure-from-motion, and 3D reconstruction. Several joint image alignment and congealing techniques have been proposed to tackle this problem, but robustness to initialisation, ability to scale to large datasets, and alignment accuracy seem to hamper their wide applicability. To overcome these limitations, we propose an unsupervised joint alignment method leveraging a densely fused spatial transformer network to estimate the warping parameters for each image and a low-capacity auto-encoder whose reconstruction error is used as an auxiliary measure of joint alignment. Experimental results on digits from multiple versions of MNIST (i.e., original, perturbed, affNIST and infiMNIST) and faces from LFW, show that our approach is capable of aligning millions of images with high accuracy and robustness to different levels and types of perturbation. Moreover, qualitative and quantitative results suggest that the proposed method outperforms state-of-the-art approaches both in terms of alignment quality and robustness to initialisation.
Drop to Adapt: Learning Discriminative Features for Unsupervised Domain Adaptation

Recent works on domain adaptation exploit adversarial training to obtain domain-invariant feature representations from the joint learning of feature extractor and domain discriminator networks. However, domain adversarial methods render suboptimal performances since they attempt to match the distributions among the domains without considering the task at hand. We propose Drop to Adapt (DTA), which leverages adversarial dropout to learn strongly discriminative features by enforcing the cluster assumption. Accordingly, we design objective functions to support robust domain adaptation. We demonstrate efficacy of the proposed method on various experiments and achieve consistent improvements in both image classification and semantic segmentation tasks. Our source code is available at https://github.com/postBG/DTA.pytorch.
NLNL: Negative Learning for Noisy Labels

Convolutional Neural Networks (CNNs) provide excellent performance when used for image classification. The classical method of training CNNs is by labeling images in a supervised manner as in "input image belongs to this label" (Positive Learning; PL), which is a fast and accurate method if the labels are assigned correctly to all images. However, if inaccurate labels, or noisy labels, exist, training with PL will provide wrong information, thus severely degrading performance. To address this issue, we start with an indirect learning method called Negative Learning (NL), in which the CNNs are trained using a complementary label as in "input image does not belong to this complementary label." Because the chances of selecting a true label as a complementary label are low, NL decreases the risk of providing incorrect information. Furthermore, to improve convergence, we extend our method by adopting PL selectively, termed as Selective Negative Learning and Positive Learning (SelNLPL). PL is used selectively to train upon expected-to-be-clean data, whose choices become possible as NL progresses, thus resulting in superior performance of filtering out noisy data. With simple semi-supervised training technique, our method achieves state-of-the-art accuracy for noisy data classification, proving the superiority of SelNLPL's noisy data filtering ability.
Adversarial Robustness vs. Model Compression, or Both?

It is well known that deep neural networks (DNNs) are vulnerable to adversarial attacks, which are implemented by adding crafted perturbations onto benign examples. Min-max robust optimization based adversarial training can provide a notion of security against adversarial attacks. However, adversarial robustness requires a significantly larger capacity of the network than that for the natural training with only benign examples. This paper proposes a framework of concurrent adversarial training and weight pruning that enables model compression while still preserving the adversarial robustness and essentially tackles the dilemma of adversarial training. Furthermore, this work studies two hypotheses about weight pruning in the conventional setting and finds that weight pruning is essential for reducing the network model size in the adversarial setting; training a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robustness nor high standard accuracy. Code is available at https://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.
On the Design of Black-Box Adversarial Examples by Leveraging Gradient-Free Optimization and Operator Splitting Method

Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models (e.g., single distortion metric and restrictive assumption on target model's feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this field, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image classification datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates.
DewarpNet: Single-Image Document Unwarping With Stacked 3D and 2D Regression Networks

Capturing document images with hand-held devices in unstructured environments is a common practice nowadays. However, "casual" photos of documents are usually unsuitable for automatic information extraction, mainly due to physical distortion of the document paper, as well as various camera positions and illumination conditions. In this work, we propose DewarpNet, a deep-learning approach for document image unwarping from a single image. Our insight is that the 3D geometry of the document not only determines the warping of its texture but also causes the illumination effects. Therefore, our novelty resides on the explicit modeling of 3D shape for document paper in an end-to-end pipeline. Also, we contribute the largest and most comprehensive dataset for document image unwarping to date - Doc3D. This dataset features multiple ground-truth annotations, including 3D shape, surface normals, UV map, albedo image, etc. Training with Doc3D, we demonstrate state-of-the-art performance for DewarpNet with extensive qualitative and quantitative evaluations. Our network also significantly improves OCR performance on captured document images, decreasing character error rate by 42% on average. Both the code and the dataset are released.
Learning Robust Facial Landmark Detection via Hierarchical Structured Ensemble

Heatmap regression-based models have significantly advanced the progress of facial landmark detection. However, the lack of structural constraints always generates inaccurate heatmaps resulting in poor landmark detection performance. While hierarchical structure modeling methods have been proposed to tackle this issue, they all heavily rely on manually designed tree structures. The designed hierarchical structure is likely to be completely corrupted due to the missing or inaccurate prediction of landmarks. To the best of our knowledge, in the context of deep learning, no work before has investigated how to automatically model proper structures for facial landmarks, by discovering their inherent relations. In this paper, we propose a novel Hierarchical Structured Landmark Ensemble (HSLE) model for learning robust facial landmark detection, by using it as the structural constraints. Different from existing approaches of manually designing structures, our proposed HSLE model is constructed automatically via discovering the most robust patterns so HSLE has the ability to robustly depict both local and holistic landmark structures simultaneously. Our proposed HSLE can be readily plugged into any existing facial landmark detection baselines for further performance improvement. Extensive experimental results demonstrate our approach significantly outperforms the baseline by a large margin to achieve a state-of-the-art performance. 
Remote Heart Rate Measurement From Highly Compressed Facial Videos: An End-to-End Deep Learning Solution With Video Enhancement

Remote photoplethysmography (rPPG), which aims at measuring heart activities without any contact, has great potential in many applications (e.g., remote healthcare). Existing rPPG approaches rely on analyzing very fine details of facial videos, which are prone to be affected by video compression. Here we propose a two-stage, end-to-end method using hidden rPPG information enhancement and attention networks, which is the first attempt to counter video compression loss and recover rPPG signals from highly compressed videos. The method includes two parts: 1) a Spatio-Temporal Video Enhancement Network (STVEN) for video enhancement, and 2) an rPPG network (rPPGNet) for rPPG signal recovery. The rPPGNet can work on its own for robust rPPG measurement, and the STVEN network can be added and jointly trained to further boost the performance especially on highly compressed videos. Comprehensive experiments are performed on two benchmark datasets to show that, 1) the proposed method not only achieves superior performance on compressed videos with high-quality videos pair, 2) it also generalizes well on novel data with only compressed videos available, which implies the promising potential for real-world applications.
Face-to-Parameter Translation for Game Character Auto-Creation

Character customization system is an important component in Role-Playing Games (RPGs), where players are allowed to edit the facial appearance of their in-game characters with their own preferences rather than using default templates. This paper proposes a method for automatically creating in-game characters of players according to an input face photo. We formulate the above "artistic creation" process under a facial similarity measurement and parameter searching paradigm by solving an optimization problem over a large set of physically meaningful facial parameters. To effectively minimize the distance between the created face and the real one, two loss functions, i.e. a "discriminative loss" and a "facial content loss", are specifically designed. As the rendering process of a game engine is not differentiable, a generative network is further introduced as an "imitator" to imitate the physical behavior of the game engine so that the proposed method can be implemented under a neural style transfer framework and the parameters can be optimized by gradient descent. Experimental results demonstrate that our method achieves a high degree of generation similarity between the input face photo and the created in-game character in terms of both global appearance and local details. Our method has been deployed in a new game last year and has now been used by players over 1 million times.
Visual Deprojection: Probabilistic Recovery of Collapsed Dimensions

We introduce visual deprojection: the task of recovering an image or video that has been collapsed along a dimension. Projections arise in various contexts, such as long-exposure photography, where a dynamic scene is collapsed in time to produce a motion-blurred image, and corner cameras, where reflected light from a scene is collapsed along a spatial dimension because of an edge occluder to yield a 1D video. Deprojection is ill-posed-- often there are many plausible solutions for a given input. We first propose a probabilistic model capturing the ambiguity of the task. We then present a variational inference strategy using convolutional neural networks as functional approximators. Sampling from the inference network at test time yields plausible candidates from the distribution of original signals that are consistent with a given input projection. We evaluate the method on several datasets for both spatial and temporal deprojection tasks. We first demonstrate the method can recover human gait videos and face images from spatial projections, and then show that it can recover videos of moving digits from dramatically motion-blurred images obtained via temporal projection. 
StructureFlow: Image Inpainting via Structure-Aware Appearance Flow

Image inpainting techniques have shown significant improvements by using deep neural networks recently. However, most of them may either fail to reconstruct reasonable structures or restore fine-grained textures. In order to solve this problem, in this paper, we propose a two-stage model which splits the inpainting task into two parts: structure reconstruction and texture generation. In the first stage, edge-preserved smooth images are employed to train a structure reconstructor which completes the missing structures of the inputs. In the second stage, based on the reconstructed structures, a texture generator using appearance flow is designed to yield image details. Experiments on multiple publicly available datasets show the superior performance of the proposed network.
Learning Fixed Points in Generative Adversarial Networks: From Image-to-Image Translation to Disease Detection and Localization

Generative adversarial networks (GANs) have ushered in a revolution in image-to-image translation. The development and proliferation of GANs raises an interesting question: can we train a GAN to remove an object, if present, from an image while otherwise preserving the image? Specifically, can a GAN "virtually heal" anyone by turning his medical image, with an unknown health status (diseased or healthy), into a healthy one, so that diseased regions could be revealed by subtracting those two images? Such a task requires a GAN to identify a minimal subset of target pixels for domain translation, an ability that we call fixed-point translation, which no GAN is equipped with yet. Therefore, we propose a new GAN, called Fixed-Point GAN, trained by (1) supervising same-domain translation through a conditional identity loss, and (2) regularizing cross-domain translation through revised adversarial, domain classification, and cycle consistency loss. Based on fixed-point translation, we further derive a novel framework for disease detection and localization using only image-level annotation. Qualitative and quantitative evaluations demonstrate that the proposed method outperforms the state of the art in multi-domain image-to-image translation and that it surpasses predominant weakly-supervised localization methods in both disease detection and localization. Implementation is available at https://github.com/jlianglab/Fixed-Point-GAN.
Generative Adversarial Training for Weakly Supervised Cloud Matting

The detection and removal of cloud in remote sensing images are essential for earth observation applications. Most previous methods consider cloud detection as a pixel-wise semantic segmentation process (cloud v.s. background), which inevitably leads to a category-ambiguity problem when dealing with semi-transparent clouds. We re-examine the cloud detection under a totally different point of view, i.e. to formulate it as a mixed energy separation process between foreground and background images, which can be equivalently implemented under an image matting paradigm with a clear physical significance. We further propose a generative adversarial framework where the training of our model neither requires any pixel-wise ground truth reference nor any additional user interactions. Our model consists of three networks, a cloud generator G, a cloud discriminator D, and a cloud matting network F, where G and D aim to generate realistic and physically meaningful cloud images by adversarial training, and F learns to predict the cloud reflectance and attenuation. Experimental results on a global set of satellite images demonstrate that our method, without ever using any pixel-wise ground truth during training, achieves comparable and even higher accuracy over other fully supervised methods, including some recent popular cloud detectors and some well-known semantic segmentation frameworks.
PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data

In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID. 
Generative Adversarial Networks for Extreme Learned Image Compression

We present a learned image compression system based on GANs, operating at extremely low bitrates. Our proposed framework combines an encoder, decoder/generator and a multi-scale discriminator, which we train jointly for a generative learned compression objective. The model synthesizes details it cannot afford to store, obtaining visually pleasing results at bitrates where previous methods fail and show strong artifacts. Furthermore, if a semantic label map of the original image is available, our method can fully synthesize unimportant regions in the decoded image such as streets and trees from the label map, proportionally reducing the storage cost. A user study confirms that for low bitrates, our approach is preferred to state-of-the-art methods, even when they use more than double the bits. 
Instance-Guided Context Rendering for Cross-Domain Person Re-Identification

Existing person re-identification (re-id) methods mostly assume the availability of large-scale identity labels for model learning in any target domain deployment. This greatly limits their scalability in practice. To tackle this limitation, we propose a novel Instance-Guided Context Rendering scheme, which transfers the source person identities into diverse target domain contexts to enable supervised re-id model learning in the unlabelled target domain. Unlike previous image synthesis methods that transform the source person images into limited fixed target styles, our approach produces more visually plausible, and diverse synthetic training data. Specifically, we formulate a dual conditional generative adversarial network that augments each source person image with rich contextual variations. To explicitly achieve diverse rendering effects, we leverage abundant unlabelled target instances as contextual guidance for image generation. Extensive experiments on Market-1501, DukeMTMC-reID and CUHK03 benchmarks show that the re-id performance can be significantly improved when using our synthetic data in cross-domain re-id model learning. 
What Else Can Fool Deep Learning? Addressing Color Constancy Errors on Deep Neural Network Performance

There is active research targeting local image manipulations that can fool deep neural networks (DNNs) into producing incorrect results. This paper examines a type of global image manipulation that can produce similar adverse effects. Specifically, we explore how strong color casts caused by incorrectly applied computational color constancy - referred to as white balance (WB) in photography - negatively impact the performance of DNNs targeting image segmentation and classification. In addition, we discuss how existing image augmentation methods used to improve the robustness of DNNs are not well suited for modeling WB errors. To address this problem, a novel augmentation method is proposed that can emulate accurate color constancy degradation. We also explore pre-processing training and testing images with a recent WB correction algorithm to reduce the effects of incorrectly white-balanced images. We examine both augmentation and pre-processing strategies on different datasets and demonstrate notable improvements on the CIFAR-10, CIFAR-100, and ADE20K datasets.
Beyond Cartesian Representations for Local Descriptors

The dominant approach for learning local patch descriptors relies on small image regions whose scale must be properly estimated a priori by a keypoint detector. In other words, if two patches are not in correspondence, their descriptors will not match. A strategy often used to alleviate this problem is to "pool" the pixel-wise features over log-polar regions, rather than regularly spaced ones. By contrast, we propose to extract the "support region" directly with a log-polar sampling scheme. We show that this provides us with a better representation by simultaneously oversampling the immediate neighbourhood of the point and undersampling regions far away from it. We demonstrate that this representation is particularly amenable to learning descriptors with deep networks. Our models can match descriptors across a much wider range of scales than was possible before, and also leverage much larger support regions without suffering from occlusions. We report state-of-the-art results on three different datasets
Distilling Knowledge From a Deep Pose Regressor Network

This paper presents a novel method to distill knowledge from a deep pose regressor network for efficient Visual Odometry (VO). Standard distillation relies on "dark knowledge" for successful knowledge transfer. As this knowledge is not available in pose regression and the teacher prediction is not always accurate, we propose to emphasize the knowledge transfer only when we trust the teacher. We achieve this by using teacher loss as a confidence score which places variable relative importance on the teacher prediction. We inject this confidence score to the main training task via Attentive Imitation Loss (AIL) and when learning the intermediate representation of the teacher through Attentive Hint Training (AHT) approach. To the best of our knowledge, this is the first work which successfully distill the knowledge from a deep pose regression network. Our evaluation on the KITTI and Malaga dataset shows that we can keep the student prediction close to the teacher with up to 92.95% parameter reduction and 2.12x faster in computation time.
Instance-Level Future Motion Estimation in a Single Image Based on Ordinal Regression

A novel algorithm to estimate instance-level future motion in a single image is proposed in this paper. We first represent the future motion of an instance with its direction, speed, and action classes. Then, we develop a deep neural network that exploits different levels of semantic information to perform the future motion estimation. For effective future motion classification, we adopt ordinal regression. Especially, we develop the cyclic ordinal regression scheme using binary classifiers. Experiments demonstrate that the proposed algorithm provides reliable performance and thus can be used effectively for vision applications, including single and multi object tracking. Furthermore, we release the future motion (FM) dataset, collected from diverse sources and annotated manually, as a benchmark for single-image future motion estimation.
Vision-Infused Deep Audio Inpainting

Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, i.e., synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI).
HAWQ: Hessian AWare Quantization of Neural Networks With Mixed-Precision

Multi-modality perception is essential to develop interactive intelligence. In this work, we consider a new task of visual information-infused audio inpainting, i.e., synthesizing missing audio segments that correspond to their accompanying videos. We identify two key aspects for a successful inpainter: (1) It is desirable to operate on spectrograms instead of raw audios. Recent advances in deep semantic image inpainting could be leveraged to go beyond the limitations of traditional audio inpainting. (2) To synthesize visually indicated audio, a visual-audio joint feature space needs to be learned with synchronization of audio and video. To facilitate a large-scale study, we collect a new multi-modality instrument-playing dataset called MUSIC-Extra-Solo (MUSICES) by enriching MUSIC dataset. Extensive experiments demonstrate that our framework is capable of inpainting realistic and varying audio segments with or without visual contexts. More importantly, our synthesized audio segments are coherent with their video counterparts, showing the effectiveness of our proposed Vision-Infused Audio Inpainter (VIAI).
Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks

Single-image super-resolution aims to generate a high-resolution version of a low-resolution image, which serves as an essential component in many image processing applications. This paper investigates the robustness of deep learning-based super-resolution methods against adversarial attacks, which can significantly deteriorate the super-resolved images without noticeable distortion in the attacked low-resolution images. It is demonstrated that state-of-the-art deep super-resolution methods are highly vulnerable to adversarial attacks. Different levels of robustness of different methods are analyzed theoretically and experimentally. We also present analysis on transferability of attacks, and feasibility of targeted attacks and universal attacks.
Overcoming Catastrophic Forgetting With Unlabeled Data in the Wild

Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8% higher accuracy and 46.5% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.
Symmetric Cross Entropy for Robust Learning With Noisy Labels

Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: the performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: our method shows up to 15.8% higher accuracy and 46.5% less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.
Few-Shot Learning With Embedded Class Models and Shot-Free Meta Training

We propose a method for learning embeddings for few-shot learning that is suitable for use with any number of shots (shot-free). Rather than fixing the class prototypes to be the Euclidean average of sample embeddings, we allow them to live in a higher-dimensional space (embedded class models) and learn the prototypes along with the model parameters. The class representation function is defined implicitly, which allows us to deal with a variable number of shots per class with a simple constant-size architecture. The class embedding encompasses metric learning, that facilitates adding new classes without crowding the class representation space. Despite being general and not tuned to the benchmark, our approach achieves state-of-the-art performance on the standard few-shot benchmark datasets.
Dual Directed Capsule Network for Very Low Resolution Image Recognition

Very low resolution (VLR) image recognition corresponds to classifying images with resolution 16x16 or less. Though it has widespread applicability when objects are captured at a very large stand-off distance (e.g. surveillance scenario) or from wide angle mobile cameras, it has received limited attention. This research presents a novel Dual Directed Capsule Network model, termed as DirectCapsNet, for addressing VLR digit and face recognition. The proposed architecture utilizes a combination of capsule and convolutional layers for learning an effective VLR recognition model. The architecture also incorporates two novel loss functions: (i) the proposed HR-anchor loss and (ii) the proposed targeted reconstruction loss, in order to overcome the challenges of limited information content in VLR images. The proposed losses use high resolution images as auxiliary data during training to "direct" discriminative feature learning. Multiple experiments for VLR digit classification and VLR face recognition are performed along with comparisons with state-of-the-art algorithms. The proposed DirectCapsNet consistently showcases state-of-the-art results; for example, on the UCCS face database, it shows over 95% face recognition accuracy when 16x16 images are matched with 80x80 images.
Recognizing Part Attributes With Insufficient Data

Recognizing the attributes of objects and their parts is central to many computer vision applications. Although great progress has been made to apply object-level recognition, recognizing the attributes of parts remains less applicable since the training data for part attributes recognition is usually scarce especially for internet-scale applications. Furthermore, most existing part attribute recognition methods rely on the part annotations which are more expensive to obtain. In order to solve the data insufficiency problem and get rid of dependence on the part annotation, we introduce a novel Concept Sharing Network (CSN) for part attribute recognition. A great advantage of CSN is its capability of recognizing the part attribute (a combination of part location and appearance pattern) that has insufficient or zero training data, by learning the part location and appearance pattern respectively from the training data that usually mix them in a single label. Extensive experiments on CUB, Celeb A, and a newly proposed human attribute dataset demonstrate the effectiveness of CSN and its advantages over other methods, especially for the attributes with few training samples. Further experiments show that CSN can also perform zero-shot part attribute recognition.
USIP: Unsupervised Stable Interest Point Detection From 3D Point Clouds

In this paper, we propose the USIP detector: an Unsupervised Stable Interest Point detector that can detect highly repeatable and accurately localized keypoints from 3D point clouds under arbitrary transformations without the need for any ground truth training data. Our USIP detector consists of a feature proposal network that learns stable keypoints from input 3D point clouds and their respective transformed pairs from randomly generated transformations. We provide degeneracy analysis and suggest solutions to prevent it. We encourage high repeatability and accurate localization of the keypoints with a probabilistic chamfer loss that minimizes the distances between the detected keypoints from the training point cloud pairs. Extensive experimental results of repeatability tests on several simulated and real-world 3D point cloud datasets from Lidar, RGB-D and CAD models show that our USIP detector significantly outperforms existing hand-crafted and deep learning-based 3D keypoint detectors. Our code is available at the project website. https://github.com/lijx10/USIP
Mixed High-Order Attention Network for Person Re-Identification

Attention has become more attractive in person re-identification (ReID) as it is capable of biasing the allocation of available resources towards the most informative parts of an input signal. However, state-of-the-art works concentrate only on coarse or first-order attention design, e.g. spatial and channels attention, while rarely exploring higher-order attention mechanism. We take a step towards addressing this problem. In this paper, we first propose the High-Order Attention (HOA) module to model and utilize the complex and high-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then, rethinking person ReID as a zero-shot learning problem, we propose the Mixed High-Order Attention Network (MHN) to further enhance the discrimination and richness of attention knowledge in an explicit manner. Extensive experiments have been conducted to validate the superiority of our MHN for person ReID over a wide variety of state-of-the-art methods on three large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP. Code is available at http://www.bhchen.cn.
Budget-Aware Adapters for Multi-Domain Learning

Multi-Domain Learning (MDL) refers to the problem of learning a set of models derived from a common deep architecture, each one specialized to perform a task in a certain domain (e.g., photos, sketches, paintings). This paper tackles MDL with a particular interest in obtaining domain-specific models with an adjustable budget in terms of the number of network parameters and computational complexity. Our intuition is that, as in real applications the number of domains and tasks can be very large, an effective MDL approach should not only focus on accuracy but also on having as few parameters as possible. To implement this idea we derive specialized deep models for each domain by adapting a pre-trained architecture but, differently from other methods, we propose a novel strategy to automatically adjust the computational complexity of the network. To this aim, we introduce Budget-Aware Adapters that select the most relevant feature channels to better handle data from a novel domain. Some constraints on the number of active switches are imposed in order to obtain a network respecting the desired complexity budget. Experimentally, we show that our approach leads to recognition accuracy competitive with state-of-the-art approaches but with much lighter networks both in terms of storage and computation.
Compact Trilinear Interaction for Visual Question Answering

In Visual Question Answering (VQA), answers have a great correlation with question meaning and visual contents. Thus, to selectively utilize image, question and answer information, we propose a novel trilinear interaction model which simultaneously learns high level associations between these three inputs. In addition, to overcome the interaction complexity, we introduce a multimodal tensor-based PARALIND decomposition which efficiently parameterizes trilinear teraction between the three inputs. Moreover, knowledge distillation is first time applied in Free-form Opened-ended VQA. It is not only for reducing the computational cost and required memory but also for transferring knowledge from trilinear interaction model to bilinear interaction model. The extensive experiments on benchmarking datasets TDIUC, VQA-2.0, and Visual7W show that the proposed compact trilinear interaction model achieves state-of-the-art results when using a single model on all three datasets.
Towards Latent Attribute Discovery From Triplet Similarities

This paper addresses the task of learning latent attributes from triplet similarity comparisons. Consider, for instance, the three shoes in Fig. 1(a). They can be compared according to color, comfort, size, or shape resulting in different rankings. Most approaches for embedding learning either make a simplifying assumption - that all inputs are comparable under a single criterion, or require expensive attribute supervision. We introduce Latent Similarity Networks (LSNs): a simple and effective technique to discover the underlying latent notions of similarity in data without any explicit attribute supervision. LSNs can be trained with standard triplet supervision and learn several latent embeddings that can be used to compare images under multiple notions of similarity. LSNs achieve state-of-the-art performance on UT-Zappos-50k Shoes and Celeb-A Faces datasets and also demonstrate the ability to uncover meaningful latent attributes.
GeoStyle: Discovering Fashion Trends and Events

Understanding fashion styles and trends is of great potential interest to retailers and consumers alike. The photos people upload to social media are a historical and public data source of how people dress across the world and at different times. While we now have tools to automatically recognize the clothing and style attributes of what people are wearing in these photographs, we lack the ability to analyze spatial and temporal trends in these attributes or make predictions about the future. In this paper we address this need by providing an automatic framework that analyzes large corpora of street imagery to (a) discover and forecast long-term trends of various fashion attributes as well as automatically discovered styles, and (b) identify spatio-temporally localized events that affect what people wear. We show that our framework makes long term trend forecasts that are > 20% more accurate than prior art, and identifies hundreds of socially meaningful events that impact fashion across the globe.
Towards Adversarially Robust Object Detection

Object detection is an important vision task and has emerged as an indispensable component in many vision system, rendering its robustness as an increasingly important performance factor for practical applications. While object detection models have been demonstrated to be vulnerable against adversarial attacks by many recent works, very few efforts have been devoted to improving their robustness. In this work, we take an initial attempt towards this direction. We first revisit and systematically analyze object detectors and many recently developed attacks from the perspective of model robustness. We then present a multi-task learning perspective of object detection and identify an asymmetric role of task losses. We further develop an adversarial training approach which can leverage the multiple sources of attacks for improving the robustness of detection models. Extensive experiments on PASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach. 
Automatic and Robust Skull Registration Based on Discrete Uniformization

 Skull registration plays a fundamental role in forensic science and is crucial for craniofacial reconstruction. The complicated topology, lack of anatomical features, and low quality reconstructed mesh make skull registration challenging. In this work, we propose an automatic skull registration method based on the discrete uniformization theory, which can handle complicated topologies and is robust to low quality meshes. We apply dynamic Yamabe flow to realize discrete uniformization, which modifies the mesh combinatorial structure during the flow and conformally maps the multiply connected skull surface onto a planar disk with circular holes. The 3D surfaces can be registered by matching their planar images using harmonic maps. This method is rigorous with theoretic guarantee, automatic without user intervention, and robust to low mesh quality. Our experimental results demonstrate the efficiency and efficacy of the method. 
Few-Shot Image Recognition With Knowledge Transfer

Human can well recognize images of novel categories just after browsing few examples of these categories. One possible reason is that they have some external discriminative visual information about these categories from their prior knowledge. Inspired from this, we propose a novel Knowledge Transfer Network architecture (KTN) for few-shot image recognition. The proposed KTN model jointly incorporates visual feature learning, knowledge inferring and classifier learning into one unified framework for their optimal compatibility. First, the visual classifiers for novel categories are learned based on the convolutional neural network with the cosine similarity optimization. To fully explore the prior knowledge, a semantic-visual mapping network is then developed to conduct knowledge inference, which enables to infer the classifiers for novel categories from base categories. Finally, we design an adaptive fusion scheme to infer the desired classifiers by effectively integrating the above knowledge and visual information. Extensive experiments are conducted on two widely-used Mini-ImageNet and ImageNet Few-Shot benchmarks to evaluate the effectiveness of the proposed method. The results compared with the state-of-the-art approaches show the encouraging performance of the proposed method, especially on 1-shot and 2-shot tasks.
Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings

We address the problem of cross-modal fine-grained action retrieval between text and video. Cross-modal retrieval is commonly achieved through learning a shared embedding space, that can indifferently embed modalities. In this paper, we propose to enrich the embedding by disentangling parts-of-speech (PoS) in the accompanying captions. We build a separate multi-modal embedding space for each PoS tag. The outputs of multiple PoS embeddings are then used as input to an integrated multi-modal space, where we perform action retrieval. All embeddings are trained jointly through a combination of PoS-aware and PoS-agnostic losses. Our proposal enables learning specialised embedding spaces that offer multiple views of the same embedded entities. We report the first retrieval results on fine-grained actions for the large-scale EPIC dataset, in a generalised zero-shot setting. Results show the advantage of our approach for both video-to-text and text-to-video action retrieval. We also demonstrate the benefit of disentangling the PoS for the generic task of cross-modal video retrieval on the MSR-VTT dataset.
Vehicle Re-Identification in Aerial Imagery: Dataset and Approach

In this work, we construct a large-scale dataset for vehicle re-identification (ReID), which contains 137k images of 13k vehicle instances captured by UAV-mounted cameras. To our knowledge, it is the largest UAV-based vehicle ReID dataset. To increase intra-class variation, each vehicle is captured by at least two UAVs at different locations, with diverse view-angles and flight-altitudes. We manually label a variety of vehicle attributes, including vehicle type, color, skylight, bumper, spare tire and luggage rack. Furthermore, for each vehicle image, the annotator is also required to mark the discriminative parts that helps them to distinguish this particular vehicle from others. Besides the dataset, we also design a specific vehicle ReID algorithm to make full use of the rich annotation information. It is capable of explicitly detecting discriminative parts for each specific vehicle and significantly outperforming the evaluated baselines and state-of-the-art vehicle ReID approaches. 
Bridging the Domain Gap for Ground-to-Aerial Image Matching

The visual entities in cross-view (e.g. ground and aerial) images exhibit drastic domain changes due to the differences in viewpoints each set of images is captured from. Existing state-of-the-art methods address the problem by learning view-invariant images descriptors. We propose a novel method for solving this task by exploiting the gener- ative powers of conditional GANs to synthesize an aerial representation of a ground-level panorama query and use it to minimize the domain gap between the two views. The synthesized image being from the same view as the ref- erence (target) image, helps the network to preserve im- portant cues in aerial images following our Joint Feature Learning approach. We fuse the complementary features from a synthesized aerial image with the original ground- level panorama features to obtain a robust query represen- tation. In addition, we employ multi-scale feature aggre- gation in order to preserve image representations at dif- ferent scales useful for solving this complex task. Experi- mental results show that our proposed approach performs significantly better than the state-of-the-art methods on the challenging CVUSA dataset in terms of top-1 and top-1% retrieval accuracies. Furthermore, we evaluate the gen- eralization of the proposed method for urban landscapes on our newly collected cross-view localization dataset with geo-reference information.
A Robust Learning Approach to Domain Adaptive Object Detection

Domain shift is unavoidable in real-world applications of object detection. For example, in self-driving cars, the target domain consists of unconstrained road environments which cannot all possibly be observed in training data. Similarly, in surveillance applications sufficiently representative training data may be lacking due to privacy regulations. In this paper, we address the domain adaptation problem from the perspective of robust learning and show that the problem may be formulated as training with noisy labels. We propose a robust object detection framework that is resilient to noise in bounding box class labels, locations and size annotations. To adapt to the domain shift, the model is trained on the target domain using a set of noisy object bounding boxes that are obtained by a detection model trained only in the source domain. We evaluate the accuracy of our approach in various source/target domain pairs and demonstrate that the model significantly improves the state-of-the-art on multiple domain adaptation scenarios on the SIM10K, Cityscapes and KITTI datasets.
Graph-Based Object Classification for Neuromorphic Vision Sensing

Neuromorphic vision sensing (NVS) devices represent visual information as sequences of asynchronous discrete events (a.k.a., "spikes'") in response to changes in scene reflectance. Unlike conventional active pixel sensing (APS), NVS allows for significantly higher event sampling rates at substantially increased energy efficiency and robustness to illumination changes. However, object classification with NVS streams cannot leverage on state-of-the-art convolutional neural networks (CNNs), since NVS does not produce frame representations. To circumvent this mismatch between sensing and processing with CNNs, we propose a compact graph representation for NVS. We couple this with novel residual graph CNN architectures and show that, when trained on spatio-temporal NVS data for object classification, such residual graph CNNs preserve the spatial and temporal coherence of spike events, while requiring less computation and memory. Finally, to address the absence of large real-world NVS datasets for complex recognition tasks, we present and make available a 100k dataset of NVS recordings of the American sign language letters, acquired with an iniLabs DAVIS240c device under real-world conditions.
Gaussian YOLOv3: An Accurate and Fast Object Detector Using Localization Uncertainty for Autonomous Driving

The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.
Sharpen Focus: Learning With Attention Separability and Consistency

The use of object detection algorithms is becoming increasingly important in autonomous vehicles, and object detection at high accuracy and a fast inference speed is essential for safe autonomous driving. A false positive (FP) from a false localization during autonomous driving can lead to fatal accidents and hinder safe and efficient driving. Therefore, a detection algorithm that can cope with mislocalizations is required in autonomous driving applications. This paper proposes a method for improving the detection accuracy while supporting a real-time operation by modeling the bounding box (bbox) of YOLOv3, which is the most representative of one-stage detectors, with a Gaussian parameter and redesigning the loss function. In addition, this paper proposes a method for predicting the localization uncertainty that indicates the reliability of bbox. By using the predicted localization uncertainty during the detection process, the proposed schemes can significantly reduce the FP and increase the true positive (TP), thereby improving the accuracy. Compared to a conventional YOLOv3, the proposed algorithm, Gaussian YOLOv3, improves the mean average precision (mAP) by 3.09 and 3.5 on the KITTI and Berkeley deep drive (BDD) datasets, respectively. Nevertheless, the proposed algorithm is capable of real-time detection at faster than 42 frames per second (fps) and shows a higher accuracy than previous approaches with a similar fps. Therefore, the proposed algorithm is the most suitable for autonomous driving applications.
Learning Semantic-Specific Graph Representation for Multi-Label Image Recognition

Recognizing multiple labels of images is a practical and challenging task, and significant progress has been made by searching semantic-aware regions and modeling label dependency. However, current methods cannot locate the semantic regions accurately due to the lack of part-level supervision or semantic guidance. Moreover, they cannot fully explore the mutual interactions among the semantic regions and do not explicitly model the label co-occurrence. To address these issues, we propose a Semantic-Specific Graph Representation Learning (SSGRL) framework that consists of two crucial modules: 1) a semantic decoupling module that incorporates category semantics to guide learning semantic-specific representations and 2) a semantic interaction module that correlates these representations with a graph built on the statistical label co-occurrence and explores their interactions via a graph propagation mechanism. Extensive experiments on public benchmarks show that our SSGRL framework outperforms current state-of-the-art methods by a sizable margin, e.g. with an mAP improvement of 2.5%, 2.6%, 6.7%, and 3.1% on the PASCAL VOC 2007 & 2012, Microsoft-COCO and Visual Genome benchmarks, respectively. Our codes and models are available at https://github.com/HCPLab-SYSU/SSGRL.
DeceptionNet: Network-Driven Domain Randomization

We present a novel approach to tackle domain adaptation between synthetic and real data. Instead, of employing "blind" domain randomization, i.e., augmenting synthetic renderings with random backgrounds or changing illumination and colorization, we leverage the task network as its own adversarial guide toward useful augmentations that maximize the uncertainty of the output. To this end, we design a min-max optimization scheme where a given task competes against a special deception network to minimize the task error subject to the specific constraints enforced by the deceiver. The deception network samples from a family of differentiable pixel-level perturbations and exploits the task architecture to find the most destructive augmentations. Unlike GAN-based approaches that require unlabeled data from the target domain, our method achieves robust mappings that scale well to multiple target distributions from source data alone. We apply our framework to the tasks of digit recognition on enhanced MNIST variants, classification and object pose estimation on the Cropped LineMOD dataset as well as semantic segmentation on the Cityscapes dataset and compare it to a number of domain adaptation approaches, thereby demonstrating similar results with superior generalization capabilities.
Pose-Guided Feature Alignment for Occluded Person Re-Identification

Persons are often occluded by various obstacles in person retrieval scenarios. Previous person re-identification (re-id) methods, either overlook this issue or resolve it based on an extreme assumption. To alleviate the occlusion problem, we propose to detect the occluded regions, and explicitly exclude those regions during feature generation and matching. In this paper, we introduce a novel method named Pose-Guided Feature Alignment (PGFA), exploiting pose landmarks to disentangle the useful information from the occlusion noise. During the feature constructing stage, our method utilizes human landmarks to generate attention maps. The generated attention maps indicate if a specific body part is occluded and guide our model to attend to the non-occluded regions. During matching, we explicitly partition the global feature into parts and use the pose landmarks to indicate which partial features belonging to the target person. Only the visible regions are utilized for the retrieval. Besides, we construct a large-scale dataset for the Occluded Person Re-ID problem, namely Occluded-DukeMTMC, which is by far the largest dataset for the Occlusion Person Re-ID. Extensive experiments are conducted on our constructed occluded re-id dataset, two partial re-id datasets, and two commonly used holistic re-id datasets. Our method largely outperforms existing person re-id methods on three occlusion datasets, while remains top performance on two holistic datasets.
Robust Person Re-Identification by Modelling Feature Uncertainty

We aim to learn deep person re-identification (ReID) models that are robust against noisy training data. Two types of noise are prevalent in practice: (1) label noise caused by human annotator errors and (2) data outliers caused by person detector errors or occlusion. Both types of noise pose serious problems for training ReID models, yet have been largely ignored so far. In this paper, we propose a novel deep network termed DistributionNet for robust ReID. Instead of representing each person image as a feature vector, DistributionNet models it as a Gaussian distribution with its variance representing the uncertainty of the extracted features. A carefully designed loss is formulated in DistributionNet to unevenly allocate uncertainty across training samples. Consequently, noisy samples are assigned large variance/uncertainty, which effectively alleviates their negative impacts on model fitting. Extensive experiments demonstrate that our model is more effective than alternative noise-robust deep models. The source code is available at: https://github.com/TianyuanYu/DistributionNet
Co-Segmentation Inspired Attention Networks for Video-Based Person Re-Identification

Person re-identification (Re-ID) is an important real-world surveillance problem that entails associating a person's identity over a network of cameras. Video-based Re-ID approaches have gained significant attention recently since a video, and not just an image, is often available. In this work, we propose a novel Co-segmentation inspired video Re-ID deep architecture and formulate a Co-segmentation based Attention Module (COSAM) that activates a common set of salient features across multiple frames of a video via mutual consensus in an unsupervised manner. As opposed to most of the prior work, our approach is able to attend to person accessories along with the person. Our plug-and-play and interpretable COSAM module applied on two deep architectures (ResNet50, SE-ResNet50) outperform the state-of-the-art methods on three benchmark datasets.
A Delay Metric for Video Object Detection: What Average Precision Fails to Tell

Average precision (AP) is a widely used metric to evaluate detection accuracy of image and video object detectors. In this paper, we analyze the object detection from video and point out that mAP alone is not sufficient to capture the temporal nature of video object detection. To tackle this problem, we propose a comprehensive metric, Average Delay (AD), to measure and compare detection delay. To facilitate delay evaluation, we carefully select a subset of ImageNet VID, which we name as ImageNet VIDT with an emphasis on complex trajectories. By extensively evaluating a wide range of detectors on VIDT, we show that most methods drastically increase the detection delay but still preserve mAP well. In other words, mAP is not sensitive enough to reflect the temporal characteristics of a video object detector. Our results suggest that video object detection methods should be evaluated with a delay metric, particularly for latency-critical applications such as autonomous vehicle perception.
IL2M: Class Incremental Learning With Dual Memory

This paper presents a class incremental learning (IL) method which exploits fine tuning and a dual memory to reduce the negative effect of catastrophic forgetting in image recognition. First, we simplify the current fine tuning based approaches which use a combination of classification and distillation losses to compensate for the limited availability of past data. We find that the distillation term actually hurts performance when a memory is allowed. Then, we modify the usual class IL memory component. Similar to existing works, a first memory stores exemplar images of past classes. A second memory is introduced here to store past class statistics obtained when they were initially learned. The intuition here is that classes are best modeled when all their data are available and that their initial statistics are useful across different incremental states. A prediction bias towards newly learned classes appears during inference because the dataset is imbalanced in their favor. The challenge is to make predictions of new and past classes more comparable. To do this, scores of past classes are rectified by leveraging contents from both memories. The method has negligible added cost, both in terms of memory and of inference complexity. Experiments with three large public datasets show that the proposed approach is more effective than a range of competitive state-of-the-art methods. 
Asymmetric Non-Local Neural Networks for Semantic Segmentation

The non-local module works as a particularly useful technique for semantic segmentation while criticized for its prohibitive computation and GPU memory occupation. In this paper, we present Asymmetric Non-local Neural Network to semantic segmentation, which has two prominent components: Asymmetric Pyramid Non-local Block (APNB) and Asymmetric Fusion Non-local Block (AFNB). APNB leverages a pyramid sampling module into the non-local block to largely reduce the computation and memory consumption without sacrificing the performance. AFNB is adapted from APNB to fuse the features of different levels under a sufficient consideration of long range dependencies and thus considerably improves the performance. Extensive experiments on semantic segmentation benchmarks demonstrate the effectiveness and efficiency of our work. In particular, we report the state-of-the-art performance of 81.3 mIoU on the Cityscapes test set. For a 256x128 input, APNB is around 6 times faster than a non-local block on GPU while 28 times smaller in GPU running memory occupation. Code is available at: https://github.com/MendelXu/ANN.git.
CCNet: Criss-Cross Attention for Semantic Segmentation

Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github.com/speedinghzl/CCNet.
Convex Shape Prior for Multi-Object Segmentation Using a Single Level Set Function

Many objects in real world have convex shapes. It is a difficult task to have representations for convex shapes with good and fast numerical solutions. This paper proposes a method to incorporate convex shape prior for multi-object segmentation using level set method. The relationship between the convexity of the segmented objects and the signed distance function corresponding to their union is analyzed theoretically. This result is combined with Gaussian mixture method for the multiple objects segmentation with convexity shape prior. Alternating direction method of multiplier (ADMM) is adopted to solve the proposed model. Special boundary conditions are also imposed to obtain efficient algorithms for 4th order partial differential equations in one step of ADMM algorithm. In addition, our method only needs one level set function regardless of the number of objects. So the increase in the number of objects does not result in the increase of model and algorithm complexity. Various numerical experiments are illustrated to show the performance and advantages of the proposed method. 
Surface Networks via General Covers

Developing deep learning techniques for geometric data is an active and fruitful research area. This paper tackles the problem of sphere-type surface learning by developing a novel surface-to-image representation. Using this representation we are able to quickly adapt successful CNN models to the surface setting. The surface-image representation is based on a covering map from the image domain to the surface. Namely, the map wraps around the surface several times, making sure that every part of the surface is well represented in the image. Differently from previous surface-to-image representations, we provide a low distortion coverage of all surface parts in a single image. Specifically, for the use case of learning spherical signals, our representation provides a low distortion alternative to several popular spherical parameterizations used in deep learning. We have used the surface-to-image representation to apply standard CNN architectures to 3D models including spherical signals. We show that our method achieves state of the art or comparable results on the tasks of shape retrieval, shape classification and semantic shape segmentation.
SSAP: Single-Shot Instance Segmentation With Affinity Pyramid

Recently, proposal-free instance segmentation has received increasing attention due to its concise and efficient pipeline. Generally, proposal-free methods generate instance-agnostic semantic segmentation labels and instance-aware features to group pixels into different object instances. However, previous methods mostly employ separate modules for these two sub-tasks and require multiple passes for inference. We argue that treating these two sub-tasks separately is suboptimal. In fact, employing multiple separate modules significantly reduces the potential for application. The mutual benefits between the two complementary sub-tasks are also unexplored. To this end, this work proposes a single-shot proposal-free instance segmentation method that requires only one single pass for prediction. Our method is based on a pixel-pair affinity pyramid, which computes the probability that two pixels belong to the same instance in a hierarchical manner. The affinity pyramid can also be jointly learned with the semantic class labeling and achieve mutual benefits. Moreover, incorporating with the learned affinity pyramid, a novel cascaded graph partition module is presented to sequentially generate instances from coarse to fine. Unlike previous time-consuming graph partition methods, this module achieves 5x speedup and 9% relative improvement on Average-Precision (AP). Our approach achieves new state of the art on the challenging Cityscapes dataset.
Learning Propagation for Arbitrarily-Structured Data

Processing an input signal that contains arbitrary structures, e.g., superpixels and point clouds, remains a big challenge in computer vision. Linear diffusion, an effective model for image processing, has been recently integrated with deep learning algorithms. In this paper, we propose to learn pairwise relations among data points in a global fashion to improve semantic segmentation with arbitrarily-structured data, through spatial generalized propagation networks (SGPN). The network propagates information on a group of graphs, which represent the arbitrarily-structured data, through a learned, linear diffusion process. The module is flexible to be embedded and jointly trained with many types of networks, e.g., CNNs. We experiment with semantic segmentation networks, where we use our propagation module to jointly train on different data -- images, superpixels, and point clouds. We show that SGPN consistently improves the performance of both pixel and point cloud segmentation, compared to networks that do not contain this module. Our method suggests an effective way to model the global pairwise relations for arbitrarily-structured data.
MultiSeg: Semantically Meaningful, Scale-Diverse Segmentations From Minimal User Input

Existing deep learning-based interactive image segmentation approaches typically assume the target-of-interest is always a single object and fail to account for the potential diversity in user expectations, thus requiring excessive user input when it comes to segmenting an object part or a group of objects instead. Motivated by the observation that the object part, full object, and a collection of objects essentially differ in size, we propose a new concept called scale-diversity, which characterizes the spectrum of segmentations w.r.t. different scales. To address this, we present MultiSeg, a scale-diverse interactive image segmentation network that incorporates a set of two-dimensional scale priors into the model to generate a set of scale-varying proposals that conform to the user input. We explicitly encourage segmentation diversity during training by synthesizing diverse training samples for a given image. As a result, our method allows the user to quickly locate the closest segmentation target for further refinement if necessary. Despite its simplicity, experimental results demonstrate that our proposed model is capable of quickly producing diverse yet plausible segmentation outputs, reducing the user interaction required, especially in cases where many types of segmentations (object parts or groups) are expected.
Robust Motion Segmentation From Pairwise Matches

In this paper we consider the problem of motion segmentation, where only pairwise correspondences are assumed as input without prior knowledge about tracks. The problem is formulated as a two-step process. First, motion segmentation is performed on image pairs independently. Secondly, we combine independent pairwise segmentation results in a robust way into the final globally consistent segmentation. Our approach is inspired by the success of averaging methods. We demonstrate in simulated as well as in real experiments that our method is very effective in reducing the errors in the pairwise motion segmentation and can cope with large number of mismatches.
InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting

Instance segmentation requires a large number of training samples to achieve satisfactory performance and benefits from proper data augmentation. To enlarge the training set and increase the diversity, previous methods have investigated using data annotation from other domain (e.g. bbox, point) in a weakly supervised mechanism. In this paper, we present a simple, efficient and effective method to augment the training set using the existing instance mask annotations. Exploiting the pixel redundancy of the background, we are able to improve the performance of Mask R-CNN for 1.7 mAP on COCO dataset and 3.3 mAP on Pascal VOC dataset by simply introducing random jittering to objects. Furthermore, we propose a location probability map based approach to explore the feasible locations that objects can be placed based on local appearance similarity. With the guidance of such map, we boost the performance of R101-Mask R-CNN on instance segmentation from 35.7 mAP to 37.9 mAP without modifying the backbone or network structure. Our method is simple to implement and does not increase the computational complexity. It can be integrated into the training pipeline of any instance segmentation model without affecting the training and inference efficiency. Our code and models have been released at https://github.com/GothicAi/InstaBoost.
Racial Faces in the Wild: Reducing Racial Bias by Information Maximization Adaptation Network

Racial bias is an important issue in biometric, but has not been thoroughly studied in deep face recognition. In this paper, we first contribute a dedicated dataset called Racial Faces in-the-Wild (RFW) database, on which we firmly validated the racial bias of four commercial APIs and four state-of-the-art (SOTA) algorithms. Then, we further present the solution using deep unsupervised domain adaptation and propose a deep information maximization adaptation network (IMAN) to alleviate this bias by using Caucasian as source domain and other races as target domains. This unsupervised method simultaneously aligns global distribution to decrease race gap at domain-level, and learns the discriminative target representations at cluster level. A novel mutual information loss is proposed to further enhance the discriminative ability of network output without label information. Extensive experiments on RFW, GBU, and IJB-A databases show that IMAN successfully learns features that generalize well across different races and across different databases.
Uncertainty Modeling of Contextual-Connections Between Tracklets for Unconstrained Video-Based Face Recognition

Unconstrained video-based face recognition is a challenging problem due to significant within-video variations caused by pose, occlusion and blur. To tackle this problem, an effective idea is to propagate the identity from high-quality faces to low-quality ones through contextual connections, which are constructed based on context such as body appearance. However, previous methods have often propagated erroneous information due to lack of uncertainty modeling of the noisy contextual connections. In this paper, we propose the Uncertainty-Gated Graph (UGG), which conducts graph-based identity propagation between tracklets, which are represented by nodes in a graph. UGG explicitly models the uncertainty of the contextual connections by adaptively updating the weights of the edge gates according to the identity distributions of the nodes during inference. UGG is a generic graphical model that can be applied at only inference time or with end-to-end training. We demonstrate the effectiveness of UGG with state-of-the-art results in the recently released challenging Cast Search in Movies and IARPA Janus Surveillance Video Benchmark dataset. 
Spatio-Temporal Fusion Based Convolutional Sequence Learning for Lip Reading

Current state-of-the-art approaches for lip reading are based on sequence-to-sequence architectures that are designed for natural machine translation and audio speech recognition. Hence, these methods do not fully exploit the characteristics of the lip dynamics, causing two main drawbacks. First, the short-range temporal dependencies, which are critical to the mapping from lip images to visemes, receives no extra attention. Second, local spatial information is discarded in the existing sequence models due to the use of global average pooling (GAP). To well solve these drawbacks, we propose a Temporal Focal block to sufficiently describe short-range dependencies and a Spatio-Temporal Fusion Module (STFM) to maintain the local spatial information and to reduce the feature dimensions as well. From the experiment results, it is demonstrated that our method achieves comparable performance with the state-of-the-art approach using much less training data and much lighter Convolutional Feature Extractor. The training time is reduced by 12 days due to the convolutional structure and the local self-attention mechanism. 
Occlusion-Aware Networks for 3D Human Pose Estimation in Video

Occlusion is a key problem in 3D human pose estimation from a monocular video. To address this problem, we introduce an occlusion-aware deep-learning framework. By employing estimated 2D confidence heatmaps of keypoints and an optical-flow consistency constraint, we filter out the unreliable estimations of occluded keypoints. When occlusion occurs, we have incomplete 2D keypoints and feed them to our 2D and 3D temporal convolutional networks (2D and 3D TCNs) that enforce temporal smoothness to produce a complete 3D pose. By using incomplete 2D keypoints, instead of complete but incorrect ones, our networks are less affected by the error-prone estimations of occluded keypoints. Training the occlusion-aware 3D TCN requires pairs of a 3D pose and a 2D pose with occlusion labels. As no such a dataset is available, we introduce a "Cylinder Man Model" to approximate the occupation of body parts in 3D space. By projecting the model onto a 2D plane in different viewing angles, we obtain and label the occluded keypoints, providing us plenty of training data. In addition, we use this model to create a pose regularization constraint, preferring the 2D estimations of unreliable keypoints to be occluded. Our method outperforms state-of-the-art methods on Human 3.6M and HumanEva-I datasets.
Context-Aware Feature and Label Fusion for Facial Action Unit Intensity Estimation With Partially Labeled Data

Facial action unit (AU) intensity estimation is a fundamental task for facial behaviour analysis. Most previous methods use a whole face image as input for intensity prediction. Considering that AUs are defined according to their corresponding local appearance, a few patch-based methods utilize image features of local patches. However, fusion of local features is always performed via straightforward feature concatenation or summation. Besides, these methods require fully annotated databases for model learning, which is expensive to acquire. In this paper, we propose a novel weakly supervised patch-based deep model on basis of two types of attention mechanisms for joint intensity estimation of multiple AUs. The model consists of a feature fusion module and a label fusion module. And we augment attention mechanisms of these two modules with a learnable task-related context, as one patch may play different roles in analyzing different AUs and each AU has its own temporal evolution rule. The context-aware feature fusion module is used to capture spatial relationships among local patches while the context-aware label fusion module is used to capture the temporal dynamics of AUs. The latter enables the model to be trained on a partially annotated database. Experimental evaluations on two benchmark expression databases demonstrate the superior performance of the proposed method.
Distill Knowledge From NRSfM for Weakly Supervised 3D Pose Learning

We propose to learn a 3D pose estimator by distilling knowledge from Non-Rigid Structure from Motion (NRSfM). Our method uses solely 2D landmark annotations. No 3D data, multi-view/temporal footage, or object specific prior is required. This alleviates the data bottleneck, which is one of the major concern for supervised methods. The challenge for using NRSfM as teacher is that they often make poor depth reconstruction when the 2D projections have strong ambiguity. Directly using those wrong depth as hard target would negatively impact the student. Instead, we propose a novel loss that ties depth prediction to the cost function used in NRSfM. This gives the student pose estimator freedom to reduce depth error by associating with image features. Validated on H3.6M dataset, our learned 3D pose estimation network achieves more accurate reconstruction compared to NRSfM methods. It also outperforms other weakly supervised methods, in spite of using significantly less supervision.
MONET: Multiview Semi-Supervised Keypoint Detection via Epipolar Divergence

This paper presents MONET---an end-to-end semi-supervised learning framework for a keypoint detector using multiview image streams. In particular, we consider general subjects such as non-human species where attaining a large scale annotated dataset is challenging. While multiview geometry can be used to self-supervise the unlabeled data, integrating the geometry into learning a keypoint detector is challenging due to representation mismatch. We address this mismatch by formulating a new differentiable representation of the epipolar constraint called epipolar divergence---a generalized distance from the epipolar lines to the corresponding keypoint distribution. Epipolar divergence characterizes when two view keypoint distributions produce zero reprojection error. We design a twin network that minimizes the epipolar divergence through stereo rectification that can significantly alleviate computational complexity and sampling aliasing in training. We demonstrate that our framework can localize customized keypoints of diverse species, e.g., humans, dogs, and monkeys.
Talking With Hands 16.2M: A Large-Scale Dataset of Synchronized Body-Finger Motion and Audio for Conversational Motion Analysis and Synthesis

We present a 16.2-million frame (50-hour) multimodal dataset of two-person face-to-face spontaneous conversations. Our dataset features synchronized body and finger motion as well as audio data. To the best of our knowledge, it represents the largest motion capture and audio dataset of natural conversations to date. The statistical analysis verifies strong intraperson and interperson covariance of arm, hand, and speech features, potentially enabling new directions on data-driven social behavior analysis, prediction, and synthesis. As an illustration, we propose a novel real-time finger motion synthesis method: a temporal neural network innovatively trained with an inverse kinematics (IK) loss, which adds skeletal structural information to the generative model. Our qualitative user study shows that the finger motion generated by our method is perceived as natural and conversation enhancing, while the quantitative ablation study demonstrates the effectiveness of IK loss.
Occlusion Robust Face Recognition Based on Mask Learning With Pairwise Differential Siamese Network

 Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of face recognition over past years. However, existing CNN models are far less accurate when handling partially occluded faces. These general face models generalize poorly for occlusions on variable facial areas. Inspired by the fact that human visual system explicitly ignores the occlusion and only focuses on the non-occluded facial areas, we propose a mask learning strategy to find and discard corrupted feature elements from recognition. A mask dictionary is firstly established by exploiting the differences between the top conv features of occluded and occlusion-free face pairs using innovatively designed pairwise differential siamese network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements from recognition. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed algorithm significantly outperforms the state-of-the-art systems.
Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection

Facial landmark detection aims to localize the anatomically defined points of human faces. In this paper, we study facial landmark detection from partially labeled facial images. A typical approach is to (1) train a detector on the labeled images; (2) generate new training samples using this detector's prediction as pseudo labels of unlabeled images; (3) retrain the detector on the labeled samples and partial pseudo labeled samples. In this way, the detector can learn from both labeled and unlabeled data and become robust. In this paper, we propose an interaction mechanism between a teacher and two students to generate more reliable pseudo labels for unlabeled data, which are beneficial to semi-supervised facial landmark detection. Specifically, the two students are instantiated as dual detectors. The teacher learns to judge the quality of the pseudo labels generated by the students and filter out unqualified samples before the retraining stage. In this way, the student detectors get feedback from their teacher and are retrained by premium data generated by itself. Since the two students are trained by different samples, a combination of their predictions will be more robust as the final prediction compared to either prediction. Extensive experiments on 300-W and AFLW benchmarks show that the interactions between teacher and students contribute to better utilization of the unlabeled data and achieves state-of-the-art performance.
A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation From a Single Depth Image

For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-to-Joint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet- 50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J's superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU.
TexturePose: Supervising Human Mesh Estimation With Texture Consistency

This work addresses the problem of model-based human pose estimation. Recent approaches have made significant progress towards regressing the parameters of parametric human body models directly from images. Because of the absence of images with 3D shape ground truth, relevant approaches rely on 2D annotations or sophisticated architecture designs. In this work, we advocate that there are more cues we can leverage, which are available for free in natural images, i.e., without getting more annotations, or modifying the network architecture. We propose a natural form of supervision, that capitalizes on the appearance constancy of a person among different frames (or viewpoints). This seemingly insignificant and often overlooked cue goes a long way for model-based pose estimation. The parametric model we employ allows us to compute a texture map for each frame. Assuming that the texture of the person does not change dramatically between frames, we can apply a novel texture consistency loss, which enforces that each point in the texture map has the same texture value across all frames. Since the texture is transferred in this common texture map space, no camera motion computation is necessary, or even an assumption of smoothness among frames. This makes our proposed supervision applicable in a variety of settings, ranging from monocular video, to multi-view images. We benchmark our approach against strong baselines that require the same or even more annotations that we do and we consistently outperform them. Simultaneously, we achieve state-of-the-art results among model-based pose estimation approaches in different benchmarks. The project website with videos, results, and code can be found at https://seas.upenn.edu/ pavlakos/projects/texturepose.
FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images

Estimating 3D hand pose from single RGB images is a highly ambiguous problem that relies on an unbiased training dataset. In this paper, we analyze cross-dataset generalization when training on existing datasets. We find that approaches perform well on the datasets they are trained on, but do not generalize to other datasets or in-the-wild scenarios. As a consequence, we introduce the first large-scale, multi-view hand dataset that is accompanied by both 3D hand pose and shape annotations. For annotating this real-world dataset, we propose an iterative, semi-automated `human-in-the-loop' approach, which includes hand fitting optimization to infer both the 3D pose and shape for each sample. We show that methods trained on our dataset consistently perform well when tested on other datasets. Moreover, the dataset allows us to train a network that predicts the full articulated hand shape from a single RGB image. The evaluation set can serve as a benchmark for articulated hand shape estimation. 
Markerless Outdoor Human Motion Capture Using Multiple Autonomous Micro Aerial Vehicles

Capturing human motion in natural scenarios means moving motion capture out of the lab and into the wild. Typical approaches rely on fixed, calibrated, cameras and reflective markers on the body, significantly limiting the motions that can be captured. To make motion capture truly unconstrained, we describe the first fully autonomous outdoor capture system based on flying vehicles. We use multiple micro-aerial-vehicles(MAVs), each equipped with a monocular RGB camera, an IMU, and a GPS receiver module. These detect the person, optimize their position, and localize themselves approximately. We then develop a markerless motion capture method that is suitable for this challenging scenario with a distant subject, viewed from above, with approximately calibrated and moving cameras. We combine multiple state-of-the-art 2D joint detectors with a 3D human body model and a powerful prior on human pose. We jointly optimize for 3D body pose and camera pose to robustly fit the 2D measurements. To our knowledge, this is the first successful demonstration of outdoor, full-body, markerless motion capture from autonomous flying vehicles.
Toyota Smarthome: Real-World Activities of Daily Living

The performance of deep neural networks is strongly influenced by the quantity and quality of annotated data. Most of the large activity recognition datasets consist of data sourced from the web, which does not reflect challenges that exist in activities of daily living. In this paper, we introduce a large real-world video dataset for activities of daily living: Toyota Smarthome. The dataset consists of 16K RGB+D clips of 31 activity classes, performed by seniors in a smarthome. Unlike previous datasets, videos were fully unscripted. As a result, the dataset poses several challenges: high intra-class variation, high class imbalance, simple and composite activities, and activities with similar motion and variable duration. Activities were annotated with both coarse and fine-grained labels. These characteristics differentiate Toyota Smarthome from other datasets for activity recognition. As recent activity recognition approaches fail to address the challenges posed by Toyota Smarthome, we present a novel activity recognition method with attention mechanism. We propose a pose driven spatio-temporal attention mechanism through 3D ConvNets. We show that our novel method outperforms state-of-the-art methods on benchmark datasets, as well as on the Toyota Smarthome dataset. We release the dataset for research use.
Relation Parsing Neural Network for Human-Object Interaction Detection

Human-Object Interaction Detection devotes to infer a triplet < human, verb, object > between human and objects. In this paper, we propose a novel model, i.e., Relation Parsing Neural Network (RPNN), to detect human-object interactions. Specifically, the network is represented by two graphs, i.e., Object-Bodypart Graph and Human-Bodypart Graph. Here, the Object-Bodypart Graph dynamically captures the relationship between body parts and the surrounding objects. The Human-Bodypart Graph infers the relationship between human and body parts, and assembles body part contexts to predict actions. These two graphs are associated through an action passing mechanism. The proposed RPNN model is able to implicitly parse a pairwise relation in two graphs without supervised labels. Experiments conducted on V-COCO and HICO-DET datasets confirm the effectiveness of the proposed RPNN network which significantly outperforms state-of-the-art methods. 
DistInit: Learning Video Representations Without a Single Labeled Video

Video recognition models have progressed significantly over the past few years, evolving from shallow classifiers trained on hand-crafted features to deep spatiotemporal networks. However, labeled video data required to train such models has not been able to keep up with the ever increasing depth and sophistication of these networks. In this work we propose an alternative approach to learning video representations that requires no semantically labeled videos, and instead leverages the years of effort in collecting and labeling large and clean still-image datasets. We do so by using state-of-the-art models pre-trained on image datasets as "teachers" to train video models in a distillation framework. We demonstrate that our method learns truly spatiotemporal features, despite being trained only using supervision from still-image networks. Moreover, it learns good representations across different input modalities, using completely uncurated raw video data sources and with different 2D teacher models. Our method obtains strong transfer performance, outperforming standard techniques for bootstrapping video architectures with image based models by 16%. We believe that our approach opens up new approaches for learning spatiotemporal representations from unlabeled video data.
Zero-Shot Anticipation for Instructional Activities

How can we teach a robot to predict what will happen next for an activity it has never seen before? We address the problem of zero-shot anticipation by presenting a hierarchical model that generalizes instructional knowledge from large-scale text-corpora and transfers the knowledge to the visual domain. Given a portion of an instructional video, our model predicts coherent and plausible actions multiple steps into the future, all in rich natural language. To demonstrate the anticipation capabilities of our model, we introduce the Tasty Videos dataset, a collection of 2511 recipes for zero-shot learning, recognition and anticipation.
Making the Invisible Visible: Action Recognition Through Walls and Occlusions

Understanding people's actions and interactions typically depends on seeing them. Automating the process of action recognition from visual data has been the topic of much research in the computer vision community. But what if it is too dark, or if the person is occluded or behind a wall? In this paper, we introduce a neural network model that can detect human actions through walls and occlusions, and in poor lighting conditions. Our model takes radio frequency (RF) signals as input, generates 3D human skeletons as an intermediate representation, and recognizes actions and interactions of multiple people over time. By translating the input to an intermediate skeleton-based representation, our model can learn from both vision-based and RF-based datasets, and allow the two tasks to help each other. We show that our model achieves comparable accuracy to vision-based action recognition systems in visible scenarios, yet continues to work accurately when people are not visible, hence addressing scenarios that are beyond the limit of today's vision-based action recognition. 
Recursive Visual Sound Separation Using Minus-Plus Net

Sounds provide rich semantics, complementary to visual data, for many tasks. However, in practice, sounds from multiple sources are often mixed together. In this paper we propose a novel framework, referred to as MinusPlus Network (MP-Net), for the task of visual sound separation. MP-Net separates sounds recursively in the order of average energy, removing the separated sound from the mixture at the end of each prediction, until the mixture becomes empty or contains only noise. In this way, MP-Net could be applied to sound mixtures with arbitrary numbers and types of sounds. Moreover, while MP-Net keeps removing sounds with large energy from the mixture, sounds with small energy could emerge and become clearer, so that the separation is more accurate. Compared to previous methods, MP-Net obtains state-of-the-art results on two large scale datasets, across mixtures with different types and numbers of sounds.
Unsupervised Video Interpolation Using Cycle Consistency

Learning to synthesize high frame rate videos via interpolation requires large quantities of high frame rate training videos, which, however, are scarce, especially at high resolutions. Here, we propose unsupervised techniques to synthesize high frame rate videos directly from low frame rate videos using cycle consistency. For a triplet of consecutive frames, we optimize models to minimize the discrepancy between the center frame and its cycle reconstruction, obtained by interpolating back from interpolated intermediate frames. This simple unsupervised constraint alone achieves results comparable with supervision using the ground truth intermediate frames. We further introduce a pseudo supervised loss term that enforces the interpolated frames to be consistent with predictions of a pre-trained interpolation model. The pseudo supervised loss term, used together with cycle consistency, can effectively adapt a pre-trained model to a new target domain. With no additional data and in a completely unsupervised fashion, our techniques significantly improve pre-trained models on new target domains, increasing PSNR values from 32.84dB to 33.05dB on the Slowflow and from 31.82dB to 32.53dB on the Sintel evaluation datasets.
Deformable Surface Tracking by Graph Matching

This paper addresses the problem of deformable surface tracking from monocular images. Specifically, we propose a graph-based approach that effectively explores the structure information of the surface to enhance tracking performance. Our approach solves simultaneously for feature correspondence, outlier rejection and shape reconstruction by optimizing a single objective function, which is defined by means of pairwise projection errors between graph structures instead of unary projection errors between matched points. Furthermore, an efficient matching algorithm is developed based on soft matching relaxation. For evaluation, our approach is extensively compared to state-of-the-art algorithms on a standard dataset of occluded surfaces, as well as a newly compiled dataset of different surfaces with rich, weak or repetitive texture. Experimental results reveal that our approach achieves robust tracking results for surfaces with different types of texture, and outperforms other algorithms in both accuracy and efficiency.
Deep Meta Learning for Real-Time Target-Aware Visual Tracking

In this paper, we propose a novel on-line visual tracking framework based on the Siamese matching network and meta-learner network, which run at real-time speeds. Conventional deep convolutional feature-based discriminative visual tracking algorithms require continuous re-training of classifiers or correlation filters, which involve solving complex optimization tasks to adapt to the new appearance of a target object. To alleviate this complex process, our proposed algorithm incorporates and utilizes a meta-learner network to provide the matching network with new appearance information of the target objects by adding target-aware feature space. The parameters for the target-specific feature space are provided instantly from a single forward-pass of the meta-learner network. By eliminating the necessity of continuously solving complex optimization tasks in the course of tracking, experimental results demonstrate that our algorithm performs at a real-time speed while maintaining competitive performance among other state-of-the-art tracking algorithms.
Looking to Relations for Future Trajectory Forecast

Inferring relational behavior between road users as well as road users and their surrounding physical space is an important step toward effective modeling and prediction of navigation strategies adopted by participants in road scenes. To this end, we propose a relation-aware framework for future trajectory forecast. Our system aims to infer relational information from the interactions of road users with each other and with the environment. The first module involves visual encoding of spatio-temporal features, which captures human-human and human-space interactions over time. The following module explicitly constructs pair-wise relations from spatio-temporal interactions and identifies more descriptive relations that highly influence future motion of the target road user by considering its past trajectory. The resulting relational features are used to forecast future locations of the target, in the form of heatmaps with an additional guidance of spatial dependencies and consideration of the uncertainty. Extensive evaluations on the public benchmark datasets demonstrate the robustness and efficacy of the proposed framework as observed by performances higher than the state-of-the-art methods.
Anchor Diffusion for Unsupervised Video Object Segmentation

Unsupervised video object segmentation has often been tackled by methods based on recurrent neural networks and optical flow. Despite their complexity, these kinds of approach tend to favour short-term temporal dependencies and are thus prone to accumulating inaccuracies, which cause drift over time. Moreover, simple (static) image segmentation models, alone, can perform competitively against these methods, which further suggests that the way temporal dependencies are modelled should be reconsidered. Motivated by these observations, in this paper we explore simple yet effective strategies to model long-term temporal dependencies. Inspired by the non-local operators, we introduce a technique to establish dense correspondences between pixel embeddings of a reference "anchor" frame and the current one. This allows the learning of pairwise dependencies at arbitrarily long distances without conditioning on intermediate frames. Without online supervision, our approach can suppress the background and precisely segment the foreground object even in challenging scenarios, while maintaining consistent performance over time. With a mean IoU of 81.7%, our method ranks first on the DAVIS-2016 leaderboard of unsupervised methods, while still being competitive against state-of-the-art online semi-supervised approaches. We further evaluate our method on the FBMS dataset and the video saliency dataset ViSal, showing results competitive with the state of the art.
Tracking Without Bells and Whistles

The problem of tracking multiple objects in a video sequence poses several challenging tasks. For tracking-by-detection, these include object re-identification, motion prediction and dealing with occlusions. We present a tracker (without bells and whistles) that accomplishes tracking without specifically targeting any of these tasks, in particular, we perform no training or optimization on tracking data. To this end, we exploit the bounding box regression of an object detector to predict the position of an object in the next frame, thereby converting a detector into a Tracktor. We demonstrate the potential of Tracktor and provide a new state-of-the-art on three multi-object tracking benchmarks by extending it with a straightforward re-identification and camera motion compensation. We then perform an analysis on the performance and failure cases of several state-of-the-art tracking methods in comparison to our Tracktor. Surprisingly, none of the dedicated tracking methods are considerably better in dealing with complex tracking scenarios, namely, small and occluded objects or missing detections. However, our approach tackles most of the easy tracking scenarios. Therefore, we motivate our approach as a new tracking paradigm and point out promising future research directions. Overall, Tracktor yields superior tracking performance than any current tracking method and our analysis exposes remaining and unsolved tracking challenges to inspire future research directions.
Perspective-Guided Convolution Networks for Crowd Counting

In this paper, we propose a novel perspective-guided convolution (PGC) for convolutional neural network (CNN) based crowd counting (i.e. PGCNet), which aims to overcome the dramatic intra-scene scale variations of people due to the perspective effect. While most state-of-the-arts adopt multi-scale or multi-column architectures to address such issue, they generally fail in modeling continuous scale variations since only discrete representative scales are considered. PGCNet, on the other hand, utilizes perspective information to guide the spatially variant smoothing of feature maps before feeding them to the successive convolutions. An effective perspective estimation branch is also introduced to PGCNet, which can be trained in either supervised setting or weakly-supervised setting when the branch has been pre-trained. Our PGCNet is single-column with moderate increase in computation, and extensive experimental results on four benchmark datasets show the improvements of our method against the state-of-the-arts. Additionally, we also introduce Crowd Surveillance, a large scale dataset for crowd counting that contains 13,000+ high-resolution images with challenging scenarios. Code is available at https://github.com/Zhaoyi-Yan/PGCNet.
End-to-End Wireframe Parsing

We present a conceptually simple yet effective algorithm to detect wireframes in a given image. Compared to the previous methods which first predict an intermediate heat map and then extract straight lines with heuristic algorithms, our method is end-to-end trainable and can directly output a vectorized wireframe that contains semantically meaningful and geometrically salient junctions and lines. To better understand the quality of the outputs, we propose a new metric for wireframe evaluation that penalizes overlapped line segments and incorrect line connectivities. We conduct extensive experiments and show that our method significantly outperforms the previous state-of-the-art wireframe and line extraction algorithms. We hope our simple approach can be served as a baseline for future wireframe parsing studies. Code has been made publicly available at https://github.com/zhou13/lcnn.
Incremental Class Discovery for Semantic Segmentation With RGBD Sensing

We present a conceptually simple yet effective algorithm to detect wireframes in a given image. Compared to the previous methods which first predict an intermediate heat map and then extract straight lines with heuristic algorithms, our method is end-to-end trainable and can directly output a vectorized wireframe that contains semantically meaningful and geometrically salient junctions and lines. To better understand the quality of the outputs, we propose a new metric for wireframe evaluation that penalizes overlapped line segments and incorrect line connectivities. We conduct extensive experiments and show that our method significantly outperforms the previous state-of-the-art wireframe and line extraction algorithms. We hope our simple approach can be served as a baseline for future wireframe parsing studies. Code has been made publicly available at https://github.com/zhou13/lcnn.
SSF-DAN: Separated Semantic Feature Based Domain Adaptation Network for Semantic Segmentation

Despite the great success achieved by supervised fully convolutional models in semantic segmentation, training the models requires a large amount of labor-intensive work to generate pixel-level annotations. Recent works exploit synthetic data to train the model for semantic segmentation, but the domain adaptation between real and synthetic images remains a challenging problem. In this work, we propose a Separated Semantic Feature based domain adaptation network, named SSF-DAN, for semantic segmentation. First, a Semantic-wise Separable Discriminator (SS-D) is designed to independently adapt semantic features across the target and source domains, which addresses the inconsistent adaptation issue in the class-wise adversarial learning. In SS-D, a progressive confidence strategy is included to achieve a more reliable separation. Then, an efficient Class-wise Adversarial loss Reweighting module (CA-R) is introduced to balance the class-wise adversarial learning process, which leads the generator to focus more on poorly adapted classes. The presented framework demonstrates robust performance, superior to state-of-the-art methods on benchmark datasets.
SpaceNet MVOI: A Multi-View Overhead Imagery Dataset

Detection and segmentation of objects in overheard imagery is a challenging task. The variable density, random orientation, small size, and instance-to-instance heterogeneity of objects in overhead imagery calls for approaches distinct from existing models designed for natural scene datasets. Though new overhead imagery datasets are being developed, they almost universally comprise a single view taken from directly overhead ("at nadir"), failing to address a critical variable: look angle. By contrast, views vary in real-world overhead imagery, particularly in dynamic scenarios such as natural disasters where first looks are often over 40 degrees off-nadir. This represents an important challenge to computer vision methods, as changing view angle adds distortions, alters resolution, and changes lighting. At present, the impact of these perturbations for algorithmic detection and segmentation of objects is untested. To address this problem, we present an open source Multi-View Overhead Imagery dataset, termed SpaceNet MVOI, with 27 unique looks from a broad range of viewing angles (-32.5 degrees to 54.0 degrees). Each of these images cover the same 665 square km geographic extent and are annotated with 126,747 building footprint labels, enabling direct assessment of the impact of viewpoint perturbation on model performance. We benchmark multiple leading segmentation and object detection models on: (1) building detection, (2) generalization to unseen viewing angles and resolutions, and (3) sensitivity of building footprint extraction to changes in resolution. We find that state of the art segmentation and object detection models struggle to identify buildings in off-nadir imagery and generalize poorly to unseen views, presenting an important benchmark to explore the broadly relevant challenge of detecting small, heterogeneous target objects in visually dynamic contexts. 
Multi-Level Bottom-Top and Top-Bottom Feature Fusion for Crowd Counting

Crowd counting presents enormous challenges in the form of large variation in scales within images and across the dataset. These issues are further exacerbated in highly congested scenes. Approaches based on straightforward fusion of multi-scale features from a deep network seem to be obvious solutions to this problem. However, these fusion approaches do not yield significant improvements in the case of crowd counting in congested scenes. This is usually due to their limited abilities in effectively combining the multi-scale features for problems like crowd counting. To overcome this, we focus on how to efficiently leverage information present in different layers of the network. Specifically, we present a network that involves: (i) a multi-level bottom-top and top-bottom fusion (MBTTBF) method to combine information from shallower to deeper layers and vice versa at multiple levels, (ii) scale complementary feature extraction blocks (SCFB) involving cross-scale residual functions to explicitly enable flow of complementary features from adjacent conv layers along the fusion paths. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we employ a principled way of generating scale-aware ground-truth density maps for training. Experiments conducted on three datasets that contain highly congested scenes (ShanghaiTech, UCF_CC_50, and UCF-QNRF) demonstrate that the proposed method is able to outperform several recent methods in all the datasets
Learning Lightweight Lane Detection CNNs by Self Attention Distillation

Training deep models for lane detection is challenging due to the very subtle and sparse supervisory signals inherent in lane annotations. Without learning from much richer context, these models often fail in challenging scenarios, e.g., severe occlusion, ambiguous lanes, and poor lighting conditions. In this paper, we present a novel knowledge distillation approach, i.e., Self Attention Distillation (SAD), which allows a model to learn from itself and gains substantial improvement without any additional supervision or labels. Specifically, we observe that attention maps extracted from a model trained to a reasonable level would encode rich contextual information. The valuable contextual information can be used as a form of 'free' supervision for further representation learning through performing top- down and layer-wise attention distillation within the net- work itself. SAD can be easily incorporated in any feed- forward convolutional neural networks (CNN) and does not increase the inference time. We validate SAD on three popular lane detection benchmarks (TuSimple, CULane and BDD100K) using lightweight models such as ENet, ResNet- 18 and ResNet-34. The lightest model, ENet-SAD, performs comparatively or even surpasses existing algorithms. Notably, ENet-SAD has 20 x fewer parameters and runs 10 x faster compared to the state-of-the-art SCNN, while still achieving compelling performance in all benchmarks.
SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation

We propose SplitNet, a method for decoupling visual perception and policy learning. By incorporating auxiliary tasks and selective learning of portions of the model, we explicitly decompose the learning objectives for visual navigation into perceiving the world and acting on that perception. We show improvements over baseline models on transferring between simulators, an encouraging step towards Sim2Real. Additionally, SplitNet generalizes better to unseen environments from the same simulator and transfers faster and more effectively to novel embodied navigation tasks. Further, given only a small sample from a target domain, SplitNet can match the performance of traditional end-to-end pipelines which receive the entire dataset
Cascaded Parallel Filtering for Memory-Efficient Image-Based Localization

Image-based localization (IBL) aims to estimate the 6DOF camera pose for a given query image. The camera pose can be computed from 2D-3D matches between a query image and Structure-from-Motion (SfM) models. Despite recent advances in IBL, it remains difficult to simultaneously resolve the memory consumption and match ambiguity problems of large SfM models. In this work, we propose a cascaded parallel filtering method that leverages the feature, visibility and geometry information to filter wrong matches under binary feature representation. The core idea is that we divide the challenging filtering task into two parallel tasks before deriving an auxiliary camera pose for final filtering. One task focuses on preserving potentially correct matches, while another focuses on obtaining high quality matches to facilitate subsequent more powerful filtering. Moreover, our proposed method improves the localization accuracy by introducing a quality-aware spatial reconfiguration method and a principal focal length enhanced pose estimation method. Experimental results on real-world datasets demonstrate that our method achieves very competitive localization performances in a memory-efficient manner.
Pixel2Mesh++: Multi-View 3D Mesh Generation via Deformation

We study the problem of shape generation in 3D mesh representation from a few color images with known camera poses. While many previous works learn to hallucinate the shape directly from priors, we resort to further improving the shape quality by leveraging cross-view information with a graph convolutional network. Instead of building a direct mapping function from images to 3D shape, our model learns to predict series of deformations to improve a coarse shape iteratively. Inspired by traditional multiple view geometry methods, our network samples nearby area around the initial mesh's vertex locations and reasons an optimal deformation using perceptual feature statistics built from multiple input images. Extensive experiments show that our model produces accurate 3D shape that are not only visually plausible from the input perspectives, but also well aligned to arbitrary viewpoints. With the help of physically driven architecture, our model also exhibits generalization capability across different semantic categories, number of input images, and quality of mesh initialization.
A Differential Volumetric Approach to Multi-View Photometric Stereo

Highly accurate 3D volumetric reconstruction is still an open research topic where the main difficulty is usually related to merging some rough estimations with high frequency details. One of the most promising methods is the fusion between multi-view stereo and photometric stereo images. Beside the intrinsic difficulties that multi-view stereo and photometric stereo in order to work reliably, supplementary problems arise when considered together. In this work, we present a volumetric approach to the multi-view photometric stereo problem. The key point of our method is the signed distance field parameterisation and its relation to the surface normal. This is exploited in order to obtain a linear partial differential equation which is solved in a variational framework, that combines multiple images from multiple points of view in a single system. In addition, the volumetric approach is naturally implemented on an octree, which allows for fast ray-tracing that reliably alleviates occlusions and cast shadows. Our approach is evaluated on synthetic and real data-sets and achieves state-of-the-art results.
Revisiting Radial Distortion Absolute Pose

To model radial distortion there are two main approaches; either the image points are undistorted such that they correspond to pinhole projections, or the pinhole projections are distorted such that they align with the image measurements. Depending on the application, either of the two approaches can be more suitable. For example, distortion models are commonly used in Structure-from-Motion since they simplify measuring the reprojection error in images. Surprisingly, all previous minimal solvers for pose estimation with radial distortion use undistortion models. In this paper we aim to fill this gap in the literature by proposing the first minimal solvers which can jointly estimate distortion models together with camera pose. We present a general approach which can handle rational models of arbitrary degree for both distortion and undistortion.
Estimating the Fundamental Matrix Without Point Correspondences With Application to Transmission Imaging

We present a general method to estimate the fundamental matrix from a pair of images under perspective projection without the need for image point correspondences. Our method is particularly well-suited for transmission imaging, where state-of-the-art feature detection and matching approaches generally do not perform well. Estimation of the fundamental matrix plays a central role in auto-calibration methods for reflection imaging. Such methods are currently not applicable to transmission imaging. Furthermore, our method extends an existing technique proposed for reflection imaging which potentially avoids the outlier-prone feature matching step from an orthographic projection model to a perspective model. Our method exploits the idea that under a linear attenuation model line integrals along corresponding epipolar lines are equal if we compute their derivatives in orthogonal direction to their common epipolar plane. We use the fundamental matrix to parametrize this equality. Our method estimates the matrix by formulating a non-convex optimization problem, minimizing an error in our measurement of this equality. We believe this technique will enable the application of the large body of work on image-based camera pose estimation to transmission imaging leading to more accurate and more general motion compensation and auto-calibration algorithms, particularly in medical X-ray and Computed Tomography imaging.
QUARCH: A New Quasi-Affine Reconstruction Stratum From Vague Relative Camera Orientation Knowledge

We present a new quasi-affine reconstruction of a scene and its application to camera self-calibration. We refer to this reconstruction as QUARCH (QUasi-Affine Reconstruction with respect to Camera centers and the Hodographs of horopters). A QUARCH can be obtained by solving a semidefinite programming problem when, (i) the images have been captured by a moving camera with constant intrinsic parameters, and (ii) a vague knowledge of the relative orientation (under or over 120 degrees) between camera pairs is available. The resulting reconstruction comes close enough to an affine one allowing thus an easy upgrade of the QUARCH to its affine and metric counterparts. We also present a constrained Levenberg-Marquardt method for nonlinear optimization subject to Linear Matrix Inequality (LMI) constraints so as to ensure that the QUARCH LMIs are satisfied during optimization. Experiments with synthetic and real data show the benefits of QUARCH in reliably obtaining a metric reconstruction.
Homography From Two Orientation- and Scale-Covariant Features

This paper proposes a geometric interpretation of the angles and scales which the orientation- and scale-covariant feature detectors, e.g. SIFT, provide. Two new general constraints are derived on the scales and rotations which can be used in any geometric model estimation tasks. Using these formulas, two new constraints on homography estimation are introduced. Exploiting the derived equations, a solver for estimating the homography from the minimal number of two correspondences is proposed. Also, it is shown how the normalization of the point correspondences affects the rotation and scale parameters, thus achieving numerically stable results. Due to requiring merely two feature pairs, robust estimators, e.g. RANSAC, do significantly fewer iterations than by using the four-point algorithm. When using covariant features, e.g. SIFT, this additional information is given at no cost. The method is tested in a synthetic environment and on publicly available real-world datasets. 
Hiding Video in Audio via Reversible Generative Models

We present a method for hiding video content inside audio files while preserving the perceptual fidelity of the cover audio. This is a form of cross-modal steganography and is particularly challenging due to the high bitrate of video. Our scheme uses recent advances in flow-based generative models, which enable mapping audio to latent codes such that nearby codes correspond to perceptually similar signals. We show that compressed video data can be concealed in the latent codes of audio sequences while preserving the fidelity of both the hidden video and the cover audio. We can embed 128x128 video inside same-duration audio, or higher-resolution video inside longer audio sequences. Quantitative experiments show that our approach outperforms relevant baselines in steganographic capacity and fidelity.
GSLAM: A General SLAM Framework and Benchmark

SLAM technology has recently seen many successes and attracted the attention of high-technological companies. However, how to unify the interface of existing or emerging algorithms, and effectively perform benchmark about the speed, robustness and portability are still problems. In this paper, we propose a novel SLAM platform named GSLAM, which not only provides evaluation functionality, but also supplies useful toolkit for researchers to quickly develop their SLAM systems. Our core contribution is an universal, cross-platform and full open-source SLAM interface for both research and commercial usage, which is aimed to handle interactions with input dataset, SLAM implementation, visualization and applications in an unified framework. Through this platform, users can implement their own functions for better performance with plugin form and further boost the application to practical usage of the SLAM.
Elaborate Monocular Point and Line SLAM With Robust Initialization

SLAM technology has recently seen many successes and attracted the attention of high-technological companies. However, how to unify the interface of existing or emerging algorithms, and effectively perform benchmark about the speed, robustness and portability are still problems. In this paper, we propose a novel SLAM platform named GSLAM, which not only provides evaluation functionality, but also supplies useful toolkit for researchers to quickly develop their SLAM systems. Our core contribution is an universal, cross-platform and full open-source SLAM interface for both research and commercial usage, which is aimed to handle interactions with input dataset, SLAM implementation, visualization and applications in an unified framework. Through this platform, users can implement their own functions for better performance with plugin form and further boost the application to practical usage of the SLAM.
Adaptive Density Map Generation for Crowd Counting

Crowd counting is an important topic in computer vision due to its practical usage in surveillance systems. The typical design of crowd counting algorithms is divided into two steps. First, the ground-truth density maps of crowd images are generated from the ground-truth dot maps (density map generation), e.g., by convolving with a Gaussian kernel. Second, deep learning models are designed to predict a density map from an input image (density map estimation). Most research efforts have concentrated on the density map estimation problem, while the problem of density map generation has not been adequately explored. In particular, the density map could be considered as an intermediate representation used to train a crowd counting network. In the sense of end-to-end training, the hand-crafted methods used for generating the density maps may not be optimal for the particular network or dataset used. To address this issue, we first show the impact of different density maps and that better ground-truth density maps can be obtained by refining the existing ones using a learned refinement network, which is jointly trained with the counter. Then, we propose an adaptive density map generator, which takes the annotation dot map as input, and learns a density map representation for a counter. The counter and generator are trained jointly within an end-to-end framework. The experiment results on popular counting datasets confirm the effectiveness of the proposed learnable density map representations.
Attention-Aware Polarity Sensitive Embedding for Affective Image Retrieval

Images play a crucial role for people to express their opinions online due to the increasing popularity of social networks. While an affective image retrieval system is useful for obtaining visual contents with desired emotions from a massive repository, the abstract and subjective characteristics make the task challenging. To address the problem, this paper introduces an Attention-aware Polarity Sensitive Embedding (APSE) network to learn affective representations in an end-to-end manner. First, to automatically discover and model the informative regions of interest, we develop a hierarchical attention mechanism, in which both polarity- and emotion-specific attended representations are aggregated for discriminative feature embedding. Second, we present a weighted emotion-pair loss to take the inter- and intra-polarity relationships of the emotional labels into consideration. Guided by attention module, we weight the sample pairs adaptively which further improves the performance of feature embedding. Extensive experiments on four popular benchmark datasets show that the proposed method performs favorably against the state-of-the-art approaches. 
Zero-Shot Emotion Recognition via Affective Structural Embedding

Image emotion recognition attracts much attention in recent years due to its wide applications. It aims to classify the emotional response of humans, where candidate emotion categories are generally defined by specific psychological theories, such as Ekman's six basic emotions. However, with the development of psychological theories, emotion categories become increasingly diverse, fine-grained, and difficult to collect samples. In this paper, we investigate zero-shot learning (ZSL) problem in the emotion recognition task, which tries to recognize the new unseen emotions. Specifically, we propose a novel affective-structural embedding framework, utilizing mid-level semantic representation, i.e., adjective-noun pairs (ANP) features, to construct an affective embedding space. By doing this, the learned intermediate space can narrow the semantic gap between low-level visual and high-level semantic features. In addition, we introduce an affective adversarial constraint to retain the discriminative capacity of visual features and the affective structural information of semantic features during training process. Our method is evaluated on five widely used affective datasets and the perimental results show the proposed algorithm outperforms the state-of-the-art approaches.
FW-GAN: Flow-Navigated Warping GAN for Video Virtual Try-On

Beyond current image-based virtual try-on systems that have attracted increasing attention, we move a step forward to developing a video virtual try-on system that precisely transfers clothes onto the person and generates visually realistic videos conditioned on arbitrary poses. Besides the challenges in image-based virtual try-on (e.g., clothes fidelity, image synthesis), video virtual try-on further requires spatiotemporal consistency. Directly adopting existing image-based approaches often fails to generate coherent video with natural and realistic textures. In this work, we propose Flow-navigated Warping Generative Adversarial Network (FW-GAN), a novel framework that learns to synthesize the video of virtual try-on based on a person image, the desired clothes image, and a series of target poses. FW-GAN aims to synthesize the coherent and natural video while manipulating the pose and clothes. It consists of: (i) a flow-guided fusion module that warps the past frames to assist synthesis, which is also adopted in the discriminator to help enhance the coherence and quality of the synthesized video; (ii) a warping net that is designed to warp clothes image for the refinement of clothes textures; (iii) a parsing constraint loss that alleviates the problem caused by the misalignment of segmentation maps from images with different poses and various clothes. Experiments on our newly collected dataset show that FW-GAN can synthesize high-quality video of virtual try-on and significantly outperforms other methods both qualitatively and quantitatively.
Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation

We propose an interactive GAN-based sketch-to-image translation method that helps novice users easily create images of simple objects. The user starts with a sparse sketch and a desired object category, and the network then recommends its plausible completion(s) and shows a corresponding synthesized image. This enables a feedback loop, where the user can edit the sketch based on the network's recommendations, while the network is able to better synthesize the image that the user might have in mind. In order to use a single model for a wide array of object classes, we introduce a gating-based approach for class conditioning, which allows us to generate distinct classes without feature mixing, from a single generator network.
Attention-Based Autism Spectrum Disorder Screening With Privileged Modality

We propose an interactive GAN-based sketch-to-image translation method that helps novice users easily create images of simple objects. The user starts with a sparse sketch and a desired object category, and the network then recommends its plausible completion(s) and shows a corresponding synthesized image. This enables a feedback loop, where the user can edit the sketch based on the network's recommendations, while the network is able to better synthesize the image that the user might have in mind. In order to use a single model for a wide array of object classes, we introduce a gating-based approach for class conditioning, which allows us to generate distinct classes without feature mixing, from a single generator network.
Image Aesthetic Assessment Based on Pairwise Comparison  A Unified Approach to Score Regression, Binary Classification, and Personalization

We propose a unified approach to three tasks of aesthetic score regression, binary aesthetic classification, and personalized aesthetics. First, we develop a comparator to estimate the ratio of aesthetic scores for two images. Then, we construct a pairwise comparison matrix for multiple reference images and an input image, and predict the aesthetic score of the input via the eigenvalue decomposition of the matrix. By varying the reference images, the proposed algorithm can be used for binary aesthetic classification and personalized aesthetics, as well as generic score regression. Experimental results demonstrate that the proposed unified algorithm provides the state-of-the-art performances in all three tasks of image aesthetics.
Delving Into Robust Object Detection From Unmanned Aerial Vehicles: A Deep Nuisance Disentanglement Approach

Object detection from images captured by Unmanned Aerial Vehicles (UAVs) is becoming increasingly useful. Despite the great success of the generic object detection methods trained on ground-to-ground images, a huge performance drop is observed when they are directly applied to images captured by UAVs. The unsatisfactory performance is owing to many UAV-specific nuisances, such as varying flying altitudes, adverse weather conditions, dynamically changing viewing angles, etc. Those nuisances constitute a large number of fine-grained domains, across which the detection model has to stay robust. Fortunately, UAVs will record meta-data that depict those varying attributes, which are either freely available along with the UAV images, or can be easily obtained. We propose to utilize those free meta-data in conjunction with associated UAV images to learn domain-robust features via an adversarial training framework dubbed Nuisance Disentangled Feature Transform (NDFT), for the specific challenging problem of object detection in UAV images, achieving a substantial gain in robustness to those nuisances. We demonstrate the effectiveness of our proposed algorithm, by showing state-of-the- art performance (single model) on two existing UAV-based object detection benchmarks. The code is available at https://github.com/TAMU-VITA/UAV-NDFT.
Bit-Flip Attack: Crushing Neural Network With Progressive Bit Search

Several important security issues of Deep Neural Network (DNN) have been raised recently associated with different applications and components. The most widely investigated security concern of DNN is from its malicious input, a.k.a adversarial example. Nevertheless, the security challenge of DNN's parameters is not well explored yet. In this work, we are the first to propose a novel DNN weight attack methodology called Bit-Flip Attack (BFA) which can crush a neural network through maliciously flipping extremely small amount of bits within its weight storage memory system (i.e., DRAM). The bit-flip operations could be conducted through well-known Row-Hammer attack, while our main contribution is to develop an algorithm to identify the most vulnerable bits of DNN weight parameters (stored in memory as binary bits), that could maximize the accuracy degradation with a minimum number of bit-flips. Our proposed BFA utilizes a Progressive Bit Search (PBS) method which combines gradient ranking and progressive search to identify the most vulnerable bit to be flipped. With the aid of PBS, we can successfully attack a ResNet-18 fully malfunction (i.e., top-1 accuracy degrade from 69.8% to 0.1%) only through 13 bit-flips out of 93 million bits, while randomly flipping 100 bits merely degrades the accuracy by less than 1%. Code is released at: https://github.com/elliothe/Neural_Network_Weight_Attack
Employing Deep Part-Object Relationships for Salient Object Detection

Despite Convolutional Neural Networks (CNNs) based methods have been successful in detecting salient objects, their underlying mechanism that decides the salient intensity of each image part separately cannot avoid inconsistency of parts within the same salient object. This would ultimately result in an incomplete shape of the detected salient object. To solve this problem, we dig into part-object relationships and take the unprecedented attempt to employ these relationships endowed by the Capsule Network (CapsNet) for salient object detection. The entire salient object detection system is built directly on a Two-Stream Part-Object Assignment Network (TSPOANet) consisting of three algorithmic steps. In the first step, the learned deep feature maps of the input image are transformed to a group of primary capsules. In the second step, we feed the primary capsules into two identical streams, within each of which low-level capsules (parts) will be assigned to their familiar high-level capsules (object) via a locally connected routing. In the final step, the two streams are integrated in the form of a fully connected layer, where the relevant parts can be clustered together to form a complete salient object. Experimental results demonstrate the superiority of the proposed salient object detection network over the state-of-the-art methods.
Self-Supervised Deep Depth Denoising

Depth perception is considered an invaluable source of information for various vision tasks. However, depth maps acquired using consumer-level sensors still suffer from non-negligible noise. This fact has recently motivated researchers to exploit traditional filters, as well as the deep learning paradigm, in order to suppress the aforementioned non-uniform noise, while preserving geometric details. Despite the effort, deep depth denoising is still an open challenge mainly due to the lack of clean data that could be used as ground truth. In this paper, we propose a fully convolutional deep autoencoder that learns to denoise depth maps, surpassing the lack of ground truth data. Specifically, the proposed autoencoder exploits multiple views of the same scene from different points of view in order to learn to suppress noise in a self-supervised end-to-end manner using depth and color information during training, yet only depth during inference. To enforce self-supervision, we leverage a differentiable rendering technique to exploit photometric supervision, which is further regularized using geometric and surface priors. As the proposed approach relies on raw data acquisition, a large RGB-D corpus is collected using Intel RealSense sensors. Complementary to a quantitative evaluation, we demonstrate the effectiveness of the proposed self-supervised denoising approach on established 3D reconstruction applications. Code is avalable at https://github.com/VCL3D/DeepDepthDenoising
Cost-Aware Fine-Grained Recognition for IoTs Based on Sequential Fixations

We consider the problem of fine-grained classification on an edge camera device that has limited power. The edge device must sparingly interact with the cloud to minimize communication bits to conserve power, and the cloud upon receiving the edge inputs returns a classification label. To deal with fine-grained classification, we adopt the perspective of sequential fixation with a foveated field-of-view to model cloud-edge interactions. We propose a novel deep reinforcement learning-based foveation model, DRIFT, that sequentially generates and recognizes mixed-acuity images. Training of DRIFT requires only image-level category labels and encourages fixations to contain task-relevant information, while maintaining data efficiency. Specifically, we train a foveation actor network with a novel Deep Deterministic Policy Gradient by Conditioned Critic and Coaching(DDPGC3) algorithm. In addition, we propose to shape the reward to provide informative feedback after each fixation to better guide RL training. We demonstrate the effectiveness of DRIFT on this task by evaluating on five fine-grained classification benchmark datasets, and show that the proposed approach achieves state-of-the-art performance with over 3X reduction in transmitted pixels.
Layout-Induced Video Representation for Recognizing Agent-in-Place Actions

We address scene layout modeling for recognizing agent-in-place actions, which are actions associated with agents who perform them and the places where they occur, in the context of outdoor home surveillance. We introduce a novel representation to model the geometry and topology of scene layouts so that a network can generalize from the layouts observed in the training scenes to unseen scenes in the test set. This Layout-Induced Video Representation (LIVR) abstracts away low-level appearance variance and encodes geometric and topological relationships of places to explicitly model scene layout. LIVR partitions the semantic features of a scene into different places to force the network to learn generic place-based feature descriptions which are independent of specific scene layouts; then, LIVR dynamically aggregates features based on connectivities of places in each specific scene to model its layout. We introduce a new Agent-in-Place Action (APA) dataset(The dataset is pending legal review and will be released upon the acceptance of this paper.) to show that our method allows neural network models to generalize significantly better to unseen scenes. 
Anomaly Detection in Video Sequence With Appearance-Motion Correspondence

Anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. We propose a deep convolutional neural network (CNN) that addresses this problem by learning a correspondence between common object appearances (e.g. pedestrian, background, tree, etc.) and their associated motions. Our model is designed as a combination of a reconstruction network and an image translation model that share the same encoder. The former sub-network determines the most significant structures that appear in video frames and the latter one attempts to associate motion templates to such structures. The training stage is performed using only videos of normal events and the model is then capable to estimate frame-level scores for an unknown input. The experiments on 6 benchmark datasets demonstrate the competitive performance of the proposed approach with respect to state-of-the-art methods.
Exploring Randomly Wired Neural Networks for Image Recognition

Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design.
Progressive Differentiable Architecture Search: Bridging the Depth Gap Between Search and Evaluation

Recently, differentiable search methods have made major progress in reducing the computational costs of neural architecture search. However, these approaches often report lower accuracy in evaluating the searched architecture or transferring it to another dataset. This is arguably due to the large gap between the architecture depths in search and evaluation scenarios. In this paper, we present an efficient algorithm which allows the depth of searched architectures to grow gradually during the training procedure. This brings two issues, namely, heavier computational overheads and weaker search stability, which we solve using search space approximation and regularization, respectively. With a significantly reduced search time ( 7 hours on a single GPU), our approach achieves state-of-the-art performance on both the proxy dataset (CIFAR10 or CIFAR100) and the target dataset (ImageNet). Code is available at https://github.com/chenxin061/pdarts
Multinomial Distribution Learning for Effective Neural Architecture Search

Architectures obtained by Neural Architecture Search (NAS) have achieved highly competitive performance in various computer vision tasks. However, the prohibitive computation demand of forward-backward propagation in deep neural networks and searching algorithms makes it difficult to apply NAS in practice. In this paper, we propose a Multinomial Distribution Learning for extremely effective NAS, which considers the search space as a joint multinomial distribution, i.e., the operation between two nodes is sampled from this distribution, and the optimal network structure is obtained by the operations with the most likely probability in this distribution. Therefore, NAS can be transformed to a multinomial distribution learning problem, i.e., the distribution is optimized to have a high expectation of the performance. Besides, a hypothesis that the performance ranking is consistent in every training epoch is proposed and demonstrated to further accelerate the learning process. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of our method. On CIFAR-10, the structure searched by our method achieves 2.55% test error, while being 6.0x (only 4 GPU hours on GTX1080Ti) faster compared with state-of-the-art NAS algorithms. On ImageNet, our model achieves 74% top1 accuracy under MobileNet settings (MobileNet V1/V2), while being 1.2x faster with measured GPU latency. Test code with pre-trained models are available at https: //github.com/tanglang96/MDENAS
Searching for MobileNetV3

We present the next generation of MobileNets based on a combination of complementary search techniques as well as a novel architecture design. MobileNetV3 is tuned to mobile phone CPUs through a combination of hardware-aware network architecture search (NAS) complemented by the NetAdapt algorithm and then subsequently improved through novel architecture advances. This paper starts the exploration of how automated search algorithms and network design can work together to harness complementary approaches improving the overall state of the art. Through this process we create two new MobileNet models for release: MobileNetV3-Large and MobileNetV3-Small which are targeted for high and low resource use cases. These models are then adapted and applied to the tasks of object detection and semantic segmentation. For the task of semantic segmentation (or any dense pixel prediction), we propose a new efficient segmentation decoder Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP). We achieve new state of the art results for mobile classification, detection and segmentation. MobileNetV3-Large is 3.2% more accurate on ImageNet classification while reducing latency by 20% compared to MobileNetV2. MobileNetV3-Small is 6.6% more accurate compared to a MobileNetV2 model with comparable latency. MobileNetV3-Large detection is over 25% faster at roughly the same accuracy as MobileNetV2 on COCO detection. MobileNetV3-Large LR-ASPP is 34% faster than MobileNetV2 R-ASPP at similar accuracy for Cityscapes segmentation.
Data-Free Quantization Through Weight Equalization and Bias Correction

We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.
A Camera That CNNs: Towards Embedded Neural Networks on Pixel Processor Arrays

We present a convolutional neural network implementation for pixel processor array (PPA) sensors. PPA hardware consists of a fine-grained array of general-purpose processing elements, each capable of light capture, data storage, program execution, and communication with neighboring elements. This allows images to be stored and manipulated directly at the point of light capture, rather than having to transfer images to external processing hardware. Our CNN approach divides this array up into 4x4 blocks of processing elements, essentially trading-off image resolution for increased local memory capacity per 4x4 "pixel". We implement parallel operations for image addition, subtraction and bit-shifting images in this 4x4 block format. Using these components we formulate how to perform ternary weight convolutions upon these images, compactly store results of such convolutions, perform max-pooling, and transfer the resulting sub-sampled data to an attached micro-controller. We train ternary weight filter CNNs for digit recognition and a simple tracking task, and demonstrate inference of these networks upon the SCAMP5 PPA system. This work represents a first step towards embedding neural network processing capability directly onto the focal plane of a sensor.
Knowledge Distillation via Route Constrained Optimization

Distillation-based learning boosts the performance of the miniaturized neural network based on the hypothesis that the representation of a teacher model can be used as structured and relatively weak supervision, and thus would be easily learned by a miniaturized model. However, we find that the representation of a converged heavy model is still a strong constraint for training a small student model, which leads to a higher lower bound of congruence loss. In this work, we consider the knowledge distillation from the perspective of curriculum learning by teacher's routing. Instead of supervising the student model with a converged teacher model, we supervised it with some anchor points selected from the route in parameter space that the teacher model passed by, as we called route constrained optimization (RCO). We experimentally demonstrate this simple operation greatly reduces the lower bound of congruence loss for knowledge distillation, hint and mimicking learning. On close-set classification tasks like CIFAR and ImageNet, RCO improves knowledge distillation by 2.14% and 1.5% respectively. For the sake of evaluating the generalization, we also test RCO on the open-set face recognition task MegaFace. RCO achieves 84.3% accuracy on one-to-million task with only 0.8 M parameters, which push the SOTA by a large margin.
Distillation-Based Training for Multi-Exit Architectures

Multi-exit architectures, in which a stack of processing layers is interleaved with early output layers, allow the processing of a test example to stop early and thus save computation time and/or energy. In this work, we propose a new training procedure for multi-exit architectures based on the principle of knowledge distillation. The method encourages early exits to mimic later, more accurate exits, by matching their probability outputs. Experiments on CIFAR100 and ImageNet show that distillation-based training significantly improves the accuracy of early exits while maintaining state-of-the-art accuracy for late ones. The method is particularly beneficial when training data is limited and also allows a straight-forward extension to semi-supervised learning, i.e. make use also of unlabeled data at training time. Moreover, it takes only a few lines to implement and imposes almost no computational overhead at training time, and none at all at test time.
Similarity-Preserving Knowledge Distillation

Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach.
Many Task Learning With Task Routing

Typical multi-task learning (MTL) methods rely on architectural adjustments and a large trainable parameter set to jointly optimize over several tasks. However, when the number of tasks increases so do the complexity of the architectural adjustments and resource requirements. In this paper, we introduce a method which applies a conditional feature-wise transformation over the convolutional activations that enables a model to successfully perform a large number of tasks. To distinguish from regular MTL, we introduce Many Task Learning (MaTL) as a special case of MTL where more than 20 tasks are performed by a single model. Our method dubbed Task Routing (TR) is encapsulated in a layer we call the Task Routing Layer (TRL), which applied in an MaTL scenario successfully fits hundreds of classification tasks in one model. We evaluate on 5 datasets and the Visual Decathlon (VD) challenge against strong baselines and state-of-the-art approaches.
Stochastic Filter Groups for Multi-Task CNNs: Learning Specialist and Generalist Convolution Kernels

The performance of multi-task learning in Convolutional Neural Networks (CNNs) hinges on the design of feature sharing between tasks within the architecture. The number of possible sharing patterns are combinatorial in the depth of the network and the number of tasks, and thus hand-crafting an architecture, purely based on the human intuitions of task relationships can be time-consuming and suboptimal. In this paper, we present a probabilistic approach to learning task-specific and shared representations in CNNs for multi-task learning. Specifically, we propose "stochastic filter groups" (SFG), a mechanism to assign convolution kernels in each layer to "specialist" and "generalist" groups, which are specific to and shared across different tasks, respectively. The SFG modules determine the connectivity between layers and the structures of task-specific and shared representations in the network. We employ variational inference to learn the posterior distribution over the possible grouping of kernels and network parameters. Experiments demonstrate the proposed method generalises across multiple tasks and shows improved performance over baseline methods. 
Transferability and Hardness of Supervised Classification Tasks

We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets---CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks)---together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for highly transferable attributes.
Moment Matching for Multi-Source Domain Adaptation

Conventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset, called DomainNet, which contains six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M3SDA), which aims to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insights specifically for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model. Dataset and Code are available at http://ai.bu.edu/M3SDA/
Unsupervised Domain Adaptation via Regularized Conditional Alignment

We propose a method for unsupervised domain adaptation that trains a shared embedding to align the joint distributions of inputs (domain) and outputs (classes), making any classifier agnostic to the domain. Joint alignment ensures that not only the marginal distributions of the domains are aligned, but the labels as well. We propose a novel objective function that encourages the class-conditional distributions to have disjoint support in feature space. We further exploit adversarial regularization to improve the performance of the classifier on the domain for which no annotated data is available.
Larger Norm More Transferable: An Adaptive Feature Norm Approach for Unsupervised Domain Adaptation

Domain adaptation enables the learner to safely generalize into novel environments by mitigating domain shifts across distributions. Previous works may not effectively uncover the underlying reasons that would lead to the drastic model degradation on the target task. In this paper, we empirically reveal that the erratic discrimination of the target domain mainly stems from its much smaller feature norms with respect to that of the source domain. To this end, we propose a novel parameter-free Adaptive Feature Norm approach. We demonstrate that progressively adapting the feature norms of the two domains to a large range of values can result in significant transfer gains, implying that those task-specific features with larger norms are more transferable. Our method successfully unifies the computation of both standard and partial domain adaptation with more robustness against the negative transfer issue. Without bells and whistles but a few lines of code, our method substantially lifts the performance on the target task and exceeds state-of-the-arts by a large margin (11.5% on Office-Home and 17.1% on VisDA2017). We hope our simple yet effective approach will shed some light on the future research of transfer learning. Code is available at https://github.com/jihanyang/AFN.
UM-Adapt: Unsupervised Multi-Task Adaptation Using Adversarial Cross-Task Distillation

Aiming towards human-level generalization, there is a need to explore adaptable representation learning methods with greater transferability. Most existing approaches independently address task-transferability and cross-domain adaptation, resulting in limited generalization. In this paper, we propose UM-Adapt - a unified framework to effectively perform unsupervised domain adaptation for spatially-structured prediction tasks, simultaneously maintaining a balanced performance across individual tasks in a multi-task setting. To realize this, we propose two novel regularization strategies; a) Contour-based content regularization (CCR) and b) exploitation of inter-task coherency using a cross-task distillation module. Furthermore, avoiding a conventional ad-hoc domain discriminator, we re-utilize the cross-task distillation loss as output of an energy function to adversarially minimize the input domain discrepancy. Through extensive experiments, we demonstrate superior generalizability of the learned representations simultaneously for multiple tasks under domain-shifts from synthetic to natural environments. UM-Adapt yields state-of-the-art transfer learning results on ImageNet classification and comparable performance on PASCAL VOC 2007 detection task, even with a smaller backbone-net. Moreover, the resulting semi-supervised framework outperforms the current fully-supervised multi-task learning state-of-the-art on both NYUD and Cityscapes dataset.
Episodic Training for Domain Generalization

Domain generalization (DG) is the challenging and topical problem of learning models that generalize to novel testing domains with different statistics than a set of known training domains. The simple approach of aggregating data from all source domains and training a single deep neural network end-to-end on all the data provides a surprisingly strong baseline that surpasses many prior published methods. In this paper we build on this strong baseline by designing an episodic training procedure that trains a single deep network in a way that exposes it to the domain shift that characterises a novel domain at runtime. Specifically, we decompose a deep network into feature extractor and classifier components, and then train each component by simulating it interacting with a partner who is badly tuned for the current domain. This makes both components more robust, ultimately leading to our networks producing state-of-the-art performance on three DG benchmarks. Furthermore, we consider the pervasive workflow of using an ImageNet trained CNN as a fixed feature extractor for downstream recognition tasks. Using the Visual Decathlon benchmark, we demonstrate that our episodic-DG training improves the performance of such a general purpose feature extractor by explicitly training a feature for robustness to novel problems. This shows that DG training can benefit standard practice in computer vision.
Domain Adaptation for Structured Output via Discriminative Patch Representations

Predicting structured outputs such as semantic segmentation relies on expensive per-pixel annotations to learn supervised models like convolutional neural networks. However, models trained on one data domain may not generalize well to other domains without annotations for model finetuning. To avoid the labor-intensive process of annotation, we develop a domain adaptation method to adapt the source data to the unlabeled target domain. We propose to learn discriminative feature representations of patches in the source domain by discovering multiple modes of patch-wise output distribution through the construction of a clustered space. With such representations as guidance, we use an adversarial learning scheme to push the feature representations of target patches in the clustered space closer to the distributions of source patches. In addition, we show that our framework is complementary to existing domain adaptation techniques and achieves consistent improvements on semantic segmentation. Extensive ablations and results are demonstrated on numerous benchmark datasets with various settings, such as synthetic-to-real and cross-city scenarios.
Semi-Supervised Learning by Augmented Distribution Alignment

In this work, we propose a simple yet effective semi-supervised learning approach called Augmented Distribution Alignment. We reveal that an essential sampling bias exists in semi-supervised learning due to the limited number of labeled samples, which often leads to a considerable empirical distribution mismatch between labeled data and unlabeled data. To this end, we propose to align the empirical distributions of labeled and unlabeled data to alleviate the bias. On one hand, we adopt an adversarial training strategy to minimize the distribution distance between labeled and unlabeled data as inspired by domain adaptation works. On the other hand, to deal with the small sample size issue of labeled data, we also propose a simple interpolation strategy to generate pseudo training samples. Those two strategies can be easily implemented into existing deep neural networks. We demonstrate the effectiveness of our proposed approach on the benchmark SVHN and CIFAR10 datasets. Our code is available at https://github.com/qinenergy/adanet .
S4L: Self-Supervised Semi-Supervised Learning

This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S4L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S4L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.
Privacy Preserving Image Queries for Camera Localization

This work tackles the problem of semi-supervised learning of image classifiers. Our main insight is that the field of semi-supervised learning can benefit from the quickly advancing field of self-supervised visual representation learning. Unifying these two approaches, we propose the framework of self-supervised semi-supervised learning (S4L) and use it to derive two novel semi-supervised image classification methods. We demonstrate the effectiveness of these methods in comparison to both carefully tuned baselines, and existing semi-supervised learning methods. We then show that S4L and existing semi-supervised methods can be jointly trained, yielding a new state-of-the-art result on semi-supervised ILSVRC-2012 with 10% of labels.
Calibration Wizard: A Guidance System for Camera Calibration Based on Modelling Geometric and Corner Uncertainty

It is well known that the accuracy of a calibration depends strongly on the choice of camera poses from which images of a calibration object are acquired. We present a system -- Calibration Wizard -- that interactively guides a user towards taking optimal calibration images. For each new image to be taken, the system computes, from all previously acquired images, the pose that leads to the globally maximum reduction of expected uncertainty on intrinsic parameters and then guides the user towards that pose. We also show how to incorporate uncertainty in corner point position in a novel principled manner, for both, calibration and computation of the next best pose. Synthetic and real-world experiments are performed to demonstrate the effectiveness of Calibration Wizard.
Gated2Depth: Real-Time Dense Lidar From Gated Images

We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth accuracy comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. Moreover, today's pulsed lidar scanners suffer from high cost, power consumption, large form-factors, and they fail in the presence of strong backscatter. We depart from point scanning and demonstrate that it is possible to turn a low-cost CMOS gated imager into a dense depth camera with at least 80m range - by learning depth from three gated images. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The proposed replacement for scanning lidar systems is real-time, handles back-scatter and provides dense depth at long ranges. We validate our approach in simulation and on real-world data acquired over 4,000km driving in northern Europe. Data and code are available at https://github.com/gruberto/Gated2Depth.
X-Section: Cross-Section Prediction for Enhanced RGB-D Fusion

Detailed 3D reconstruction is an important challenge with application to robotics, augmented and virtual reality, which has seen impressive progress throughout the past years. Advancements were driven by the availability of depth cameras (RGB-D), as well as increased compute power, e.g. in the form of GPUs -- but also thanks to inclusion of machine learning in the process. Here, we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep learning to make object-level predictions about thicknesses that can be readily integrated into a volumetric multi-view fusion process, where we propose an extension to the popular KinectFusion approach. In essence, our method allows to complete shape in general indoor scenes behind what is sensed by the RGB-D camera, which may be crucial e.g. for robotic manipulation tasks or efficient scene exploration. Predicting object thicknesses rather than volumes allows us to work with comparably high spatial resolution without exploding memory and training data requirements on the employed Convolutional Neural Networks. In a series of qualitative and quantitative evaluations, we demonstrate how we accurately predict object thickness and reconstruct general 3D scenes containing multiple objects.
Learning an Event Sequence Embedding for Dense Event-Based Deep Stereo

Today, a frame-based camera is the sensor of choice for machine vision applications. However, these cameras, originally developed for acquisition of static images rather than for sensing of dynamic uncontrolled visual environments, suffer from high power consumption, data rate, latency and low dynamic range. An event-based image sensor addresses these drawbacks by mimicking a biological retina. Instead of measuring the intensity of every pixel in a fixed time-interval, it reports events of significant pixel intensity changes. Every such event is represented by its position, sign of change, and timestamp, accurate to the microsecond. Asynchronous event sequences require special handling, since traditional algorithms work only with synchronous, spatially gridded data. To address this problem we introduce a new module for event sequence embedding, for use in difference applications. The module builds a representation of an event sequence by firstly aggregating information locally across time, using a novel fully-connected layer for an irregularly sampled continuous domain, and then across discrete spatial domain. Based on this module, we design a deep learning-based stereo method for event-based cameras. The proposed method is the first learning-based stereo method for an event-based camera and the only method that produces dense results. We show that large performance increases on the Multi Vehicle Stereo Event Camera Dataset (MVSEC), which became the standard set for benchmarking of event-based stereo methods. 
Point-Based Multi-View Stereo Network

We introduce Point-MVSNet, a novel point-based deep framework for multi-view stereo (MVS). Distinct from existing cost volume approaches, our method directly processes the target scene as point clouds. More specifically, our method predicts the depth in a coarse-to-fine manner. We first generate a coarse depth map, convert it into a point cloud and refine the point cloud iteratively by estimating the residual between the depth of the current iteration and that of the ground truth. Our network leverages 3D geometry priors and 2D texture information jointly and effectively by fusing them into a feature-augmented point cloud, and processes the point cloud to estimate the 3D flow for each point. This point-based architecture allows higher accuracy, more computational efficiency and more flexibility than cost-volume-based counterparts. Experimental results show that our approach achieves a significant improvement in reconstruction quality compared with state-of-the-art methods on the DTU and the Tanks and Temples dataset. Our source code and trained models are available at https://github.com/callmeray/PointMVSNet.
Discrete Laplace Operator Estimation for Dynamic 3D Reconstruction

We present a general paradigm for dynamic 3D reconstruction from multiple independent and uncontrolled image sources having arbitrary temporal sampling density and distribution. Our graph-theoretic formulation models the spatio-temporal relationships among our observations in terms of the joint estimation of their 3D geometry and its discrete Laplace operator. Towards this end, we define a tri-convex optimization framework that leverages the geometric properties and dependencies found among a Euclidean shape-space and the discrete Laplace operator describing its local and global topology. We present a reconstructability analysis, experiments on motion capture data and multi-view image datasets, as well as explore applications to geometry-based event segmentation and data association.
Deep Non-Rigid Structure From Motion

Current non-rigid structure from motion (NRSfM) algorithms are mainly limited with respect to: (i) the number of images, and (ii) the type of shape variability they can handle. This has hampered the practical utility of NRSfM for many applications within vision. In this paper we propose a novel deep neural network to recover camera poses and 3D points solely from an ensemble of 2D image coordinates. The proposed neural network is mathematically interpretable as a multi-layer block sparse dictionary learning problem, and can handle problems of unprecedented scale and shape complexity. Extensive experiments demonstrate the impressive performance of our approach where we exhibit superior precision and robustness against all available state-of-the-art works in the order of magnitude. We further propose a quality measure (based on the network weights) which circumvents the need for 3D ground-truth to ascertain the confidence we have in the reconstruction. 
Equivariant Multi-View Networks

Several popular approaches to 3D vision tasks process multiple views of the input independently with deep neural networks pre-trained on natural images, where view permutation invariance is achieved through a single round of pooling over all views. We argue that this operation discards important information and leads to subpar global descriptors. In this paper, we propose a group convolutional approach to multiple view aggregation where convolutions are performed over a discrete subgroup of the rotation group, enabling, thus, joint reasoning over all views in an equivariant (instead of invariant) fashion, up to the very last layer. We further develop this idea to operate on smaller discrete homogeneous spaces of the rotation group, where a polar view representation is used to maintain equivariance with only a fraction of the number of input views. We set the new state of the art in several large scale 3D shape retrieval tasks, and show additional applications to panoramic scene classification.
Interpolated Convolutional Networks for 3D Point Cloud Understanding

Point cloud is an important type of 3D representation. However, directly applying convolutions on point clouds is challenging due to the sparse, irregular and unordered data structure. In this paper, we propose a novel Interpolated Convolution operation, InterpConv, to tackle the point cloud feature learning and understanding problem. The key idea is to utilize a set of discrete kernel weights and interpolate point features to neighboring kernel-weight coordinates by an interpolation function for convolution. A normalization term is introduced to handle neighborhoods of different sparsity levels. Our InterpConv is shown to be permutation and sparsity invariant, and can directly handle irregular inputs. We further design Interpolated Convolutional Neural Networks (InterpCNNs) based on InterpConv layers to handle point cloud recognition tasks including shape classification, object part segmentation and indoor scene semantic parsing. Experiments show that the networks can capture both fine-grained local structures and global shape context information effectively. The proposed approach achieves state-of-the-art performance on public benchmarks including ModelNet40, ShapeNet Parts and S3DIS.
Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data

Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy ( 92%). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page https://hkust-vgd.github.io/scanobjectnn/.
PointCloud Saliency Maps

3D point-cloud recognition with PointNet and its variants has received remarkable progress. A missing ingredient, however, is the ability to automatically evaluate point-wise importance w.r.t.  classification performance, which is usually reflected by a saliency map. A saliency map is an important tool as it allows one to perform further processes on point-cloud data. In this paper, we propose a novel way of characterizing critical points and segments to build point-cloud saliency maps. Our method assigns each point a score reflecting its contribution to the model-recognition loss. The saliency map explicitly explains which points are the key for model recognition. Furthermore, aggregations of highly-scored points indicate important segments/subsets in a point-cloud. Our motivation for constructing a saliency map is by point dropping, which is a non-differentiable operator. To overcome this issue, we approximate point-dropping with a differentiable procedure of shifting points towards the cloud centroid. Consequently, each saliency score can be efficiently measured by the corresponding gradient of the loss w.r.t the point under the spherical coordinates. Extensive evaluations on several state-of-the-art point-cloud recognition models, including PointNet, PointNet++ and DGCNN, demonstrate the veracity and generality of our proposed saliency map. Code for experiments is released on https://github.com/tianzheng4/PointCloud-Saliency-Maps
ShellNet: Efficient Point Cloud Convolutional Neural Networks Using Concentric Shells Statistics

Deep learning with 3D data has progressed significantly since the introduction of convolutional neural networks that can handle point order ambiguity in point cloud data. While being able to achieve good accuracies in various scene understanding tasks, previous methods often have low training speed and complex network architecture. In this paper, we address these problems by proposing an efficient end-to-end permutation invariant convolution for point cloud deep learning. Our simple yet effective convolution operator named ShellConv uses statistics from concentric spherical shells to define representative features and resolve the point order ambiguity, allowing traditional convolution to perform on such features. Based on ShellConv we further build an efficient neural network named ShellNet to directly consume the point clouds with larger receptive fields while maintaining less layers. We demonstrate the efficacy of ShellNet by producing state-of-the-art results on object classification, object part segmentation, and semantic scene segmentation while keeping the network very fast to train.
Unsupervised Deep Learning for Structured Shape Matching

We present a novel method for computing correspondences across 3D shapes using unsupervised learning. Our method computes a non-linear transformation of given descriptor functions, while optimizing for global structural properties of the resulting maps, such as their bijectivity or approximate isometry. To this end, we use the functional maps framework, and build upon the recent FMNet architecture for descriptor learning. Unlike that approach, however, we show that learning can be done in a purely unsupervised setting, without having access to any ground truth correspondences. This results in a very general shape matching method that we call SURFMNet for Spectral Unsupervised FMNet, and which can be used to establish correspondences within 3D shape collections without any prior information. We demonstrate on a wide range of challenging benchmarks, that our approach leads to state-of-the-art results compared to the existing unsupervised methods and achieves results that are comparable even to the supervised learning techniques. Moreover, our framework is an order of magnitude faster, and does not rely on geodesic distance computation or expensive post-processing.
Linearly Converging Quasi Branch and Bound Algorithms for Global Rigid Registration

In recent years, several branch-and-bound (BnB) algorithms have been proposed to globally optimize rigid registration problems. In this paper, we suggest a general framework to improve upon the BnB approach, which we name Quasi BnB. Quasi BnB replaces the linear lower bounds used in BnB algorithms with quadratic quasi-lower bounds which are based on the quadratic behavior of the energy in the vicinity of the global minimum. While quasi-lower bounds are not truly lower bounds, the Quasi-BnB algorithm is globally optimal. In fact we prove that it exhibits linear convergence -- it achieves epsilon accuracy in O(log(1/epsilon)) time while the time complexity of other rigid registration BnB algorithms is polynomial in 1/epsilon. Our experiments verify that Quasi-BnB is significantly more efficient than state-of-the-art BnB algorithms, especially for problems where high accuracy is desired. 
Consensus Maximization Tree Search Revisited

 Consensus maximization is widely used for robust fitting in computer vision. However, solving it exactly, i.e., finding the globally optimal solution, is intractable. A* tree search, which has been shown to be fixed-parameter tractable, is one of the most efficient exact methods, though it is still limited to small inputs. We make two key contributions towards improving A* tree search. First, we show that the consensus maximization tree structure used previously actually contains paths that connect nodes at both adjacent and non-adjacent levels. Crucially, paths connecting non-adjacent levels are redundant for tree search, but they were not avoided previously. We propose a new acceleration strategy that avoids such redundant paths. In the second contribution, we show that the existing branch pruning technique also deteriorates quickly with the problem dimension. We then propose a new branch pruning technique that is less dimension-sensitive to address this issue. Experiments show that both new techniques can significantly accelerate A* tree search, making it reasonably efficient on inputs that were previously out of reach. Demo code is available at https://github.com/ZhipengCai/MaxConTreeSearch.
Quasi-Globally Optimal and Efficient Vanishing Point Estimation in Manhattan World

The image lines projected from parallel 3D lines intersect at a common point called the vanishing point (VP). Manhattan world holds for the scenes with three orthogonal VPs. In Manhattan world, given several lines in a calibrated image, we aim at clustering them by three unknown-but-sought VPs. The VP estimation can be reformulated as computing the rotation between the Manhattan frame and the camera frame. To compute this rotation, state-of-the-art methods are based on either data sampling or parameter search, and they fail to guarantee the accuracy and efficiency simultaneously. In contrast, we propose to hybridize these two strategies. We first compute two degrees of freedom (DOF) of the above rotation by two sampled image lines, and then search for the optimal third DOF based on the branch-and-bound. Our sampling accelerates our search by reducing the search space and simplifying the bound computation. Our search is not sensitive to noise and achieves quasi-global optimality in terms of maximizing the number of inliers. Experiments on synthetic and real-world images showed that our method outperforms state-of-the-art approaches in terms of accuracy and/or efficiency.
An Efficient Solution to the Homography-Based Relative Pose Problem With a Common Reference Direction

In this paper, we propose a novel approach to two-view minimal-case relative pose problems based on homography with a common reference direction. We explore the rank-1 constraint on the difference between the Euclidean homography matrix and the corresponding rotation, and propose an efficient two-step solution for solving both the calibrated and partially calibrated (unknown focal length) problems. We derive new 3.5-point, 3.5-point, 4-point solvers for two cameras such that the two focal lengths are unknown but equal, one of them is unknown, and both are unknown and possibly different, respectively. We present detailed analyses and comparisons with existing 6 and 7-point solvers, including results with smart phone images.
A Quaternion-Based Certifiably Optimal Solution to the Wahba Problem With Outliers

The Wahba problem, also known as rotation search, seeks to find the best rotation to align two sets of vector observations given putative correspondences, and is a fundamental routine in many computer vision and robotics applications. This work proposes the first polynomial-time certifiably optimal approach for solving the Wahba problem when a large number of vector observations are outliers. Our first contribution is to formulate the Wahba problem using a Truncated Least Squares (TLS) cost that is insensitive to a large fraction of spurious correspondences. The second contribution is to rewrite the problem using unit quaternions and show that the TLS cost can be framed as a Quadratically-Constrained Quadratic Program (QCQP). Since the resulting optimization is still highly non-convex and hard to solve globally, our third contribution is to develop a convex Semidefinite Programming (SDP) relaxation. We show that while a naive relaxation performs poorly in general, our relaxation is tight even in the presence of large noise and outliers. We validate the proposed algorithm, named QUASAR (QUAternion-based Semidefinite relAxation for Robust alignment), in both synthetic and real datasets showing that the algorithm outperforms RANSAC, robust local optimization techniques, global outlier-removal procedures, and Branch-and-Bound methods. QUASAR is able to compute certifiably optimal solutions (i.e. the relaxation is exact) even in the case when 95% of the correspondences are outliers.
PLMP - Point-Line Minimal Problems in Complete Multi-View Visibility

We present a complete classification of all minimal problems for generic arrangements of points and lines completely observed by calibrated perspective cameras. We show that there are only 30 minimal problems in total, no problems exist for more than 6 cameras, for more than 5 points, and for more than 6 lines. We present a sequence of tests for detecting minimality starting with counting degrees of freedom and ending with full symbolic and numeric verification of representative examples. For all minimal problems discovered, we present their algebraic degrees, i.e. the number of solutions, which measure their intrinsic difficulty. It shows how exactly the difficulty of problems grows with the number of views. Importantly, several new mini- mal problems have small degrees that might be practical in image matching and 3D reconstruction.
Variational Few-Shot Learning

We propose a variational Bayesian framework for enhancing few-shot learning performance. This idea is motivated by the fact that single point based metric learning approaches are inherently noise-vulnerable and easy-to-be-biased. In a nutshell, stochastic variational inference is invoked to approximate bias-eliminated class specific sample distributions. In the meantime, a classifier-free prediction is attained by leveraging the distribution statistics on novel samples. Extensive experimental results on several benchmarks well demonstrate the effectiveness of our distribution-driven few-shot learning framework over previous point estimates based methods, in terms of superior classification accuracy and robustness.
Generative Adversarial Minority Oversampling

Class imbalance is a long-standing problem relevant to a number of real-world applications of deep learning. Oversampling techniques, which are effective for handling class imbalance in classical learning systems, can not be directly applied to end-to-end deep learning systems. We propose a three-player adversarial game between a convex generator, a multi-class classifier network, and a real/fake discriminator to perform oversampling in deep learning systems. The convex generator generates new samples from the minority classes as convex combinations of existing instances, aiming to fool both the discriminator as well as the classifier into misclassifying the generated samples. Consequently, the artificial samples are generated at critical locations near the peripheries of the classes. This, in turn, adjusts the classifier induced boundaries in a way which is more likely to reduce misclassification from the minority classes. Extensive experiments on multiple class imbalanced image datasets establish the efficacy of our proposal. 
Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection

Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder "generalizes" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE. 
Topological Map Extraction From Overhead Images

We propose a new approach, named PolyMapper, to circumvent the conventional pixel-wise segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper directly extracts the topological map of a city from overhead images as collections of building footprints and road networks. In order to unify the shape representation for different types of objects, we also propose a novel sequentialization method that reformulates a graph structure as closed polygons. Experiments are conducted on both existing and self-collected large-scale datasets of several cities. Our empirical results demonstrate that our end-to-end learnable model is capable of drawing polygons of building footprints and road networks that very closely approximate the structure of existing online map services, in a fully automated manner. Quantitative and qualitative comparison to the state-of-the-arts also show that our approach achieves good levels of performance. To the best of our knowledge, the automatic extraction of large-scale topological maps is a novel contribution in the remote sensing community that we believe will help develop models with more informed geometrical constraints.
Exploiting Temporal Consistency for Real-Time Video Depth Estimation

Accuracy of depth estimation from static images has been significantly improved recently, by exploiting hierarchical features from deep convolutional neural networks (CNNs). Compared with static images, vast information exists among video frames and can be exploited to improve the depth estimation performance. In this work, we focus on exploring temporal information from monocular videos for depth estimation. Specifically, we take the advantage of convolutional long short-term memory (CLSTM) and propose a novel spatial-temporal CSLTM (ST-CLSTM) structure. Our ST-CLSTM structure can capture not only the spatial features but also the temporal correlations/consistency among consecutive video frames with negligible increase in computational cost. Additionally, in order to maintain the temporal consistency among the estimated depth frames, we apply the generative adversarial learning scheme and design a temporal consistency loss. The temporal consistency loss is combined with the spatial loss to update the model in an end-to-end fashion. By taking advantage of the temporal information, we build a video depth estimation framework that runs in real-time and generates visually pleasant results. Moreover, our approach is flexible and can be generalized to most existing depth estimation frameworks. Code is available at: https://tinyurl.com/STCLSTM
The Sound of Motions

Sounds originate from object motions and vibrations of surrounding air. Inspired by the fact that humans is capable of interpreting sound sources from how objects move visually, we propose a novel system that explicitly captures such motion cues for the task of sound localization and separation. Our system is composed of an end-to-end learnable model called Deep Dense Trajectory (DDT), and a curriculum learning scheme. It exploits the inherent coherence of audio-visual signals from a large quantities of unlabeled videos. Quantitative and qualitative evaluations show that comparing to previous models that rely on visual appearance cues, our motion based system improves performance in separating musical instrument sounds. Furthermore, it separates sound components from duets of the same category of instruments, a challenging problem that has not been addressed before.
SC-FEGAN: Face Editing Generative Adversarial Network With User's Sketch and Color

We present a novel image editing system that generates images as the user provides free-form masks, sketches and color as inputs. Our system consists of an end-to-end trainable convolutional network. In contrast to the existing methods, our system utilizes entirely free-form user input in terms of color and shape. This allows the system to respond to the user's sketch and color inputs, using them as guidelines to generate an image. In this work, we trained the network with an additional style loss, which made it possible to generate realistic results despite large portions of the image being removed. Our proposed network architecture SC-FEGAN is well suited for generating high-quality synthetic images using intuitive user inputs.
Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style

Image captioning is a research hotspot where encoder-decoder models combining convolutional neural network (CNN) and long short-term memory (LSTM) achieve promising results. Despite significant progress, these models generate sentences differently from human cognitive styles. Existing models often generate a complete sentence from the first word to the end, without considering the influence of the following words on the whole sentence generation. In this paper, we explore the utilization of a human-like cognitive style, i.e., building overall cognition for the image to be described and the sentence to be constructed, for enhancing computer image understanding. This paper first proposes a Mutual-aid network structure with Bidirectional LSTMs (MaBi-LSTMs) for acquiring overall contextual information. In the training process, the forward and backward LSTMs encode the succeeding and preceding words into their respective hidden states by simultaneously constructing the whole sentence in a complementary manner. In the captioning process, the LSTM implicitly utilizes the subsequent semantic information contained in its hidden states. In fact, MaBi-LSTMs can generate two sentences in forward and backward directions. To bridge the gap between cross-domain models and generate a sentence with higher quality, we further develop a cross-modal attention mechanism to retouch the two sentences by fusing their salient parts as well as the salient areas of the image. Experimental results on the Microsoft COCO dataset show that the proposed model improves the performance of encoder-decoder models and achieves state-of-the-art results.
Order-Aware Generative Modeling Using the 3D-Craft Dataset

In this paper, we study the problem of sequentially building houses in the game of Minecraft, and demonstrate that learning the ordering can make for more effective autoregressive models. Given a partially built house made by a human player, our system tries to place additional blocks in a human-like manner to complete the house. We introduce a new dataset, HouseCraft, for this new task. HouseCraft contains the sequential order in which 2,500 Minecraft houses were built from scratch by humans. The human action sequences enable us to learn an order-aware generative model called Voxel-CNN. In contrast to many generative models where the sequential generation ordering either does not matter (e.g. holistic generation with GANs), or is manually/arbitrarily set by simple rules (e.g. raster-scan order), our focus is on an ordered generation that imitates humans. To evaluate if a generative model can accurately predict human-like actions, we propose several novel quantitative metrics. We demonstrate that our Voxel-CNN model is simple and effective at this creative task, and can serve as a strong baseline for future research in this direction. The HouseCraft dataset and code with baseline models will be made publicly available.
Crowd Counting With Deep Structured Scale Integration Network

Automatic estimation of the number of people in unconstrained crowded scenes is a challenging task and one major difficulty stems from the huge scale variation of people. In this paper, we propose a novel Deep Structured Scale Integration Network (DSSINet) for crowd counting, which addresses the scale variation of people by using structured feature representation learning and hierarchically structured loss function optimization. Unlike conventional methods which directly fuse multiple features with weighted average or concatenation, we first introduce a Structured Feature Enhancement Module based on conditional random fields (CRFs) to refine multiscale features mutually with a message passing mechanism. Specifically, each scale-specific feature is considered as a continuous random variable and passes complementary information to refine the features at other scales. Second, we utilize a Dilated Multiscale Structural Similarity loss to enforce our DSSINet to learn the local correlation of people's scales within regions of various size, thus yielding high-quality density maps. Extensive experiments on four challenging benchmarks well demonstrate the effectiveness of our method. In particular, our DSSINet achieves improvements of 9.5% error reduction on Shanghaitech dataset and 24.9% on UCF-QNRF dataset against the state-of-the-art methods.
Bidirectional One-Shot Unsupervised Domain Mapping

We study the problem of mapping between a domain A, in which there is a single training sample and a domain B, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain B. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST.
Evolving Space-Time Neural Architectures for Videos

We present a new method for finding video CNN architectures that more optimally capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing CNN video architectures. We here develop a novel evolutionary algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new diverse and interesting video architectures that were unknown previously. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on four datasets: Kinetics, Charades, Moments in Time and HMDB. We will open source the code and models, to encourage future model development. 
Universally Slimmable Networks and Improved Training Techniques

Slimmable networks are a family of neural networks that can instantly adjust the runtime width. The width can be chosen from a predefined widths set to adaptively optimize accuracy-efficiency trade-offs at runtime. In this work, we propose a systematic approach to train universally slimmable networks (US-Nets), extending slimmable networks to execute at arbitrary width, and generalizing to networks both with and without batch normalization layers. We further propose two improved training techniques for US-Nets, named the sandwich rule and inplace distillation, to enhance training process and boost testing accuracy. We show improved performance of universally slimmable MobileNet v1 and MobileNet v2 on ImageNet classification task, compared with individually trained ones and 4-switch slimmable network baselines. We also evaluate the proposed US-Nets and improved training techniques on tasks of image super-resolution and deep reinforcement learning. Extensive ablation experiments on these representative tasks demonstrate the effectiveness of our proposed methods. Our discovery opens up the possibility to directly evaluate FLOPs-Accuracy spectrum of network architectures. Code and models are available at: https://github.com/JiahuiYu/slimmable_networks.
AutoDispNet: Improving Disparity Estimation With AutoML

Much research work in computer vision is being spent on optimizing existing network architectures to obtain a few more percentage points on benchmarks. Recent AutoML approaches promise to relieve us from this effort. However, they are mainly designed for comparatively small-scale classification tasks. In this work, we show how to use and extend existing AutoML techniques to efficiently optimize large-scale U-Net-like encoder-decoder architectures. In particular, we leverage gradient-based neural architecture search and Bayesian optimization for hyperparameter search. The resulting optimization does not require a large-scale compute cluster. We show results on disparity estimation that clearly outperform the manually optimized baseline and reach state-of-the-art performance.
Deep Meta Functionals for Shape Representation

We present a new method for 3D shape reconstruction from a single image, in which a deep neural network directly maps an image to a vector of network weights. The network parametrized by these weights represents a 3D shape by classifying every point in the volume as either within or outside the shape. The new representation has virtually unlimited capacity and resolution, and can have an arbitrary topology. Our experiments show that it leads to more accurate shape inference from a 2D projection than the existing methods, including voxel-, silhouette-, and mesh-based methods. The code will be available at: https: //github.com/gidilittwin/Deep-Meta. 
Differentiable Kernel Evolution

This paper proposes a differentiable kernel evolution (DKE) algorithm to find a better layer-operator for the convolutional neural network. Unlike most of the other neural architecture searching (NAS) technologies, we consider the searching space in a fundamental scope: kernel space, which encodes the assembly of basic multiply-accumulate (MAC) operations into a conv-kernel. We first deduce a strict form of the generalized convolutional operator by some necessary constraints and construct a continuous searching space for its extra freedom-of-degree, namely, the connection of each MAC. Then a novel unsupervised greedy evolution algorithm called gradient agreement guided searching (GAGS) is proposed to learn the optimal location for each MAC in the spatially continuous searching space. We leverage DKE on multiple kinds of tasks such as object classification, face/object detection, large-scale fine-grained and recognition, with various kinds of backbone architecture. Not to mention the consistent performance gain, we found the proposed DKE can further act as an auto-dilated operator, which makes it easy to boost the performance of miniaturized neural networks in multiple tasks.
Batch Weight for Domain Adaptation With Mass Shift

Unsupervised domain transfer is the task of transferring or translating samples from a source distribution to a different target distribution. Current solutions unsupervised domain transfer often operate on data on which the modes of the distribution are well-matched, for instance have the same frequencies of classes between source and target distributions. However, these models do not perform well when the modes are not well-matched, as would be the case when samples are drawn independently from two different, but related, domains. This mode imbalance is problematic as generative adversarial networks (GANs), a successful approach in this setting, are sensitive to mode frequency, which results in a mismatch of semantics between source samples and generated samples of the target distribution. We propose a principled method of re-weighting training samples to correct for such mass shift between the transferred distributions, which we call batch weight. We also provide rigorous probabilistic setting for domain transfer and new simplified objective for training transfer networks, an alternative to complex, multi-component loss functions used in the current state-of-the art image-to-image translation models. The new objective stems from the discrimination of joint distributions and enforces cycle-consistency in an abstract, high-level, rather than pixel-wise, sense. Lastly, we experimentally show the effectiveness of the proposed methods in several image-to-image translation tasks. 
SRM: A Style-Based Recalibration Module for Convolutional Neural Networks

Following the advance of style transfer with Convolutional Neural Networks (CNNs), the role of styles in CNNs has drawn growing attention from a broader perspective. In this paper, we aim to fully leverage the potential of styles to improve the performance of CNNs in general vision tasks. We propose a Style-based Recalibration Module (SRM), a simple yet effective architectural unit, which adaptively recalibrates intermediate feature maps by exploiting their styles. SRM first extracts the style information from each channel of the feature maps by style pooling, then estimates per-channel recalibration weight via channel-independent style integration. By incorporating the relative importance of individual styles into feature maps, SRM effectively enhances the representational ability of a CNN. The proposed module is directly fed into existing CNN architectures with negligible overhead. We conduct comprehensive experiments on general image recognition as well as tasks related to styles, which verify the benefit of SRM over recent approaches such as Squeeze-and-Excitation (SE). To explain the inherent difference between SRM and SE, we provide an in-depth comparison of their representational properties.
Switchable Whitening for Deep Representation Learning

Normalization methods are essential components in convolutional neural networks (CNNs). They either standardize or whiten data using statistics estimated in predefined sets of pixels. Unlike existing works that design normalization techniques for specific tasks, we propose Switchable Whitening (SW), which provides a general form unifying different whitening methods as well as standardization methods. SW learns to switch among these operations in an end-to-end manner. It has several advantages. First, SW adaptively selects appropriate whitening or standardization statistics for different tasks (see Fig.1), making it well suited for a wide range of tasks without manual design. Second, by integrating benefits of different normalizers, SW shows consistent improvements over its counterparts in various challenging benchmarks. Third, SW serves as a useful tool for understanding the characteristics of whitening and standardization techniques. We show that SW outperforms other alternatives on image classification (CIFAR-10/100, ImageNet), semantic segmentation (ADE20K, Cityscapes), domain adaptation (GTA5, Cityscapes), and image style transfer (COCO). For example, without bells and whistles, we achieve state-of-the-art performance with 45.33% mIoU on the ADE20K dataset.
Adaptative Inference Cost With Convolutional Neural Mixture Models

Despite the outstanding performance of convolutional neural networks (CNNs) for many vision tasks, the required computational cost during inference is problematic when resources are limited. In this context, we propose Convolutional Neural Mixture Models (CNMMs), a probabilistic model embedding a large number of CNNs that can be jointly trained and evaluated in an efficient manner. Within the proposed framework, we present different mechanisms to prune subsets of CNNs from the mixture, allowing to easily adapt the computational cost required for inference. Image classification and semantic segmentation experiments show that our method achieve excellent accuracy-compute trade-offs. Moreover, unlike most of previous approaches, a single CNMM provides a large range of operating points along this trade-off, without any re-training. 
On Network Design Spaces for Visual Recognition

Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.
Improved Techniques for Training Adaptive Deep Networks

Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.
Resource Constrained Neural Network Architecture Search: Will a Submodularity Assumption Help?

The design of neural network architectures is frequently either based on human expertise using trial/error and empirical feedback or tackled via large scale reinforcement learning strategies performed over distinct discrete architecture choices. In the latter case, the optimization is often non-differentiable and also not very amenable to derivative-free optimization methods. Most methods in use today require sizable computational resources. And if we want networks that additionally satisfy resource constraints, the above challenges are exacerbated because the search must now balance accuracy with certain budget constraints on resources. We formulate this problem as the optimization of a set function -- we find that the empirical behavior of this set function often (but not always) satisfies marginal gain and monotonicity principles -- properties central to the idea of submodularity. Based on this observation, we adapt algorithms within discrete optimization to obtain heuristic schemes for neural network architecture search, where we have resource constraints on the architecture. This simple scheme when applied on CIFAR-100 and ImageNet, identifies resource-constrained architectures with quantifiably better performance than current state-of-the-art models designed for mobile devices. Specifically, we find high-performing architectures with fewer parameters and computations by a search method that is much faster. 
ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks

As designing appropriate Convolutional Neural Network (CNN) architecture in the context of a given application usually involves heavy human works or numerous GPU hours, the research community is soliciting the architecture-neutral CNN structures, which can be easily plugged into multiple mature architectures to improve the performance on our real-world applications. We propose Asymmetric Convolution Block (ACB), an architecture-neutral structure as a CNN building block, which uses 1D asymmetric convolutions to strengthen the square convolution kernels. For an off-the-shelf architecture, we replace the standard square-kernel convolutional layers with ACBs to construct an Asymmetric Convolutional Network (ACNet), which can be trained to reach a higher level of accuracy. After training, we equivalently convert the ACNet into the same original architecture, thus requiring no extra computations anymore. We have observed that ACNet can improve the performance of various models on CIFAR and ImageNet by a clear margin. Through further experiments, we attribute the effectiveness of ACB to its capability of enhancing the model's robustness to rotational distortions and strengthening the central skeleton parts of square convolution kernels.
A Comprehensive Overhaul of Feature Distillation

We investigate the design aspects of feature distillation methods achieving network compression and propose a novel feature distillation method in which the distillation loss is designed to make a synergy among various aspects: teacher transform, student transform, distillation feature position and distance function. Our proposed distillation loss includes a feature transform with a newly designed margin ReLU, a new distillation feature position, and a partial L2 distance function to skip redundant information giving adverse effects to the compression of student. In ImageNet, our proposed method achieves 21.65% of top-1 error with ResNet50, which outperforms the performance of the teacher network, ResNet152. Our proposed method is evaluated on various tasks such as image classification, object detection and semantic segmentation and achieves a significant performance improvement in all tasks. The code is available at project page.
Transferable Semi-Supervised 3D Object Detection From RGB-D Data

We investigate the direction of training a 3D object detector for new object classes from only 2D bounding box labels of these new classes, while simultaneously transferring information from 3D bounding box labels of the existing classes. To this end, we propose a transferable semi-supervised 3D object detection model that learns a 3D object detector network from training data with two disjoint sets of object classes - a set of strong classes with both 2D and 3D box labels, and another set of weak classes with only 2D box labels. In particular, we suggest a relaxed reprojection loss, box prior loss and a Box-to-Point Cloud Fit network that allow us to effectively transfer useful 3D information from the strong classes to the weak classes during training, and consequently, enable the network to detect 3D objects in the weak classes during inference. Experimental results show that our proposed algorithm outperforms baseline approaches and achieves promising results compared to fully-supervised approaches on the SUN-RGBD and KITTI datasets. Furthermore, we show that our Box-to-Point Cloud Fit network improves performances of the fully-supervised approaches on both datasets.
DPOD: 6D Pose Object Detector and Refiner

In this paper we present a novel deep learning method for 3D object detection and 6D pose estimation from RGB images. Our method, named DPOD (Dense Pose Object Detector), estimates dense multi-class 2D-3D correspondence maps between an input image and available 3D models. Given the correspondences, a 6DoF pose is computed via PnP and RANSAC. An additional RGB pose refinement of the initial pose estimates is performed using a custom deep learning-based refinement scheme. Our results and comparison to a vast number of related works demonstrate that a large number of correspondences is beneficial for obtaining high-quality 6D poses both before and after refinement. Unlike other methods that mainly use real data for training and do not train on synthetic renderings, we perform evaluation on both synthetic and real training data demonstrating superior results before and after refinement when compared to all recent detectors. While being precise, the presented approach is still real-time capable.
STD: Sparse-to-Dense 3D Object Detector for Point Cloud

We propose a two-stage 3D object detection framework, named sparse-to-dense 3D Object Detector (STD). The first stage is a bottom-up proposal generation network that uses raw point clouds as input to generate accurate proposals by seeding each point with a new spherical anchor. It achieves a higher recall with less computation compared with prior works. Then, PointsPool is applied for proposal feature generation by transforming interior point features from sparse expression to compact representation, which saves even more computation. In box prediction, which is the second stage, we implement a parallel intersection-over-union (IoU) branch to increase awareness of localization accuracy, resulting in further improved performance. We conduct experiments on KITTI dataset, and evaluate our method on 3D object and Bird's Eye View (BEV) detection. Our method outperforms other methods by a large margin, especially on the hard set, with 10+ FPS inference speed.
DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense

Neural networks are vulnerable to adversarial examples, which poses a threat to their application in security sensitive systems. We propose a Denoiser and UPsampler Network (DUP-Net) structure as defenses for 3D adversarial point cloud classification, where the two modules reconstruct surface smoothness by dropping or adding points. In this paper, statistical outlier removal (SOR) and a data-driven upsampling network are considered as denoiser and upsampler respectively. Compared with baseline defenses, DUP-Net has three advantages. First, with DUP-Net as a defense, the target model is more robust to white-box adversarial attacks. Second, the statistical outlier removal provides added robustness since it is a non-differentiable denoising operation. Third, the upsampler network can be trained on a small dataset and defends well against adversarial attacks generated from other point cloud datasets. We conduct various experiments to validate that DUP-Net is very effective as defense in practice. Our best defense eliminates 83.8% of C&W and l2 loss based attack (point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point adding) and 9.0% of saliency map based attack (point dropping) under 200 dropped points on PointNet. 
Learning Rich Features at High-Speed for Single-Shot Object Detection

Single-stage object detection methods have received significant attention recently due to their characteristic realtime capabilities and high detection accuracies. Generally, most existing single-stage detectors follow two common practices: they employ a network backbone that is pretrained on ImageNet for the classification task and use a top-down feature pyramid representation for handling scale variations. Contrary to common pre-training strategy, recent works have demonstrated the benefits of training from scratch to reduce the task gap between classification and localization, especially at high overlap thresholds. However, detection models trained from scratch require significantly longer training time compared to their typical finetuning based counterparts. We introduce a single-stage detection framework that combines the advantages of both fine-tuning pretrained models and training from scratch. Our framework constitutes a standard network that uses a pre-trained backbone and a parallel light-weight auxiliary network trained from scratch. Further, we argue that the commonly used top-down pyramid representation only focuses on passing high-level semantics from the top layers to bottom layers. We introduce a bi-directional network that efficiently circulates both low-/mid-level and high-level semantic information in the detection framework. Experiments are performed on MS COCO and UAVDT datasets. Compared to the baseline, our detector achieives an absolute gain of 7.4% and 4.2% in average precision (AP) on MS COCO and UAVDT datasets, respectively using VGG backbone. For a 300x300 input on the MS COCO test set, our detector with ResNet backbone surpasses existing single-stage detection methods for single-scale inference achieving 34.3 AP, while operating at an inference time of 19 milliseconds on a single Titan X GPU. Code is avail- able at https://github.com/vaesl/LRF-Net.
Detecting Unseen Visual Relations Using Analogies

We seek to detect visual relations in images of the form of triplets t = (subject, predicate, object), such as "person riding dog", where training examples of the individual entities are available but their combinations are unseen at training. This is an important set-up due to the combinatorial nature of visual relations : collecting sufficient training data for all possible triplets would be very hard. The contributions of this work are three-fold. First, we learn a representation of visual relations that combines (i) individual embeddings for subject, object and predicate together with (ii) a visual phrase embedding that represents the relation triplet. Second, we learn how to transfer visual phrase embeddings from existing training triplets to unseen test triplets using analogies between relations that involve similar objects. Third, we demonstrate the benefits of our approach on three challenging datasets : on HICO-DET, our model achieves significant improvement over a strong baseline for both frequent and unseen triplets, and we observe similar improvement for the retrieval of unseen triplets with out-of-vocabulary predicates on the COCO-a dataset as well as the challenging unusual triplets in the UnRel dataset.
Disentangling Monocular 3D Object Detection

In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies and set a new state-of-the-art on the KITTI3D Car class.
STM: SpatioTemporal and Motion Encoding for Action Recognition

Spatiotemporal and motion features are two complementary and crucial information for video action recognition. Recent state-of-the-art methods adopt a 3D CNN stream to learn spatiotemporal features and another flow stream to learn motion features. In this work, we aim to efficiently encode these two features in a unified 2D framework. To this end, we first propose a STM block, which contains a Channel-wise SpatioTemporal Module (CSTM) to present the spatiotemporal features and a Channel-wise Motion Module (CMM) to efficiently encode motion features. We then replace original residual blocks in the ResNet architecture with STM blcoks to form a simple yet effective STM network by introducing very limited extra computation cost. Extensive experiments demonstrate that the proposed STM network outperforms the state-of-the-art methods on both temporal-related datasets (i.e., Something-Something v1 & v2 and Jester) and scene-related datasets (i.e., Kinetics-400, UCF-101, and HMDB-51) with the help of encoding spatiotemporal and motion features together.
Dynamic Context Correspondence Network for Semantic Alignment

Establishing semantic correspondence is a core problem in computer vision and remains challenging due to large intra-class variations and lack of annotated data. In this paper, we aim to incorporate global semantic context in a flexible manner to overcome the limitations of prior work that relies on local semantic representations. To this end, we first propose a context-aware semantic representation that incorporates spatial layout for robust matching against local ambiguities. We then develop a novel dynamic fusion strategy based on attention mechanism to weave the advantages of both local and context features by integrating semantic cues from multiple scales. We instantiate our strategy by designing an end-to-end learnable deep network, named as Dynamic Context Correspondence Network (DCCNet). To train the network, we adopt a multi-auxiliary task loss to improve the efficiency of our weakly-supervised learning procedure. Our approach achieves superior or competitive performance over previous methods on several challenging datasets, including PF-Pascal, PF-Willow, and TSS, demonstrating its effectiveness and generality.
Fooling Network Interpretation in Image Classification

Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.
Unconstrained Foreground Object Search

Many people search for foreground objects to use when editing images. While existing methods can retrieve candidates to aid in this, they are constrained to returning objects that belong to a pre-specified semantic class. We instead propose a novel problem of unconstrained foreground object (UFO) search and introduce a solution that supports efficient search by encoding the background image in the same latent space as the candidate foreground objects. A key contribution of our work is a cost-free, scalable approach for creating a large-scale training dataset with a variety of foreground objects of differing semantic categories per image location. Quantitative and human-perception experiments with two diverse datasets demonstrate the advantage of our UFO search solution over related baselines.
Embodied Amodal Recognition: Learning to Move to Perceive Objects

Passive visual systems typically fail to recognize objects in the amodal setting where they are heavily occluded. In contrast, humans and other embodied agents have the ability to move in the environment and actively control the viewing angle to better understand object shapes and semantics. In this work, we introduce the task of Embodied Amodel Recognition (EAR): an agent is instantiated in a 3D environment close to an occluded target object, and is free to move in the environment to perform object classification, amodal object localization, and amodal object segmentation. To address this problem, we develop a new model called Embodied Mask R-CNN for agents to learn to move strategically to improve their visual recognition abilities. We conduct experiments using a simulator for indoor environments. Experimental results show that: 1) agents with embodiment (movement) achieve better visual recognition performance than passive ones and 2) in order to improve visual recognition abilities, agents can learn strategic paths that are different from shortest paths.
SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition

Understanding the spatial relations between objects in images is a surprisingly challenging task. A chair may be "behind" a person even if it appears to the left of the person in the image (depending on which way the person is facing). Two students that appear close to each other in the image may not in fact be "next to" each other if there is a third student between them. We introduce SpatialSense, a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. SpatialSense is constructed through adversarial crowdsourcing, in which human annotators are tasked with finding spatial relations that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Adversarial crowdsourcing significantly reduces dataset bias and samples more interesting relations in the long tail compared to existing datasets. On SpatialSense, state-of-the-art recognition models perform comparably to simple baselines, suggesting that they rely on straightforward cues instead of fully reasoning about this complex task. The SpatialSense benchmark provides a path forward to advancing the spatial reasoning capabilities of computer vision systems. The dataset and code are available at https://github.com/princeton-vl/SpatialSense.
TensorMask: A Foundation for Dense Object Segmentation

Sliding-window object detectors that generate bounding-box object predictions over a dense, regular grid have advanced rapidly and proven popular. In contrast, modern instance segmentation approaches are dominated by methods that first detect object bounding boxes, and then crop and segment these regions, as popularized by Mask R-CNN. In this work, we investigate the paradigm of dense sliding-window instance segmentation, which is surprisingly under-explored. Our core observation is that this task is fundamentally different than other dense prediction tasks such as semantic segmentation or bounding-box object detection, as the output at every spatial location is itself a geometric structure with its own spatial dimensions. To formalize this, we treat dense instance segmentation as a prediction task over 4D tensors and present a general framework called TensorMask that explicitly captures this geometry and enables novel operators on 4D tensors. We demonstrate that the tensor view leads to large gains over baselines that ignore this structure, and leads to results comparable to Mask R-CNN. These promising results suggest that TensorMask can serve as a foundation for novel advances in dense mask prediction and a more complete understanding of the task. Code will be made available.
Integral Object Mining via Online Attention Accumulation

Object attention maps generated by image classifiers are usually used as priors for weakly-supervised segmentation approaches. However, normal image classifiers produce attention only at the most discriminative object parts, which limits the performance of weakly-supervised segmentation task. Therefore, how to effectively identify entire object regions in a weakly-supervised manner has always been a challenging and meaningful problem. We observe that the attention maps produced by a classification network continuously focus on different object parts during training. In order to accumulate the discovered different object parts, we propose an online attention accumulation (OAA) strategy which maintains a cumulative attention map for each target category in each training image so that the integral object regions can be gradually promoted as the training goes. These cumulative attention maps, in turn, serve as the pixel-level supervision, which can further assist the network in discovering more integral object regions. Our method (OAA) can be plugged into any classification network and progressively accumulate the discriminative regions into integral objects as the training process goes. Despite its simplicity, when applying the resulting attention maps to the weakly-supervised semantic segmentation task, our approach improves the existing state-of-the-art methods on the PASCAL VOC 2012 segmentation benchmark, achieving a mIoU score of 66.4% on the test set. Code is available at https://mmcheng.net/oaa/.
Accelerated Gravitational Point Set Alignment With Altered Physical Laws

This work describes Barnes-Hut Rigid Gravitational Approach (BH-RGA) -- a new rigid point set registration method relying on principles of particle dynamics. Interpreting the inputs as two interacting particle swarms, we directly minimise the gravitational potential energy of the system using non-linear least squares. Compared to solutions obtained by solving systems of second-order ordinary differential equations, our approach is more robust and less dependent on the parameter choice. We accelerate otherwise exhaustive particle interactions with a Barnes-Hut tree and efficiently handle massive point sets in quasilinear time while preserving the globally multiply-linked character of interactions. Among the advantages of BH-RGA is the possibility to define boundary conditions or additional alignment cues through varying point masses. Systematic experiments demonstrate that BH-RGA surpasses performances of baseline methods in terms of the convergence basin and accuracy when handling incomplete, noisy and perturbed data. The proposed approach also positively compares to the competing method for the alignment with prior matches. 
Domain Adaptation for Semantic Segmentation With Maximum Squares Loss

 Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised learning methods have been applied to UDA and achieved state-of-the-art performance. One of the most popular approaches in semi-supervised learning is the entropy minimization method. However, when applying the entropy minimization to UDA for semantic segmentation, the gradient of the entropy is biased towards samples that are easy to transfer. To balance the gradient of well-classified target samples, we propose the maximum squares loss. Our maximum squares loss prevents the training process being dominated by easy-to-transfer samples in the target domain. Besides, we introduce the image-wise weighting ratio to alleviate the class imbalance in the unlabeled target domain. Both synthetic-to-real and cross-city adaptation experiments demonstrate the effectiveness of our proposed approach. The code is released at https://github. com/ZJULearning/MaxSquareLoss.
Domain Randomization and Pyramid Consistency: Simulation-to-Real Generalization Without Accessing Target Domain Data

We propose to harness the potential of simulation for semantic segmentation of real-world self-driving scenes in a domain generalization fashion. The segmentation network is trained without any information about target domains and tested on the unseen target domains. To this end, we propose a new approach of domain randomization and pyramid consistency to learn a model with high generalizability. First, we propose to randomize the synthetic images with styles of real images in terms of visual appearances using auxiliary datasets, in order to effectively learn domain-invariant representations. Second, we further enforce pyramid consistency across different "stylized" images and within an image, in order to learn domain-invariant and scale-invariant features, respectively. Extensive experiments are conducted on generalization from GTA and SYNTHIA to Cityscapes, BDDS, and Mapillary; and our method achieves superior results over the state-of-the-art techniques. Remarkably, our generalization results are on par with or even better than those obtained by state-of-the-art simulation-to-real domain adaptation methods, which access the target domain data at training time.
Semi-Supervised Skin Detection by Network With Mutual Guidance

We propose to harness the potential of simulation for semantic segmentation of real-world self-driving scenes in a domain generalization fashion. The segmentation network is trained without any information about target domains and tested on the unseen target domains. To this end, we propose a new approach of domain randomization and pyramid consistency to learn a model with high generalizability. First, we propose to randomize the synthetic images with styles of real images in terms of visual appearances using auxiliary datasets, in order to effectively learn domain-invariant representations. Second, we further enforce pyramid consistency across different "stylized" images and within an image, in order to learn domain-invariant and scale-invariant features, respectively. Extensive experiments are conducted on generalization from GTA and SYNTHIA to Cityscapes, BDDS, and Mapillary; and our method achieves superior results over the state-of-the-art techniques. Remarkably, our generalization results are on par with or even better than those obtained by state-of-the-art simulation-to-real domain adaptation methods, which access the target domain data at training time.
ACE: Adapting to Changing Environments for Semantic Segmentation

Deep neural networks exhibit exceptional accuracy when they are trained and tested on the same data distributions. However, neural classifiers are often extremely brittle when confronted with domain shift---changes in the input distribution that occur over time. We present ACE, a framework for semantic segmentation that dynamically adapts to changing environments over time. By aligning the distribution of labeled training data from the original source domain with the distribution of incoming data in a shifted domain, ACE synthesizes labeled training data for environments as it sees them. This stylized data is then used to update a segmentation model so that it performs well in new environments. To avoid forgetting knowledge from past environments, we introduce a memory that stores feature statistics from previously seen domains. These statistics can be used to replay images in any of the previously observed domains, thus preventing catastrophic forgetting. In addition to standard batch training using stochastic gradient decent (SGD), we also experiment with fast adaptation methods based on adaptive meta-learning. Extensive experiments are conducted on two datasets from SYNTHIA, the results demonstrate the effectiveness of the proposed approach when adapting to a number of tasks. 
Efficient Segmentation: Learning Downsampling Near Semantic Boundaries

Many automated processes such as auto-piloting rely on a good semantic segmentation as a critical component. To speed up performance, it is common to downsample the input frame. However, this comes at the cost of missed small objects and reduced accuracy at semantic boundaries. To address this problem, we propose a new content-adaptive downsampling technique that learns to favor sampling locations near semantic boundaries of target classes. Cost-performance analysis shows that our method consistently outperforms the uniform sampling improving balance between accuracy and computational efficiency. Our adaptive sampling gives segmentation with better quality of boundaries and more reliable support for smaller-size objects.
Recurrent U-Net for Resource-Constrained Segmentation

State-of-the-art segmentation methods rely on very deep networks that are not always easy to train without very large training datasets and tend to be relatively slow to run on standard GPUs. In this paper, we introduce a novel recurrent U-Net architecture that preserves the compactness of the original U-Net, while substantially increasing its performance to the point where it outperforms the state of the art on several benchmarks. We will demonstrate its effectiveness for several tasks, including hand segmentation, retina vessel segmentation, and road segmentation. We also introduce a large-scale dataset for hand segmentation. 
Detecting the Unexpected via Image Resynthesis

Classical semantic segmentation methods, including the recent deep learning ones, assume that all classes observed at test time have been seen during training. In this paper, we tackle the more realistic scenario where unexpected objects of unknown classes can appear at test time. The main trends in this area either leverage the notion of prediction uncertainty to flag the regions with low confidence as unknown, or rely on autoencoders and highlight poorly-decoded regions. Having observed that, in both cases, the detected regions typically do not correspond to unexpected objects, in this paper, we introduce a drastically different strategy: It relies on the intuition that the network will produce spurious labels in regions depicting unexpected objects. Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image. In other words, we translate the problem of detecting unknown classes to one of identifying poorly-resynthesized image regions. We show that this outperforms both uncertainty- and autoencoder-based methods.
Self-Supervised Monocular Depth Hints

Monocular depth estimators can be trained with various forms of self-supervision from binocular-stereo data to circumvent the need for high-quality laser-scans or other ground-truth data. The disadvantage, however, is that the photometric reprojection losses used with self-supervised learning typically have multiple local minima. These plausible-looking alternatives to ground-truth can restrict what a regression network learns, causing it to predict depth maps of limited quality. As one prominent example, depth discontinuities around thin structures are often incorrectly estimated by current state-of-the-art methods. Here, we study the problem of ambiguous reprojections in depth-prediction from stereo-based self-supervision, and introduce Depth Hints to alleviate their effects. Depth Hints are complementary depth-suggestions obtained from simple off-the-shelf stereo algorithms. These hints enhance an existing photometric loss function, and are used to guide a network to learn better weights. They require no additional data, and are assumed to be right only sometimes. We show that using our Depth Hints gives a substantial boost when training several leading self-supervised-from-stereo models, not just our own. Further, combined with other good practices, we produce state-of-the-art depth predictions on the KITTI benchmark.
3D Scene Reconstruction With Multi-Layer Depth and Epipolar Transformers

We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel "Epipolar Feature Transformer" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.
How Do Neural Networks See Depth in Single Images?

Deep neural networks have lead to a breakthrough in depth estimation from single images. Recent work shows that the quality of these estimations is rapidly increasing. It is clear that neural networks can see depth in single images. However, to the best of our knowledge, no work currently exists that analyzes what these networks have learned. In this work we take four previously published networks and investigate what depth cues they exploit. We find that all networks ignore the apparent size of known obstacles in favor of their vertical position in the image. The use of the vertical position requires the camera pose to be known; however, we find that these networks only partially recognize changes in camera pitch and roll angles. Small changes in camera pitch are shown to disturb the estimated distance towards obstacles. The use of the vertical image position allows the networks to estimate depth towards arbitrary obstacles - even those not appearing in the training set - but may depend on features that are not universally present.
On Boosting Single-Frame 3D Human Pose Estimation via Monocular Videos

The premise of training an accurate 3D human pose estimation network is the possession of huge amount of richly annotated training data. Nonetheless, manually obtaining rich and accurate annotations is, even not impossible, tedious and slow. In this paper, we propose to exploit monocular videos to complement the training dataset for the single-image 3D human pose estimation tasks. At the beginning, a baseline model is trained with a small set of annotations. By fixing some reliable estimations produced by the resulting model, our method automatically collects the annotations across the entire video as solving the 3D trajectory completion problem. Then, the baseline model is further trained with the collected annotations to learn the new poses. We evaluate our method on the broadly-adopted Human3.6M and MPI-INF-3DHP datasets. As illustrated in experiments, given only a small set of annotations, our method successfully makes the model to learn new poses from unlabelled monocular videos, promoting the accuracies of the baseline model by about 10%. By contrast with previous approaches, our method does not rely on either multi-view imagery or any explicit 2D keypoint annotations.
Canonical Surface Mapping via Geometric Cycle Consistency

We explore the task of Canonical Surface Mapping (CSM). Specifically, given an image, we learn to map pixels on the object to their corresponding locations on an abstract 3D model of the category. But how do we learn such a mapping? A supervised approach would require extensive manual labeling which is not scalable beyond a few hand-picked categories. Our key insight is that the CSM task (pixel to 3D), when combined with 3D projection (3D to pixel), completes a cycle. Hence, we can exploit a geometric cycle consistency loss, thereby allowing us to forgo the dense manual supervision. Our approach allows us to train a CSM model for a diverse set of classes, without sparse or dense keypoint annotation, by leveraging only foreground mask labels for training. We show that our predictions also allow us to infer dense correspondence between two images, and compare the performance of our approach against several methods that predict correspondence by leveraging varying amount of supervision.
GP2C: Geometric Projection Parameter Consensus for Joint 3D Pose and Focal Length Estimation in the Wild

We present a joint 3D pose and focal length estimation approach for object categories in the wild. In contrast to previous methods that predict 3D poses independently of the focal length or assume a constant focal length, we explicitly estimate and integrate the focal length into the 3D pose estimation. For this purpose, we combine deep learning techniques and geometric algorithms in a two-stage approach: First, we estimate an initial focal length and establish 2D-3D correspondences from a single RGB image using a deep network. Second, we recover 3D poses and refine the focal length by minimizing the reprojection error of the predicted correspondences. In this way, we exploit the geometric prior given by the focal length for 3D pose estimation. This results in two advantages: First, we achieve significantly improved 3D translation and 3D pose accuracy compared to existing methods. Second, our approach finds a geometric consensus between the individual projection parameters, which is required for precise 2D-3D alignment. We evaluate our proposed approach on three challenging real-world datasets (Pix3D, Comp, and Stanford) with different object categories and significantly outperform the state-of-the-art by up to 20% absolute in multiple different metrics.
Moulding Humans: Non-Parametric 3D Human Shape Estimation From Single Images

In this paper, we tackle the problem of 3D human shape estimation from single RGB images. While the recent progress in convolutional neural networks has allowed impressive results for 3D human pose estimation, estimating the full 3D shape of a person is still an open issue. Model-based approaches can output precise meshes of naked under-cloth human bodies but fail to estimate details and un-modelled elements such as hair or clothing. On the other hand, non-parametric volumetric approaches can potentially estimate complete shapes but, in practice, they are limited by the resolution of the output grid and cannot produce detailed estimates. In this work, we propose a non-parametric approach that employs a double depth map to represent the 3D shape of a person: a visible depth map and a "hidden" depth map are estimated and combined, to reconstruct the human 3D shape as done with a "mould". This representation through 2D depth maps allows a higher resolution output with a much lower dimension than voxel-based volumetric representations. Additionally, our fully derivable depth-based model allows us to efficiently incorporate a discriminator in an adversarial fashion to improve the accuracy and "humanness" of the 3D output. We train and quantitatively validate our approach on SURREAL and on 3D-HUMANS, a new photorealistic dataset made of semi-synthetic in-house videos annotated with 3D ground truth surfaces.
3DPeople: Modeling the Geometry of Dressed Humans

Recent advances in 3D human shape estimation build upon parametric representations that model very well the shape of the naked body, but are not appropriate to represent the clothing geometry. In this paper, we present an approach to model dressed humans and predict their geometry from single images. We contribute in three fundamental aspects of the problem, namely, a new dataset, a novel shape parameterization algorithm and an end-to-end deep generative network for predicting shape. First, we present 3DPeople, a large-scale synthetic dataset with 2 Million photo-realistic images of 80 subjects performing 70 activities and wearing diverse outfits. Besides providing textured 3D meshes for clothes and body we annotated the dataset with segmentation masks, skeletons, depth, normal maps and optical flow. All this together makes 3DPeople suitable for a plethora of tasks. We then represent the 3D shapes using 2D geometry images. To build these images we propose a novel spherical area-preserving parameterization algorithm based on the optimal mass transportation method. We show this approach to improve existing spherical maps which tend to shrink the elongated parts of the full body models such as the arms and legs, making the geometry images incomplete. Finally, we design a multi-resolution deep generative network that, given an input image of a dressed human, predicts his/her geometry image (and thus the clothed body shape) in an end-to-end manner. We obtain very promising results in jointly capturing body pose and clothing shape, both for synthetic validation and on the wild images. 
Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop

Model-based human pose estimation is currently approached through two different paradigms. Optimization-based methods fit a parametric body model to 2D observations in an iterative manner, leading to accurate image-model alignments, but are often slow and sensitive to the initialization. In contrast, regression-based methods, that use a deep network to directly estimate the model parameters from pixels, tend to provide reasonable, but not pixel accurate, results while requiring huge amounts of supervision. In this work, instead of investigating which approach is better, our key insight is that the two paradigms can form a strong collaboration. A reasonable, directly regressed estimate from the network can initialize the iterative optimization making the fitting faster and more accurate. Similarly, a pixel accurate fit from iterative optimization can act as strong supervision for the network. This is the core of our proposed approach SPIN (SMPL oPtimization IN the loop). The deep network initializes an iterative optimization routine that fits the body model to 2D joints within the training loop, and the fitted estimate is subsequently used to supervise the network. Our approach is self-improving by nature, since better network estimates can lead the optimization to better solutions, while more accurate optimization fits provide better supervision for the network. We demonstrate the effectiveness of our approach in different settings, where 3D ground truth is scarce, or not available, and we consistently outperform the state-of-the-art model-based pose estimation approaches by significant margins. The project website with videos, results, and code can be found at https://seas.upenn.edu/ nkolot/projects/spin.
Optimizing Network Structure for 3D Human Pose Estimation

A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation power when used for estimating 3D poses. We overcome the limitation by introducing Locally Connected Network (LCN) which is naturally implemented by this generic formulation. It notably improves the representation capability over GCN. In addition, since every joint is only connected to a few joints in its neighborhood, it has strong generalization power. The experiments on public datasets show it: (1) outperforms the state-of-the-arts; (2) is less data hungry than alternative models; (3) generalizes well to unseen actions and datasets.
Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks

Despite great progress in 3D pose estimation from single-view images or videos, it remains a challenging task due to the substantial depth ambiguity and severe self-occlusions. Motivated by the effectiveness of incorporating spatial dependencies and temporal consistencies to alleviate these issues, we propose a novel graph-based method to tackle the problem of 3D human body and 3D hand pose estimation from a short sequence of 2D joint detections. Particularly, domain knowledge about the human hand (body) configurations is explicitly incorporated into the graph convolutional operations to meet the specific demand of the 3D pose estimation. Furthermore, we introduce a local-to-global network architecture, which is capable of learning multi-scale features for the graph-based representations. We evaluate the proposed method on challenging benchmark datasets for both 3D hand pose estimation and 3D body pose estimation. Experimental results show that our method achieves state-of-the-art performance on both tasks.
Resolving 3D Human Pose Ambiguities With 3D Scene Constraints

To understand and analyze human behavior, we need to capture humans moving in, and interacting with, the world. Most existing methods perform 3D human pose estimation without explicitly considering the scene. We observe however that the world constrains the body and vice-versa. To motivate this, we show that current 3D human pose estimation methods produce results that are not consistent with the 3D scene. Our key contribution is to exploit static 3D scene structure to better estimate human pose from monocular images. The method enforces Proximal Relationships with Object eXclusion and is called PROX. To test this, we collect a new dataset composed of 12 different 3D scenes and RGB sequences of 20 subjects moving in and interacting with the scenes. We represent human pose using the 3D human body model SMPL-X and extend SMPLify-X to estimate body pose using scene constraints. We make use of the 3D scene information by formulating two main constraints. The inter-penetration constraint penalizes intersection between the body model and the surrounding 3D scene. The contact constraint encourages specific parts of the body to be in contact with scene surfaces if they are close enough in distance and orientation. For quantitative evaluation we capture a separate dataset with 180 RGB frames in which the ground-truth body pose is estimated using a motion capture system. We show quantitatively that introducing scene constraints significantly reduces 3D joint error and vertex error. Our code and data are available for research at https://prox.is.tue.mpg.de.
Tex2Shape: Detailed Full Human Body Geometry From a Single Image

We present a simple yet effective method to infer detailed full human body shape from only a single photograph. Our model can infer full-body shape including face, hair, and clothing including wrinkles at interactive frame-rates. Results feature details even on parts that are occluded in the input image. Our main idea is to turn shape regression into an aligned image-to-image translation problem. The input to our method is a partial texture map of the visible region obtained from off-the-shelf methods. From a partial texture, we estimate detailed normal and vector displacement maps, which can be applied to a low-resolution smooth body model to add detail and clothing. Despite being trained purely with synthetic data, our model generalizes well to real-world photographs. Numerous results demonstrate the versatility and robustness of our method. 
PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization

We introduce Pixel-aligned Implicit Function (PIFu), an implicit representation that locally aligns pixels of 2D images with the global context of their corresponding 3D object. Using PIFu, we propose an end-to-end deep learning method for digitizing highly detailed clothed humans that can infer both 3D surface and texture from a single image, and optionally, multiple input images. Highly intricate shapes, such as hairstyles, clothing, as well as their variations and deformations can be digitized in a unified way. Compared to existing representations used for 3D deep learning, PIFu produces high-resolution surfaces including largely unseen regions such as the back of a person. In particular, it is memory efficient unlike the voxel representation, can handle arbitrary topology, and the resulting surface is spatially aligned with the input image. Furthermore, while previous techniques are designed to process either a single image or multiple views, PIFu extends naturally to arbitrary number of views. We demonstrate high-resolution and robust reconstructions on real world images from the DeepFashion dataset, which contains a variety of challenging clothing types. Our method achieves state-of-the-art performance on a public benchmark and outperforms the prior work for clothed human digitization from a single image.
DF2Net: A Dense-Fine-Finer Network for Detailed 3D Face Reconstruction

Reconstructing the detailed geometric structure from a single face image is a challenging problem due to its ill-posed nature and the fine 3D structures to be recovered. This paper proposes a deep Dense-Fine-Finer Network (DF2Net) to address this challenging problem. DF2Net decomposes the reconstruction process into three stages, each of which is processed by an elaborately-designed network, namely D-Net, F-Net, and Fr-Net. D-Net exploits a U-net architecture to map the input image to a dense depth image. F-Net refines the output of D-Net by integrating features from depth and RGB domains, whose output is further enhanced by Fr-Net with a novel multi-resolution hypercolumn architecture. In addition, we introduce three types of data to train these networks, including 3D model synthetic data, 2D image reconstructed data, and fine facial images. We elaborately exploit different datasets (or combination) together with well-designed losses to train different networks. Qualitative evaluation indicates that our DF2Net can effectively reconstruct subtle facial details such as small crow's feet and wrinkles. Our DF2Net achieves performance superior or comparable to state-of-the-art algorithms in qualitative and quantitative analyses on real-world images and the BU-3DFE dataset. Code and the collected 70K image-depth data will be publicly available.
Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking

Monocular 3D human-pose estimation from static images is a challenging problem, due to the curse of dimensionality and the ill-posed nature of lifting 2D-to-3D. In this paper, we propose a Deep Conditional Variational Autoencoder based model that synthesizes diverse anatomically plausible 3D-pose samples conditioned on the estimated 2D-pose. We show that CVAE-based 3D-pose sample set is consistent with the 2D-pose and helps tackling the inherent ambiguity in 2D-to-3D lifting. We propose two strategies for obtaining the final 3D pose- (a) depth-ordering/ordinal relations to score and weight-average the candidate 3D-poses, referred to as OrdinalScore, and (b) with supervision from an Oracle. We report close to state-of-the-art results on two benchmark datasets using OrdinalScore, and state-of-the-art results using the Oracle. We also show that our pipeline yields competitive results without paired image-to-3D annotations. The training and evaluation code is available at https://github.com/ssfootball04/generative_pose.
Aligning Latent Spaces for 3D Hand Pose Estimation

Hand pose estimation from monocular RGB inputs is a highly challenging task. Many previous works for monocular settings only used RGB information for training despite the availability of corresponding data in other modalities such as depth maps. In this work, we propose to learn a joint latent representation that leverages other modalities as weak labels to boost the RGB-based hand pose estimator. By design, our architecture is highly flexible in embedding various diverse modalities such as heat maps, depth maps and point clouds. In particular, we find that encoding and decoding the point cloud of the hand surface can improve the quality of the joint latent representation. Experiments show that with the aid of other modalities during training, our proposed method boosts the accuracy of RGB-based hand pose estimation systems and significantly outperforms state-of-the-art on two public benchmarks.
HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation

Estimating 3D human pose from a single image is a challenging task. This work attempts to address the uncertainty of lifting the detected 2D joints to the 3D space by introducing an intermediate state - Part-Centric Heatmap Triplets (HEMlets), which shortens the gap between the 2D observation and the 3D interpretation. The HEMlets utilize three joint-heatmaps to represent the relative depth information of the end-joints for each skeletal body part. In our approach, a Convolutional Network(ConvNet) is first trained to predict HEMlests from the input image, followed by a volumetric joint-heatmap regression. We leverage on the integral operation to extract the joint locations from the volumetric heatmaps, guaranteeing end-to-end learning. Despite the simplicity of the network design, the quantitative comparisons show a significant performance improvement over the best-of-grade method (by 20% on Human3.6M). The proposed method naturally supports training with "in-the-wild" images, where only weakly-annotated relative depth information of skeletal joints is available. This further improves the generalization ability of our model, as validated by qualitative comparisons on outdoor images.
End-to-End Hand Mesh Recovery From a Monocular RGB Image

In this paper, we present a HAnd Mesh Recovery (HAMR) framework to tackle the problem of reconstructing the full 3D mesh of a human hand from a single RGB image. In contrast to existing research on 2D or 3D hand pose estimation from RGB or/and depth image data, HAMR can provide a more expressive and useful mesh representation for monocular hand image understanding. In particular, the mesh representation is achieved by parameterizing a generic 3D hand model with shape and relative 3D joint angles. By utilizing this mesh representation, we can easily compute the 3D joint locations via linear interpolations between the vertexes of the mesh, while obtain the 2D joint locations with a projection of the 3D joints. To this end, a differentiable re-projection loss can be defined in terms of the derived representations and the ground-truth labels, thus making our framework end-to-end trainable. Qualitative experiments show that our framework is capable of recovering appealing 3D hand mesh even in the presence of severe occlusions. Quantitatively, our approach also outperforms the state-of-the-art methods for both 2D and 3D hand pose estimation from a monocular RGB image on several benchmark datasets.
Robust Multi-Modality Multi-Object Tracking

Multi-sensor perception is crucial to ensure the reliability and accuracy in autonomous driving system, while multi-object tracking (MOT) improves that by tracing sequential movement of dynamic objects. Most current approaches for multi-sensor multi-object tracking are either lack of reliability by tightly relying on a single input source (e.g., center camera), or not accurate enough by fusing the results from multiple sensors in post processing without fully exploiting the inherent information. In this study, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and could further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. We conduct extensive experiments to evaluate the effectiveness of the proposed framework on the challenging KITTI benchmark and report state-of-the-art performance. Code and models are available at https://github.com/ZwwWayne/mmMOT.
The Trajectron: Probabilistic Multi-Agent Trajectory Modeling With Dynamic Spatiotemporal Graphs

Developing safe human-robot interaction systems is a necessary step towards the widespread integration of autonomous agents in society. A key component of such systems is the ability to reason about the many potential futures (e.g. trajectories) of other agents in the scene. Towards this end, we present the Trajectron, a graph-structured model that predicts many potential future trajectories of multiple agents simultaneously in both highly dynamic and multimodal scenarios (i.e. where the number of agents in the scene is time-varying and there are many possible highly-distinct futures for each agent). It combines tools from recurrent sequence modeling and variational deep generative modeling to produce a distribution of future trajectories for each agent in a scene. We demonstrate the performance of our model on several datasets, obtaining state-of-the-art results on standard trajectory prediction metrics as well as introducing a new metric for comparing models that output distributions.
'Skimming-Perusal' Tracking: A Framework for Real-Time and Robust Long-Term Tracking

Compared with traditional short-term tracking, long-term tracking poses more challenges and is much closer to realistic applications. However, few works have been done and their performance have also been limited. In this work, we present a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules. The perusal module consists of an effective bounding box regressor to generate a series of candidate proposals and a robust target verifier to infer the optimal candidate with its confidence score. Based on this score, our tracker determines whether the tracked object being present or absent, and then chooses the tracking strategies of local search or global search respectively in the next frame. To speed up the image-wide global search, a novel skimming module is designed to efficiently choose the most possible regions from a large number of sliding windows. Numerous experimental results on the VOT-2018 long-term and OxUvA long-term benchmarks demonstrate that the proposed method achieves the best performance and runs in real-time. The source codes are available at https://github.com/iiau-tracker/SPLT.
TASED-Net: Temporally-Aggregating Spatial Encoder-Decoder Network for Video Saliency Detection

TASED-Net is a 3D fully-convolutional network architecture for video saliency detection. It consists of two building blocks: first, the encoder network extracts low-resolution spatiotemporal features from an input clip of several consecutive frames, and then the following prediction network decodes the encoded features spatially while aggregating all the temporal information. As a result, a single prediction map is produced from an input clip of multiple frames. Frame-wise saliency maps can be predicted by applying TASED-Net in a sliding-window fashion to a video. The proposed approach assumes that the saliency map of any frame can be predicted by considering a limited number of past frames. The results of our extensive experiments on video saliency detection validate this assumption and demonstrate that our fully-convolutional model with temporal aggregation method is effective. TASED-Net significantly outperforms previous state-of-the-art approaches on all three major large-scale datasets of video saliency detection: DHF1K, Hollywood2, and UCFSports. After analyzing the results qualitatively, we observe that our model is especially better at attending to salient moving objects.
Attacking Optical Flow

Deep neural nets achieve state-of-the-art performance on the problem of optical flow estimation. Since optical flow is used in several safety-critical applications like self-driving cars, it is important to gain insights into the robustness of those techniques. Recently, it has been shown that adversarial attacks easily fool deep neural networks to misclassify objects. The robustness of optical flow networks to adversarial attacks, however, has not been studied so far. In this paper, we extend adversarial patch attacks to optical flow networks and show that such attacks can compromise their performance. We show that corrupting a small patch of less than 1% of the image size can significantly affect optical flow estimates. Our attacks lead to noisy flow estimates that extend significantly beyond the region of the attack, in many cases even completely erasing the motion of objects in the scene. While networks using an encoder-decoder architecture are very sensitive to these attacks, we found that networks using a spatial pyramid architecture are less affected. We analyse the success and failure of attacking both architectures by visualizing their feature maps and comparing them to classical optical flow techniques which are robust to these attacks. We also demonstrate that such attacks are practical by placing a printed pattern into real scenes.
Pro-Cam SSfM: Projector-Camera System for Structure and Spectral Reflectance From Motion

In this paper, we propose a novel projector-camera system for practical and low-cost acquisition of a dense object 3D model with the spectral reflectance property. In our system, we use a standard RGB camera and leverage an off-the-shelf projector as active illumination for both the 3D reconstruction and the spectral reflectance estimation. We first reconstruct the 3D points while estimating the poses of the camera and the projector, which are alternately moved around the object, by combining multi-view structured light and structure-from-motion (SfM) techniques. We then exploit the projector for multispectral imaging and estimate the spectral reflectance of each 3D point based on a novel spectral reflectance estimation model considering the geometric relationship between the reconstructed 3D points and the estimated projector positions. Experimental results on several real objects demonstrate that our system can precisely acquire a dense 3D model with the full spectral reflectance property using off-the-shelf devices.
Mop Moire Patterns Using MopNet

Moire pattern is a common image quality degradation caused by frequency aliasing between monitors and cameras when taking screen-shot photos. The complex frequency distribution, imbalanced magnitude in colour channels, and diverse appearance attributes of moire pattern make its removal a challenging problem. In this paper, we propose a Moire pattern Removal Neural Network (MopNet) to solve this problem. All core components of MopNet are specially designed for unique properties of moire patterns, including the multi-scale feature aggregation addressing complex frequency, the channel-wise target edge predictor to exploit imbalanced magnitude among colour channels, and the attribute-aware classifier to characterize the diverse appearance for better modelling Moire patterns. Quantitative and qualitative experimental comparison validate the state-of-the-art performance of MopNet.
Kernel Modeling Super-Resolution on Real Low-Resolution Images

Deep convolutional neural networks (CNNs), trained on corresponding pairs of high- and low-resolution images, achieve state-of-the-art performance in single-image super-resolution and surpass previous signal-processing based approaches. However, their performance is limited when applied to real photographs. The reason lies in their training data: low-resolution (LR) images are obtained by bicubic interpolation of the corresponding high-resolution (HR) images. The applied convolution kernel significantly differs from real-world camera-blur. Consequently, while current CNNs well super-resolve bicubic-downsampled LR images, they often fail on camera-captured LR images. To improve generalization and robustness of deep super-resolution CNNs on real photographs, we present a kernel modeling super-resolution network (KMSR) that incorporates blur-kernel modeling in the training. Our proposed KMSR consists of two stages: we first build a pool of realistic blur-kernels with a generative adversarial network (GAN) and then we train a super-resolution network with HR and corresponding LR images constructed with the generated kernels. Our extensive experimental validations demonstrate the effectiveness of our single-image super-resolution approach on photographs with unknown blur-kernels.
Learning to Jointly Generate and Separate Reflections

Existing learning-based single image reflection removal methods using paired training data have fundamental limitations about the generalization capability on real-world reflections due to the limited variations in training pairs. In this work, we propose to jointly generate and separate reflections within a weakly-supervised learning framework, aiming to model the reflection image formation more comprehensively with abundant unpaired supervision. By imposing the adversarial losses and combinable mapping mechanism in a multi-task structure, the proposed framework elegantly integrates the two separate stages of reflection generation and separation into a unified model. The gradient constraint is incorporated into the concurrent training process of the multi-task learning as well. In particular, we built up an unpaired reflection dataset with 4,027 images, which is useful for facilitating the weakly-supervised learning of reflection removal model. Extensive experiments on a public benchmark dataset show that our framework performs favorably against state-of-the-art methods and consistently produces visually appealing results.
Deep Multi-Model Fusion for Single-Image Dehazing

This paper presents a deep multi-model fusion network to attentively integrate multiple models to separate layers and boost the performance in single-image dehazing. To do so, we first formulate the attentional feature integration module to maximize the integration of the convolutional neural network (CNN) features at different CNN layers and generate the attentional multi-level integrated features (AMLIF). Then, from the AMLIF, we further predict a haze-free result for an atmospheric scattering model, as well as for four haze-layer separation models, and then fuse the results together to produce the final haze-free image. To evaluate the effectiveness of our method, we compare our network with several state-of-the-art methods on two widely-used dehazing benchmark datasets, as well as on two sets of real-world hazy images. Experimental results demonstrate clear quantitative and qualitative improvements of our method over the state-of-the-arts.
Deep Learning for Seeing Through Window With Raindrops

When taking pictures through glass window in rainy day, the images are comprised and corrupted by the raindrops adhered to glass surfaces. It is a challenging problem to remove the effect of raindrops from an image. The key task is how to accurately and robustly identify the raindrop regions in an image. This paper develops a convolutional neural network (CNN) for removing the effect of raindrops from an image. In the proposed CNN, we introduce a double attention mechanism that concurrently guides the CNN using shape-driven attention and channel re-calibration. The shape-driven attention exploits physical shape priors of raindrops, i.e. convexness and contour closedness, to accurately locate raindrops, and the channel re-calibration improves the robustness when processing raindrops with varying appearances. The experimental results show that the proposed CNN outperforms the state-of-the-art approaches in terms of both quantitative metrics and visual quality.
Mask-ShadowGAN: Learning to Remove Shadows From Unpaired Data

This paper presents a new method for shadow removal using unpaired data, enabling us to avoid tedious annotations and obtain more diverse training samples. However, directly employing adversarial learning and cycle-consistency constraints is insufficient to learn the underlying relationship between the shadow and shadow-free domains, since the mapping between shadow and shadow-free images is not simply one-to-one. To address the problem, we formulate Mask-ShadowGAN, a new deep framework that automatically learns to produce a shadow mask from the input shadow image and then takes the mask to guide the shadow generation via re-formulated cycle-consistency constraints. Particularly, the framework simultaneously learns to produce shadow masks and learns to remove shadows, to maximize the overall performance. Also, we prepared an unpaired dataset for shadow removal and demonstrated the effectiveness of Mask-ShadowGAN on various experiments, even it was trained on unpaired data.
Spatio-Temporal Filter Adaptive Network for Video Deblurring

Video deblurring is a challenging task due to the spatially variant blur caused by camera shake, object motions, and depth variations, etc. Existing methods usually estimate optical flow in the blurry video to align consecutive frames or approximate blur kernels. However, they tend to generate artifacts or cannot effectively remove blur when the estimated optical flow is not accurate. To overcome the limitation of separate optical flow estimation, we propose a Spatio-Temporal Filter Adaptive Network (STFAN) for the alignment and deblurring in a unified framework. The proposed STFAN takes both blurry and restored images of the previous frame as well as blurry image of the current frame as input, and dynamically generates the spatially adaptive filters for the alignment and deblurring. We then propose the new Filter Adaptive Convolutional (FAC) layer to align the deblurred features of the previous frame with the current frame and remove the spatially variant blur from the features of the current frame. Finally, we develop a reconstruction network which takes the fusion of two transformed features to restore the clear frames. Both quantitative and qualitative evaluation results on the benchmark datasets and real-world videos demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in terms of accuracy, speed as well as model size.
Learning Deep Priors for Image Dehazing

Image dehazing is a well-known ill-posed problem, which usually requires some image priors to make the problem well-posed. We propose an effective iteration algorithm with deep CNNs to learn haze-relevant priors for image dehazing. We formulate the image dehazing problem as the minimization of a variational model with favorable data fidelity terms and prior terms to regularize the model. We solve the variational model based on the classical gradient descent method with built-in deep CNNs so that iteration-wise image priors for the atmospheric light, transmission map and clear image can be well estimated. Our method combines the properties of both the physical formation of image dehazing as well as deep learning approaches. We show that it is able to generate clear images as well as accurate atmospheric light and transmission maps. Extensive experimental results demonstrate that the proposed algorithm performs favorably against state-of-the-art methods in both benchmark datasets and real-world images.
JPEG Artifacts Reduction via Deep Convolutional Sparse Coding

To effectively reduce JPEG compression artifacts, we propose a deep convolutional sparse coding (DCSC) network architecture. We design our DCSC in the framework of classic learned iterative shrinkage-threshold algorithm. To focus on recognizing and separating artifacts only, we sparsely code the feature maps instead of the raw image. The final de-blocked image is directly reconstructed from the coded features. We use dilated convolution to extract multi-scale image features, which allows our single model to simultaneously handle multiple JPEG compression levels. Since our method integrates model-based convolutional sparse coding with a learning-based deep neural network, the entire network structure is compact and more explainable. The resulting lightweight model generates comparable or better de-blocking results when compared with state-of-the-art methods.
Self-Guided Network for Fast Image Denoising

During the past years, tremendous advances in image restoration tasks have been achieved using highly complex neural networks. Despite their good restoration performance, the heavy computational burden hinders the deployment of these networks on constrained devices, e.g. smart phones and consumer electronic products. To tackle this problem, we propose a self-guided network (SGN), which adopts a top-down self-guidance architecture to better exploit image multi-scale information. SGN directly generates multi-resolution inputs with the shuffling operation. Large-scale contextual information extracted at low resolution is gradually propagated into the higher resolution sub-networks to guide the feature extraction processes at these scales. Such a self-guidance strategy enables SGN to efficiently incorporate multi-scale information and extract good local features to recover noisy images. We validate the effectiveness of SGN through extensive experiments. The experimental results demonstrate that SGN greatly improves the memory and runtime efficiency over state-of-the-art efficient methods, without trading off PSNR accuracy. 
Non-Local Intrinsic Decomposition With Near-Infrared Priors

Intrinsic image decomposition is a highly under-constrained problem that has been extensively studied by computer vision researchers. Previous methods impose additional constraints by exploiting either empirical or data-driven priors. In this paper, we revisit intrinsic image decomposition with the aid of near-infrared (NIR) imagery. We show that NIR band is considerably less sensitive to textures and can be exploited to reduce ambiguity caused by reflectance variation, promoting a simple yet powerful prior for shading smoothness. With this observation, we formulate intrinsic decomposition as an energy minimisation problem. Unlike existing methods, our energy formulation decouples reflectance and shading estimation, into a convex local shading component based on NIR-RGB image pair, and a reflectance component that encourages reflectance homogeneity both locally and globally. We further show the minimisation process can be approached by a series of multi-dimensional kernel convolutions, each within linear time complexity. To validate the proposed algorithm, a NIR-RGB dataset is captured over real-world objects, where our NIR-assisted approach demonstrates clear superiority over RGB methods.
VideoMem: Constructing, Analyzing, Predicting Short-Term and Long-Term Video Memorability

Humans share a strong tendency to memorize/forget some of the visual information they encounter. This paper focuses on understanding the intrinsic memorability of visual content. To address this challenge, we introduce a large scale dataset (VideoMem) composed of 10,000 videos with memorability scores. In contrast to previous work on image memorability -- where memorability was measured a few minutes after memorization -- memory performance is measured twice: a few minutes and again 24-72 hours after memorization. Hence, the dataset comes with short-term and long-term memorability annotations. After an in-depth analysis of the dataset, we investigate various deep neural network-based models for the prediction of video memorability. Our best model using a ranking loss achieves a Spearman's rank correlation of 0.494 (respectively 0.256) for short-term (resp. long-term) memorability prediction, while our model with attention mechanism provides insights of what makes a content memorable. The VideoMem dataset with pre-extracted features is publicly available.
Rescan: Inductive Instance Segmentation for Indoor RGBD Scans

 In depth-sensing applications ranging from home robotics to AR/VR, it will be common to acquire 3D scans of interior spaces repeatedly at sparse time intervals (e.g., as part of regular daily use). We propose an algorithm that analyzes these "rescans" to infer a temporal model of a scene with semantic instance information. Our algorithm operates inductively by using the temporal model resulting from past observations to infer an instance segmentation of a new scan, which is then used to update the temporal model. The model contains object instance associations across time and thus can be used to track individual objects, even though there are only sparse observations. During experiments with a new benchmark for the new task, our algorithm outperforms alternate approaches based on state-of-the-art networks for semantic instance segmentation. 
End-to-End CAD Model Retrieval and 9DoF Alignment in 3D Scans

We present a novel, end-to-end approach to align CAD models to an 3D scan of a scene, enabling transformation of a noisy, incomplete 3D scan to a compact, CAD reconstruction with clean, complete object geometry. Our main contribution lies in formulating a differentiable Procrustes alignment that is paired with a symmetry-aware dense object correspondence prediction. To simultaneously align CAD models to all the objects of a scanned scene, our approach detects object locations, then predicts symmetry-aware dense object correspondences between scan and CAD geometry in a unified object space, as well as a nearest neighbor CAD model, both of which are then used to inform a differentiable Procrustes alignment. Our approach operates in a fully-convolutional fashion, enabling alignment of CAD models to the objects of a scan in a single forward pass. This enables our method to outperform state-of-the-art approaches by 19.04% for CAD model alignment to scans, with approximately 250x faster runtime than previous data-driven approaches.
Making History Matter: History-Advantage Sequence Training for Visual Dialog

We study the multi-round response generation in visual dialog, where a response is generated according to a visually grounded conversational history. Given a triplet: an image, Q&A history, and current question, all the prevailing methods follow a codec (i.e., encoder-decoder) fashion in a supervised learning paradigm: a multimodal encoder encodes the triplet into a feature vector, which is then fed into the decoder for the current answer generation, supervised by the ground-truth. However, this conventional supervised learning does NOT take into account the impact of imperfect history, violating the conversational nature of visual dialog and thus making the codec more inclined to learn history bias but not contextual reasoning. To this end, inspired by the actor-critic policy gradient in reinforcement learning, we propose a novel training paradigm called History Advantage Sequence Training (HAST). Specifically, we intentionally impose wrong answers in the history, obtaining an adverse critic, and see how the historic error impacts the codec's future behavior by History Advantage -- a quantity obtained by subtracting the adverse critic from the gold reward of ground-truth history. Moreover, to make the codec more sensitive to the history, we propose a novel attention network called History-Aware Co-Attention Network (HACAN) which can be effectively trained by using HAST. Experimental results on three benchmarks: VisDial v0.9&v1.0 and GuessWhat?!, show that the proposed HAST strategy consistently outperforms the state-of-the-art supervised counterparts.
Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization

This paper tackles the problem of large-scale image-based localization (IBL) where the spatial location of a query image is determined by finding out the most similar reference images in a large database. For solving this problem, a critical task is to learn discriminative image representation that captures informative information relevant for localization. We propose a novel representation learning method having higher location-discriminating power. It provides the following contributions: 1) we represent a place (location) as a set of exemplar images depicting the same landmarks and aim to maximize similarities among intra-place images while minimizing similarities among inter-place images; 2) we model a similarity measure as a probability distribution on L_2-metric distances between intra-place and inter-place image representations; 3) we propose a new Stochastic Attraction and Repulsion Embedding (SARE) loss function minimizing the KL divergence between the learned and the actual probability distributions; 4) we give theoretical comparisons between SARE, triplet ranking and contrastive losses. It provides insights into why SARE is better by analyzing gradients. Our SARE loss is easy to implement and pluggable to any CNN. Experiments show that our proposed method improves the localization performance on standard benchmarks by a large margin. Demonstrating the broad applicability of our method, we obtained the third place out of 209 teams in the 2018 Google Landmark Retrieval Challenge. Our code and model are available at https://github.com/Liumouliu/deepIBL.
Scene Graph Prediction With Limited Labels

Visual knowledge bases such as Visual Genome power numerous applications in computer vision, including visual question answering and captioning, but suffer from sparse, incomplete relationships. All scene graph models to date are limited to training on a small set of visual relationships that have thousands of training labels each. Hiring human annotators is expensive, and using textual knowledge base completion methods are incompatible with visual data. In this paper, we introduce a semi-supervised method that assigns probabilistic relationship labels to a large number of unlabeled images using few labeled examples. We analyze visual relationships to suggest two types of image-agnostic features that are used to generate noisy heuristics, whose outputs are aggregated using a factor graph-based generative model. With as few as 10 labeled examples per relationship, the generative model creates enough training data to train any existing state-of-the-art scene graph model. We demonstrate that our method outperforms all baseline approaches on scene graph prediction by5.16 recall@100 for PREDCLS. In our limited label setting, we define a complexity metric for relationships that serves as an indicator (R^2 = 0.778) for conditions under which our method succeeds over transfer learning, the de-facto approach for training with limited labels.
Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded

Many vision and language models suffer from poor visual grounding -- often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. In this work, we propose a generic approach called Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. HINT encourages deep networks to be sensitive to the same input regions as humans. Our approach optimizes the alignment between human attention maps and gradient-based network importances -- ensuring that models learn not just to look at but rather rely on visual concepts that humans found relevant for a task when making predictions. We apply HINT to Visual Question Answering and Image Captioning tasks, outperforming top approaches on splits that penalize over-reliance on language priors (VQA-CP and robust captioning) using human attention demonstrations for just 6% of the training data. 
Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment

We address the problem of grounding free-form textual phrases by using weak supervision from image-caption pairs. We propose a novel end-to-end model that uses caption-to-image retrieval as a downstream task to guide the process of phrase localization. Our method, as a first step, infers the latent correspondences between regions-of-interest (RoIs) and phrases in the caption and creates a discriminative image representation using these matched RoIs. In the subsequent step, this learned representation is aligned with the caption. Our key contribution lies in building this "caption-conditioned" image encoding, which tightly couples both the tasks and allows the weak supervision to effectively guide visual grounding. We provide extensive empirical and qualitative analysis to investigate the different components of our proposed model and compare it with competitive baselines. For phrase localization, we report an improvement of 4.9% and 1.3% (absolute) over the prior state-of-the-art on the VisualGenome and Flickr30k Entities datasets. We also report results that are at par with the state-of-the-art on the downstream caption-to-image retrieval task on COCO and Flickr30k datasets.
Adaptive Reconstruction Network for Weakly Supervised Referring Expression Grounding

 Weakly supervised referring expression grounding aims at localizing the referential object in an image according to the linguistic query, where the mapping between the referential object and query is unknown in the training stage. To address this problem, we propose a novel end-to-end adaptive reconstruction network (ARN). It builds the correspondence between image region proposal and query in an adaptive manner: adaptive grounding and collaborative reconstruction. Specifically, we first extract the subject, location and context features to represent the proposals and the query respectively. Then, we design the adaptive grounding module to compute the matching score between each proposal and query by a hierarchical attention model. Finally, based on attention score and proposal features, we reconstruct the input query with a collaborative loss of language reconstruction loss, adaptive reconstruction loss, and attribute classification loss. This adaptive mechanism helps our model to alleviate the variance of different referring expressions. Experiments on four large-scale datasets show ARN outperforms existing state-of-the-art methods by a large margin. Qualitative results demonstrate that the proposed ARN can better handle the situation where multiple objects of a particular category situated together.
Hierarchy Parsing for Image Captioning

It is always well believed that parsing an image into constituent visual patterns would be helpful for understanding and representing an image. Nevertheless, there has not been evidence in support of the idea on describing an image with a natural-language utterance. In this paper, we introduce a new design to model a hierarchy from instance level (segmentation), region level (detection) to the whole image to delve into a thorough image understanding for captioning. Specifically, we present a HIerarchy Parsing (HIP) architecture that novelly integrates hierarchical structure into image encoder. Technically, an image decomposes into a set of regions and some of the regions are resolved into finer ones. Each region then regresses to an instance, i.e., foreground of the region. Such process naturally builds a hierarchal tree. A tree-structured Long Short-Term Memory (Tree-LSTM) network is then employed to interpret the hierarchal structure and enhance all the instance-level, region-level and image-level features. Our HIP is appealing in view that it is pluggable to any neural captioning models. Extensive experiments on COCO image captioning dataset demonstrate the superiority of HIP. More remarkably, HIP plus a top-down attention-based LSTM decoder increases CIDEr-D performance from 120.1% to 127.2% on COCO Karpathy test split. When further endowing instance-level and region-level features from HIP with semantic relation learnt through Graph Convolutional Networks (GCN), CIDEr-D is boosted up to 130.6%.
HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips

Learning text-video embeddings usually requires a dataset of video clips with manually provided captions. However, such datasets are expensive and time consuming to create and therefore difficult to obtain on a large scale. In this work, we propose instead to learn such embeddings from video data with readily available natural language annotations in the form of automatically transcribed narrations. The contributions of this work are three-fold. First, we introduce HowTo100M: a large-scale dataset of 136 million video clips sourced from 1.22M narrated instructional web videos depicting humans performing and describing over 23k different visual tasks. Our data collection procedure is fast, scalable and does not require any additional manual annotation. Second, we demonstrate that a text-video embedding trained on this data leads to state-of-the-art results for text-to-video retrieval and action localization on instructional video datasets such as YouCook2 or CrossTask. Finally, we show that this embedding transfers well to other domains: fine-tuning on generic Youtube videos (MSR-VTT dataset) and movies (LSMDC dataset) outperforms models trained on these datasets alone. Our dataset, code and models are publicly available.
Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network

In this paper, we propose to guide the video caption generation with Part-of-Speech (POS) information, based on a gated fusion of multiple representations of input videos. We construct a novel gated fusion network, with one particularly designed cross-gating (CG) block, to effectively encode and fuse different types of representations, e.g., the motion and content features of an input video. One POS sequence generator relies on this fused representation to predict the global syntactic structure, which is thereafter leveraged to guide the video captioning generation and control the syntax of the generated sentence. Specifically, a gating strategy is proposed to dynamically and adaptively incorporate the global syntactic POS information into the decoder for generating each word. Experimental results on two benchmark datasets, namely MSR-VTT and MSVD, demonstrate that the proposed model can well exploit complementary information from multiple representations, resulting in improved performances. Moreover, the generated global POS information can well capture the global syntactic structure of the sentence, and thus be exploited to control the syntactic structure of the description. Such POS information not only boosts the video captioning performance but also improves the diversity of the generated captions. Our code is at: https://github.com/vsislab/Controllable_XGating.
Multi-View Stereo by Temporal Nonparametric Fusion

We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder-decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from nearby views. We train the encoder-decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices.
Floor-SP: Inverse CAD for Floorplans by Sequential Room-Wise Shortest Path

We propose a novel idea for depth estimation from multi-view image-pose pairs, where the model has capability to leverage information from previous latent-space encodings of the scene. This model uses pairs of images and poses, which are passed through an encoder-decoder model for disparity estimation. The novelty lies in soft-constraining the bottleneck layer by a nonparametric Gaussian process prior. We propose a pose-kernel structure that encourages similar poses to have resembling latent spaces. The flexibility of the Gaussian process (GP) prior provides adapting memory for fusing information from nearby views. We train the encoder-decoder and the GP hyperparameters jointly end-to-end. In addition to a batch method, we derive a lightweight estimation scheme that circumvents standard pitfalls in scaling Gaussian process inference, and demonstrate how our scheme can run in real-time on smart devices.
Polarimetric Relative Pose Estimation

In this paper we consider the problem of relative pose estimation from two images with per-pixel polarimetric information. Using these additional measurements we derive a simple minimal solver for the essential matrix which only requires two point correspondences. The polarization constraints allow us to pointwise recover the 3D surface normal up to a two-fold ambiguity for the diffuse reflection. Since this ambiguity exists per point, there is a combinatorial explosion of possibilities. However, since our solver only requires two point correspondences, we only need to consider 16 configurations when solving for the relative pose. Once the relative orientation is recovered, we show that it is trivial to resolve the ambiguity for the remaining points. For robustness, we also propose a joint optimization between the relative pose and the refractive index to handle the refractive distortion. In experiments, on both synthetic and real data, we demonstrate that by leveraging the additional information available from polarization cameras, we can improve over classical methods which only rely on the 2D-point locations to estimate the geometry. Finally, we demonstrate the practical applicability of our approach by integrating it into a state-of-the-art global Structure-from-Motion pipeline.
Closed-Form Optimal Two-View Triangulation Based on Angular Errors

In this paper we consider the problem of relative pose estimation from two images with per-pixel polarimetric information. Using these additional measurements we derive a simple minimal solver for the essential matrix which only requires two point correspondences. The polarization constraints allow us to pointwise recover the 3D surface normal up to a two-fold ambiguity for the diffuse reflection. Since this ambiguity exists per point, there is a combinatorial explosion of possibilities. However, since our solver only requires two point correspondences, we only need to consider 16 configurations when solving for the relative pose. Once the relative orientation is recovered, we show that it is trivial to resolve the ambiguity for the remaining points. For robustness, we also propose a joint optimization between the relative pose and the refractive index to handle the refractive distortion. In experiments, on both synthetic and real data, we demonstrate that by leveraging the additional information available from polarization cameras, we can improve over classical methods which only rely on the 2D-point locations to estimate the geometry. Finally, we demonstrate the practical applicability of our approach by integrating it into a state-of-the-art global Structure-from-Motion pipeline.
Pix2Vox: Context-Aware 3D Reconstruction From Single and Multi-View Images

Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.
Unsupervised Robust Disentangling of Latent Characteristics for Image Synthesis

Recovering the 3D representation of an object from single-view or multi-view RGB images by deep neural networks has attracted increasing attention in the past few years. Several mainstream works (e.g., 3D-R2N2) use recurrent neural networks (RNNs) to fuse multiple feature maps extracted from input images sequentially. However, when given the same set of input images with different orders, RNN-based approaches are unable to produce consistent reconstruction results. Moreover, due to long-term memory loss, RNNs cannot fully exploit input images to refine reconstruction results. To solve these problems, we propose a novel framework for single-view and multi-view 3D reconstruction, named Pix2Vox. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. Then, a context-aware fusion module is introduced to adaptively select high-quality reconstructions for each part (e.g., table legs) from different coarse 3D volumes to obtain a fused 3D volume. Finally, a refiner further refines the fused 3D volume to generate the final output. Experimental results on the ShapeNet and Pix3D benchmarks indicate that the proposed Pix2Vox outperforms state-of-the-arts by a large margin. Furthermore, the proposed method is 24 times faster than 3D-R2N2 in terms of backward inference time. The experiments on ShapeNet unseen 3D categories have shown the superior generalization abilities of our method.
SROBB: Targeted Perceptual Loss for Single Image Super-Resolution

By benefiting from perceptual losses, recent studies have improved significantly the performance of the super-resolution task, where a high-resolution image is resolved from its low-resolution counterpart. Although such objective functions generate near-photorealistic results, their capability is limited, since they estimate the reconstruction error for an entire image in the same way, without considering any semantic information. In this paper, we propose a novel method to benefit from perceptual loss in a more objective way. We optimize a deep network-based decoder with a targeted objective function that penalizes images at different semantic levels using the corresponding terms. In particular, the proposed method leverages our proposed OBB (Object, Background and Boundary) labels, generated from segmentation labels, to estimate a suitable perceptual loss for boundaries, while considering texture similarity for backgrounds. We show that our proposed approach results in more realistic textures and sharper edges, and outperforms other state-of-the-art algorithms in terms of both qualitative results on standard benchmarks and results of extensive user studies.
An Internal Learning Approach to Video Inpainting

We propose a novel video inpainting algorithm that simultaneously hallucinates missing appearance and motion (optical flow) information, building upon the recent 'Deep Image Prior' (DIP) that exploits convolutional network architectures to enforce plausible texture in static images. In extending DIP to video we make two important contributions. First, we show that coherent video inpainting is possible without a priori training. We take a generative approach to inpainting based on internal (within-video) learning without reliance upon an external corpus of visual data to train a one-size-fits-all model for the large space of general videos. Second, we show that such a framework can jointly generate both appearance and flow, whilst exploiting these complementary modalities to ensure mutual consistency. We show that leveraging appearance statistics specific to each video achieves visually plausible results whilst handling the challenging problem of long-term consistency.
Deep CG2Real: Synthetic-to-Real Translation via Image Disentanglement

We present a method to improve the visual realism of low-quality, synthetic images, e.g. OpenGL renderings. Training an unpaired synthetic-to-real translation network in image space is severely under-constrained and produces visible artifacts. Instead, we propose a semi-supervised approach that operates on the disentangled shading and albedo layers of the image. Our two-stage pipeline first learns to predict accurate shading in a supervised fashion using physically-based renderings as targets, and further increases the realism of the textures and shading with an improved CycleGAN network. Extensive evaluations on the SUNCG indoor scene dataset demonstrate that our approach yields more realistic images compared to other state-of-the-art approaches. Furthermore, networks trained on our generated "real" images predict more accurate depth and normals than domain adaptation approaches, suggesting that improving the visual realism of the images can be more effective than imposing task-specific losses.
Adversarial Defense via Learning to Generate Diverse Attacks

With the remarkable success of deep learning, Deep Neural Networks (DNNs) have been applied as dominant tools to various machine learning domains. Despite this success, however, it has been found that DNNs are surprisingly vulnerable to malicious attacks; adding a small, perceptually indistinguishable perturbations to the data can easily degrade classification performance. Adversarial training is an effective defense strategy to train a robust classifier. In this work, we propose to utilize the generator to learn how to create adversarial examples. Unlike the existing approaches that create a one-shot perturbation by a deterministic generator, we propose a recursive and stochastic generator that produces much stronger and diverse perturbations that comprehensively reveal the vulnerability of the target classifier. Our experiment results on MNIST and CIFAR-10 datasets show that the classifier adversarially trained with our method yields more robust performance over various white-box and black-box attacks.
Image Generation From Small Datasets via Batch Statistics Adaptation

Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing, even when the dataset is small ( 100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. Our method makes it possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain. Code is available at github.com/nogu-atsu/small-dataset-image-generation
Lifelong GAN: Continual Learning for Conditional Image Generation

Lifelong learning is challenging for deep neural networks due to their susceptibility to catastrophic forgetting. Catastrophic forgetting occurs when a trained network is not able to maintain its ability to accomplish previously learned tasks when it is trained to perform new tasks. We study the problem of lifelong learning for generative models, extending a trained network to new conditional generation tasks without forgetting previous tasks, while assuming access to the training data for the current task only. In contrast to state-of-the-art memory replay based approaches which are limited to label-conditioned image generation tasks, a more generic framework for continual learning of generative models under different conditional image generation settings is proposed in this paper. Lifelong GAN employs knowledge distillation to transfer learned knowledge from previous networks to the new network. This makes it possible to perform image-conditioned generation tasks in a lifelong learning setting. We validate Lifelong GAN for both image-conditioned and label-conditioned generation tasks, and provide qualitative and quantitative results to show the generality and effectiveness of our method.
Bayesian Relational Memory for Semantic Visual Navigation

We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure.
Mono-SF: Multi-View Geometry Meets Single-View Depth for Monocular Scene Flow Estimation of Dynamic Traffic Scenes

Existing 3D scene flow estimation methods provide the 3D geometry and 3D motion of a scene and gain a lot of interest, for example in the context of autonomous driving. These methods are traditionally based on a temporal series of stereo images. In this paper, we propose a novel monocular 3D scene flow estimation method, called Mono-SF. Mono-SF jointly estimates the 3D structure and motion of the scene by combining multi-view geometry and single-view depth information. Mono-SF considers that the scene flow should be consistent in terms of warping the reference image in the consecutive image based on the principles of multi-view geometry. For integrating single-view depth in a statistical manner, a convolutional neural network, called ProbDepthNet, is proposed. ProbDepthNet estimates pixel-wise depth distributions from a single image rather than single depth values. Additionally, as part of ProbDepthNet, a novel recalibration technique for regression problems is proposed to ensure well-calibrated distributions. Our experiments show that Mono-SF outperforms state-of-the-art monocular baselines and ablation studies support the Mono-SF approach and ProbDepthNet design.
Prior Guided Dropout for Robust Visual Localization in Dynamic Environments

Existing 3D scene flow estimation methods provide the 3D geometry and 3D motion of a scene and gain a lot of interest, for example in the context of autonomous driving. These methods are traditionally based on a temporal series of stereo images. In this paper, we propose a novel monocular 3D scene flow estimation method, called Mono-SF. Mono-SF jointly estimates the 3D structure and motion of the scene by combining multi-view geometry and single-view depth information. Mono-SF considers that the scene flow should be consistent in terms of warping the reference image in the consecutive image based on the principles of multi-view geometry. For integrating single-view depth in a statistical manner, a convolutional neural network, called ProbDepthNet, is proposed. ProbDepthNet estimates pixel-wise depth distributions from a single image rather than single depth values. Additionally, as part of ProbDepthNet, a novel recalibration technique for regression problems is proposed to ensure well-calibrated distributions. Our experiments show that Mono-SF outperforms state-of-the-art monocular baselines and ablation studies support the Mono-SF approach and ProbDepthNet design.
Drive&Act: A Multi-Modal Dataset for Fine-Grained Driver Behavior Recognition in Autonomous Vehicles

We introduce the novel domain-specific Drive&Act benchmark for fine-grained categorization of driver behavior. Our dataset features twelve hours and over 9.6 million frames of people engaged in distractive activities during both, manual and automated driving. We capture color, infrared, depth and 3D body pose information from six views and densely label the videos with a hierarchical annotation scheme, resulting in 83 categories. The key challenges of our dataset are: (1) recognition of fine-grained behavior inside the vehicle cabin; (2) multi-modal activity recognition, focusing on diverse data streams; and (3) a cross view recognition benchmark, where a model handles data from an unfamiliar domain, as sensor type and placement in the cabin can change between vehicles. Finally, we provide challenging benchmarks by adopting prominent methods for video- and body pose-based action recognition.
Depth Completion From Sparse LiDAR Data With Depth-Normal Constraints

Depth completion aims to recover dense depth maps from sparse depth measurements. It is of increasing importance for autonomous driving and draws increasing attention from the vision community. Most of the current competitive methods directly train a network to learn a mapping from sparse depth inputs to dense depth maps, which has difficulties in utilizing the 3D geometric constraints and handling the practical sensor noises. In this paper, to regularize the depth completion and improve the robustness against noise, we propose a unified CNN framework that 1) models the geometric constraints between depth and surface normal in a diffusion module and 2) predicts the confidence of sparse LiDAR measurements to mitigate the impact of noise. Specifically, our encoder-decoder backbone predicts the surface normal, coarse depth and confidence of LiDAR inputs simultaneously, which are subsequently inputted into our diffusion refinement module to obtain the final completion results. Extensive experiments on KITTI depth completion dataset and NYU-Depth-V2 dataset demonstrate that our method achieves state-of-the-art performance. Further ablation study and analysis give more insights into the proposed components and demonstrate the generalization capability and stability of our model.
PRECOG: PREdiction Conditioned on Goals in Visual Multi-Agent Settings

For autonomous vehicles (AVs) to behave appropriately on roads populated by human-driven vehicles, they must be able to reason about the uncertain intentions and decisions of other drivers from rich perceptual information. Towards these capabilities, we present a probabilistic forecasting model of future interactions between a variable number of agents. We perform both standard forecasting and the novel task of conditional forecasting, which reasons about how all agents will likely respond to the goal of a controlled agent (here, the AV). We train models on real and simulated data to forecast vehicle trajectories given past positions and LIDAR. Our evaluation shows that our model is substantially more accurate in multi-agent driving scenarios compared to existing state-of-the-art. Beyond its general ability to perform conditional forecasting queries, we show that our model's predictions of all agents improve when conditioned on knowledge of the AV's goal, further illustrating its capability to model agent interactions.
LPD-Net: 3D Point Cloud Learning for Large-Scale Place Recognition and Environment Analysis

Point cloud based place recognition is still an open issue due to the difficulty in extracting local features from the raw 3D point cloud and generating the global descriptor, and it's even harder in the large-scale dynamic environments. In this paper, we develop a novel deep neural network, named LPD-Net (Large-scale Place Description Network), which can extract discriminative and generalizable global descriptors from the raw 3D point cloud. Two modules, the adaptive local feature extraction module and the graph-based neighborhood aggregation module, are proposed, which contribute to extract the local structures and reveal the spatial distribution of local features in the large-scale point cloud, with an end-to-end manner. We implement the proposed global descriptor in solving point cloud based retrieval tasks to achieve the large-scale place recognition. Comparison results show that our LPD-Net is much better than PointNetVLAD and reaches the state-of-the-art. We also compare our LPD-Net with the vision-based solutions to show the robustness of our approach to different weather and light conditions.
Local Supports Global: Deep Camera Relocalization With Sequence Enhancement

We propose to leverage the local information in a image sequence to support global camera relocalization. In contrast to previous methods that regress global poses from single images, we exploit the spatial-temporal consistency in sequential images to alleviate uncertainty due to visual ambiguities by incorporating a visual odometry (VO) component. Specifically, we introduce two effective steps called content-augmented pose estimation and motion-based refinement. The content-augmentation step focuses on alleviating the uncertainty of pose estimation by augmenting the observation based on the co-visibility in local maps built by the VO stream. Besides, the motion-based refinement is formulated as a pose graph, where the camera poses are further optimized by adopting relative poses provided by the VO component as additional motion constraints. Thus, the global consistency can be guaranteed. Experiments on the public indoor 7-Scenes and outdoor Oxford RobotCar benchmark datasets demonstrate that benefited from local information inherent in the sequence, our approach outperforms state-of-the-art methods, especially in some challenging cases, e.g., insufficient texture, highly repetitive textures, similar appearances, and over-exposure.
Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry

We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.
TextPlace: Visual Place Recognition and Topological Localization Through Reading Scene Texts

Visual place recognition is a fundamental problem for many vision based applications. Sparse feature and deep learning based methods have been successful and dominant over the decade. However, most of them do not explicitly leverage high-level semantic information to deal with challenging scenarios where they may fail. This paper proposes a novel visual place recognition algorithm, termed TextPlace, based on scene texts in the wild. Since scene texts are high-level information invariant to illumination changes and very distinct for different places when considering spatial correlation, it is beneficial for visual place recognition tasks under extreme appearance changes and perceptual aliasing. It also takes spatial-temporal dependence between scene texts into account for topological localization. Extensive experiments show that TextPlace achieves state-of-the-art performance, verifying the effectiveness of using high-level scene texts for robust visual place recognition in urban areas.
CamNet: Coarse-to-Fine Retrieval for Camera Re-Localization

Camera re-localization is an important but challenging task in applications like robotics and autonomous driving. Recently, retrieval-based methods have been considered as a promising direction as they can be easily generalized to novel scenes. Despite significant progress has been made, we observe that the performance bottleneck of previous methods actually lies in the retrieval module. These methods use the same features for both retrieval and relative pose regression tasks which have potential conflicts in learning. To this end, here we present a coarse-to-fine retrieval-based deep learning framework, which includes three steps, i.e., image-based coarse retrieval, pose-based fine retrieval and precise relative pose regression. With our carefully designed retrieval module, the relative pose regression task can be surprisingly simpler. We design novel retrieval losses with batch hard sampling criterion and two-stage retrieval to locate samples that adapt to the relative pose regression task. Extensive experiments show that our model (CamNet) outperforms the state-of-the-art methods by a large margin on both indoor and outdoor datasets.
Situational Fusion of Visual Representation for Visual Navigation

A complex visual navigation task puts an agent in different situations which call for a diverse range of visual perception abilities. For example, to "go to the nearest chair", the agent might need to identify a chair in a living room using semantics, follow along a hallway using vanishing point cues, and avoid obstacles using depth. Therefore, utilizing the appropriate visual perception abilities based on a situational understanding of the visual environment can empower these navigation models in unseen visual environments. We propose to train an agent to fuse a large set of visual representations that correspond to diverse visual perception abilities. To fully utilize each representation, we develop an action-level representation fusion scheme, which predicts an action candidate from each representation and adaptively consolidate these action candidates into the final action. Furthermore, we employ a data-driven inter-task affinity regularization to reduce redundancies and improve generalization. Our approach leads to a significantly improved performance in novel environments over ImageNet-pretrained baseline and other fusion methods.
Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking

Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.
6-DOF GraspNet: Variational Grasp Generation for Object Manipulation

Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.
DAGMapper: Learning to Map by Discovering Lane Topology

One of the fundamental challenges to scale self-driving is being able to create accurate high definition maps (HD maps) with low cost. Current attempts to automate this pro- cess typically focus on simple scenarios, estimate independent maps per frame or do not have the level of precision required by modern self driving vehicles. In contrast, in this paper we focus on drawing the lane boundaries of complex highways with many lanes that contain topology changes due to forks and merges. Towards this goal, we formulate the problem as inference in a directed acyclic graphical model (DAG), where the nodes of the graph encode geo- metric and topological properties of the local regions of the lane boundaries. Since we do not know a priori the topology of the lanes, we also infer the DAG topology (i.e., nodes and edges) for each region. We demonstrate the effectiveness of our approach on two major North American Highways in two different states and show high precision and recall as well as 89% correct topology.
3D-LaneNet: End-to-End 3D Multiple Lane Detection

We introduce a network that directly predicts the 3D layout of lanes in a road scene from a single image. This work marks a first attempt to address this task with on-board sensing without assuming a known constant lane width or relying on pre-mapped environments. Our network architecture, 3D-LaneNet, applies two new concepts: intra-network inverse-perspective mapping (IPM) and anchor-based lane representation. The intra-network IPM projection facilitates a dual-representation information flow in both regular image-view and top-view. An anchor-per-column output representation enables our end-to-end approach which replaces common heuristics such as clustering and outlier rejection, casting lane estimation as an object detection problem. In addition, our approach explicitly handles complex situations such as lane merges and splits. Results are shown on two new 3D lane datasets, a synthetic and a real one. For comparison with existing methods, we test our approach on the image-only tuSimple lane detection benchmark, achieving performance competitive with state-of-the-art.
Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once

Modern deep neural networks are often vulnerable to adversarial samples. Based on the first optimization-based attacking method, many following methods are proposed to improve the attacking performance and speed. Recently, generation-based methods have received much attention since they directly use feed-forward networks to generate the adversarial samples, which avoid the time-consuming iterative attacking procedure in optimization-based and gradient-based methods. However, current generation-based methods are only able to attack one specific target (category) within one model, thus making them not applicable to real classification systems that often have hundreds/thousands of categories. In this paper, we propose the first Multi-target Adversarial Network (MAN), which can generate multi-target adversarial samples with a single model. By incorporating the specified category information into the intermediate features, it can attack any category of the target classification model during runtime. Experiments show that the proposed MAN can produce stronger attack results and also have better transferability than previous state-of-the-art methods in both multi-target attack task and single-target attack task. We further use the adversarial samples generated by our MAN to improve the robustness of the classification model. It can also achieve better classification accuracy than other methods when attacked by various methods.
Attention Bridging Network for Knowledge Transfer

The attention of a deep neural network obtained by back-propagating gradients can effectively explain the decision of the network. They can further be used to explicitly access to the network response to a specific pattern. Considering objects of the same category but from different domains share similar visual patterns, we propose to treat the network attention as a bridge to connect objects across domains. In this paper, we use knowledge from the source domain to guide the network's response to categories shared with the target domain. With weights sharing and domain adversary training, this knowledge can be successfully transferred by regularizing the network's response to the same category in the target domain. Specifically, we transfer the foreground prior from a simple single-label dataset to another complex multi-label dataset, leading to improvement of attention maps. Experiments about the weakly-supervised semantic segmentation task show the effectiveness of our method. Besides, we further explore and validate that the proposed method is able to improve the generalization ability of a classification network in domain adaptation and domain generalization settings. 
Recover and Identify: A Generative Dual Model for Cross-Resolution Person Re-Identification

Person re-identification (re-ID) aims at matching images of the same identity across camera views. Due to varying distances between cameras and persons of interest, resolution mismatch can be expected, which would degrade person re-ID performance in real-world scenarios. To overcome this problem, we propose a novel generative adversarial network to address cross-resolution person re-ID, allowing query images with varying resolutions. By advancing adversarial learning techniques, our proposed model learns resolution-invariant image representations while being able to recover the missing details in low-resolution input images. The resulting features can be jointly applied for improving person re-ID performance due to preserving resolution invariance and recovering re-ID oriented discriminative details. Our experiments on five benchmark datasets confirm the effectiveness of our approach and its superiority over the state-of-the-art methods, especially when the input resolutions are unseen during training.
Aggregation via Separation: Boosting Facial Landmark Detector With Semi-Supervised Style Translation

Facial landmark detection, or face alignment, is a fundamental task that has been extensively studied. In this paper, we investigate a new perspective of facial landmark detection and demonstrate it leads to further notable improvement. Given that any face images can be factored into space of style that captures lighting, texture and image environment, and a style-invariant structure space, our key idea is to leverage disentangled style and shape space of each individual to augment existing structures via style translation. With these augmented synthetic samples, our semi-supervised model surprisingly outperforms the fully-supervised one by a large margin. Extensive experiments verify the effectiveness of our idea with state-of-the-art results on WFLW, 300W, COFW, and AFLW datasets. Our proposed structure is general and could be assembled into any face alignment frameworks. The code is made publicly available at https://github.com/thesouthfrog/stylealign.
Field1
Text
 R2-B2: Recursive Reasoning-Based Bayesian Optimization for No-Regret Learning in Games 
 This paper presents a recursive reasoning formalism of Bayesian optimization (BO) to model the reasoning process in the interactions between boundedly rational, self-interested agents with unknown, complex, and costly-to-evaluate payoff functions in repeated games, which we call Recursive Reasoning-Based BO (R2-B2). Our R2-B2 algorithm is general in that it does not constrain the relationship among the payoff functions of different agents and can thus be applied to various types of games such as constant-sum, general-sum, and common-payoff games. We prove that by reasoning at level 2 or more and at one level higher than the other agents, our R2-B2 agent can achieve faster asymptotic convergence to no regret than that without utilizing recursive reasoning. We also propose a computationally cheaper variant of R2-B2 called R2-B2-Lite at the expense of a weaker convergence guarantee. The performance and generality of our R2-B2 algorithm are empirically demonstrated using synthetic games, adversarial machine learning, and multi-agent reinforcement learning.
 On Relativistic f-Divergences 
 We take a more rigorous look at Relativistic Generative Adversarial Networks (RGANs) and prove that the objective function of the discriminator is a statistical divergence for any concave function $f$ with minimal properties ($f(0)=0$, $f'(0) \neq 0$, $\sup_x f(x)>0$). We devise additional variants of relativistic $f$-divergences. We show that the Wasserstein distance is weaker than $f$-divergences which are weaker than relativistic $f$-divergences. Given the good performance of RGANs, this suggests that Wasserstein GAN does not performs well primarily because of the weak metric, but rather because of regularization and the use of a relativistic discriminator. We introduce the minimum-variance unbiased estimator (MVUE) for Relativistic paired GANs (RpGANs; originally called RGANs which could bring confusion) and show that it does not perform better. We show that the estimator of Relativistic average GANs (RaGANs) is asymptotically unbiased and that the finite-sample bias is small; removing this bias does not improve performance.
 Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks 
 A fundamental question in modern machine learning is how overparameterized deep neural networks can generalize. We address this question using 1) an equivalence between training infinitely wide neural networks and performing kernel regression with a deterministic kernel called the Neural Tangent Kernel (NTK), and 2) theoretical tools from statistical physics. We derive analytical expressions for learning curves for kernel regression, and use them to evaluate how the test loss of a trained neural network depends on the number of samples. Our approach allows us not only to compute the total test risk but also the decomposition of the risk due to different spectral components of the kernel. Complementary to recent results showing that during gradient descent, neural networks fit low frequency components first, we identify a new type of frequency principle: as the size of the training set size grows, kernel machines and neural networks begin to fit successively higher frequency modes of the target function. We verify our theory with simulations of kernel regression and training wide artificial neural networks. 
 Attentive Group Equivariant Convolutional Networks 
 Although group convolutional networks are able to learn powerful representations based on symmetry patterns, they lack explicit means to learn meaningful relationships among them (e.g., relative positions and poses). In this paper, we present attentive group equivariant convolutions, a generalization of the group convolution, in which attention is applied during the course of convolution to accentuate meaningful symmetry combinations and suppress non-plausible, misleading ones. We indicate that prior work on visual attention can be described as special cases of our proposed framework and show empirically that our attentive group equivariant convolutional networks consistently outperform conventional group convolutional networks on benchmark image datasets. Simultaneously, we provide interpretability to the learned concepts through the visualization of equivariant attention maps.
 Data Amplification: Instance-Optimal Property Estimation  
 The best-known and most commonly used technique for distribution-property estimation uses a plug-in estimator, with empirical frequency replacing the underlying distribution. We present novel linear-time-computable estimators that significantly ``amplify'' the effective amount of data available. For a large variety of distribution properties including four of the most popular ones and for every underlying distribution, they achieve the accuracy that the empirical-frequency plug-in estimators would attain using a logarithmic-factor more samples. Specifically, for Shannon entropy and a broad class of Lipschitz properties including the $L_1$ distance to a fixed distribution, the new estimators use $n$ samples to achieve the accuracy attained by the empirical estimators with $n\log n$ samples. For support-size and coverage, the new estimators use $n$ samples to achieve the performance of empirical frequency with sample size $n$ times the logarithm of the property value. Significantly strengthening the traditional min-max formulation, these results hold not only for the worst distributions, but for each and every underlying distribution. Furthermore, the logarithmic amplification factors are optimal. Experiments on a wide variety of distributions show that the new estimators outperform the previous state-of-the-art estimators designed for each specific property.  
 Best Arm Identification for Cascading Bandits in the Fixed Confidence Setting 
 We design and analyze CascadeBAI, an algorithm for finding the best set of K items, also called an arm, within the framework of cascading bandits. An upper bound on the time complexity of CascadeBAI is derived by overcoming a crucial analytical challenge, namely, that of probabilistically estimating the amount of available feedback at each step. To do so, we define a new class of random variables (r.v.'s) which we term as left-sided sub-Gaussian r.v.'s; these are r.v.'s whose cumulant generating functions (CGFs) can be bounded by a quadratic only for non-positive arguments of the CGFs. This enables the application of a sufficiently tight Bernstein-type concentration inequality. We show, through the derivation of a lower bound on the time complexity, that the performance of CascadeBAI is optimal in some practical regimes. Finally, extensive numerical simulations corroborate the efficacy of CascadeBAI as well as the tightness of our upper bound on its time complexity.
 Deep Coordination Graphs 
 This paper introduces the deep coordination graph (DCG) for collaborative multi-agent reinforcement learning. DCG strikes a flexible trade-off between representational capacity and generalization by factoring the joint value function of all agents according to a coordination graph into payoffs between pairs of agents. The value can be maximized by local message passing along the graph, which allows training of the value function end-to-end with Q-learning. Payoff functions are approximated with deep neural networks that employ parameter sharing and low-rank approximations to significantly improve sample efficiency. We show that DCG can solve predator-prey tasks that highlight the relative overgeneralization pathology, as well as challenging StarCraft II micromanagement tasks.
 Measuring Non-Expert Comprehension of Machine Learning Fairness Metrics 
 Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions.  We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a lay audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of three such definitions--demographic parity, equal opportunity, and equalized odds. We evaluate this metric using an online survey, and investigate the relationship between comprehension and sentiment, demographics, and the definition itself.
 Accelerated Message Passing for Entropy-Regularized MAP Inference 
 Maximum a posteriori (MAP) inference is a fundamental problem in machine learning that involves identifying the most likely configuration of a discrete-valued Markov random field. Due to the difficulty of this combinatorial problem, linear programming (LP) relaxations are commonly used to derive specialized message passing algorithms that are often interpreted as coordinate descent on the dual LP. To achieve more desirable computational properties, a number of methods regularize the LP with an entropy term, leading to a class of smooth message passing algorithms with convergence guarantees. In this paper, we present randomized methods for accelerating these algorithms by leveraging techniques that underlie classical accelerated gradient methods. Crucially, the proposed algorithms incorporate the familiar steps of standard smooth message passing algorithms, which can be viewed as coordinate minimization steps. We show that the accelerated variants achieve faster rates for finding $\epsilon$-optimal points of the unregularized problem. When the LP is tight, we prove that the proposed algorithms recover the true MAP solution in fewer iterations than the best-known results.
 Efficient Intervention Design for Causal Discovery with Latents 
 We consider recovering a causal graph in presence of latent variables, where we seek to minimize the cost of interventions used in the recovery process. We consider two intervention cost models: (1) a linear cost model where the cost of an intervention on a subset of variables has a linear form, and (2) an identity cost model where the cost of an intervention is the same, regardless of what variables it is on, i.e., the goal is just to minimize the number of interventions. Under the linear cost model, we give an algorithm to identify the ancestral relations of the underlying causal graph, achieving within a 2-factor of the optimal intervention cost. This approximation factor can be improved to 1+eps for any eps > 0, under some mild restrictions. Under the identity cost model, we bound the number of interventions needed to recover the entire causal graph, including the latent variables, using a parameterization of the causal graph  through a special type of colliders. In particular, we introduce the notion of p-colliders, that are colliders between pair of nodes arising from a specific type of conditioning in the causal graph, and provide an upper bound on the number of interventions as a function of the maximum number of p-colliders between any two nodes in the causal graph.
 Dual-Path Distillation: A Unified Framework to Improve Black-Box Attacks 
 We study the problem of constructing black-box adversarial attacks, where no model information is revealed except for the feedback knowledge of the given inputs. To obtain sufficient knowledge for crafting adversarial examples, previous methods query the target model with inputs that are perturbed with different searching directions. However, these methods suffer from poor query efficiency since the employed searching directions are sampled randomly. To mitigate this issue, we formulate the goal of mounting efficient attacks as an optimization problem in which the adversary tries to fool the target model with a limited number of queries. Under such settings, the adversary has to select appropriate searching directions to reduce the number of model queries. By solving the efficient-attack problem, we find that we need to distill the knowledge in both the path of the adversarial examples and the path of the searching directions. Therefore, we propose a novel framework, dual-path distillation, that utilizes the feedback knowledge not only to craft adversarial examples but also to alter the  searching directions to achieve efficient attacks.  Experimental results suggest that our framework can significantly increase the query efficiency.
 Implicit Regularization of Random Feature Models 
 Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with $P$ features, $N$ data points, and a ridge $\lambda$, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an \textit{effective ridge} $\tilde{\lambda}$. We show that $\tilde{\lambda} > \lambda$ and $\tilde{\lambda} \searrow \lambda$ monotonically as $P$ grows, thus revealing the \textit{implicit regularization effect} of finite RF sampling. We then compare the risk (i.e. test error) of the $\tilde{\lambda}$-KRR predictor with the average risk of the $\lambda$-RF predictor and obtain a precise and explicit bound on their difference.
Finally, we empirically find an extremely good agreement between the test errors of the average $\lambda$-RF predictor and $\tilde{\lambda}$-KRR predictor.

 Consistent Estimators for Learning to Defer to an Expert 
 Learning algorithms are often used in conjunction with expert decision makers in practical scenarios, however this fact is largely ignored when designing these algorithms. In this paper we explore how to learn predictors that can either predict or choose to defer the decision to a downstream expert. Given samples of the expert's decisions, we give a procedure based on learning a classifier and a rejector and analyze it theoretically. Our approach is based on a reduction to cost sensitive learning where we give a novel calibrated surrogate loss that resolves the open problem of (Ni et al., 2019) for multiclass rejection learning. We show the effectiveness of the new surrogate loss and approach on image and text classification tasks.
 Inexact Tensor Methods with Dynamic Accuracies 
 In this paper, we study inexact high-order Tensor Methods for solving convex optimization problems with composite objective. At every step of such methods, we use approximate solution of the auxiliary problem, defined by the bound for the residual in function value. We propose two dynamic strategies for choosing the inner accuracy: the first one is decreasing as $1/k^{p + 1}$, where $p \geq 1$ is the order of the method and $k$ is the iteration counter, and the second approach is using for the inner accuracy the last progress in the target objective. We show that inexact Tensor Methods with these strategies achieve the same global convergence rate as in the error-free case. For the second approach we also establish local superlinear rates (for $p \geq 2$), and propose the accelerated scheme. Lastly, we present computational results on a variety of machine learning problems for several methods and different accuracy policies.
 Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors 
 Bayesian neural networks (BNNs) demonstrate promising success in improving the robustness and uncertainty quantification of modern neural networks. However, they generally struggle with underfitting at scale and parameter efficiency. On the other hand, deep ensembles have emerged as an alternative for uncertainty quantification that, while outperforming BNNs on certain problems, also suffers from efficiency issues. It remains unclear how to combine the strengths of these two approaches and remediate their common issues. To tackle this challenge, we propose a rank-1 parameterization of BNNs, where each weight matrix involves only a distribution on a rank-1 subspace. We also revisit the use of mixture approximate posteriors to capture multiple modes where unlike typical mixtures, this approach admits a significantly smaller memory increase (e.g., only a 0.4\% increase for a ResNet-50 mixture of size 10). We perform a systematic empirical study on the choices of prior, variational posterior, and methods to improve training. For ResNet-50 on ImageNet and Wide ResNet 28-10 on CIFAR-10/100, rank-1 BNNs demonstrate improved performance across log-likelihood, accuracy, and calibration on the test set and out-of-distribution variants.
 Understanding and Estimating the Adaptability of Domain-Invariant Representations 
 Learning domain-invariant representations is a popular approach to unsupervised domain adaptation, i.e., generalizing from a source domain with labels to an unlabeled target domain. In this work, we aim to better understand and estimate the effect of domain-invariant representations on generalization to the target. In particular, we study the effect of the complexity of the latent, domain-invariant representation, and find that it has a significant influence on the target risk. Based on these findings, we propose a general approach for addressing this complexity tradeoff in neural networks. We also propose a method for estimating how well a model based on domain-invariant representations will perform on the target domain, without having seen any target labels. Applications of our results include model selection, deciding early stopping, and predicting the adaptability of a model between domains.
 Spectral Clustering with Graph Neural Networks for Graph Pooling 
 Spectral clustering (SC) is a popular clustering technique to find strongly connected communities on a graph.
SC can be used in Graph Neural Networks (GNNs) to implement pooling operations that aggregate nodes belonging to the same cluster.
However, the eigendecomposition of the Laplacian is expensive and, since clustering results are graph-specific, pooling methods based on SC must perform a new optimization for each new sample.
In this paper, we propose a graph clustering approach that addresses these limitations of SC.
We formulate a continuous relaxation of the normalized minCUT problem and train a GNN to compute cluster assignments that minimize this objective. 
Our GNN-based implementation is differentiable, does not require to compute the spectral decomposition, and learns a clustering function that can be quickly evaluated on out-of-sample graphs.
From the proposed clustering method, we design a graph pooling operator that overcomes some important limitations of state-of-the-art graph pooling techniques and achieves the best performance in several supervised and unsupervised tasks.
 Feature Noise Induces Loss Discrepancy Across Groups 
 It has been observed that the performance of standard learning procedures differs widely across groups. Recent studies usually attribute this loss discrepancy to an information deficiency for one group (e.g., one group has less data). In this work, we point to a more subtle source of loss discrepancy---feature noise. Our main result is that even when there is no information deficiency specific to one group (e.g., both groups have infinite data), adding the same amount of feature noise to all individuals leads to loss discrepancy. For linear regression, we characterize this loss discrepancy in terms of the amount of noise and difference between moments of the two groups. We then study the time it takes for an estimator to adapt to a shift in the population that makes the groups have the same mean. We finally validate our results on three real-world datasets.
 Deep Reasoning Networks for Unsupervised Pattern De-mixing with Constraint Reasoning 
 We introduce Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with constraint reasoning for solving pattern de-mixing problems, typically in an unsupervised or very-weakly-supervised setting. DRNets exploit problem structure and prior knowledge by tightly combining constraint reasoning with stochastic-gradient-based neural network optimization. Our motivating task is from materials discovery and concerns inferring crystal structures of materials from X-ray diffraction data (Crystal-Structure-Phase-Mapping). Given the complexity of its underlying scientific domain, we start by introducing DRNets on an analogous but much simpler task: de-mixing overlapping hand-written Sudokus (Multi-MNIST-Sudoku). On Multi-MNIST-Sudoku, DRNets almost perfectly recovered the mixed Sudokus' digits, with 100\% digit accuracy, outperforming the supervised state-of-the-art MNIST de-mixing models. On Crystal-Structure-Phase-Mapping, DRNets significantly outperform the state of the art and experts' capabilities, recovering more precise and physically meaningful crystal structures.
 Loss Function Search for Face Recognition 
 In face recognition, designing margin-based (\textit{e.g.}, angular, additive, additive angular margins) softmax loss functions plays an important role to learn discriminative features. However, these hand-crafted heuristic methods may be sub-optimal because they require much effort to explore the large design space. Recently, an AutoML for loss function search method AM-LFS has been derived, which leverages reinforcement learning to search loss functions during the training process. But its search space is complex and unstable that hindering its superiority. In this paper, we first analyze that the key to enhance the feature discrimination is actually \textbf{how to reduce the softmax probability}. We then design a unified formulation for the current margin-based softmax losses. Accordingly, we define a novel search space and develop a reward-guided search method to automatically obtain the best candidate. Experimental results on a variety of face recognition benchmarks have demonstrated the effectiveness of our method over the state-of-the-art alternatives.
 PackIt: A Virtual Environment for Geometric Planning 
 The ability to jointly understand the geometry of objects and plan actions for manipulating them is crucial for intelligent agents. We refer to this ability as geometric planning. Recently, many interactive environments have been proposed to evaluate intelligent agents on various skills, however, none of them cater to the needs of geometric planning. We present PackIt, a virtual environment to evaluate and potentially learn the ability to do geometric planning. In this environment, an agent needs to take a sequence of actions to pack a set of objects into a box with limited space. We also construct a set of challenging packing tasks using an evolutionary algorithm. Further, we study various baselines for the task that include model-free learning-based and heuristic-based methods, as well as search-based optimization methods that assume access to the model of the environment.
 Thompson Sampling via Local Uncertainty 
 Thompson sampling is an efficient algorithm for sequential decision making, which exploits the posterior uncertainty to address the exploration-exploitation dilemma. There has been significant recent interest in integrating  Bayesian neural networks into Thompson sampling. Most of these methods rely on global variable uncertainty for exploration. In this paper, we propose a new probabilistic modeling framework for Thompson sampling, where local latent variable uncertainty is used to sample the mean reward. Variational inference is used to approximate the posterior of the local variable, and semi-implicit structure is further introduced to enhance its expressiveness. Our experimental results on eight  contextual bandits benchmark datasets show that Thompson sampling guided by local uncertainty achieves state-of-the-art performance while having low computational complexity.
 Adversarial Learning Guarantees for Linear Hypotheses and Neural Networks 
 Adversarial or test time robustness measures the susceptibility of a classifier to perturbations to the test input. While there has been a flurry of recent work on designing defenses against such perturbations, the theory of adversarial robustness is not well understood. In order to make progress on this, we focus on the problem of understanding generalization in adversarial settings, via the lens of Rademacher complexity.

We give upper and lower bounds for the adversarial empirical Rademacher complexity of linear hypotheses with adversarial perturbations measured in $l_r$-norm for an arbitrary $r \geq 1$. This generalizes the recent result of Yin et al.~\cite{YinRamchandranBartlett2019} that studies the case of $r = \infty$, and provides a finer analysis of the dependence on the input dimensionality as compared to the recent work of Khim and Loh~\cite{khim2018adversarial} on linear hypothesis classes and additionally provides matching lower bounds.

We then extend our analysis to provide Rademacher complexity lower and upper bounds for a single ReLU unit. Finally, we give adversarial Rademacher complexity bounds for feed-forward neural networks with one hidden layer. Unlike previous works we directly provide bounds on the adversarial Rademacher complexity of the given network, as opposed to a bound on a surrogate. A by-product of our analysis also leads to tighter bounds for the Rademacher complexity of linear hypotheses, for which we give a detailed analysis and present a comparison with existing bounds. 

 Sequential Cooperative Bayesian Inference 
 Cooperation is often implicitly assumed when learning from other agents. Cooperation implies that the agent selecting the data, and the agent learning from the data, have the same goal, that the learner infer the intended hypothesis. Recent models in human and machine learning have demonstrated the possibility of cooperation. We seek foundational theoretical results for cooperative inference by Bayesian agents through sequential data. We develop novel approaches analyzing consistency, rate of convergence and stability of Sequential Cooperative Bayesian Inference (SCBI). Our analysis of the effectiveness, sample efficiency and robustness show that cooperation is not only possible but theoretically well-founded. We discuss implications for human-human and human-machine cooperation.
 PENNI: Pruned Kernel Sharing for Efficient CNN Inference 
 Although state-of-the-art (SOTA) CNNs achieve outstanding performance on various tasks, their high computation demand and massive number of parameters make it difficult to deploy these SOTA CNNs onto resource-constrained devices. Previous works on CNN acceleration utilize low-rank approximation of the original convolution layers to reduce computation cost. However, these methods are very difficult to conduct upon sparse models, which limits execution speedup since redundancies within the CNN model are not fully exploited. We argue that kernel granularity decomposition can be conducted with low-rank assumption while exploiting the redundancy within the remaining compact coefficients. Based on this observation, we propose PENNI, a CNN model compression framework that is able to achieve model compactness and hardware efficiency simultaneously by (1) implementing kernel sharing in convolution layers via a small number of basis kernels and (2) alternately adjusting bases and coefficients with sparse constraints. Experiments show that we can prune 97% parameters and 92% FLOPs on ResNet18 CIFAR10 with no accuracy loss, and achieve a 44% reduction in run-time memory consumption and a 53% reduction in inference latency.
 Learning with Multiple Complementary Labels 
 A complementary label (CL) simply indicates an incorrect class of an example, but learning with CLs results in multi-class classifiers that can predict the correct class. Unfortunately, the problem setting of previous research only allows a single CL for each example, which notably limits its potential since our labelers may easily identify multiple complementary labels (MCLs) to one example. In this paper, we propose a novel problem setting to allow MCLs for each example and two ways for learning with MCLs. In the first way, we design two wrappers that decompose MCLs into many single CLs in different manners, so that we could use any method for learning with CLs. However, we find that the supervision information that MCLs hold is conceptually diluted after decomposition. Thus, in the second way, we derive an unbiased risk estimator; minimizing it processes each set of MCLs as a whole and possesses an estimation error bound. In addition, we improve the second way into minimizing properly chosen upper bounds for practical implementation. Experiments show that the former way works well for learning with MCLs while the latter is even better on various benchmark datasets.
 Double Trouble in Double Descent:  Bias and Variance(s) in the Lazy Regime 
 Deep neural networks can achieve remarkable generalization performances while interpolating the training data. Rather than the U-curve emblematic of the bias-variance trade-off, their test error often follows a ``double descent"---a mark of the beneficial role of overparametrization. In this work, we develop a quantitative theory for this phenomenon in the so-called lazy learning regime of neural networks, by considering the problem of learning a high-dimensional function with random features regression. We obtain a precise asymptotic expression for the bias-variance decomposition of the test error, and show that the bias displays a phase transition at the interpolation threshold, beyond it which it remains constant. We disentangle the variances stemming from the sampling of the dataset, from the additive noise corrupting the labels, and from the initialization of the weights. We demonstrate that the latter two contributions are the crux of the double descent: they lead to the overfitting peak at the interpolation threshold and to the decay of the test error upon overparametrization. We quantify how they are suppressed by ensembling the outputs of $K$ independently initialized estimators. For $K\rightarrow \infty$, the test error is monotonously decreasing and remains constant beyond the interpolation threshold. We further compare the effects of overparametrizing, ensembling and regularizing. Finally, we present numerical experiments on classic deep learning setups to show that our results hold qualitatively in realistic lazy learning scenarios. 
 Prediction-Guided Multi-Objective Reinforcement Learning for Continuous Robot Control 
 Many real-world control problems involve conflicting objectives where we desire a dense and high-quality set of control policies that are optimal for different objective preferences (called Pareto-optimal). While extensive research in multi-objective reinforcement learning (MORL) has been conducted to tackle such problems, multi-objective optimization for complex continuous robot control is still under-explored. In this work, we propose an efficient evolutionary learning algorithm to find the Pareto set approximation for continuous robot control problems, by extending a state-of-the-art RL algorithm and presenting a novel prediction model to guide the learning process. In addition to efficiently discovering the individual policies on the Pareto front, we construct a continuous set of Pareto-optimal solutions by Pareto analysis and interpolation. Furthermore, we design six multi-objective RL environments with continuous action space, which is the first benchmark platform to evaluate MORL algorithms on various robot control problems. We test the previous methods on the proposed benchmark problems, and the experiments show that our approach is able to find a much denser and higher-quality set of Pareto policies than the existing algorithms.
 Semismooth Newton Algorithm for Efficient Projections onto $\ell_{1, \infty}$-norm Ball 
 Structured sparsity-inducing $\ell_{1, \infty}$-norm, as a generalization of the classical $\ell_1$-norm, plays an important role in jointly sparse models which select or remove simultaneously all the variables forming a group. However, its resulting problem is more difficult to solve than the conventional $\ell_1$-norm constrained problem. In this paper, we propose an efficient algorithm for Euclidean projection onto $\ell_{1, \infty}$-norm ball. We tackle the projection problem via semismooth Newton algorithm to solve the system of semismooth equations. Meanwhile, exploiting the structure of Jacobian matrix via LU decomposition yields an equivalent algorithm which is proved to terminate after a finite number  of iterations. Empirical studies demonstrate that our proposed algorithm outperforms the existing state-of-the-art solver and is promising for the optimization of learning problems with $\ell_{1, \infty}$-norm ball constraint.
 Constructive universal distribution generation through deep ReLU networks 
 We present an explicit deep network construction that transforms uniformly distributed one-dimensional noise into an arbitrarily close approximation of any two-dimensional target distribution of finite differential entropy and Lipschitz-continuous pdf. The key ingredient of our design is a generalization of the  "space-filling'' property of sawtooth functions introduced in (Bailey & Telgarsky, 2018). We elicit the importance of depth 
in our construction in driving the Wasserstein distance between the target distribution and its approximation realized by the proposed neural network to zero. Finally, we outline how our construction can be extended to output distributions of arbitrary dimension.
 Conditional Augmentation for Generative Modeling 
 We present conditional augmentation (CondAugment), a simple and powerful method of regularizing generative models. Core to our approach is applying augmentation functions to data and then conditioning the generative model on the specific function used. Unlike typical data augmentation, CondAugment allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 150M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures, objectives, and problem domains.
 On conditional versus marginal bias in multi-armed bandits 
 The bias of the sample means of the arms in multi-armed bandits is an important issue in adaptive data analysis that has recently received considerable attention in the literature. Existing results relate in precise ways the sign and magnitude of the bias to various sources of data adaptivity, but do not apply to the conditional inference setting in which the sample means are computed only if some specific conditions are satisfied. In this paper, we characterize the sign of the conditional bias of monotone functions of the rewards, including the sample mean. Our results hold for arbitrary conditioning events and leverage natural monotonicity properties of the data collection policy. We further demonstrate, through several examples from sequential testing and best arm identification, that the sign of the conditional and unconditional bias of the sample mean of an arm can be different, depending on the conditioning event. Our analysis offers new and interesting perspectives on the subtleties of assessing the bias in data adaptive settings.
 Don't Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript 
 Recent years have witnessed intensive research interests on training deep neural networks (DNNs) more efficiently by quantization-based compression methods, which facilitate DNNs training in two ways: (1) activations are quantized to shrink the memory consumption, and (2) gradients are quantized to decrease the communication cost. However, existing methods mostly use a uniform mechanism that quantizes the values evenly. Such a scheme may cause a large quantization variance and slow down the convergence in practice.

In this work, we introduce TinyScript, which applies a non-uniform quantization algorithm to both activations and gradients. TinyScript models the original values by a family of Weibull distributions and searches for ''quantization knobs'' that minimize quantization variance. We also discuss the convergence of the non-uniform quantization algorithm on DNNs with varying depths, shedding light on the number of bits required for convergence. Experiments show that TinyScript always obtains lower quantization variance, and achieves comparable model qualities against full precision training using 1-2 bits less than the uniform-based counterpart.
 Contrastive Multi-View Representation Learning on Graphs 
 We introduce a self-supervised approach for learning node and graph level 
representations by contrasting structural views of graphs. We show that unlike 
visual representation learning, increasing the number of views to more than two or 
contrasting multi-scale encodings do not improve performance, and the best 
performance is achieved by contrasting encodings from first-order neighbors and 
a graph diffusion. We achieve new state-of-the-art results in self-supervised 
learning on 8 out of 8 node and graph classification benchmarks under the linear 
evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) 
classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 
5.5% and 2.4% relative improvements over previous state-of-the-art. When 
compared to supervised baselines, our approach outperforms them in 4 out of 8 
benchmarks.
 VideoOneNet: Bidirectional Convolutional Recurrent OneNet with Trainable Data Steps for Video Processing 
 Deep Neural Networks (DNNs) achieve the state-of-the-art results on a wide range of image processing tasks, however, the majority of such solutions are problem-specific, like most AI algorithms. The One Network to Solve Them All (OneNet) procedure has been suggested to resolve this issue by exploiting a DNN as the proximal operator in Alternating Direction Method of Multipliers (ADMM) solvers for various imaging problems. In this work, we make two contributions, both facilitating end-to-end learning using backpropagation. First, we generalize OneNet to videos by augmenting its convolutional prior network with bidirectional recurrent connections; second, we extend the fixed fully connected linear ADMM data step with another trainable bidirectional convolutional recurrent network. In our computational experiments on the Rotated MNIST, Scanned CIFAR-10 and UCF-101 data sets, the proposed modifications improve performance by a large margin compared to end-to-end convolutional OneNet and 3D Wavelet sparsity on several video processing problems: pixelwise inpainting-denoising, blockwise inpainting, scattered inpainting, super resolution, compressive sensing, deblurring, frame interpolation, frame prediction and colorization. Our two contributions are complementary, and using them together yields the best results.
 Interference and Generalization in Temporal Difference Learning 
 We study the link between generalization and interference in temporal-difference (TD) learning. Interference is defined as the inner product of two different gradients, representing their alignment; this quantity emerges as being of interest from a variety of observations about neural networks, parameter sharing and the dynamics of learning. We find that TD easily leads to low-interference, under-generalizing parameters, while the effect seems reversed in supervised learning. We hypothesize that the cause can be traced back to the interplay between the dynamics of interference and bootstrapping. This is supported empirically by several observations: the negative relationship between the generalization gap and interference in TD, the negative effect of bootstrapping on interference and the local coherence of targets, and the contrast between the propagation rate of information in TD(0) versus TD($\lambda$) and regression tasks such as Monte-Carlo policy evaluation. We hope that these new findings can guide the future discovery of better bootstrapping methods.
 Generating Programmatic Referring Expressions via Program Synthesis 
 Incorporating symbolic reasoning into machine learning algorithms is a promising approach to improve performance on learning tasks that require logical reasoning. We study the problem of generating a programmatic variant of referring expressions that we call referring relational programs. In particular, given a symbolic representation of an image and a target object in that image, the goal is to generate a relational program that uniquely identifies the target object in terms of its attributes and its relations to other objects in the image. We propose a neurosymbolic program synthesis algorithm that combines a policy neural network with enumerative search to generate such relational programs. The policy neural network employs a program interpreter that provides immediate feedback on the consequences of the decisions made by the policy, and also takes into account the uncertainty in the symbolic representation of the image. We evaluate our algorithm on challenging benchmarks based on the CLEVR dataset, and demonstrate that our approach significantly outperforms several baselines.
 Proper Network Interpretability Helps Adversarial Robustness in Classification 
 Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), and interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretability discrepancy, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on robust interpretation (without the need of resorting to adversarial loss minimization). We show that our defense achieves both robust classification and robust interpretation, outperforming state-of-the-art adversarial training methods against   attacks of large perturbation in particular.
 Inverse Active Sensing: Modeling and Understanding Timely Decision-Making 
 Evidence-based decision-making entails collecting (costly) observations about an underlying phenomenon of interest, and subsequently committing to an (informed) decision on the basis of accumulated evidence. In this setting, *active sensing* is the goal-oriented problem of efficiently selecting which acquisitions to make, and when and what decision to settle on. As its complement, *inverse active sensing* seeks to uncover an agent's preferences and strategy given their observable decision-making behavior. In this paper, we develop an expressive, unified framework for the general setting of evidence-based decision-making under endogenous, context-dependent time pressure---which requires negotiating (subjective) tradeoffs between accuracy, speediness, and cost of information. Using this language, we demonstrate how it enables *modeling* intuitive notions of surprise, suspense, and optimality in decision strategies (the forward problem). Finally, we illustrate how this formulation enables *understanding* decision-making behavior by quantifying preferences implicit in observed decision strategies (the inverse problem).
 Differentiating through the Fréchet Mean 
 Recent advances in deep representation learning on Riemannian manifolds extend classical deep learning operations to better capture the geometry of the manifold. One possible extension is the Fréchet mean, the generalization of the Euclidean mean; however, it has been difficult to apply because it lacks a closed form with an easily computable derivative. In this paper, we show how to differentiate through the Fréchet mean for arbitrary Riemannian manifolds. Then, focusing on hyperbolic space, we derive explicit gradient expressions and a fast, accurate, and hyperparameter-free Fréchet mean solver. This fully integrates the Fréchet mean into the hyperbolic neural network pipeline. To demonstrate this integration, we present two case studies. First, we apply our Fréchet mean to the existing Hyperbolic Graph Convolutional Network, replacing its projected aggregation to obtain state-of-the-art results on datasets with high hyperbolicity. Second, to demonstrate the Fréchet mean’s capacity to generalize Euclidean neural network operations, we develop a hyperbolic batch normalization method that gives an improvement parallel to the one observed in the Euclidean setting.
 History-Gradient Aided Batch Size Adaptation for Variance Reduced Algorithms 
 Variance-reduced algorithms, although achieve great theoretical performance, can run slowly in practice due to the periodic gradient estimation with a large batch of data. Batch-size adaptation thus arises as a promising approach to accelerate such algorithms. However, existing schemes either apply prescribed batch-size adaption rule or exploit the information along optimization path via additional backtracking and condition verification steps. In this paper, we propose a novel scheme, which eliminates backtracking line search but still exploits the information along optimization path by adapting the batch size via history stochastic gradients. We further theoretically show that such a scheme substantially reduces the overall complexity for popular variance-reduced algorithms SVRG and SARAH/SPIDER for both conventional nonconvex optimization and reinforcement learning problems. To this end, we develop a new convergence analysis framework to handle the dependence of the batch size on history stochastic gradients. Extensive experiments validate the effectiveness of the proposed batch-size adaptation scheme. 
 Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics 
 According to previous studies, one of the major impediments to accurate off-policy learning is the overestimation bias. This paper investigates a novel way to alleviate the overestimation bias in a continuous control setting. Our method---Truncated Quantile Critics, TQC,---blends three ideas: distributional representation of a critic, truncation of critics prediction, and ensembling of multiple critics. We show that all components are key for the achieved performance. Distributional representation combined with truncation allows for arbitrary granular overestimation control, and ensembling further improves the results of our method. TQC significantly outperforms the current state of the art on all environments from the continuous control benchmark suite, demonstrating 25% improvement on the most challenging Humanoid environment.
 Training Binary Neural Networks through Learning with Noisy Supervision 
 This paper formalizes the binarization operations over neural networks from a learning perspective. In contrast to classical hand crafted rules (\eg hard thresholding) to binarize full-precision neurons, we propose to learn a mapping from full-precision neurons to the target binary ones. Each individual weight entry will not be binarized independently. Instead, they are taken as a whole to accomplish the binarization, just as they work together in generating convolution features. To help the training of the binarization mapping, the full-precision neurons after taking sign operations is regarded as some auxiliary supervision signal, which is noisy but still has valuable guidance.  An unbiased estimator is therefore introduced to mitigate the influence of the supervision noise. Experimental results on benchmark datasets indicate that the proposed binarization technique attains consistent improvements over baselines.
 Graph Homomorphism Convolution 
 In this paper, we study the graph classification problem from the graph homomorphism perspective. We consider the homomorphisms from $F$ to $G$, where $G$ is a graph of interest (e.g. molecules or social networks) and $F$ belongs to some family of graphs (e.g. paths or non-isomorphic trees). We proved that graph homomorphism numbers provide a natural universally invariant embedding maps which can be used for graph classifications. We also discovered that the graph homomorphism method unifies connectivity preserving methods. In practice, by observing that graph classification datasets often have bounded treewidths, we show that our method is not only competitive in classification accuracy but also run much faster than other state-of-the-art. Finally, based on our theoretical analysis, we propose the Graph Homomorphism Convolution module which has promising performance in the graph classification task.
 Instance-hiding Schemes for Private Distributed Learning 
 An important problem today is how to allow a group of decentralized entities to compute on their private data on a centralized deep net while protecting data privacy. Classic cryptographic techniques are too inefficient, so other methods have recently been suggested, e.g., differentially private Federated Learning. Here, a new method is introduced, inspired by the classic notion of {\em instance hiding} in cryptography. It uses the Mixup technique, proposed by {Zhang et al, ICLR 2018} as a way to improve generalization and robustness. Usual mixup involves training on nonnegative combinations of inputs. The new ideas in the current paper are: (a) new variants of mixup with negative as well as positive coefficients, and extend the sample-wise mixup to be pixel-wise.  (b) Experiments demonstrating the effectiveness of this in protecting privacy against known attacks while preserving utility. (c)  Theoretical analysis suggesting why this method is effective, using ideas from analyses of attacks. (d) Estimates of security and the release of a challenge dataset to allow the design of attack schemes.
 Overfitting in adversarially robust deep learning 
 It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models (L-infinity and L-2). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting.  Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. 
 Robustness to Programmable String Transformations via Augmented Abstract Training 
 Deep neural networks for natural language processing tasks are vulnerable to adversarial input perturbations. Existing works have proposed to improve the robustness against specific adversarial input perturbations (e.g., token substitutions), but do not consider general perturbations such as token insertions, token deletions, token swaps, etc. To fill this gap, we present a technique to train models that are robust to user-defined string transformations. Our technique combines data augmentation---to detect worst-case transformed inputs---and verifiable training using abstract interpretation---to further increase the robustness of the model on the worst-case transformed inputs. We use our technique to train models on the AG and SST2 datasets and show that the resulting models are robust to combinations of user-defined transformations mimicking spelling mistakes and other meaning-preserving transformations.
 Hierarchical Generation of Molecular Graphs using Structural Motifs 
 Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.
 Logistic Regression for Massive Data with Rare Events 
 This paper studies binary logistic regression for rare events data, or imbalanced data, where the number of events (observations in one class, often called cases) is significantly smaller than the number of nonevents (observations in the other class, often called controls). We first derive the asymptotic distribution of the maximum likelihood estimator (MLE) of the unknown parameter, which shows that the asymptotic variance convergences to zero in a rate of the inverse of the number of the events instead of the inverse of the full data sample size. This indicates that the available information in rare events data is at the scale of the number of events instead of the full data sample size. Furthermore, we prove that under-sampling a small proportion of the nonevents, the resulting under-sampled estimator may have identical asymptotic distribution to the full data MLE. This demonstrates the advantage of under-sampling nonevents for rare events data, because this procedure may significantly reduce the computation and/or data collection costs. Another common practice in analyzing rare events data is to over-sample (replicate) the events, which has a higher computational cost. We show that this procedure may even result in efficiency loss in terms of parameter estimation.
 Acceleration for Compressed Gradient Descent in Distributed Optimization 
 Due to the high communication cost in distributed and federated learning problems, methods relying on sparsification or quantization of communicated messages are becoming increasingly popular. While in other contexts the best performing gradient-type  methods invariably rely on some form of acceleration to reduce the number of iterations, there are no methods which combine the benefits of both  gradient compression and acceleration. In this paper, we remedy this situation and propose the first {\em accelerated compressed gradient descent (ACGD)} methods. In the single machine regime, we prove that ACGD enjoys the rate $O((1+\omega)\sqrt{\nicefrac{L}{\mu}}\log \nicefrac{1}{\epsilon})$ for $\mu$-strongly convex problems and $O((1+\omega)\sqrt{\nicefrac{L}{\epsilon}})$ for convex problems, respectively, where $L$ is the smoothness constant and $\omega$ is the variance parameter of an unbiased compression operator. Our results improve upon the existing non-accelerated rates $O\left((1+\omega)\nicefrac{L}{\mu}\log \nicefrac{1}{\epsilon}\right)$ and $O\left((1+\omega)\nicefrac{L}{\epsilon}\right)$, respectively, and recover the best known rates of accelerated gradient descent as a special case when no compression ($\omega=0$) is applied. We further propose a distributed variant  of ACGD and establish the rate $\tilde{O}\left(\omega+\sqrt{\nicefrac{L}{\mu}} +\sqrt{(\nicefrac{\omega}{n}+\sqrt{\nicefrac{\omega}{n}})\nicefrac{\omega L}{\mu}}\right)$, where $n$ is the number of machines and $\tilde{O}$ hides the logarithmic factor $\log \nicefrac{1}{\epsilon}$ . This improves upon the previous best result $\tilde{O}\left(\omega + \nicefrac{L}{\mu}+\nicefrac{\omega L}{n\mu} \right)$ achieved by the DIANA method. Finally, we conduct several experiments on real-world datasets which corroborate  our theoretical results and confirm the practical superiority of our methods.
 Provable guarantees for decision tree induction: the agnostic setting  
 We give strengthened provable guarantees on the performance of widely employed and empirically successful {\sl top-down decision tree learning heuristics}.  While prior works have focused on the realizable setting, we consider the more realistic and challenging {\sl agnostic} setting.  We show that for all monotone functions $f$ and $s\in \N$, these heuristics construct a decision tree  of size $s^{\tilde{O}((\log s)/\eps^2)}$ that achieves error $\le \opt_s + \eps$, where $\opt_s$ denotes the error of the optimal size-$s$ decision tree for $f$.  Previously such a guarantee was not known to be achievable by any algorithm, even one that is not based on top-down heuristics.  We complement our algorithmic guarantee with a near-matching $s^{\tilde{\Omega}(\log s)}$ lower bound. 
 Born-again Tree Ensembles 
 The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles, in particular, offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise.
 On Convergence-Diagnostic based Step Sizes for Stochastic Gradient Descent 
 Constant step-size Stochastic Gradient Descent exhibits two phases: a transient phase during which iterates make fast progress towards the optimum, followed by a stationary phase during which iterates oscillate around the optimal point. In this paper, we show that efficiently detecting this transition and appropriately decreasing the step size can lead to fast convergence rates. We analyse the classical statistical test proposed by Pflug (1983), based on the inner product between consecutive stochastic gradients. Even in the simple case where the objective function is quadratic we show that this test cannot lead to an adequate convergence diagnostic. We propose then a novel and simple statistical procedure that accurately detects stationarity and we provide experimental results showing state-of-the-art performance on synthetic and real-word datasets.
 A Finite-Time Analysis of  Q-Learning with Neural Network Function Approximation 
 Q-learning with neural network function approximation (neural Q-learning for short) is among the most prevalent deep reinforcement learning algorithms. Despite its empirical success, the non-asymptotic convergence rate of neural Q-learning remains virtually unknown. In this paper, we present a finite-time analysis of a neural Q-learning algorithm, where the data are generated from a Markov decision process and the action-value function is approximated by a deep ReLU neural network. We prove that neural Q-learning finds the optimal policy with $O(1/\sqrt{T})$ convergence rate if the neural function approximator is sufficiently overparameterized, where $T$ is the number of iterations. To our best knowledge, our result is the first finite-time analysis of neural Q-learning under non-i.i.d. data assumption.
 Understanding Contrastive Representation Learning through Geometry on the Hypersphere 
 Contrastive representation learning has been outstandingly successful in practice. In this work, we identify two key properties related to the contrastive loss: (1) alignment (closeness) of features from positive pairs, and (2) uniformity of the induced distribution of the (normalized) features on the hypersphere. We prove that, asymptotically, the contrastive loss indeed optimizes these properties, and analyze their positive effects on downstream tasks. Empirically, we introduce an optimizable metric to quantify each property. Extensive experiments on standard image classification and depth prediction datasets confirm the strong agreement between both metrics and downstream task performance. Remarkably, directly optimizing for these two metrics leads to representations with comparable or better performance at downstream tasks than contrastive learning.
 Evolutionary Topology Search for Tensor Network Decomposition 
 Tensor network (TN) decomposition is a promising framework to represent extremely high-dimensional problems with few parameters. However, it is challenging to search the (near-)optimal topological structure for TN decomposition, since the number of possible solutions exponentially grows with increasing the order of tensor. In this paper, we claim that this issue can be practically tackled by evolutionary algorithms in an efficient manner. We encode the complex topological structures into binary string, and develop a simple yet efficient genetic-based algorithm (GA) to search the optimal topology on Hamming space. The experimental results by both synthetic and real-world data demonstrate that our method can efficiently discovers the groundtruth topology or even better structures with few number of generations, and significantly boost the representational power of TN decomposition compared with well-known tensor-train (TT) or tensor-ring (TR) models.
 Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks 
 The deep layers of modern neural networks extract a rather rich set of features as an input propagates through the network. This paper sets out to harvest these rich intermediate representations for quantization with minimal accuracy loss while significantly reducing the memory footprint and compute intensity of the DNN. This paper utilizes knowledge distillation through teacher-student paradigm (Hinton et al., 2015) in a novel setting that exploits the feature extraction capability of DNNs for higher-accuracy quantization. As such, our algorithm logically divides a pretrained full-precision DNN to multiple sections, each of which exposes intermediate features to train a team of students independently in the quantized domain. This divide and conquer strategy, in fact, makes the training of each student section possible in isolation while all these independently trained sections are later stitched together to form the equivalent fully quantized network. Our algorithm is a sectional approach towards knowledge distillation and is not treating the intermediate representation as a hint for pretraining before one knowledge distillation pass over the entire network (Romero et al., 2015). Experiments on various DNNs (AlexNet, LeNet, ResNet-18, ResNet-20, SVHN and VGG-11) show that, on average, this approach—called DCQ (Divide and Conquer Quantization)—on average closes the accuracy gap between a state-of-the-art quantized training technique, DoReFa-Net (Zhou et al., 2016) and the full-precision runs by 85% and 92% for binary and ternary quantization of the weights, respectively. Additionally, we show that our approach, DCQ, can improve performance of existing state-of-the art knowledge-distillation based approaches (Mishra et al., 2018) by 1.75% on average for both weight and activation quantization.
 Improved Sleeping Bandits with Stochastic Action Sets and Adversarial Rewards 
 In this paper, we consider the problem of sleeping bandits with stochastic action sets and adversarial rewards. In this setting, in contrast to most work in bandits, the actions may not be available at all times. For instance, some products might be out of stock in item recommendation. The best existing efficient (i.e., polynomial-time) algorithms for this problem only guarantee a $O(T^{2/3})$ upper-bound on the regret. Yet, inefficient algorithms based on EXP4 can achieve $O(\sqrt{T})$.  In this paper, we provide a new computationally efficient algorithm inspired by EXP3 satisfying a regret of order  $O(\sqrt{T})$ when the availabilities of each action $i \in \cA$ are independent. We then study the most general version of the problem where at each round available sets are generated from some unknown arbitrary distribution (i.e., without the independence assumption) and propose an efficient algorithm with $O(\sqrt {2^K T})$ regret guarantee. Our theoretical results are corroborated with experimental evaluations.
 On the consistency of top-k surrogate losses 
 The top-$k$ error is often employed to evaluate performance for challenging classification tasks in computer vision as it is designed to compensate for ambiguity in ground truth labels. This practical success motivates our theoretical analysis of consistent top-$k$ classification. To this end, we provide a characterization of Bayes optimality by defining a top-$k$ preserving property, which is new and fixes a non-uniqueness gap in prior work. Then, we define top-$k$ calibration and show it is necessary and sufficient for consistency.  Based on the top-$k$ calibration analysis, we propose a rich class of top-$k$ calibrated Bregman divergence surrogates. Our analysis continues by showing previously proposed hinge-like top-$k$ surrogate losses are not top-$k$ calibrated and thus inconsistent. On the other hand, we propose two new hinge-like losses, one which is similarly inconsistent, and one which is consistent.
Our empirical results highlight theoretical claims, confirming our analysis of the consistency of these losses.
 Can Increasing Input Dimensionality Improve Deep Reinforcement Learning? 
 Deep reinforcement learning (RL) algorithms have recently achieved remarkable successes in various sequential decision making tasks, leveraging advances in methods for training large deep networks.
However, these methods usually require large amounts of training data, which is often a big problem for real-world applications. One natural question to ask is whether learning good representations for states and using larger networks helps in learning better policies. In this paper, we try to study if increasing input dimensionality helps improve performance and sample efficiency of model-free deep RL algorithms. To do so, we propose an online feature extractor network (OFENet) that uses neural nets to produce \textit{good} representations to be used as inputs to an off-policy RL algorithm. Even though the high dimensionality of input is usually thought to make learning of RL agents more difficult, we show that the RL agents in fact learn more efficiently with the high-dimensional representation than with the lower-dimensional state observations. We believe that stronger feature propagation together with larger networks allows RL agents to learn more complex functions of states and thus improves the sample efficiency. Through numerical experiments, we show that the proposed method achieves much higher sample efficiency and better performance.
 An Optimistic Perspective on Offline Deep Reinforcement Learning 
 Off-policy reinforcement learning (RL) using a fixed offline dataset of logged interactions is an important consideration in real world applications. This paper studies offline RL using the DQN replay dataset comprising the entire replay experience of a DQN agent on 60 Atari 2600 games. We demonstrate that recent off-policy deep RL algorithms, even when trained solely on this fixed dataset, outperform the fully trained DQN agent. To enhance generalization in the offline setting, we present Random Ensemble Mixture (REM), a robust Q-learning algorithm that enforces optimal Bellman consistency on random convex combinations of multiple Q-value estimates. Offline REM trained on the DQN replay dataset surpasses strong RL baselines. Ablation studies highlight the role of offline dataset size and diversity as well as the algorithm choice in our positive results. Overall, the results here present an optimistic view that robust RL algorithms trained on sufficiently large and diverse offline datasets can lead to high quality policies. The DQN replay dataset can serve as an offline RL benchmark and is open-sourced.
 Train Big, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers 
 Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.
 Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study 
 Contemporary work on learning in continuous games has commonly overlooked the hierarchical decision-making structure present in machine learning problems formulated as games, instead treating them as simultaneous play games and adopting the Nash equilibrium solution concept. We deviate from this paradigm and provide a comprehensive study of learning in Stackelberg games. This work provides insights into the optimization landscape of zero-sum games by establishing connections between Nash and Stackelberg equilibria along with the limit points of simultaneous gradient descent. We derive novel gradient-based learning dynamics emulating the natural structure of a Stackelberg game using the Implicit Function Theorem and provide convergence analysis for deterministic and stochastic updates for zero-sum and general-sum games. Notably, in zero-sum games using deterministic updates, we show the only critical points the dynamics converge to are Stackelberg equilibria and provide a local convergence rate. Empirically, the proposed learning dynamics mitigate rotational behavior and exhibit benefits for training Generative Adversarial Networks compared to gradient play.
 Uncertainty quantification for nonconvex tensor completion: Confidence intervals, heteroscedasticity and optimality 
 We study the distribution and uncertainty of nonconvex optimization for noisy tensor completion --- the problem of estimating a low-rank tensor given incomplete and corrupted observations of its entries. Focusing on a two-stage nonconvex estimation algorithm, we characterize the distribution of this estimator down to fine scales. This distributional theory in turn allows one to construct valid and short confidence intervals for both the unseen tensor entries and its underlying tensor factors. The proposed inferential procedure enjoys several important features: (1) it is fully adaptive to noise heteroscedasticity, and (2) it is data-driven and adapts automatically to unknown noise distributions. Furthermore, our findings unveil the statistical optimality of nonconvex tensor completion: it attains un-improvable estimation accuracy --- including both the rates and the pre-constants --- under i.i.d. Gaussian noise.
 Near-Tight Margin-Based Generalization Bounds for Support Vector Machines 
 Support Vector Machines (SVMs) are among the most fundamental tools for binary classification. 

In its simplest formulation, an SVM produces a hyperplane separating two classes of data using the largest possible margin to the data. 
The focus on maximizing the margin has been well motivated through numerous generalization bounds. 

In this paper, we revisit and improve the classic generalization bounds in terms of margins. 
Furthermore, we complement our new generalization bound by a nearly matching lower bound, thus almost settling the generalization performance of SVMs in terms of margins.
 Reinforcement Learning in Feature Space: Matrix Bandit, Kernels, and Regret Bound 
 Exploration in reinforcement learning (RL) suffers from the curse of dimensionality when the state-action space is large. A common practice is to parameterize the high-dimensional value and policy functions using given features. However existing methods either have no theoretical guarantee or suffer a regret that is exponential in the planning horizon $H$.In this paper, we propose an online RL algorithm, namely the MatrixRL, that leverages ideas from linear bandit  to learn a low-dimensional representation of the probability  transition model while carefully balancing the exploitation-exploration tradeoff. We show that MatrixRL achieves a regret bound ${O}\big(H^2d\log T\sqrt{T}\big)$ where $d$ is the number of features, independent with the number of state-action pairs. MatrixRL has an equivalent kernelized version, which is able to work with an arbitrary kernel Hilbert space without using explicit features. In this case, the kernelized MatrixRL satisfies a regret bound ${O}\big(H^2\wt{d}\log T\sqrt{T}\big)$, where $\wt{d}$ is the effective dimension of the kernel space.
 Graph Structure of Neural Networks 
 Neural networks are often represented as graphs of connections between the neurons. However, despite their wide use there is currently no understanding of the relationship between the graph structure of a neural network and its predictive performance. Here we systematically investigate this relationship, via developing a novel graph-based representation of neural networks called relational graph, where computation is specified by rounds of message exchange along the graph structure. Using our novel framework we show that (1) there is a “sweet spot”, where relational graphs within certain range of average path length and clustering coefficient lead to neural networks with significant improvements in predictive performance; (2) perhaps even more surprisingly, we find that these sweet spots tend to highly correlate across different architectures and datasets; and, (3) we show that discovering top-performing relational graphs only requires a few epochs of training. Overall, our results suggest promising avenues for designing and understanding neural networks with graph representations.
 Causal Inference using Gaussian Processes with Structured Latent Confounders 
 Latent confounders---unobserved variables that influence both treatment and outcome---can bias estimates of causal effects. In some cases, these confounders are shared across observations, e.g. all students in a school are influenced by the school's culture in addition to any educational interventions they receive individually. This paper shows how to model latent confounders that have this structure and thereby improve estimates of causal effects. The key innovations are a hierarchical Bayesian model, Gaussian processes with structured latent confounders (GP-SLC), and a Monte Carlo inference algorithm for this model based on elliptical slice sampling. GP-SLC provides principled Bayesian uncertainty estimates of individual treatment effect without requiring parametric assumptions about the functional forms relating confounders, covariates, treatment, and outcomes. This paper also proves that, for linear functional forms, accounting for the structure in latent confounders is sufficient for asymptotically consistent estimates of causal effect. Finally, this paper shows GP-SLC is competitive with or more accurate than widely used causal inference techniques such as multi-level linear models and Bayesian additive regression trees. Benchmark datasets include the Infant Health and Development Program and a dataset showing the effect of changing temperatures on state-wide energy consumption across New England.
 Accelerating Large-Scale Inference with Anisotropic Vector Quantization 
 Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach achieves state-of-the-art results on the public benchmarks available at \url{ann-benchmarks.com}.
 IPBoost – Non-Convex Boosting via Integer Programming 
 Recently non-convex optimization approaches for solving machine learning problems have gained significant attention. In this paper we explore non-convex boosting in classification by means of integer programming and demonstrate real-world practicability of the approach while circumvent- ing shortcomings of convex boosting approaches. We report results that are comparable to or better than the current state-of-the-art.
 Lower Complexity Bounds for Finite-Sum Convex-Concave Minimax Optimization Problems 
 This paper studies the lower bound complexity for minimax optimization problem whose objective function is the average of $n$ individual smooth convex-concave functions. We consider the algorithm which gets access to gradient and proximal oracle for each individual component. For the strongly-convex-strongly-concave case, we prove such an algorithm can not reach an $\varepsilon$-suboptimal point in fewer than $\Omega\left((n+\kappa)\log(1/\varepsilon)\right)$ iterations, where $\kappa$ is the condition number of the objective function. This lower bound matches the upper bound of the existing incremental first-order oracle algorithm stochastic variance-reduced extragradient. We develop a novel construction to show the above result, which partitions the tridiagonal matrix of classical examples into $n$ groups. This construction is friendly to the analysis of incremental gradient and proximal oracle and we also extend the analysis to general convex-concave cases.
 Closed Loop Neural-Symbolic Learning via Integrating Neural Perception, Grammar Parsing, and Symbolic Reasoning 
 The goal of neural-symbolic computation is to integrate the connectionist and symbolist paradigms. Prior methods learn the neural-symbolic models using reinforcement learning (RL) approaches, which ignore the error propagation in the symbolic reasoning module and thus converge slowly with sparse rewards. In this paper, we address these issues and close the loop of neural-symbolic learning by (1) introducing the grammar model as a symbolic prior to bridge neural perception and symbolic reasoning, and (2) proposing a novel back-search algorithm which mimics the top-down human-like learning procedure to propagate the error through the symbolic reasoning module efficiently. We further interpret the proposed learning framework as maximum likelihood estimation using Markov chain Monte Carlo sampling and the back-search algorithm as a Metropolis-Hastings sampler. The experiments are conducted on two weakly-supervised neural-symbolic tasks: (1) handwritten formula recognition on the newly introduced HWF dataset; (2) visual question answering on the CLEVR dataset. The results show that our approach significantly outperforms the RL methods in terms of performance, converging speed, and data efficiency. Our code and data are released at https://liqing-ustc.github.io/NGS.
 Latent Bernoulli Autoencoder 
 In this work, we pose a question whether it is possible to design and train an
autoencoder model in an end-to-end fashion to learn latent representations in
multivariate Bernoulli space, and achieve performance comparable with the
current state-of-the-art variational methods. Moreover, we investigate how to
generate novel samples and perform smooth interpolation in the binary latent
space.  To meet our objective, we propose a simplified deterministic model
with a straight-through estimator to learn the binary latents and show its
competitiveness with the latest VAE methods.  Furthermore, we propose a novel
method based on a random hyperplane rounding for sampling and smooth
interpolation in the multivariate Bernoulli latent space.  Although not a main
objective, we demonstrate that our methods perform on par or better than the
current state-of-the-art methods on common CelebA, CIFAR-10 and MNIST 
datasets. PyTorch code and trained models to reproduce published results 
will be released with the camera ready version.
 Estimating the Error of Randomized Newton Methods: A Bootstrap Approach 
 Randomized Newton methods have recently become the focus of intense research activity in large-scale and distributed optimization. Generally, these methods are based on a "computation-accuracy trade-off", which allows the user to gain scalability in exchange for error in the solution. However, the user does not know how much error is created by the randomization, which can be detrimental in two ways: On one hand, the user may try to manage the unknown error with theoretical worst-case error bounds, but this approach is impractical when the bounds involve unknown constants, and it typically leads to excessive computation. On the other hand, the user may select tuning parameters or stopping criteria in a heuristic manner, but this is generally unreliable. Motivated by these difficulties, we develop a bootstrap method for directly estimating the unknown error, which avoids excessive computation and offers greater reliability. Also, we provide non-asymptotic theoretical guarantees to show that the error estimates are valid for several error metrics and algorithms (including GIANT and Newton Sketch). Lastly, we show that the proposed method adds relatively little cost to existing randomized Newton methods, and that it performs well in a range of experimental conditions.
 Bandits for BMO Functions 
 We study the bandit problem where the underlying expected reward is a Bounded Mean Oscillation (BMO) function. BMO functions are allowed to be discontinuous and unbounded, and are useful in modeling signals with singularities in the domain.  For example, BMO functions can model the intensity field of several radioactive emitting sources.  A bandit BMO algorithm can help us quickly locate the strongest emitting source. We develop a toolset for BMO bandits, and provide an algorithm that can achieve poly-log $\delta$-regret -- a regret measured against an arm that is optimal after removing a $\delta$-sized portion of the arm space. 
 Fair Learning with Private Demographic Data 
 Sensitive attributes such as race are rarely available to learners in real world settings as their collection is often restricted by laws and regulations. We give a scheme that allows individuals to release their sensitive information privately while still allowing any downstream entity to learn non-discriminatory predictors. We show how to adapt non-discriminatory learners to work with privatized protected attributes giving theoretical guarantees on performance. Finally, we highlight how the methodology could apply to learning fair predictors in settings where protected attributes are only available for a subset of the data.
 Lookahead-Bounded Q-learning 
 We introduce the lookahead-bounded Q-learning (LBQL) algorithm, a new, provably convergent variant of Q-learning that seeks to improve the performance of standard Q-learning in stochastic environments through the use of “lookahead” upper and lower bounds. To do this, LBQL employs previously collected experience and each iteration’s state-action values as dual feasible penalties to construct a sequence of sampled information relaxation problems. The solutions to these problems provide estimated upper and lower bounds on the optimal value, which we track via stochastic approximation. These quantities are then used to constrain the iterates to stay within the bounds at every iteration. Numerical experiments confirm the fast convergence of LBQL as compared to the standard Q-learning algorithm and several related techniques.
 Representations for Stable Off-Policy Reinforcement Learning 
 Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical SARSA algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case of a defective transition matrix, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks.
 Zeno++: Robust Fully Asynchronous SGD 
 We propose Zeno++, a new robust asynchronous Stochastic Gradient Descent(SGD) procedure, intended to  tolerate Byzantine failures of  workers. In contrast to previous work, Zeno++ removes several unrealistic restrictions on worker-server communication, now allowing for fully asynchronous updates from anonymous workers, for arbitrarily stale worker updates, and for the possibility of an unbounded number of Byzantine workers. The key idea is to estimate the descent of the loss value after the candidate gradient is applied, where large descent values indicate that the update results in optimization progress. We prove the convergence of Zeno++ for non-convex problems under Byzantine failures. Experimental results show that Zeno++ outperforms existing Byzantine-tolerant asynchronous SGD algorithms.
 Upper bounds for Model-Free Row-Sparse Principal Component Analysis 
 Sparse principal component analysis (PCA) is a widely-used dimensionality reduction tool in statistics and machine learning. Most methods mentioned in literature are either heuristics for good primal feasible solutions under statistical assumptions or ADMM-type algorithms with stationary/critical points convergence property for the regularized reformulation of sparse PCA. However, none of these methods can efficiently verify the quality of the solutions via comparing current objective values with their dual bounds, especially in model-free case. We propose a new framework that finds out upper (dual) bounds for the sparse PCA within polynomial time via solving a convex integer program (IP). We show that, in the worst-case, the dual bounds provided by the convex IP is within an affine function of the global optimal value. Moreover, in contrast to the semi-definition relaxation, this framework is much easier to scale on large cases. Numerical results on both artificial and real cases are reported to demonstrate the advantages of our method.
 Aligned Cross Entropy for Non-Autoregressive Machine Translation 
 Non-autoregressive machine translation models significantly speed up decoding by allowing for parallel prediction of the entire target sequence. However, modeling word order is more challenging due to the lack of autoregressive factors in the model. This difficultly is compounded during training with cross entropy loss, which can highly penalize small shifts in word order. In this paper, we propose aligned cross entropy (AXE) as an alternate loss function for training of non-autoregressive models. AXE uses a differentiable dynamic program to assign loss based on the best possible monotonic alignment between target tokens and model predictions. AXE-based non-monotonic training of conditional masked language models (CMLMs) improves performance by 3 and 5 BLEU points respectively on WMT 16 EN-RO and WMT 14 EN-DE. It also significantly outperforms the state-of-the-art non-autoregressive models on a range of translation benchmarks.
 Stochastic Flows and Geometric Optimization on the Orthogonal Group 
 We present a new class of stochastic, geometrically-driven optimization algorithms on the orthogonal group O(d) and naturally reductive homogeneous manifolds obtained from the action of the rotation group SO(d). We theoretically and experimentally demonstrate that our methods can be applied in various fields of machine learning including deep, convolutional and recurrent neural networks, reinforcement learning, normalizing flows and metric learning. We show an intriguing connection between efficient stochastic optimization on the orthogonal group and graph theory (e.g. matching problem, partition functions over graphs, graph-coloring). We leverage the theory of Lie groups and provide theoretical results for the designed class of algorithms. We demonstrate broad applicability of our methods by showing strong performance on the seemingly unrelated tasks of learning world models to obtain stable policies for the most difficult Humanoid agent from OpenAI Gym and improving convolutional neural networks.
 Fast and Three-rious: Speeding Up Weak Supervision with Triplet Methods 
 Weak supervision is a popular method for building machine learning models without relying on ground truth annotations. Instead, it generates probabilistic training labels by estimating the accuracies of multiple noisy labeling sources (e.g., heuristics, crowd workers). Existing approaches use latent variable estimation to model the noisy sources, but these methods can be computationally expensive, scaling superlinearly in the data. In this work, we show that, for a class of latent variable models highly applicable to weak supervision, we can find a closed-form solution to model parameters, obviating the need for iterative solutions like stochastic gradient descent (SGD). We use this insight to build FlyingSquid, a weak supervision framework that runs orders of magnitude faster than previous weak supervision approaches and requires fewer assumptions. In particular, we prove bounds on generalization error without assuming that the latent variable model can exactly parameterize the underlying data distribution. Empirically, we validate FlyingSquid on benchmark weak supervision datasets and find that it achieves the same or higher quality compared to previous approaches without the need to tune an SGD procedure, recovers model parameters 170 times faster on average, and enables new video analysis and online learning applications.
 Implicit Geometric Regularization for Learning Shapes 
 Representing shapes as level-sets of neural networks has been recently proved to be useful for different shape analysis and reconstruction tasks. So far, such representations were computed using either: (i) pre-computed implicit shape representations; or (ii) loss functions explicitly defined over the neural level-sets. 

In this paper we offer a new paradigm for computing high fidelity implicit neural representations directly from raw data (i.e., point clouds, with or without normal information). We observe that a rather simple loss function, encouraging the neural network to vanish on the input point cloud and to have a unit norm gradient, possesses an implicit geometric regularization property that favors smooth and natural zero level-set surfaces, avoiding bad zero-loss solutions. 
We provide a theoretical analysis of this property for the linear case, and show that, in practice, our method leads to state-of-the-art implicit neural representations with higher level-of-details and fidelity compared to previous methods. 
 Accelerated Stochastic Gradient-free and Projection-free Methods 
 In the paper, we propose a class of accelerated stochastic gradient-free and projection-free (a.k.a., zeroth-order Frank Wolfe) methods to solve the problem of constrained stochastic and finite-sum nonconvex optimization. Specifically, we propose an accelerated stochastic zeroth-order Frank Wolfe (Acc-SZOFW)
method based on the variance reduced technique and a novel momentum technique. Moreover, under some mild conditions, we prove that the Acc-SZOFW has the function query complexity of $O(d\sqrt{n}\epsilon^{-2})$ for finding an $\epsilon$-stationary point in the finite-sum problem, which improves the exiting best result by a factor of $O(\sqrt{n}\epsilon^{-2})$, and has the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem, which improves the exiting best result by a factor of $O(\epsilon^{-1})$. Further, we propose a novel accelerated stochastic zeroth-order Frank Wolfe (Acc-SZOFW*) to relax the large mini-batch size required in the Acc-SZOFW. In particular, we prove that the Acc-SZOFW* still has the function query complexity of $O(d\epsilon^{-3})$ in the stochastic problem. Finally, we use extensive experiments including black-box adversarial attack and robust black-box classification to verify the efficiency of our algorithms.
 XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation 
 Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We will release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.


 Does the Markov Decision Process Fit the Data: Testing for the Markov Property in Sequential Decision Making 
 The Markov assumption (MA) is fundamental to the empirical validity of reinforcement learning. In this paper, we propose a novel Forward-Backward Learning procedure to test MA in sequential decision making. The proposed test does not assume any parametric form on the joint distribution of the observed data and plays an important role for identifying the optimal policy in high-order Markov decision processes and partially observable MDPs. We apply our test to both synthetic datasets and a real data example from mobile health studies to illustrate its usefulness.
 Context Aware Local Differential Privacy 
 Local differential privacy (LDP) is a strong notion of privacy that often leads to a significant drop in utility. The original definition of LDP assumes that all the elements in the data domain are equally sensitive. However, in many real-life applications, some elements are more sensitive than others. We propose a context-aware framework for LDP that allows the privacy level to vary across the data domain, enabling system designers to place privacy constraints where they matter without paying the cost where they do not. For binary data domains, we provide a universally optimal privatization scheme and highlight its connections to Warner’s randomized response and Mangat’s improved response. Motivated by geo-location and web search applications, for k-ary data domains, we consider two special cases of context-aware LDP: block-structured LDP and high-low LDP. We study minimax discrete distribution estimation under both cases and provide communication-efficient, sample-optimal schemes, and information-theoretic lower bounds. We show, using worst-case analyses and experiments on Gowalla’s 3.6 million check-ins to 43,750 locations, that context-aware LDP achieves a far better accuracy under the same number of samples.
 Accountable Off-Policy Evaluation via a Kernelized Bellman Statistics 
 Off-policy evaluation plays an important role in modern reinforcement learning.
However, most of the existing off-policy evaluation only focus on the value estimation, without providing an accountable confidence interval, that can reflect the uncertainty caused by limited observed data and algorithmic errors. Recently,  Feng  et  al.  (2019) proposed a novel kernel loss for learning value functions, which can also be used to test whether the learned value function satisfies the Bellman equation. In this work, we investigate the statistical properties of the kernel loss, which allows us to find a feasible set that contains the true value function with high probability. We further utilize this set to construct an accountable confidence interval for off-policy value estimation, and a post-hoc diagnosis for existing estimators. Empirical results show that our methods yield a tight yet accountable confidence interval in different settings, which demonstrate the effectiveness of our method.
 Is There a Trade-Off Between Fairness and Accuracy? A Perspective Using Mismatched Hypothesis Testing 
 A trade-off between accuracy and fairness is almost taken as a given in the existing literature on fairness in machine learning. Yet, it is not preordained that accuracy should decrease with increased fairness. Novel to this work, we examine fair classification through the lens of mismatched hypothesis testing: trying to find a classifier that distinguishes between two ideal distributions when given two mismatched distributions that are biased. Using Chernoff information, a tool in information theory, we theoretically demonstrate that, contrary to popular belief, there always exist ideal distributions such that optimal fairness and accuracy (with respect to the ideal distributions) are achieved simultaneously: there is no trade-off. Moreover, the same classifier yields the lack of a trade-off with respect to ideal distributions while yielding a trade-off when accuracy is measured with respect to the given (possibly biased) dataset. To complement our main result, we formulate an optimization to find ideal distributions and derive fundamental limits to explain why a trade-off exists on the given biased dataset. We also derive conditions under which active data collection can alleviate the fairness-accuracy trade-off in the real world. Our results lead us to contend that it is problematic to measure accuracy with respect to data that may reflect bias and inequity, and instead, we should be considering accuracy with respect to ideal, unbiased data.
 Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates 
 Learning with noisy labels is a common problem in supervised learning. Existing approaches require practitioners to specify \emph{noise rates}, i.e., a set of parameters controlling the severity of label noises in the problem. The specifications are either assumed to be given or estimated using additional approaches. In this work, we introduce a new family of loss functions that we name as \emph{peer loss} functions, which enables learning from noisy labels that does not require a priori specification of the noise rates.Our approach uses a standard empirical risk minimization (ERM) framework with peer loss functions. Peer loss functions associate each training sample with a certain form of ``peer" samples, which evaluate a classifier' predictions jointly. 

We show that, under mild conditions, performing ERM with peer loss functions on the noisy dataset leads to the optimal or a near optimal classifier as if performing ERM over the clean training data, which we do not have access to. We pair our results with an extensive set of experiments, where we compare with state-of-the-art techniques of learning with noisy labels. Our results show that peer loss functions based method consistently outperforms the baseline benchmarks, as well as some recent new results. Peer loss provides a way to simplify model development when facing potentially noisy training labels, and can be promoted as a robust candidate loss function in such situations.
 On the Expressivity of Neural Networks for Deep Reinforcement Learning 
 We compare the model-free reinforcement learning with the model-based approaches through the lens of the expressive power of neural networks for policies, Q-functions, and dynamics. We show, theoretically and empirically, that even for one-dimensional continuous state space, there are many MDPs whose optimal Q-functions and policies are much more complex than the dynamics. For these MDPs, model-based planning is a favorable algorithm, because the resulting policies can approximate the optimal policy significantly better than a neural network parameterization can, and model-free or model-based policy optimization rely on policy parameterization. Motivated by the theory, we apply a simple multi-step model-based bootstrapping planner (BOOTS) to bootstrap a weak Q-function into a stronger policy. Empirical results show that applying BOOTS on top of model-based or model-free policy optimization algorithms at the test time improves the performance on benchmark tasks.
 Uniform Convergence of Rank-weighted Learning  
 The decision-theoretic foundations of classical machine learning models have largely focused on estimating model parameters that minimize the expectation of a given loss function. However, as machine learning models are deployed in varied contexts, such as in high-stakes decision-making and societal settings, it is clear that these models are not just evaluated by their average performances. In this work, we study a novel notion of L-Risk based on the classical idea of rank-weighted learning. These L-Risks, induced by rank-dependent weighting functions with bounded variation, is a unification of popular risk measures such as conditional value-at-risk and those defined by cumulative prospect theory. We give uniform convergence bounds of this broad class of risk measures and study their consequences on a logistic regression example.
 Label-Noise Robust Domain Adaptation 
 Domain adaptation aims to correct the classifiers when faced with distribution shift between source (training) and target (test) domains. State-of-the-art domain adaptation methods make use of deep networks to extract domain-invariant representations. However, existing methods assume that all the instances in the source domain are correctly labeled; while in reality, it is unsurprising that we may obtain a source domain with noisy labels. In this paper, we first investigate how label noise could adversely affect existing domain adaptation methods in various scenarios. Focusing on the generalized target shift scenario, where both label distribution $P_Y$ and the class-conditional distribution $P_{X|Y}$ can change, we propose a new Denoising Conditional Invariant Component (DCIC) framework, which provably ensures (1) extracting invariant representations given examples with noisy labels in the source domain and unlabeled examples in the target domain and (2) estimating the label distribution in the target domain with no bias. Experimental results on both synthetic and real-world data verify the effectiveness of the proposed method.
 Streaming k-Submodular Maximization under Noise subject to Size Constraint 
 Maximizing on k-submodular functions subject to size constraint has received extensive attention recently. In this paper, we investigate a more realistic scenario of this problem that (1) obtaining exact evaluation of an objective function is impractical, instead, its noisy version is acquired; and (2) algorithms are required to take only one single pass over dataset, producing solutions in a timely manner. We propose two novel streaming algorithms, namely DStream and RStream, with their theoretical performance guarantees. We further demonstrate the efficiency of our algorithms in two application, showing that our algorithms can return comparative results to state-of-the-art non-streaming methods while using a much fewer number of queries.
 LEEP: A New Measure to Evaluate Transferability of Learned Representations 
 We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.
 Implicit Generative Modeling for Efficient Exploration 
 Efficient exploration remains a challenging problem in reinforcement learning, especially for those tasks where rewards from environments are sparse. 
In this work, we introduce an exploration approach based on a novel implicit generative modeling algorithm to estimate a Bayesian uncertainty of the agent's belief of the environment dynamics. Each random draw from our generative model is a neural network that instantiates the dynamic function, hence multiple draws would approximate the posterior, and the variance in the predictions based on this posterior is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on the amortized Stein Variational Gradient Descent. In experiments, we demonstrate the effectiveness of this exploration algorithm in both pure exploration tasks and a downstream task, comparing with state-of-the-art intrinsic reward-based exploration approaches, including two recent approaches based on an ensemble of dynamic models. In challenging exploration tasks, our implicit generative model consistently outperforms competing approaches regarding data efficiency in exploration.
 Multi-fidelity Bayesian Optimization with Max-value Entropy Search and its Parallelization 
 In a standard setting of Bayesian optimization (BO), the objective function evaluation is assumed to be highly expensive. Multi-fidelity Bayesian optimization (MFBO) accelerates BO by incorporating lower fidelity observations available with a lower sampling cost. In this paper, we focus on the information-based approach, which is a popular and empirically successful approach in BO. For MFBO, however, existing information-based methods are plagued by difficulty in estimating the information gain. We propose an approach based on max-value entropy search (MES), which greatly facilitates computations by considering the entropy of the optimal function value instead of the optimal input point. We show that, in our multi-fidelity MES (MF-MES), most of additional computations, compared with usual MES, is reduced to analytical computations. Although an additional numerical integration is necessary for the information across different fidelities, this is only in one dimensional space, which can be performed efficiently and accurately. Further, we also propose parallelization of MF-MES. Since there exist a variety of different sampling costs, queries typically occur asynchronously in MFBO. We show that similar simple computations can be derived for asynchronous parallel MFBO. We demonstrate effectiveness of our approach by using benchmark datasets and a real-world application to materials science data.
 Data-Efficient Image Recognition with Contrastive Predictive Coding 
 Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on PASCAL VOC 2007, surpassing fully supervised pre-trained ImageNet classifiers. 
 Efficient Identification in Linear Structural Causal Models with Auxiliary Cutsets 
 We develop a a new polynomial-time algorithm for identification in linear Structural Causal Models that subsumes previous non-exponential identification methods when applied to direct effects, and unifies several disparate approaches to identification in linear systems. Leveraging these new results and  understanding, we develop a procedure for identifying total causal effects. 
 Spectral Subsampling MCMC for Stationary Time Series 
 Bayesian inference using Markov Chain Monte Carlo (MCMC) on large datasets has developed rapidly in recent years. However, the underlying methods are generally limited to relatively simple settings where the data have specific forms of independence. We propose a novel technique for speeding up MCMC for time series data by efficient data subsampling in the frequency domain. For several challenging time series models, we demonstrate a speedup of up to two orders of magnitude while incurring negligible bias compared to MCMC on the full dataset. We also propose alternative control variates for variance reduction based on data grouping and coreset constructions.
 Variance Reduction in Stochastic Particle-Optimization Sampling 
 Stochastic particle-optimization sampling (SPOS) is a recently-developed
scalable Bayesian sampling framework that unifies stochastic gradient MCMC
(SG-MCMC) and Stein variational gradient descent (SVGD) algorithms based
on Wasserstein gradient flows. With a rigorous non-asymptotic convergence
theory developed recently, SPOS avoids the particle-collapsing pitfall of SVGD.
Nevertheless, variance reduction in SPOS has never been studied. In this
paper, we bridge the gap by presenting several variance-reduction techniques
for SPOS. Specifically, we propose three variants of variance-reduced SPOS,
called SAGA particle-optimization sampling (SAGA-POS), SVRG particle optimization sampling (SVRG-POS) and a variant of SVRG-POS which avoids
full gradient computations, denoted as SVRG-POS+. Importantly, we provide
non-asymptotic convergence guarantees for these algorithms in terms of 2-
Wasserstein metric and analyze their complexities. Remarkably, the results
show our algorithms yield better convergence rates than existing variance reduced variants of stochastic Langevin dynamics, even though more space
is required to store the particles in training. Our theory well aligns with
experimental results on both synthetic and real datasets.
 Refined bounds for algorithm configuration: The knife-edge of dual class approximability 
 Automating algorithm configuration is growing increasingly necessary as algorithms come with more and more tunable parameters. It is common to tune parameters using machine learning, optimizing algorithmic performance (runtime or solution quality, for example) using a training set of problem instances from the specific domain at hand. We investigate a fundamental question about these techniques: how large should the training set be to ensure that a parameter’s average empirical performance over the training set is close to its expected, future performance? We answer this question for algorithm configuration problems that exhibit a widely-applicable structure: the algorithm's performance as a function of its parameters can be approximated by a “simple” function. We show that if this approximation holds under the L∞-norm, we can provide strong sample complexity bounds, but if the approximation holds only under the Lp-norm for p < ∞, it is not possible to provide meaningful sample complexity bounds in the worst case. We empirically evaluate our bounds in the context of integer programming, obtaining sample complexity bounds that are up to 700 times smaller than the previously best-known bounds.
 Learning with Good Feature Representations in Bandits and in RL with a Generative Model 
 The construction in the recent paper by Du et al. [2019] implies that searching for a near-optimal action in a bandit sometimes requires examining essentially all the actions, even if the learner is given linear features in R^d that approximate the rewards with a small uniform error. We use the Kiefer-Wolfowitz theorem to prove a positive result that by checking only a few actions, a learner can always find an action that is suboptimal with an error of at most O(ε√d) where ε is the approximation error of the features. Thus, features are useful when the approximation error is small relative to the dimensionality of the features. The idea is applied to stochastic bandits and reinforcement learning with a generative model where the learner has access to d-dimensional linear features that approximate the action-value functions for all policies to an accuracy of ε. For linear bandits, we prove a bound on the regret of order d√(n log(k)) + εn√d log(n) with k the number of actions and n the horizon. For RL we show that approximate policy iteration can learn a policy that is optimal up to an additive error of order ε√d/(1 − γ)^2 and using about d/(ε^2(1 − γ)^4) samples from the generative model. These bounds are independent of the finer details of the features. We also investigate how the structure of the feature set impacts the tradeoff between sample complexity and estimation error.
 Online Pricing with Offline Data: Phase Transition and Inverse Square Law 
 This paper investigates the impact of pre-existing offline data on online learning, in the context of dynamic pricing. We study a single-product dynamic pricing problem over a selling horizon of T periods. The demand in each period is determined by the price of the product according to a linear demand model with unknown parameters. We assume that the seller already has some pre-existing offline data before the start of the selling horizon. The seller wants to utilize both the pre-existing offline data and the sequential online data to minimize the regret of the online learning process. We characterize the joint effect of the size, location and dispersion of the offline data on the optimal regret of the online learning process. Our results reveal surprising transformations of the optimal regret rate with respect to the size of the offline data, which we refer to as phase transitions. In addition, our results demonstrate that the location and dispersion of the offline data also have an intrinsic effect on the optimal regret, and we quantify this effect via the inverse-square law.
 Voice Separation with an Unknown Number of Multiple Speakers 
 We present a new method for separating a mixed audio sequence, in which multiple voices speak simultaneously. The new method employs gated neural networks that are trained to separate the voices at multiple processing steps, while maintaining the speaker in each output channel fixed. A different model is trained for every number of possible speakers, and a the model with the largest number of speakers is employed to select the actual number of speakers in a given sample. Our method greatly outperforms the current state of the art, which, as we show, is not competitive for more than two speakers. 
 Adaptive Droplet Routing in Digital Microfluidic Biochips Using Deep Reinforcement Learning 
 We present and investigate a novel application domain for deep reinforcement learning (RL): droplet routing on digital microfluidic biochips (DMFBs). A DMFB, composed of a two-dimensional electrode array, manipulates discrete fluid droplets to automatically execute biochemical protocols such as high-throughput DNA sequencing and point-of-care clinical diagnosis. However, a major concern associated with the use of DMFBs is that electrodes in a biochip can degrade over time. Droplet-transportation operations associated with the degraded electrodes can fail, thereby compromising the integrity of the bioassay outcome. While it is not feasible to detect the degradation of an electrode by simply examining its appearance, we show that casting droplet transportation as an RL problem enables the training of deep network policies to capture the underlying health conditions of electrodes and to provide reliable fluidic operations. We propose a new RL-based droplet-routing flow that can be used for various sizes of DMFBs, and demonstrate reliable execution of an epigenetic bioassay with the RL droplet router on a fabricated DMFB. To facilitate further research, we also present a simulation environment based on the OpenAI Gym Interface for RL-guided droplet-routing problems on DMFBs.
 Sub-Goal Trees -- a Framework for Goal-Based Reinforcement Learning 
 Many AI problems, in robotics and other domains, are goal-directed, essentially seeking a trajectory leading to some goal state. Reinforcement learning (RL), building on Bellman's optimality equation, naturally optimizes for a single goal, yet can be made goal-directed by augmenting the state with the goal. Instead, we propose a new RL framework, derived from a dynamic programming equation for the all pairs shortest path (APSP) problem, which naturally solves goal-directed queries. We show that this approach has computational benefits for both standard and approximate dynamic programming.
Interestingly, our formulation prescribes a novel protocol for computing a trajectory: instead of predicting the next state given its predecessor, as in standard RL, a goal-conditioned trajectory is constructed by first predicting an intermediate state between start and goal, partitioning the trajectory into two. Then, recursively, predicting intermediate points on each sub-segment, until a complete trajectory is obtained. We call this trajectory structure a sub-goal tree. Building on it, we additionally extend the policy gradient methodology to recursively predict sub-goals, resulting in novel goal-based algorithms. Finally, we apply our method to neural motion planning, where we demonstrate significant improvements compared to standard RL on navigating a 7-DoF robot arm between obstacles.
 Graph Convolutional Network for Recommendation with Low-pass Collaborative Filters 
 Graph Convolutional Neural Network (GCN) is widely used in graph data learning tasks such as recommendation. When facing to a large graph, the graph convolution is very computational expensive thus is simplified in all existing GCNs, while is seriously impaired due to the oversimplification. To address this gap, we leverage the original graph convolution in GCN and propose a Low-pass Collaborative Filter (LCF) to make it applicable to the large graph. LCF is designed to remove the noise in observed data, and it also reduces the complexity of graph convolution without hurting its ability. Experiments show that LCF improves the effectiveness and efficiency of graph convolution and our GCN outperforms existing GCNs significantly.
 Reliable Fidelity and Diversity Metrics for Generative Models 
 Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Frechet Inception Distance (FID) score. Since it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics.
 Expert Learning through Generalized Inverse Multiobjective Optimization: Models, Insights and Algorithms 
 We study a new unsupervised learning task of inferring objective functions or constraints of a multiobjective decision making model, based on a set of observed decisions. Specifically, we formulate such a learning problem as an inverse multiobjective optimization problem (IMOP) and propose its first sophisticated model with statistical guarantees. Then, we some fundamental connections between IMOP, K-means clustering and manifold learning. More precisely, we prove that every K-means clustering problem can be transformed equivalently into an IMOP, and every IMOP can be conversely interpreted as a constrained K-means clustering problem. In addition, we show that the Pareto optimal set is a piecewise continuous manifold with an intrinsic dimension of $ p-1 $ (where $ p $ is the number of objectives) under suitable conditions. Hence, IMOP can also be interpreted as a manifold learning problem. Leveraging these critical insights and connections, we propose two algorithms to solve IMOP through manifold learning and clustering. Numerical results confirm the effectiveness of our model and the computational efficacy of algorithms.
 Towards Adaptive Residual Network Training: A Neural-ODE Perspective 
 Serving as a crucial factor, the depth of residual networks balances model capacity, performance, and training efficiency. However, depth has been long fixed as a hyper-parameter and needs laborious tuning, due to the lack of theories describing its dynamics. Here, we conduct theoretical analysis on network depth and introduce adaptive residual network training, which gradually increases model depth during training. Specifically, from an ordinary differential equation perspective, we describe the effect of depth growth with embedded errors, characterize the impact of model depth with truncation errors, and derive bounds for them. Illuminated by these derivations, we propose an adaptive training algorithm for residual networks, LipGrow, which automatically increases network depth and accelerates model training. In our experiments, it achieves better or comparable performance while reducing ~50% of training time.
 Frequency Bias in Neural Networks for Input of Non-Uniform Density 
 Recent works have partly attributed the generalization ability of over-parameterized neural networks to frequency bias -- networks trained with gradient descent on data drawn from a uniform distribution find a low frequency fit before high frequency ones. As realistic training sets are not drawn from a uniform distribution, we here use the Neural Tangent Kernel (NTK) model to explore the effect of variable density on training dynamics. Our results, which combine analytic and empirical observations, show that when learning a pure harmonic function of frequency $\kappa$, convergence at a point $\x \in \Sphere^{d-1}$ occurs in time $O(\kappa^d/p(\x))$ where $p(\x)$ denotes the local density at $\x$. Specifically, for data in $\Sphere^1$ we analytically derive the eigenfunctions of the kernel associated with the NTK for two-layer networks. We further prove convergence results for deep, fully connected networks with respect to the spectral decomposition of the NTK. Our empirical study highlights similarities and differences between deep and shallow networks in this model.
 Communication-Efficient Distributed Stochastic AUC Maximization with Deep Neural Networks 
 In this paper, we study distributed algorithms for large-scale AUC maximization with a deep neural network as a predictive model.  
Although distributed learning techniques have been investigated extensively  in deep learning, they are not directly applicable to stochastic AUC maximization with deep neural networks due to its striking differences from standard loss minimization problems (e.g., cross-entropy).  Towards addressing this challenge,  we propose and analyze a communication-efficient distributed optimization algorithm  based on a {\it non-convex concave} reformulation of the AUC maximization, in which the communication of both the primal variable and the dual variable between each worker and the parameter server only occurs after multiple steps of gradient-based updates in each worker.  Compared with the naive parallel version of an existing algorithm that computes stochastic gradients at individual machines and averages them for updating the model parameter, our algorithm requires a much less number of communication rounds and still achieves linear speedup in theory. To the best of our knowledge, this is the \textbf{first} work that solves the {\it non-convex concave min-max} problem for  AUC maximization with deep neural networks in a communication-efficient distributed manner while still maintaining the linear speedup property in theory.   Our experiments on several benchmark datasets show the effectiveness of our algorithm and also confirm our theory. 
 Inductive Bias-driven Reinforcement Learning For Efficient Schedules in Heterogeneous Clusters 
 The problem of scheduling of workloads onto heterogeneous processors (e.g., CPUs, GPUs, FPGAs) is of fundamental importance in modern datacenters.
Current system schedulers rely on application/system-specific heuristics that have to be built on a case-by-case basis.
Recent work has demonstrated ML techniques to automate this heuristic search using black box approaches which require significant training data and time, which make them challenging to use in practice.
This paper addresses the challenge in two ways: 
(i) a domain-driven Bayesian reinforcement learning (RL) model for scheduling, which inherently models the resource dependencies identified from the  system architecture; and
(ii) a sampling-based technique which allows the computation of gradients of a Bayesian model without performing full probabilistic inference.
Together, these techniques reduce both the amount of training-data and -time required to produce scheduling policies that significantly outperform black box approaches by up to 2.2$\times$.
 Learning Flat Latent Manifolds with VAEs 
 Measuring the similarity between data points often requires domain knowledge, which can in parts be compensated by relying on unsupervised methods such as latent-variable models, where similarity/distance is estimated in a more compact latent space. Prevalent is the use of the Euclidean metric, which has the drawback of ignoring information about similarity of data stored in the decoder, as captured by the framework of Riemannian geometry. We propose an extension to the framework of variational auto-encoders allows learning flat latent manifolds, where the Euclidean metric is a proxy for the similarity between data points. This is achieved by defining the latent space as a Riemannian manifold and by regularising the metric tensor to be a scaled identity matrix. Additionally, we replace the compact prior typically used in variational auto-encoders with a recently presented, more expressive hierarchical one---and formulate the learning problem as a constrained optimisation problem. We evaluate our method on a range of data-sets, including a video-tracking benchmark, where the performance of our unsupervised approach nears that of state-of-the-art supervised approaches, while retaining the computational efficiency of straight-line-based approaches.

 Finding trainable sparse networks through Neural Tangent Transfer  
 Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria.  In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.
 Learning To Stop While Learning To Predict 
 There is a recent surge of interest in designing deep architectures based on the update steps in traditional algorithms, or learning neural networks to improve and replace traditional algorithms. While traditional algorithms have certain stopping criteria for outputting results at different iterations, many algorithm-inspired deep models are restricted to a ``fixed-depth'' for all inputs. Similar to algorithms, the optimal depth of a deep architecture may be different for different input instances, either to avoid ``over-thinking'', or because we want to compute less for operations converged already. In this paper, we tackle this varying depth problem using a steerable architecture, where a feed-forward deep model and a variational stopping policy are learned together to sequentially determine the optimal number of layers for each input instance. Training such architecture is very challenging. We provide a variational Bayes perspective and design a novel and effective training procedure which decomposes the task into an oracle model learning stage and an imitation stage. Experimentally, we show that the learned deep model along with the stopping policy improves the performances on a diverse set of tasks, including learning sparse recovery, few-shot meta learning, and computer vision tasks.
 Better depth-width trade-offs for neural networks through the lens of dynamical systems 
 The expressivity of neural networks as a function of their depth, width and type of activation units has been an important question in deep learning theory. Recently, depth separation results for ReLU networks were obtained via a new connection with dynamical systems, using a generalized notion of fixed points of a continuous map $f$, called periodic points. In this work, we strengthen the connection with dynamical systems and we improve the existing width lower bounds along several aspects. Our first main result is period-specific width lower bounds that hold under the stronger notion of $L^1$-approximation error, instead of the weaker classification error. Our second contribution is that we provide sharper width lower bounds, still yielding meaningful exponential depth-width separations, in regimes where previous results wouldn't apply. A byproduct of our results is that there exists a universal constant characterizing the depth-width trade-offs, as long as $f$ has odd periods. Technically, our results follow by unveiling a tighter connection between the following three quantities of a given function: its period, its Lipschitz constant and the growth rate of the number of oscillations arising under compositions of the function $f$ with itself.
 On Contrastive Learning for Likelihood-free Inference  
 Likelihood-free methods perform parameter inference in stochastic simulator models where evaluating the likelihood is intractable but sampling synthetic data is possible. One class of methods for this likelihood-free problem uses a classifier to distinguish between pairs of parameter-observation samples generated using the simulator and samples drawn from some reference distribution, implicitly learning a density ratio proportional to the likelihood. Another popular class of methods proposes to fit a conditional distribution to the parameter posterior directly, and a particular recent variant allows for the use of flexible neural density estimators for this task. In this work, we show that both of these approaches can be unified under a general contrastive learning scheme, and clarify how they should be run and compared.
 Sequence Generation with Mixed Representations 
 Tokenization is the first step of many natural language processing (NLP) tasks and plays an important role for neural NLP models. Tokenizaton method such as byte-pair encoding (BPE), which can greatly reduce the large vocabulary and deal with out-of-vocabulary words, has shown to be effective and is widely adopted for sequence generation tasks. While various tokenization methods exist, there is no common acknowledgement which is the best. In this work, we propose to leverage the mixed representations from different tokenization methods for sequence generation tasks, in order to boost the model performance with unique characteristics and advantages of individual tokenization methods. Specifically, we introduce a new model architecture to incorporate mixed representations and a co-teaching algorithm to better utilize the diversity of different tokenization methods. Our approach  achieves significant improvements on neural machine translation (NMT) tasks with six language pairs (e.g., English$\leftrightarrow$German, English$\leftrightarrow$Romanian), as well as an abstractive summarization task.
 Ordinal Non-negative Matrix Factorization for Recommendation 
 We introduce a new non-negative matrix factorization (NMF) method for ordinal data, called OrdNMF. Ordinal data are categorical data which exhibit a natural ordering between the categories. In particular, they can be found in recommender systems, either with explicit data (such as ratings) or implicit data (such as quantized play counts). OrdNMF is a probabilistic latent factor model that generalizes Bernoulli-Poisson factorization (BePoF) and Poisson factorization (PF) applied to binarized data. Contrary to these methods, OrdNMF circumvents binarization and can exploit a more informative representation of the data. We design an efficient variational algorithm based on a suitable model augmentation and related to variational PF. In particular, our algorithm preserves the scalability of PF and can be applied to huge sparse datasets. We report recommendation experiments on explicit and implicit datasets, and show that OrdNMF outperforms BePoF and PF applied to binarized data.
 Student Specialization in Deep Rectified Networks With Finite Width and Input Dimension 
 We consider a deep ReLU / Leaky ReLU student network trained from the output of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD). The student network is \emph{over-realized}: at each layer $l$, the number $n_l$ of student nodes is more than that ($m_l$) of teacher. Under mild conditions on dataset and teacher network, we prove that when the gradient is small at every data sample, each teacher node is \emph{specialized} by at least one student node \emph{at the lowest layer}. For two-layer network, such specialization can be achieved by training on any dataset of \emph{polynomial} size $\cO( K^{5/2} d^3 \epsilon^{-1})$ (sample size including augmentation) until the gradient magnitude drops to $\cO(\epsilon/K^{3/2}\sqrt{d})$, where $d$ is the input dimension, $K = m_1 + n_1$ is the total number of neurons in the lowest layer of teacher and student. To our best knowledge, we are the first to give polynomial sample complexity for student specialization of training two-layer (Leaky) ReLU networks with finite depth and width in teacher-student setting, and finite complexity for the lowest layer specialization in multi-layer case, without parametric assumption of the input (like Gaussian). Our theory suggests that teacher nodes with large fan-out weights get specialized first when the gradient is still large, while others are specialized with small gradient, which suggests inductive bias in training. This shapes the stage of training as empirically observed in multiple previous works. Experiments on synthetic and CIFAR10 verify our findings.
 On a projective ensemble approach to two sample test for equality of distributions 
 In this work, we propose a robust test for the multivariate two-sample problem through projective ensemble, which is a generalization of the Cramer-von Mises statistic. The proposed test statistic has a simple closed-form expression without any tuning parameters involved, it is easy to implement can be computed in quadratic time. Moreover, our test is insensitive to the dimension and consistent against all fixed alternatives, it does not require the moment assumption and is robust to the presence of outliers. We study the asymptotic behaviors of the test statistic under the null and two kinds of alternative hypotheses. We also suggest a permutation procedure to approximate critical values and employ its consistency. We demonstrate the effectiveness of our test through extensive simulation studies and a real data application.
 Planning to Explore via Latent Disagreement 
 To solve complex tasks, intelligent agents first need to explore their environments. However, providing manual feedback to agents during exploration can be challenging. This work focuses on task-agnostic exploration, where an agent explores a visual environment without yet knowing the tasks it will later be asked to solve. While current methods often learn reactive exploration behaviors to maximize retrospective novelty, we learn a world model trained from images to plan for expected surprise. Novelty is estimated as ensemble disagreement in the latent space of the world model. Exploring and learning the world model without rewards, our approach, latent disagreement (LD), efficiently adapts to a range of control tasks with high-dimensional image inputs.
 Scalable Differential Privacy with Certified Robustness in Adversarial Learning 
 In this paper, we aim to develop a scalable algorithm to preserve differential privacy (DP) in adversarial learning for deep neural networks (DNNs), with certified robustness to adversarial examples. By leveraging the sequential composition theory in DP, we randomize both input and latent spaces to strengthen our certified robustness bounds. To address the trade-off among model utility, privacy loss, and robustness, we design an original adversarial objective function, based on the post-processing property in DP, to tighten the sensitivity of our model. A new stochastic batch training is proposed to apply our mechanism on large DNNs and datasets, by bypassing the vanilla iterative batch-by-batch training in DP DNNs. An end-to-end theoretical analysis and evaluations show that our mechanism notably improves the robustness and scalability of DP DNNs.
 Agent57: Outperforming the Atari Human Benchmark 
 Atari games have been a long-standing benchmark in the reinforcement learning (RL) community for the past decade. This benchmark was proposed to test general competency of RL algorithms. Previous work has achieved good average performance by doing outstandingly well on many games of the set, but very poorly in several of the most challenging games. We propose Agent57, the first deep RL agent that outperforms the standard human benchmark on all 57 Atari games. To achieve this result, we train a neural network which parameterizes a family of policies ranging from very exploratory to purely exploitative. We propose an adaptive mechanism to choose which policy to prioritize throughout the training process. Additionally, we utilize a novel parameterization of the architecture that allows for more consistent and stable learning.
 Quantum Boosting 
 Boosting is a technique that boosts a weak and inaccurate machine learning algorithm into a strong accurate learning algorithm. The AdaBoost algorithm by Freund and Schapire (for which they were awarded the G{\"o}del prize in 2003) is one of the widely used boosting algorithms, with many applications in theory and practice. Suppose we have a gamma-weak learner for a Boolean concept class C that takes time R(C), then the time complexity of AdaBoost scales as VC(C)poly(R(C), 1/gamma), where VC(C) is the VC-dimension of C. In this paper, we show how quantum techniques can improve the time complexity of classical AdaBoost. To this end, suppose we have a gamma-weak quantum learning algorithm for a Boolean concept class C that takes time Q(C), we introduce a quantum boosting algorithm whose complexity scales as sqrt{VC(C)}poly(Q(C),1/gamma); thereby achieving quadratic quantum improvement over classical AdaBoost in terms of  VC(C). 
 On the (In)tractability of Computing Normalizing Constants for the Product of Determinantal Point Processes 
 We consider the product of determinantal point processes (DPPs), a point process whose probability mass is proportional to the product of principal minors of multiple matrices as a natural, promising generalization of DPPs.
We study the computational complexity of computing its normalizing constant, which is among the most essential probabilistic inference tasks.
Our complexity-theoretic results (almost) rule out the existence of efficient algorithms for this task, unless input matrices are forced to have favorable structures.
In particular, we prove the following:
(1) Computing $\sum_{S} \det(\mat{A}_{S,S})^p$ exactly for every (fixed) positive even integer $p$ is UP-hard and Mod3P-hard, which gives a negative answer to an open question posed by Kulesza and Taskar (2012).
(2) $\sum_{S} \det(\mat{A}_{S,S}) \det(\mat{B}_{S,S}) \det(\mat{C}_{S,S})$ is NP-hard to approximate within a factor of $ 2^{O(|I|^{1-\epsilon})} $ for any $\epsilon > 0$, where $|I|$ is the input size. This result is stronger than #P-hardness for the case of two matrices by Gillenwater (2014).
(3) There exists a $ k^{O(k)} |I|^{O(1)} $-time algorithm for computing $\sum_{S} \det(\mat{A}_{S,S}) \det(\mat{B}_{S,S})$, where $k$ is ``the maximum rank of $\mat{A}$ and $\mat{B}$'' or ``the treewidth of the graph formed by non-zero entries of $\mat{A}$ and $\mat{B}$.'' Such parameterized algorithms are said to be fixed-parameter tractable.
 Decoupled Greedy Learning of CNNs 
 A commonly cited inefficiency of neural network training by back-propagation is
the update locking problem: each layer must wait for the signal to propagate through
the network before updating. In recent years multiple authors have considered
alternatives that can alleviate this issue. In this context, we consider a simpler, but
more effective, substitute that uses minimal feedback, which we call Decoupled
Greedy Learning (DGL). It is based on a greedy relaxation of the joint training
objective, recently shown to be effective in the context of Convolutional Neural
Networks (CNNs) on large-scale image classification. We consider an optimization
of this objective that permits us to decouple the layer training, allowing for layers
or modules in networks to be trained with a potentially linear parallelization in
layers. We show theoretically and empirically that this approach converges. Then,
we empirically find that it can lead to better generalization than sequential greedy
optimization and sometimes end-to-end back-propagation. We show an extension
of this approach to asynchronous settings, where modules can operate with large
communication delays, is possible with the use of a replay buffer. We demonstrate
the effectiveness of DGL on the CIFAR-10 dataset against alternatives and on the
large-scale ImageNet dataset.
 Encoding Musical Style with Transformer Autoencoders 
 We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global representation with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and an internal dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.
 Explainable k-Means and k-Medians Clustering 
 Clustering is a popular unsupervised learning method for geometric data. Unfortunately, many clustering algorithms use global properties of the data, and there are no simple explanations for cluster assignments. To improve interpretability, we consider using a small threshold tree to partition a dataset into clusters. This leads to cluster assignments that can be explained by very few feature values in a straightforward manner. We study this problem from a theoretical viewpoint, measuring the output quality by the k-means and k-medians objectives. In terms of negative results, we show that popular top-down decision tree algorithms may lead to clusterings with arbitrarily large cost, and we prove that any explainable clustering must incur an \Omega(\log k) approximation compared to the optimal clustering. On the upper bound side, we design efficient algorithms that produce explainable clusters using a tree with k leaves. For two means/medians, we show that a single threshold cut suffices to achieve a constant factor approximation, which is a surprising result that nearly matches our lower bounds. For general k \geq 2, our algorithm is an O(k) approximation to the optimal k-medians and an O(k^2) approximation to the optimal k-means. Prior to our work, no algorithms were known with provable guarantees independent of the dimensionality and input size. 
 ROMA: Multi-Agent Reinforcement Learning with Emergent Roles 
 The role concept provides a useful tool to design and understand complex multi-agent systems, which allows agents with a similar role to share similar behaviors. However, existing role-based methods use prior domain knowledge and predefine role structures and behaviors. In contrast, multi-agent reinforcement learning (MARL) provides flexibility and adaptability, but less efficiency in complex tasks. In this paper, we synergize these two paradigms and propose a role-oriented MARL framework (ROMA). In this framework, roles are emergent, and agents with similar roles tend to share their learning to be specialized on certain sub-tasks. To this end, we construct a stochastic role embedding space by introducing two novel regularizers and conditioning individual policies on roles. Experiments show that our method can learn dynamic, versatile, identifiable, and specialized roles, which help our method push forward the state of the art on the StarCraft II micromanagement benchmark. Demonstrative videos are available at https://sites.google.com/view/romarl/.
 Model-free Reinforcement Learning in Infinite-horizon Average-reward Markov Decision Processes 
 Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm reduces the problem to the discounted-reward version and achieves O(T^{2/3}) regret after T steps, under the minimal assumption of weakly communicating MDPs. The second algorithm makes use of recent advances in adaptive algorithms for adversarial multi-armed bandits and improves the regret to O(\sqrt{T}), albeit with a stronger ergodic assumption. To the best of our knowledge, these are the first model-free algorithms with sub-linear regret (that is polynomial in all parameters) in the infinite-horizon average-reward setting.
 LowFER: Low-rank Bilinear Pooling for Link Prediction 
 Knowledge graphs are incomplete by nature, representing only a limited number of observed facts about the world knowledge as relations between entities. An important task in statistical relational learning is that of link prediction or knowledge graph completion to partly address this issue. Both linear and non-linear (deep learning based) models have been proposed to solve the problem, with former being parameter efficient and interpretable. Bilinear models, while expressive, are prone to overfitting and lead to quadratic growth of parameters in number of relations. Simpler models have become more standard, with certain constraints on bilinear maps as relation parameters. In this work, we propose a factorized bilinear pooling model, commonly used in multi-modal learning, for better fusion of entities and relations, leading to an efficient and constraints free model. We prove that our model is fully expressive and provide bounds on the entity and relation embedding dimensions and the factorization rank. Our model naturally generalizes TuckER model (Balazevic et al., 2019), which has shown to generalize other models as special cases, by efficient low-rank approximation without compromising much on performance. The model complexity can be controlled by the factorization rank as opposed to the cubic growth of core tensor in TuckER model when entities and relations share the same space. Empirically, we evaluate on real-world datasets, reaching on par or state-of-the-art performance. In extreme low-ranks, the model already outperforms many of the recently proposed methods.
 Black-Box Methods for Restoring Monotonicity 
 In many practical applications, heuristic or approximation algorithms
are used to efficiently solve the task at hand. However their
solutions frequently do not satisfy natural monotonicity properties
expected to hold in the optimum. In this work we develop algorithms that are
able to restore monotonicity in the parameters of interest.

Specifically, given oracle access to a possibly non monotone function, 
we provide an algorithm that restores monotonicity while
degrading the expected value of the function by at most $\epsilon$. The
number of queries required is at most logarithmic in $1/\epsilon$ and
exponential in the number of parameters. We also give a lower bound
showing that this exponential dependence is necessary.

Finally, we obtain improved query complexity bounds for restoring the
weaker property of $k$-marginal monotonicity. Under this property, every
$k$-dimensional projection of the function is required to be
monotone. The query complexity we obtain only scales exponentially with $k$ and is polynomial in the number of parameters.

 Time Series Deconfounder: Estimating Treatment Effects over Time in the Presence of Hidden Confounders 
 The estimation of treatment effects is a pervasive problem in medicine. Existing methods for estimating treatment effects from longitudinal observational data assume that there are no hidden confounders. This assumption is not testable in practice and, if it does not hold, leads to biased estimates. In this paper, we develop the Time Series Deconfounder, a method that leverages the assignment of multiple treatments over time to enable the estimation of treatment effects in the presence of multi-cause hidden confounders. The Time Series Deconfounder uses a novel recurrent neural network architecture with multitask output to build a factor model over time and infer substitute confounders that render the assigned treatments conditionally independent.  Then it performs causal inference using the substitute confounders. We provide a theoretical analysis for obtaining unbiased causal effects of time-varying exposures using the Time Series Deconfounder. Using both simulations and real data to show the effectiveness of our method in deconfounding the estimation of treatment responses in longitudinal data. 
 Scalable Deep Generative Modeling for Sparse Graphs 
 Learning graph generative models is a challenging task for deep learning and has wide applicability to a range of domains like chemistry, biology and social science. However current deep neural methods suffer from limited scalability: for a graph with $n$ nodes and $m$ edges, existing deep neural methods require $\Omega(n^2)$ complexity by building up the  adjacency matrix. On the other hand, many real world graphs are actually sparse in the sense that $m\ll n^2$. Based on this, we develop a novel autoregressive model, named BiGG, that utilizes this sparsity to avoid generating the full adjacency matrix, and importantly reduces the graph generation time complexity to $O((n + m)\log n)$. Furthermore, during training this autoregressive model can be parallelized with $O(\log n)$ synchronization stages, which makes it much more efficient than other autoregressive models that require $\Omega(n)$. Experiments on several benchmarks show that the proposed approach not only scales to orders of magnitude larger graphs than previously possible with deep autoregressive graph generative models, but also yields better graph generation quality.
 Debiased Sinkhorn barycenters 
 Entropy regularization in optimal transport (OT) has been the driver of many recent interests for Wasserstein metrics and barycenters in machine learning. It allows to keep the appealing geometrical properties of the unregularized Wasserstein distance while having a significantly lower complexity thanks to Sinkhorn's algorithm. However, entropy brings some inherent smoothing bias, resulting for example in blurred barycenters. This side effect has prompted an increasing temptation in the community to settle for a slower algorithm such as log-domain stabilized Sinkhorn which breaks the parallel structure that can be leveraged on GPUs, or even go back to unregularized OT. Here we show how this bias is tightly linked to the reference measure that defines the entropy regularizer and propose debiased Sinkhorn barycenters that preserve the best of worlds: fast Sinkhorn-like iterations without entropy smoothing. Theoretically, we prove that this debiasing is perfect for Gaussian distributions with equal variance. Empirically, we illustrate the reduced blurring and the computational advantage.
 On Learning Language-Invariant Representations for Universal Machine Translation 
 The goal of universal machine translation is to learn to translate between any pair of languages, given pairs of translated documents for \emph{some} of these languages. Despite impressive empirical results and an increasing interest in massively multilingual models, theoretical analysis on translation errors made by such universal machine translation models is only nascent. In this paper, we take one step towards better understanding of universal machine translation by first proving an impossibility theorem in the general case. In particular, we derive a lower bound on the translation error in the many-to-one translation setting, which shows that any algorithm aiming to learn shared sentence representations among multiple language pairs has to make a large translation error on at least one of the translation tasks, if no assumption on the structure of the languages is made. On the positive side, we show that if the documents follow a natural encoder-decoder generative process, then we can expect a natural notion of ``generalization'': a linear number of pairs, rather than quadratic, suffices. Our theory also explains what kinds of connection graphs between pairs of languages are better suited: ones with longer paths result in worse sample complexity in terms of the total number of documents per language pair needed.

 Generative Flows with Matrix Exponential 
 Flow-based generative models are a family of generative models which enjoy the properties of tractable exact likelihood and efficient training and sampling. They are composed of a sequence of invertible functions. In this paper, we incorporate
matrix exponential into generative flows. Matrix exponential is a map from matrices to invertible matrices, this property is suitable for generative flows. Based on matrix exponential, we propose matrix exponential coupling layers which are a general case of affine coupling layers and a stable version of invertible 1 x 1 convolutions which do not collapse during training. And we modify the networks architecture to make training stable and significantly speed up the training process. Our experiments show that our model achieves great performance on density estimation amongst flow-based models.
 Task-Oriented Active Perception and Planning in Environments with Partially Known Semantics 
 We consider an agent that is assigned with a temporal logic task in an environment whose semantic representation is only partially known. We represent the semantics of the environment with a set of state properties, called \textit{atomic propositions}. The agent holds a probabilistic belief over the atomic propositions and updates it as new sensory measurements arrive. The goal is to design a policy for the agent that realizes the task with high probability. 
We develop a planning strategy that takes the semantic uncertainties into account and by doing so provides probabilistic guarantees on the task success. Furthermore, as new data arrive, the belief over the atomic propositions evolves and, subsequently, the planning strategy adapts accordingly. We evaluate the proposed method on various finite-horizon tasks in planar navigation settings. The empirical results show that the proposed method provides reliable task performance that also improves as the knowledge about the environment enhances.
 Composable Sketches for  Functions of Frequencies: Beyond the Worst Case 
 Recently there has been increased interest in using machine learning techniques to improve classical algorithms.
In this paper we study when it is possible to construct compact, composable sketches
 for weighted
sampling and statistics estimation according to functions of data
frequencies.  Such structures are now central components of
large-scale data analytics and machine learning pipelines. Many common
functions, however, such as thresholds and
$p$th frequency moments with $p>2$, are known to require polynomial
size sketches in the worst case.  We explore performance beyond the
worst case under two different types of assumptions.  The first is
having access to noisy \emph{advice} on item frequencies. This
continues the line of work of Hsu et al.~(ICLR 2019), who assume
predictions are provided by a machine learning model.
 The second is providing guaranteed performance on a restricted class of
input frequency distributions that are better aligned with what is
observed in practice. This extends the work on heavy hitters under Zipfian distributions in a seminal paper of Charikar et al.~(ESA 2002).
Surprisingly, we show analytically and empirically that ``in practice'' small polylogarithmic-size sketches provide
accuracy for ``hard'' functions.
 New Oracle-Efficient Algorithms for Private Synthetic Data Release 
 We present three new algorithms for constructing differentially
  private synthetic data---a sanitized version of a sensitive dataset
  that approximately preserves the answers to a large collection of
  statistical queries. All three algorithms are
  \emph{oracle-efficient} in the sense that they are computationally
  efficient when given access to an optimization oracle. Such an oracle can
  be implemented using many existing (non-private)
  optimization tools such as sophisticated integer program solvers. While the
  accuracy of the synthetic data is contingent on the oracle's
  optimization performance, the algorithms satisfy differential
  privacy even in the worst case. For all three algorithms, we provide
  theoretical guarantees for both accuracy and privacy. Through
  empirical evaluation, we demonstrate that our methods scale well
  with both the dimensionality of the data and the number of queries. In the high privacy
  regime (corresponding to low privacy loss $\eps$), our algorithms achieve better accuracy than state-of-the-art techniques, including the High-Dimensional Matrix Mechanism (McKenna et al.~VLDB 2018).

 Optimal Estimator for Unlabeled Linear Regression 
 Unlabeled linear regression, or ``linear regression with an  unknown permutation'',  has attracted increasing  attentions due to its applications in linkage record and
de-anonymization.  However, its computation proves to be cumbersome and
all existing algorithms require considerable time in the  high dimensional regime. 
This paper proposes a one-step estimator which  are optimal from both the computational and statistical  sense.  From the computational perspective, our estimator  exhibits the same order of computational time  as that of the oracle case, where the covariates are known in advance and only the permutation 
needs recovery.  From the statistical  perspective, when comparing with the necessary conditions for permutation recovery,  our requirement on \emph{signal-to-noise ratio} ($\snr$) agrees up to $O\bracket{\log \log n}$ difference  in certain regimes.  Numerical experiments have also been provided to  corroborate the above claims.
 Estimation of Bounds on Potential Outcomes For Decision Making 
 Estimation of individual treatment effects is often used as the basis for contextual decision making in fields such as healthcare, education, and economics. However, in many real-world applications it is sufficient for the decision maker to have estimates of upper and lower bounds on the potential outcomes under treatment and non-treatment. In these cases, we can get better finite sample efficiency by estimating simple functions that correctly bound the potential outcomes instead of directly estimating the potential outcomes, which may be complex, and hard to estimate. Our theoretical analysis highlights a tradeoff between the complexity of the learning task and the confidence with which the resulting bounds cover the true potential outcomes. Guided by our theoretical findings, we develop an algorithm for learning upper and lower bounds on the potential outcomes under treatment and non-treatment. Our algorithm finds the optimal bound estimates that maximize an objective function defined by the decision maker without violating a required false coverage rate. We demonstrate our algorithm's performance and highlight how it can be used to guide decision making using a clinical dataset, and a well-known causality benchmark. We show that our algorithm outperforms the state-of-the-art, providing tighter intervals without violating the required false coverage rate. 
 Pseudo-Masked Language Models for Unified Language Model Pre-Training 
 We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.
 Self-Attentive Associative Memory 
 Heretofore, neural networks with external memory are restricted to single memory with lossy representations of memory interactions. A rich representation of relationships between memory pieces urges a high-order and segregated relational memory. In this paper, we propose to separate the storage of individual experiences (item memory) and their occurring relationships (relational memory). The idea is implemented through a novel Self-attentive Associative Memory (SAM) operator. Found upon outer product, SAM forms a set of associative memories that represent the hypothetical high-order relationships between arbitrary pairs of memory elements, through which a relational memory is constructed from an item memory. The two memories are wired into a single sequential model capable of both memorization and relational reasoning. We achieve competitive results with our proposed two-memory model in a diversity of machine learning tasks, from challenging synthetic problems to practical testbeds such as geometry, graph, reinforcement learning, and question answering. 
 The k-tied Normal Distribution: A Compact Parameterization of Gaussian Mean Field Posteriors in Bayesian Neural Networks 
 Variational Bayesian Inference is a popular methodology for approximating posterior distributions over Bayesian neural network weights. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibit strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. Furthermore, we find that such factorized parameterizations improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.
 A Sequential Self Teaching Approach for Improving Generalization in Sound Event Recognition 
 An important problem in machine auditory perception is to recognize and detect sound events. In this paper, we propose a sequential self-teaching
approach to learn sounds. Our main proposition is that it is harder to learn sounds in adverse situations such as from weakly labeled or noisy labeled
data and in these situations a single stage of learning is not sufficient. Our proposal is a sequential stage-wise learning process that improves generalization capabilities of a given modeling system. We justify this via technical results. On
Audioset, the largest sound events dataset, our sequential learning approach can lead to up to 9% improvement in performance. A comprehensive evaluation also shows that the model leads to
improved transferability of knowledge from previously trained models, thereby leading to improved generalization capabilities on transfer learning tasks as well.
 Policy Teaching via Environment Poisoning: Training-time Adversarial Attacks against Reinforcement Learning 
 We study a security threat to reinforcement learning where an attacker poisons the learning environment to force the agent into executing a target policy chosen by the attacker. As a victim, we consider RL agents whose objective is to find a policy that maximizes average reward in undiscounted infinite-horizon problem settings. The attacker can manipulate the rewards or the transition dynamics in the learning environment at training-time and is interested in doing so in a stealthy manner. We propose an optimization framework for finding an \emph{optimal stealthy attack} for different measures of attack cost. We provide sufficient technical conditions under which the attack is feasible and provide lower/upper bounds on the attack cost. We instantiate our attacks in two settings: (i) an \emph{offline} setting where the agent is doing planning in the poisoned environment, and (ii) an \emph{online} setting where the agent is learning a policy using a regret-minimization framework with poisoned feedback. Our results show that the attacker can easily succeed in teaching any target policy to the victim under mild conditions and highlight a significant security threat to reinforcement learning agents in practice.
 Converging to Team-Maxmin Equilibria in Zero-Sum Multiplayer Games 
 Efficiently computing equilibria for multiplayer games is still an open challenge in computational game theory. This paper focuses on computing Team-Maxmin Equilibria (TMEs), which is an important solution concept for zero-sum multiplayer
games where players in a team having the same utility function play against an adversary independently. Existing algorithms are inefficient to compute TMEs in large games, especially when the strategy space is too large to be represented due to limited memory. In two-player games, the Incremental Strategy Generation (ISG) algorithm is an efficient approach to avoid enumerating all pure strategies. However, the study of ISG for computing TMEs is completely unexplored. To fill this gap, we first study the properties of ISG for multiplayer games, showing that ISG converges to a Nash equilibrium (NE) but may not converge to a TME. Second, we design an ISG variant for TMEs (ISGT) by exploiting that a TME is an NE maximizing the team’s utility and show that ISGT converges to a TME and the impossibility of relaxing conditions in ISGT. Third, to further improve the scalability, we design an ISGT variant (CISGT) by using the strategy space for computing an equilibrium that is close to TME but is easier to be computed as the initial strategy space of ISGT. Finally, extensive experimental results show that CISGT is orders of magnitude faster than ISGT and the state-of-the-art algorithm to compute TMEs in large games.
 Cost-effective Interactive Attention Learning with Neural Attention Process 
 We propose a novel interactive learning framework which we refer to as Interactive Attention Learning (IAL), in which the human supervisors interactively manipulate the allocated attentions, to correct the model's behavior by updating the attention-generating network. However, such a model is prone to overfitting due to scarcity of human annotations, and requires costly retraining. Moreover, it is almost infeasible for the human annotators to examine attentions on tons of instances and features. We tackle these challenges by proposing a sample-efficient attention mechanism and a cost-effective reranking algorithm for instances and features. First, we propose Neural Attention Process (NAP), which is an attention generator that can update its behavior by incorporating new attention-level supervisions without any retraining. Secondly, we propose an algorithm which prioritizes the instances and the features by their negative impacts, such that the model can yield large improvements with minimal human feedback. We validate IAL on various time-series datasets from multiple domains (healthcare, real-estate, and computer vision) on which it significantly outperforms baselines with conventional attention mechanisms, or without cost-effective reranking, with substantially less retraining and human-model interaction cost.
 Adversarial Nonnegative Matrix Factorization 
 Nonnegative Matrix Factorization (NMF) has become an increasingly important research topic in machine learning. It targets at searching for two non-negative matrices (\textit{i.e.}, feature matrix \textbf{A} and weight matrix \textbf{X}) whose product can well approximate the original matrix \textbf{Y}. Despite all the practical success, most of existing NMF models are still vulnerable to adversarial attacks. To overcome this limitation, we propose a novel Adversarial NMF (ANMF) approach in which an adversary can exercise some control over the perturbed data generation process. Different from the traditional NMF models which focus on either the regular input or certain types of noise, our model considers potential test adversaries that are beyond the pre-defined constraints, which can cope with various noises (or perturbations). We formulate the proposed model as a bilevel optimization problem and use Alternating Direction Method of Multipliers (ADMM) to solve it with convergence analysis. Theoretically, the robustness analysis of ANMF is established under mild conditions dedicating asymptotically unbiased prediction. Extensive experiments verify that ANMF is robust to a broad categories of perturbations, and achieves state-of-the-art performances on distinct real-world benchmark datasets.
 DeepCoDA: personalized interpretability for compositional health 
 Interpretability allows the domain-expert to directly evaluate the model's relevance and reliability, a practice that offers assurance and builds trust. In the healthcare setting, interpretable models should implicate relevant biological mechanisms independent of technical factors like data pre-processing. We define personalized interpretability as a measure of sample-specific feature attribution, and view it as a minimum requirement for a precision health model to justify its conclusions. Some health data, especially those generated by high-throughput sequencing experiments, have nuances that compromise precision health models and their interpretation. These data are compositional, meaning that each feature is conditionally dependent on all other features. We propose the DeepCoDA framework to extend precision health modelling to high-dimensional compositional data, and to provide personalized interpretability through patient-specific weights. Our architecture maintains state-of-the-art performance across 25 real-world data sets, all while producing interpretations that are both personalized and fully coherent for compositional data.
 Input-Sparsity Low Rank Approximation in Schatten Norm 
 We give the first input-sparsity time algorithms for the rank-$k$ low rank approximation problem in every Schatten norm. Specifically, for a given $n\times n$ matrix $A$, our algorithm computes $Y,Z\in \R^{n\times k}$, which, with high probability, satisfy $\|A-YZ^T\|_p \leq (1+\eps)\|A-A_k\|_p$, where $\|M\|_p = \left (\sum_{i=1}^n \sigma_i(M)^p \right )^{1/p}$ is the Schatten $p$-norm of a matrix $M$ with singular values $\sigma_1(M), \ldots, \sigma_n(M)$, and where $A_k$ is the best rank-$k$ approximation to $A$. Our algorithm runs in time $\tilde{O}(\nnz(A) + n^{\alpha_p}\poly(k/\eps))$, where $\alpha_p = 1$ for $p\in [1,2)$ and $\alpha_p = 1 + (\omega-1)(1-2/p)$ for $p>2$ and $\omega \approx 2.374$ is the exponent of matrix multiplication. For the important case of $p = 1$, which corresponds to the more ``robust'' nuclear norm, we obtain $\tilde{O}(\nnz(A) + n \cdot \poly(k/\epsilon))$ time, which was previously only known for the Frobenius norm $(p = 2)$. Moreover, since $\alpha_p < \omega$ for every $p$, our algorithm has a better dependence on $n$ than that in the singular value decomposition for every $p$. Crucial to our analysis is the use of dimensionality reduction for Ky-Fan $p$-norms. 
 Efficient Continuous Pareto Exploration in Multi-Task Learning 
 Tasks in multi-task learning often correlate, conflict, or even compete with each other. As a result, a single solution that is optimal for all tasks rarely exists. Recent papers introduced the concept of Pareto optimality to this field and directly cast multi-task learning as multi-objective optimization problems, but solutions returned by existing methods are typically finite, sparse, and discrete. We present a novel, efficient method that generates locally continuous Pareto sets and Pareto fronts, which opens up the possibility of continuous analysis of Pareto optimal solutions in machine learning problems. We scale up a recent theoretical result in multi-objective optimization to modern machine learning problems by proposing a sample-based sparse linear system, for which standard Hessian-free solvers in machine learning can be applied. We compare our method to the state-of-the-art algorithms and demonstrate its usage of analyzing local Pareto sets on various multi-task classification problems. The experimental results confirm that our algorithm reveals the primary directions in local Pareto sets for trade-off balancing, finds more solutions with different trade-offs efficiently, and scales well to tasks with millions of parameters.
 On the Generalization Effects of Linear Transformations in Data Augmentation 
 Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations which preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations which mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST.

Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms RandAugment by 1.24% on CIFAR-100 using Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA Adversarial Autoaugment on CIFAR datasets.
 Control Frequency Adaptation via Action Persistence in Batch Reinforcement Learning 
 The choice of the control frequency of a system has a relevant impact on the ability of \emph{reinforcement learning algorithms} to learn a highly performing policy. In this paper, we introduce the notion of \emph{action persistence} that consists in the repetition of an action for a fixed number of decision steps, having the effect of modifying the control frequency. We start analyzing how action persistence affects the performance of the optimal policy, and then we present a novel algorithm, \emph{Persistent Fitted Q-Iteration} (PFQI), that extends FQI, with the goal of learning the optimal value function at a given persistence. After having provided a theoretical study of PFQI and a heuristic approach to identify the optimal persistence, we present an experimental campaign on benchmark domains to show the advantages of action persistence and proving the effectiveness of our persistence selection method.
 The continuous categorical: a novel simplex-valued exponential family 
 Simplex-valued data appear throughout statistics and machine learning, for example in the context of transfer learning and compression of deep networks. Existing models for this class of data rely on the Dirichlet distribution or other related loss functions; here we show these standard choices suffer systematically from a number of limitations, including bias and numerical issues that frustrate the use of flexible network models upstream of these distributions. We resolve these limitations by introducing a novel exponential family of distributions for modeling simplex-valued data – the continuous categorical, which arises as a nontrivial multivariate generalization of the recently discovered continuous Bernoulli. Unlike the Dirichlet and other typical choices, the continuous categorical results in a well-behaved probabilistic loss function that produces unbiased estimators, while preserving the mathematical simplicity of the Dirichlet. As well as exploring its theoretical properties, we introduce sampling methods for this distribution that are amenable to the reparameterization trick, and evaluate their performance. Lastly, we demonstrate that the continuous categorical outperforms standard choices empirically, across a simulation study, an applied example on multi-party elections, and a neural network compression task.
 Self-Modulating Nonparametric Event-Tensor Factorization 
 Tensor factorization is a fundamental framework to analyze high-order interactions in data. Despite the success of the existing methods, the valuable temporal information are severely underused. The timestamps of the interactions are either ignored or discretized into crude steps. The recent work although formulates event-tensors to keep the timestamps in factorization and can capture mutual excitation effects among the interaction events, it overlooks another important type of temporal influence, inhibition. In addition, it uses a local window to exclude all the long-term dependencies. To overcome these limitations, we propose a self-modulating nonparametric Bayesian factorization model. We use the latent factors to construct mutually governed, general random point processes, which can capture various short-term/long-term, excitation/inhibition effects, so as to encode the complex temporal dependencies into factor representations.  In addition, our model couples with a latent Gaussian process to estimate and fuse nonlinear yet static relationships between the entities. For efficient inference, we derive a fully decomposed model evidence lower bound to dispense with the huge kernel matrix and costly summations inside the rate and log rate functions. We then develop an efficient stochastic optimization algorithm. We show the advantage of our method in four real-world applications. 
 Uncertainty-Aware Lookahead Factor Models for Improved Quantitative Investing 
 On a periodic basis, publicly traded companies are required to report fundamentals: financial data such as revenue, earnings, debt, etc., providing insight into the company’s financial health. Quantitative finance research has identified several factors—computed features of the reported data—that have been demonstrated in retrospective analysis to outperform market averages. In this paper, we first show through simulation that if we could (clairvoyantly) select stocks using factors calculated on future fundamentals (via oracle), then our portfolios would far outperform a standard factor approach.  Motivated by this analysis, we train MLP and LSTM neural networks to forecast future fundamentals based on a trailing window of five years. We propose lookahead factor models to act upon these predictions,  plugging the predicted future fundamentals into traditional factors. Finally, we incorporate uncertainty estimates from both neural heteroscedastic regression and a dropout-based heuristic, demonstrating gains from adjusting our portfolios to avert risk.  In a retrospective analysis using an industry-grade stock portfolio simulator (backtester), we show simultaneous improvement in annualized return and Sharpe ratio (a common measure of risk-adjusted returns).  Specifically, the simulated annualized return for the uncertainty-aware model is 17.7% (vs 14.0% for a standard factor model) and the Sharpe ratio is 0.84 (vs 0.52).
 Non-Stationary Bandits with Intermediate Observations 
 Online recommender systems often face long delays in receiving feedback, especially when optimizing for some long-term metrics. While mitigating the effects of delays in learning is well-understood in stationary environments, the problem becomes much more challenging when the environment changes. In fact, if the timescale of the change is comparable to the delay, it is impossible to learn about the environment, since the available observations are already obsolete.
However, the arising issues can be addressed if intermediate signals are available without delay, such that given those signals, the long-term behavior of the system is stationary. To model this situation, we 
introduce the problem of stochastic, non-stationary, delayed bandits with intermediate observations. We develop a computationally efficient algorithm based on $\UCRL$, and prove sublinear regret guarantees for its performance. Experimental results demonstrate that our method is able to learn in non-stationary delayed environments where existing methods fail. 
 Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks 
 The optimization of multilayer neural networks typically leads to a solution with zero training error, yet the landscape can exhibit spurious local minima and the minima can be disconnected. In this paper, we shed light on this phenomenon: we show that the combination of stochastic gradient descent (SGD) and over-parameterization makes the landscape of multilayer neural networks approximately connected and thus more favorable to optimization. More specifically, we prove that SGD solutions are connected via a piecewise linear path, and the increase in loss along this path vanishes as the number of neurons grows large. This result is a consequence of the fact that the parameters found by SGD are increasingly dropout stable as the network becomes wider. We show that, if we remove part of the neurons (and suitably rescale the remaining ones), the change in loss is independent of the total number of neurons, and it depends only on how many neurons are left. Our results exhibit a mild dependence on the input dimension: they are dimension-free for two-layer networks and require the number of neurons to scale linearly with the dimension for multilayer networks. We validate our theoretical findings with numerical experiments for different architectures and classification tasks.
 Counterfactual Cross-Validation: Stable Model Selection Procedure for Causal Inference Models 
 We study the model selection problem in \textit{conditional average treatment effect} (CATE) prediction. Unlike previous works on this topic, we focus on preserving the rank order of the performance of candidate CATE predictors to enable accurate and stable model selection. To this end, we analyze the model performance ranking problem and formulate guidelines to obtain a better evaluation metric. We then propose a novel metric that can identify the ranking of the performance of CATE predictors with high confidence. Empirical evaluations demonstrate that our metric outperforms existing metrics in both model selection and hyperparameter tuning tasks.
 Learning to Learn Kernels with Variational Random Features 
 We introduce kernels with random Fourier features in the meta-learning framework for few-shot learning.
We propose meta variational random features (MetaVRF) to learn adaptive kernels for the base-learner, which is developed in a latent variable model by treating the random feature basis as the latent variable. We formulate the optimization of MetaVRF as a variational inference problem by deriving an evidence lower bound under the meta-learning framework. To incorporate shared knowledge from related tasks, we propose a context inference of the posterior, which is established by an LSTM architecture. The LSTM-based inference network can effectively integrate the context information of previous tasks with task-specific information, generating informative and adaptive features. The learned MetaVRF can produce kernels of high representational power with a relatively low spectral sampling rate and also enables fast adaptation to new tasks. Experimental results on a variety of few-shot regression and classification tasks demonstrate that MetaVRF delivers much better, or at least competitive, performance compared to existing meta-learning alternatives.
 Multi-Task Learning with User Preferences: Gradient Descent with Controlled Ascent in Pareto Optimization 
 Multi-Task Learning (MTL) is a well established learning paradigm for jointly learning models for multiple correlated tasks. Often the tasks conflict requiring trade-offs between them during optimization. Recent advances in multi-objective optimization based MTL  have enabled us to use large-scale deep networks to find one or more Pareto optimal solutions. However, they cannot be used to find exact Pareto optimal solutions satisfying user-specified preferences with respect to task-specific losses, that is not only a common requirement in applications but also a useful way to explore the infinite set of Pareto optimal solutions. We develop the first gradient-based multi-objective MTL algorithm to address this problem. Our unique approach combines multiple gradient descent with carefully controlled ascent, that enables it to trace the Pareto front in a principled manner and makes it robust to initialization. Assuming only differentiability of the task-specific loss functions, we provide theoretical guarantees for convergence. We empirically demonstrate the superiority of our algorithm over state-of-the-art methods.
 Optimization from Structured Samples for Coverage Functions 
 We revisit the optimization from samples (OPS) model, which studies the problem of optimizing objective functions directly from the sample data. Previous results showed that we cannot obtain a constant approximation ratio for the maximum coverage problem using polynomially many independent samples of the form $\{S_i, f(S_i)\}_{i=1}^t$ (Balkanski et al., 2017), even if coverage functions are $(1 - \epsilon)$-PMAC learnable using these samples (Badanidiyuru et al., 2012), which means most of the function values can be approximately learned very well with high probability. In this work, to circumvent the impossibility result of OPS, we propose a stronger model called optimization from structured samples (OPSS) for coverage functions, where the data samples encode the structural information of the functions. We show that under three general assumptions on the sample distributions, we can design efficient OPSS algorithms that achieve a constant approximation for the maximum coverage problem. We further prove a constant lower bound under these assumptions, which is tight when not considering computational efficiency. Moreover, we also show that if we remove any one of the three assumptions, OPSS for the maximum coverage problem has no constant approximation.
 Ready Policy One: World Building Through Active Learning 
 Model-Based Reinforcement Learning (MBRL) offers a promising direction for sample efficient learning, often achieving state of the art results for continuous control tasks. However many existing MBRL methods rely on combining greedy policies with exploration heuristics, and even those which utilize principled exploration bonuses construct dual objectives in an ad hoc fashion. In this paper we introduce Ready Policy One (RP1), a framework that views MBRL as an active learning problem, where we aim to improve the world model in the fewest samples possible. RP1 achieves this by utilizing a hybrid objective function, which crucially adapts during optimization, allowing the algorithm to trade off reward v.s. exploration at different stages of learning. In addition, we introduce a principled mechanism to terminate sample collection once we have a rich enough trajectory batch to improve the model. We rigorously evaluate our method on a variety of continuous control tasks, and demonstrate statistically significant gains over existing approaches.
 Inferring DQN structure for high-dimensional continuous control 
 Despite recent advancements in the field of Deep Reinforcement Learning, Deep Q-network (DQN) models still show lackluster performance on problems with high-dimensional action spaces.
The problem is even more pronounced for cases with high-dimensional continuous action spaces due to a combinatorial increase in the number of the outputs. 
Recent works approach the problem by dividing the network into multiple parallel or sequential (action) modules responsible for different discretized actions. 
However, there are drawbacks to both the parallel and the sequential approaches.
Parallel module architectures lack coordination between action modules, leading to extra complexity in the task, while a sequential structure can result in the vanishing gradients problem and exploding parameter space. 
In this work, we show that the compositional structure of the action modules has a significant impact on model performance.
We propose a novel approach to infer the network structure for DQN models operating with high-dimensional continuous actions.
Our method is based on the uncertainty estimation techniques introduced in the paper.
Our approach achieves state-of-the-art performance on MuJoCo environments with high-dimensional continuous action spaces.
Furthermore, we demonstrate the improvement of the introduced approach on a realistic AAA sailing simulator game.
 Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks 
 Adversarial training yields robust models against a specific threat model, e.g., $L_\infty$ adversarial examples. Typically robustness does not generalize to previously unseen threat models, e.g., other $L_p$ norms, or larger perturbations. Our confidence-calibrated adversarial training (CCAT) tackles this problem by biasing the model towards low confidence predictions on adversarial examples. By allowing to reject examples with low confidence, robustness generalizes beyond the threat model employed during training. CCAT, trained only on $L_\infty$ adversarial examples, increases robustness against larger $L_\infty$, $L_2$, $L_1$ and $L_0$ attacks, adversarial frames, distal adversarial examples and corrupted examples and yields better clean accuracy compared to adversarial training. For thorough evaluation we developed novel white- and black-box attacks directly attacking CCAT by maximizing confidence. For each threat model, we use $7$ attacks with up to $50$ restarts and $5000$ iterations and report worst-case robust test error, extended to our confidence-thresholded setting, across all attacks.
 Optimizing Black-box Metrics with Adaptive Surrogates 
 We address the problem of training models with black-box and hard-to-optimize metrics by expressing the metric as a monotonic function of a small number of easy-to-optimize surrogates. We pose the training problem as an optimization over a relaxed surrogate space, which we solve by estimating local gradients for the metric and performing inexact convex projections. We analyze  gradient estimates based on finite differences and local linear interpolations, and show convergence of our approach under smoothness  assumptions with respect to the surrogates. Experimental results on classification and ranking problems verify the proposal performs on par with methods that know the mathematical formulation, and adds notable value when the form of the metric is unknown.
 NADS: Neural Architecture Distribution Search for Uncertainty Awareness 
 Machine learning (ML) systems often encounter Out-of-Distribution (OoD) errors when dealing with testing data coming from a distribution different from training data. It becomes important for ML systems in critical applications to accurately quantify its predictive uncertainty and screen out these anomalous inputs. However, existing OoD detection approaches are prone to errors and even sometimes assign higher likelihoods to OoD samples. Unlike standard learning tasks, there is currently no well established guiding principle for designing OoD detection architectures that can accurately quantify uncertainty. To address these problems, we first seek to identify guiding principles for designing uncertainty-aware architectures, by proposing Neural Architecture Distribution Search (NADS). NADS searches for a distribution of architectures that perform well on a given task, allowing us to identify common building blocks among all uncertainty-aware architectures. With this formulation, we are able to optimize a stochastic OoD detection objective and construct an ensemble of models to perform OoD detection. We perform multiple OoD detection experiments and observe that our NADS performs favorably compared to state-of-the-art OoD detection methods.
 Robust learning with the Hilbert-Schmidt independence criterion 
 We investigate the use of a non-parametric independence measure, the Hilbert-Schmidt Independence Criterion (HSIC), as a loss-function for learning robust regression and classification models. This loss-function encourages learning models where the distribution of the residuals between the label and the model prediction is statistically independent of the distribution of the instances themselves. This loss-function was first proposed by \citet{mooij2009regression} in the context of learning causal graphs. We adapt it to the task of learning for unsupervised covariate shift: learning on a source domain without access to any instances or labels from the unknown target domain, but with the assumption that $p(y|x)$  (the conditional probability of labels given instances) remains the same in the target domain. We show that the proposed loss is expected to give rise to models that generalize well on a class of target domains characterised by the complexity of their description within a reproducing kernel Hilbert space. Experiments on unsupervised covariate shift tasks  demonstrate that models learned with the proposed loss-function outperform models learned with standard loss functions, achieving state-of-the-art results on a challenging cell-microscopy unsupervised covariate shift task.
 On the Iteration Complexity of Hypergradient Computations 
 We study a general class of bilevel optimization problems, in which the upper-level objective is defined via the solution of a fixed point equation. Important instances arising in machine learning include hyper-parameter optimization, meta-learning, graph and recurrent neural networks. Typically the gradient of the upper-level objective is not known explicitly or is hard to compute exactly, which has raised the interest in approximation methods. We investigate two popular approaches to compute the hypergradient, based on reverse mode iterative differentiation and approximate implicit differentiation. We present a unified analysis which allows for the first time to quantitatively compare these methods, providing explicit bounds for their iteration complexity. This analysis suggests a hierarchy in terms of computational efficiency among the above methods, with approximate implicit differentiation based on conjugate gradient performing best. We present an extensive experimental comparison among the methods which confirm the theoretical findings.
 Retrieval Augmented Language Model Pre-Training 
 Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.
 Predictive Sampling with Forecasting Autoregressive Models 
 Autoregressive models (ARMs) currently hold state-of-the-art performance in likelihood-based modeling of image and audio data. Generally, neural network based ARMs are designed to allow fast inference, but sampling from these models is impractically slow. In this paper, we introduce the predictive sampling algorithm: a procedure that exploits the fast inference property of ARMs in order to speed up sampling, while keeping the model intact. We propose two variations of predictive sampling, namely sampling with ARM fixed-point iteration and learned forecasting modules. Their effectiveness is demonstrated in two settings: i) explicit likelihood modeling on binary MNIST, SVHN and CIFAR10, and ii) discrete latent modeling in an autoencoder trained on SVHN, CIFAR10 and Imagenet32. Empirically, we show considerable improvements over baselines in number of ARM inference calls and sampling speed.
 Black-Box Variational Inference as a Parametric Approximation to Langevin Dynamics 
 Variational inference (VI) and Markov chain Monte Carlo (MCMC) are approximate posterior inference algorithms that are often said to have complementary strengths, with VI being fast but biased and MCMC being slower but asymptotically unbiased. In this paper, we analyze gradient-based MCMC and VI procedures and find theoretical and empirical evidence that these procedures are not as different as one might think. In particular, a close examination of the Fokker-Planck equation that governs the Langevin dynamics (LD) MCMC procedure reveals that LD implicitly follows a gradient flow that corresponds to an VI procedure based on optimizing a nonparametric normalizing flow. The evolution under gradient descent of real-world VI approximations that use tractable, parametric flows can thus be seen as an approximation to the evolution of a population of LD-MCMC chains. This result suggests that the transient bias of LD (due to the Markov chain not having burned in) may track that of VI (due to the optimizer not having converged), up to differences due to VI’s asymptotic bias and parameter geometry. Empirically, we find that the transient biases of these algorithms (and their momentum-accelerated counterparts) do evolve similarly. This suggests that practitioners with a limited time budget may get more accurate results by running an MCMC procedure (even if it is stopped before fully burning in) than a VI procedure, as long as the variance of the MCMC estimator can be dealt with (e.g., by running many parallel chains).
 Fiedler Regularization: Learning Neural Networks with Graph Sparsity 
 We introduce a novel regularization approach for deep learning that incorporates and respects the underlying graphical structure of the neural network. Existing regularization methods often focus on dropping/penalizing weights in a global manner that ignores the connectivity structures of the neural network. We propose to use the Fiedler value of the neural network's underlying graph as a tool for regularization. We provide theoretical support for this approach via Spectral Graph Theory. We demonstrate the convexity of this penalty and provide an approximate, variational approach for fast computation in practical training of neural networks. We provide bounds on such approximations. We provide an alternative but equivalent formulation of this framework in the form of a structurally weighted L1 penalty, thus linking our approach to sparsity induction. We trained neural networks on various datasets to compare Fiedler Regularization with traditional regularization methods such as Dropout and weight decay. Results demonstrate the efficacy of Fiedler Regularization.
 State Space Expectation Propagation: Efficient Inference Schemes for Temporal Gaussian Processes 
 We formulate expectation propagation (EP), a state-of-the-art method for approximate Bayesian inference, as a nonlinear Kalman smoother, showing that it generalises a wide class of classical smoothing algorithms. Specifically we show how power EP recovers the Extended and Unscented Kalman smoothers, with the distinction between the two being the choice of method for performing moment matching. EP provides some benefits over the traditional methods via introduction of the so-called cavity distribution, and by allowing fractional updates. We combine these benefits with the computational efficiency of Kalman smoothing, and provide extensive empirical analysis demonstrating the efficacy of various algorithms under this unifying framework. The resulting schemes enable inference in Gaussian process models in linear time complexity in the number of data, making them ideal for large temporal and spatio-temporal scenarios. Our results show that an extension of the Extended Kalman filter in which the linearisations are iteratively refined via EP-style updates is both efficient and performant, whilst its ease of implementation makes it a convenient plug-and-play approach to many non-conjugate regression and classification problems.
 Graph Optimal Transport for Cross-Domain Alignment 
 Cross-domain alignment between two sets of entities (e.g., objects in an image, words in a sentence) is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, where no training signals are provided to explicitly encourage alignment. Plus, the learned attention matrices are often dense and difficult to interpret. We propose Graph Optimal Transport (GOT), a principled framework that builds upon recent advances in Optimal Transport (OT). In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities as a dynamically-constructed graph. Two types of OT distances are considered: (i) Wasserstein distance (WD) for node (entity) matching; and (ii) Gromov-Wasserstein distance (GWD) for edge (structure) matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer.  
The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization. 
 Does label smoothing mitigate label noise? 
 Label smoothing is commonly used in training deep learning models, wherein one-hot training labels are mixed with uniform label vectors. Empirically, smoothing has been shown to improve both predictive performance and model calibration. In this paper, we study whether label smoothing is also effective as a means of coping with label noise. While label smoothing apparently amplifies this problem --- being equivalent to injecting symmetric noise to the labels --- we show how it relates to a general family of loss-correction techniques from the label noise literature. Building on this connection, we show that label smoothing can be competitive with loss-correction techniques under label noise. Further, we show that when performing distillation under label noise, label smoothing of the teacher can be beneficial; this is in contrast to recent findings for noise-free problems, and sheds further light on settings where label smoothing is beneficial.
 Balancing Competing Objectives with Noisy Data: Score-Based Classifiers for Welfare-Aware Machine Learning 
 While real-world decisions involve many competing objectives, algorithmic decisions are often evaluated with a single objective function. In this paper, we study algorithmic policies which explicitly trade off between a private objective (such as profit) and a public objective (such as social welfare). We analyze a natural class of policies which trace an empirical Pareto frontier based on learned scores, and focus on how such decisions can be made in noisy or data-limited regimes. Our theoretical results characterize the optimal strategies in this class, bound the Pareto errors due to inaccuracies in the scores, and show an equivalence between optimal strategies and a rich class of fairness-constrained profit-maximizing policies. We then present empirical results in two different contexts --- online content recommendation and sustainable abalone fisheries --- to underscore the generality of our approach to a wide range of practical decisions. Taken together, these results shed light on inherent trade-offs in using machine learning for decisions that impact social welfare.
 Revisiting Fundamentals of Experience Replay 
 Experience replay is central to off-policy algorithms in deep reinforcement learning (RL), but there remain significant gaps in our understanding. We therefore present a systematic and extensive analysis of experience replay in Q-learning methods, focusing on two fundamental properties: the replay capacity and the ratio of learning updates to experience collected (replay ratio). Our additive and ablative studies upend conventional wisdom around experience replay - greater capacity is found to substantially increase the performance of certain algorithms, while leaving others unaffected. Counter-intuitively we show that theoretically ungrounded, uncorrected n-step returns are uniquely beneficial while other techniques confer limited benefit for sifting through larger memory. Separately by directly controlling the replay ratio
we contextualize previous observations in the literature and empirically measure the importance across three deep RL algorithms. Finally, we conclude by testing a set of hypotheses on the nature of these performance benefits.
 Hierarchically Decoupled Morphological Transfer 
 Learning long-range behaviors on complex high-dimensional agents is a fundamental problem in robot learning. For such tasks, we argue that transferring learned information from a morphologically simpler agent can massively improve the sample efficiency of a more complex one. To this end, we propose a hierarchical decoupling of policies into two parts: an independently learned low-level policy and a transferable high-level policy. To remedy poor transfer performance due to mismatch in morphologies, we contribute two key ideas. First, we show that incentivizing a complex agent's low-level to imitate a simpler agent's low-level significantly improves zero-shot high-level transfer. Second, we show that KL-regularized training of the high level stabilizes learning and prevents mode-collapse. Finally, on a suite of navigation and manipulation environments, we demonstrate the applicability of hierarchical transfer on long-range tasks across morphologies.
 Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack 
 The evaluation of robustness against adversarial manipulation of neural networks-based classifiers is mainly tested with empirical attacks as methods for the exact computation, even when available, do not scale to large networks. We propose in this paper a new white-box adversarial attack wrt the $l_p$-norms for $p \in \{1,2,\infty\}$ aiming at finding the minimal perturbation necessary to change the class of a given input. It has an intuitive geometric meaning, yields quickly high quality results, minimizes the size of the perturbation (so that it returns the robust accuracy at every threshold with a single run). It performs better or similar to state-of-the-art attacks which are partially specialized to one $l_p$-norm, and is robust to the phenomenon of gradient obfuscation.
 Improved Bounds on Minimax Regret under Logarithmic Loss via Self-Concordance 
 We study the classical problem of forecasting under logarithmic loss while competing against an arbitrary class of experts. We present a novel approach to bounding the minimax regret that exploits the self-concordance property of logarithmic loss. Our regret bound depends on the metric entropy of the expert class and matches previous best known results for arbitrary expert classes. We improve the dependence on the time horizon for classes with metric entropy under the supremum norm of order $\Omega(\gamma^{-p})$ when $p>1$, which includes, for example, Lipschitz functions of dimension greater than 1. 
 AR-DAE: Towards Unbiased Neural Entropy Gradient Estimation 
 Entropy is ubiquitous in machine learning, but it is in general intractable to compute the entropy of the distribution of an arbitrary continuous random variable. In this paper, we propose the amortized residual denoising autoencoder (AR-DAE) to approximate the gradient of the log density function, which can be used to estimate the gradient of entropy. Amortization allows us to significantly reduce the error of the gradient approximator by approaching asymptotic optimality of a regular DAE, in which case the estimation is in theory unbiased. We conduct theoretical and experimental analyses on the approximation error of the proposed method, as well as extensive studies on heuristics to ensure its robustness. Finally, using the proposed gradient approximator to estimate the gradient of entropy, we demonstrate state-of-the-art performance on density estimation with variational autoencoders and continuous control with soft actor-critic.
 Robust Graph Representation Learning via Neural Sparsification 
 Graph representation learning serves as the core of important prediction tasks, ranging from product recommendation to fraud detection. Real-life graphs usually have complex information in the local neighborhood, where each node is described by a rich set of features and connects to dozens or even hundreds of neighbors. Despite the success of neighborhood aggregation in graph neural networks, task-irrelevant information is mixed into nodes' neighborhood, making learned models suffer from sub-optimal generalization performance. In this paper, we present NeuralSparse, a supervised graph sparsification technique that improves generalization power by learning to remove potentially task-irrelevant edges from input graphs. Our method takes both structural and non-structural information as input, utilizes deep neural networks to parameterize sparsification processes, and optimizes the parameters by feedback signals from downstream tasks. Under the NeuralSparse framework, supervised graph sparsification could seamlessly connect with existing graph neural networks for more robust performance. Experimental results on both benchmark and private datasets show that NeuralSparse can yield up to 7.2% improvement in testing accuracy when working with existing graph neural networks on node classification tasks.
 DeBayes: a Bayesian method for debiasing network embeddings 
 As machine learning algorithms are increasingly deployed for high-impact automated decision making, ethical and increasingly also legal standards demand that they treat all individuals fairly, without discrimination based on their age, gender, race or other sensitive traits. In recent years much progress has been made on ensuring fairness and reducing bias in standard machine learning settings. Yet, for network embedding, with applications in vulnerable domains ranging from social network analysis to recommender systems, current options remain limited both in number and performance. We thus propose DeBayes: a conceptually elegant Bayesian method that is capable of learning debiased embeddings by using a biased prior. Our experiments show that these representations can then be used to perform link prediction that is significantly more fair in terms of popular metrics such as demographic parity and equalized opportunity.
 Tuning-free Plug-and-Play Proximal Algorithm for Inverse Imaging Problems 
 Plug-and-play (PnP) is a non-convex framework that combines ADMM or other proximal algorithms  with advanced denoiser priors.  Recently, PnP has achieved great empirical  success, especially with the integration of deep learning-based denoisers. However, a key problem of PnP based approaches is that they require manual parameter tweaking. It is necessary to obtain high-quality results across the high discrepancy in terms of imaging conditions and varying scene content.
In this work, we present a tuning-free PnP proximal algorithm, which can automatically  determine the internal parameters including  the penalty parameter, the denoising strength  and the terminal time. A key part of our approach is to develop a policy network for automatic search of parameters, which can be effectively learned via mixed model-free and model-based deep reinforcement learning. We demonstrate, through numerical and visual experiments, that the learned policy can customize different parameters for different states, and often more efficient and effective than existing handcrafted criteria. Moreover, we discuss the practical considerations of the plugged denoisers, which together with our learned policy yield  state-of-the-art results. This is prevalent on both linear and nonlinear exemplary inverse imaging problems, and in particular, we show promising results on Compressed Sensing MRI and phase retrieval. 
 Neural Network Control Policy Verification With Persistent Adversarial Perturbation 
 Deep neural networks are known to be fragile to small adversarial perturbations. This issue becomes more critical when a neural network is interconnected with a physical system in a closed loop. In this paper, we show how to combine recent works on static neural network certification tools with robust control theory to certify a neural network policy in a control loop. We give a sufficient condition and an algorithm to ensure that the closed loop state and control constraints are satisfied when the persistent adversarial perturbation is linf norm bounded. Our method is based on finding a positively invariant set of the closed loop dynamical system, and thus we do not require the continuity of the neural network policy. Along with the verification result, we also develop an effective attack strategy for neural network control systems that outperforms exhaustive Monte-Carlo search significantly. We show that our certification algorithm works well on learned models and achieves 5 times better result than the traditional Lipschitz-based method to certify the robustness of a neural network policy on multiple control problems.
 Full Law Identification In Graphical Models Of Missing Data: Completeness Results 
 Missing data has the potential to affect analyses conducted in all fields of scientific study, including healthcare, economics, and the social sciences. Several approaches to unbiased inference in the presence of non-ignorable missingness rely on the specification of the target distribution and its missingness process as a probability distribution that factorizes with respect to a directed acyclic graph. In this paper, we address the longstanding question of the characterization of models that are identifiable within this class of missing data distributions. We provide the first completeness result in this field of study -- necessary and sufficient graphical conditions under which, the full data distribution can be recovered from the observed data distribution. We then simultaneously address issues that may arise due to the presence of both missing data and unmeasured confounding, by extending these graphical conditions and proofs of completeness, to settings where some variables are not just missing, but completely unobserved.
 Transformer Hawkes Process 
 Modern data acquisition routinely produce massive amounts of event sequence data in various domains, such as social media, healthcare, and financial markets. These data often exhibit complicated short-term and long-term temporal dependencies. However, most of the existing recurrent neural network-based point process models fail to capture such dependencies, and yield unreliable prediction performance. To address this issue, we propose a Transformer Hawkes Process (THP) model, which leverages the self-attention mechanism to capture long-term dependencies and meanwhile enjoys computational efficiency. Numerical experiments on various datasets show that THP outperforms existing models in terms of both likelihood and event prediction accuracy by a notable margin. Moreover, THP is quite general and can incorporate additional structural knowledge. We provide a concrete example, where THP achieves improved prediction performance for learning multiple point processes when incorporating their relational information.
 Generalization Error of Generalized Linear Models in High Dimensions 
 At the heart of machine learning lies the question of generalizability of learned rules over previously unseen data.  While over-parameterized models based on neural networks are now ubiquitous in machine learning applications, our understanding of their generalization capabilities is incomplete.  This task is made harder by the non-convexity of the underlying learning problems.  
We provide a general framework to characterize the asymptotic generalization error for single-layer neural networks (i.e., generalized linear models) with arbitrary non-linearities, making it applicable to regression as well as classification problems.  This framework enables analyzing the effect of (i) over-parameterization and non-linearity during modeling; and (ii) choices of loss function, initialization, and regularizer during learning.  Our model also captures mismatch between training and test distributions.  As examples, we analyze a few special cases, namely linear regression, and logistic regression.  We are also able to rigorously and analytically explain the \emph{double descent} phenomenon in generalized linear models.  
 Multi-Step Greedy Reinforcement Learning Algorithms 
 Multi-step greedy policies have been extensively used in model-based Reinforcement Learning (RL), both when a model of the environment is available (e.g.,~in the game of Go) and when it is learned. In this paper, we explore the benefits of multi-step greedy policies in model-free RL when employed using the multi-step Dynamic Programming algorithms: $\kappa$-Policy Iteration ($\kappa$-PI) and $\kappa$-Value Iteration ($\kappa$-VI). These methods iteratively compute the next policy ($\kappa$-PI) and value function ($\kappa$-VI) by solving a surrogate decision problem with a shaped reward and a smaller discount factor. We derive model-free RL algorithms based on $\kappa$-PI and $\kappa$-VI in which the surrogate decision problem is solved by DQN and TRPO. We call the resulting algorithms $\kappa$-PI-DQN, $\kappa$-VI-DQN, $\kappa$-PI-TRPO, and $\kappa$-VI-TRPO and evaluate them on Atari and MuJoCo benchmarks. Our results indicate that for the right range of $\kappa$, our algorithms outperform DQN and TRPO. Moreover, we identify the importance of a hyper-parameter that controls the extent to which the surrogate decision problem is solved, and show how to set this parameter. Finally, we establish that $\kappa$-PI-TRPO coincides with the popular GAE algorithm. 
 Gamification of Pure Exploration for Linear Bandits 
  We investigate an active \emph{pure-exploration} setting, that includes \emph{best-arm identification}, in the context of \emph{linear stochastic bandits}.  While asymptotically optimal algorithms exist for standard \emph{multi-armed bandits}, the existence of such algorithms for the best-arm identification in linear bandits has been elusive despite several attempts to address it.  First, we provide a thorough comparison and new insight over different notions of optimality in the linear case, including G-optimality, transductive optimality from optimal experimental design and asymptotic optimality.  Second, we design the first asymptotically optimal algorithm for fixed-confidence pure exploration in linear bandits. As a consequence, our algorithm naturally bypasses the pitfall caused by a simple but difficult instance, that most prior algorithms had to be engineered to deal with explicitly.  Finally, we avoid the need to fully solve an optimal design problem by providing an approach that entails an efficient implementation. 
 Robust Outlier Arm Identification 
 We study the problem of Robust Outlier Arm Identification (ROAI), where the goal is to identify arms whose expected rewards deviate substantially from the majority, by adaptively sampling from their reward distributions. We compute the outlier threshold using the median and median absolute deviation of the expected rewards. This is a robust choice for the threshold compared to using the mean and standard deviation, since it can correctly identify all outlier arms even in the presence of extreme outlier values. Our setting is different from existing pure exploration problems where the threshold is pre-specified as a given value or rank. This is useful in applications where the goal is to identify the set of promising items but the cardinality of this set is unknown, such as finding promising drugs for a new disease or identifying items favored by a population. We propose two computationally efficient delta-PAC algorithms for ROAI, which includes the first UCB-style algorithm for outlier detection, and derive upper bounds on their sample complexity. We also prove a matching, up to logarithmic factors, worst case lower bound for the problem, indicating that our upper bounds are generally unimprovable. Experimental results show that our algorithms are both robust and at least 5x sample efficient compared to state-of-the-art.
 Growing Adaptive Multi-hyperplane Machines 
 Adaptive Multi-hyperplane Machine (AMM) is an online algorithm for learning Multi-hyperplane Machine (MM), a classification model which allows multiple hyperplanes per class. AMM is based on Stochastic Gradient Descent (SGD), with training time comparable to linear Support Vector Machine (SVM) and significantly higher accuracy. On the other hand, empirical results indicate there is a large accuracy gap between AMM and non-linear SVMs. In this paper we show that this performance gap is not due to limited representability of MM model, as it can represent arbitrary concepts. We set to explain a connection between AMM and LVQ, and introduce a novel Growing AMM (GAMM) algorithm motivated by Growing LVQ, that imputes duplicate hyperplanes into MM model during SGD training. We provide theoretical results showing that GAMM has favorable convergence properties, and analyze the generalization bound of MM models. Experiments indicate that GAMM achieves significantly improved accuracy on non-linear problems with only slightly slower training compared to AMM. On some tasks GAMM is even more accurate than non-linear SVM and other popular classifiers such as Neural Networks and Random Forests.
 GANs May Have No Nash Equilibria 
 Generative adversarial networks (GANs) represent a zero-sum game between two machine players, a generator and a discriminator, designed to learn the distribution of data. While GANs have achieved state-of-the-art performance in several benchmark learning tasks, GAN minimax optimization still poses great theoretical and empirical challenges. GANs trained using first-order optimization methods commonly fail to converge to a stable solution where the players cannot improve their objective, i.e., the Nash equilibrium of the underlying game. Such issues raise the question of the existence of Nash equilibria in the GAN zero-sum game. In this work, we show through several theoretical and numerical results that indeed GAN zero-sum games may not have any Nash equilibria. To characterize an equilibrium notion applicable to GANs, we consider the equilibrium of a new zero-sum game with an objective function given by a proximal operator applied to the original objective, a solution we call the proximal equilibrium. Unlike the Nash equilibrium, the proximal equilibrium captures the sequential nature of GANs, in which the generator moves first followed by the discriminator. We prove that the optimal generative model in Wasserstein GAN problems provides a proximal equilibrium. Inspired by these results, we propose a new approach, which we call proximal training, for solving GAN problems. We discuss several numerical experiments demonstrating the existence of proximal equilibria in GAN problems.
 Provable Self-Play Algorithms for Competitive Reinforcement Learning 
   Self-play, where the algorithm learns by playing against itself without requiring any direct supervision, has become the new weapon in modern Reinforcement Learning (RL) for achieving superhuman performance in practice. However, the majority of exisiting theory in reinforcement learning only applies to the setting where the agent plays against a fixed environment. It remains largely open whether self-play algorithms can be provably effective, especially when it is necessary to manage the exploration/exploitation tradeoff.

  We study self-play in competitive reinforcement learning under the setting of Markov games, a generalization of Markov decision processes to the two-player case. We introduce a self-play algorithm---Value Iteration with Upper/Lower Confidence Bound (VI-ULCB), and show that it achieves regret $\tilde{O}(\sqrt{T})$ after playing $T$ steps of the game. The regret is measured by the agent's performance against a \emph{fully adversarial} opponent who can exploit the agent's strategy at \emph{any} step. We also introduce an explore-then-exploit style algorithm, which achieves a slightly worse regret of $\tilde{O}(T^{2/3})$, but is guaranteed to run in polynomial time even in the worst case. To the best of our knowledge, our work presents the first line of provably sample-efficient self-play algorithms for competitive reinforcement learning.
 Linear Lower Bounds and Conditioning of Differentiable Games 
 Recent successes of game-theoretic formulations in ML have caused a resurgence of research interest in differentiable games. Overwhelmingly, that research focuses on methods and upper bounds. In this work, we approach the question of fundamental iteration complexity by providing lower bounds to complement the linear (i.e. geometric) upper bounds observed in the literature on a wide class of problems. We cast saddle-point and min-max problems as 2-player games. We leverage tools from single-objective convex optimisation to propose new linear lower bounds for convex-concave games. Notably, we give a linear lower bound for n-player differentiable games, by using the spectral properties of the update operator. We then propose a new definition of the condition number arising from our lower bound analysis. Unlike past definitions, our condition number captures the fact that linear rates are possible in games, even in the absence of strong convex-concavity.
 Detecting Out-of-Distribution Examples with Gram Matrices 
 When presented with Out-of-Distribution (OOD) examples, deep neural networks yield confident, incorrect predictions. Detecting OOD examples is challenging, and the potential risks are high. In this paper, we propose to detect OOD examples by identifying inconsistencies between activity patterns and predicted class. We find that characterizing activity patterns by Gram matrices and identifying anomalies in Gram matrix values can yield high OOD detection rates. We identify anomalies in the Gram matrices by simply comparing each value with its respective range observed over the training data. Unlike many approaches, this can be used with any pre-trained softmax classifier and neither requires access to OOD data for fine-tuning hyperparameters, nor does it require OOD access for inferring parameters. We empirically demonstrate applicability across a variety of architectures and vision datasets and, for the important and surprisingly hard task of detecting far out-of-distribution examples, it generally performs better than or equal to state-of-the-art OOD detection methods (including those that do assume access to OOD examples).
 Low Bias Low Variance Gradient Estimates for Hierarchical Boolean Stochastic Networks 
 Stochastic neural networks with discrete random variables are an important class of models for their expressiveness and interpretability. Since direct differentiation and backpropagation is not possible, Monte Carlo gradient estimation techniques are a popular alternative. Efficient stochastic gradient estimators, such Straight-Through and Gumbel-Softmax, work well for shallow stochastic models. Their performance, however, suffers with hierarchical, more complex models. We focus on hierarchical stochastic networks with multiple layers of Boolean latent variables. To analyze such networks, we introduce the framework of harmonic analysis for Boolean functions to derive an analytic formulation for the bias and variance in the Straight-Through estimator. Exploiting these formulations, we propose \emph{FouST}, a low-bias and low-variance gradient estimation algorithm that is just as efficient. Extensive experiments show that FouST performs favorably compared to state-of-the-art biased estimators and is much faster than unbiased ones.
 Optimizer Benchmarking Needs to Account for Hyperparameter Tuning 
 The performance of optimizers, particularly in deep learning, depends considerably on their chosen hyperparameter configuration. The efficacy of optimizers is often studied under near-optimal problem-specific hyperparameters, and finding these settings may be prohibitively costly for practitioners. In this work, we argue that a fair assessment of optimizers' performance must take the computational cost of hyperparameter tuning into account, i.e., how easy it is to find good hyperparameter configurations using an automatic hyperparameter search. Evaluating a variety of optimizers on an extensive set of standard datasets and architectures, our results indicate that Adam is the most practical solution, particularly in low-budget scenarios.
 When are Non-Parametric Methods Robust? 
 A growing body of research has shown that many classifiers are susceptible to adversarial examples -- small strategic modifications to test inputs that lead to misclassification. In this work, we study general non-parametric methods, with a view towards understanding when they are robust to these modifications. We establish general conditions under which non-parametric methods are r-consistent -- in the sense that they converge to optimally robust and accurate classifiers in the large sample limit. 

Concretely, our results show that when data is well-separated, nearest neighbors and kernel classifiers are r-consistent, while histograms are not. For general data distributions, we prove that preprocessing by Adversarial Pruning (Yang et. al., 2019)-- that makes data well-separated -- followed by nearest neighbors or kernel classifiers also leads to r-consistency. 
 Boosting Frank-Wolfe by Chasing Gradients 
 The Frank-Wolfe algorithm has become a popular first-order optimization algorithm for it is simple and projection-free, and it has been successfully applied to a variety of real-world problems. Its main drawback however lies in its convergence rate, which can be excessively slow due to naive descent directions. We propose to speed up the Frank-Wolfe algorithm by better aligning the descent direction with that of the negative gradient via a subroutine. This subroutine chases the negative gradient direction in a matching pursuit-style while still preserving the projection-free property. Although the approach is reasonably natural, it produces very significant results. We derive convergence rates $\mathcal{O}(1/t)$ to $\mathcal{O}(e^{-\omega t})$ of our method and we demonstrate its competitive advantage both per iteration and in CPU time over the state-of-the-art in a series of computational experiments.
 Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks 
 Optimizing convolutional neural networks for fast inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network.
Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions.

In this paper, we present an in-depth analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains.
To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss.
Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with little or no retraining cost.
 Progressive Graph Learning for Open-Set Domain Adaptation 
 Domain shift is a fundamental problem in visual recognition which typically arises when the source and target data follow different distributions. The existing domain adaptation approaches which tackle this problem work in the "closed-set" setting with the assumption that the source and the target data share exactly the same classes of objects. In this paper, we tackle a more realistic problem of the "open-set" domain shift where the target data contains additional classes that were not present in the source data. More specifically, we introduce an end-to-end Progressive Graph Learning (PGL) framework where a graph neural network with episodic training is integrated to suppress underlying conditional shift and adversarial learning is adopted to close the gap between the source and target distributions. Compared to the existing open-set adaptation approaches, our approach guarantees to achieve a tighter upper bound of the target error. Extensive experiments on three standard open-set benchmarks evidence that our approach significantly outperforms the state-of-the-arts in open-set domain adaptation.
 Certified Robustness to Label-Flipping Attacks via Randomized Smoothing 
 Machine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. In this work, we propose a strategy for building linear classifiers that are certifiably robust against a strong variant of label flipping, where each test example is targeted independently. In other words, for each test point, our classifier includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Our approach leverages randomized smoothing, a technique that has previously been used to guarantee---with high probability---test-time robustness to adversarial manipulation of the input to a classifier. We derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with minimal additional runtime complexity over standard classification and no assumptions on the train or test distributions. We generalize our results to the multi-class case, providing the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.
 Temporal Phenotyping using Deep Predictive Clustering of Disease Progression 
 Due to the wider availability of modern electronic health records, patient care data is often being stored in the form of time-series. Clustering such time-series data is crucial for patient phenotyping, anticipating patients’ prognoses by identifying “similar” patients, and designing treatment guidelines that are tailored to homogeneous patient subgroups. In this paper, we develop a deep learning approach for clustering time-series data, where each cluster comprises patients who share similar future outcomes of interest (e.g., adverse events, the onset of comorbidities). To encourage each cluster to have homogeneous future outcomes, the clustering is carried out by learning discrete representations that best describe the future outcome distribution based on novel loss functions. Experiments on two real-world datasets show that our model achieves superior clustering performance over state-of-the-art benchmarks and identifies meaningful clusters that can be translated into actionable information for clinical decision-making.
 Statistical Bias in Dataset Replication 
 Dataset replication is a useful tool for assessing whether models have overfit to a specific validation set or the exact circumstances under which it was generated. In this paper, we highlight the importance of statistical modeling in dataset replication: we present unintuitive yet pervasive ways in which statistical bias, when left unmitigated, can skew results.  Specifically, we examine ImageNet-v2, a replication of the ImageNet dataset that induces a significant drop in model accuracy, presumed to be caused by a benign distribution shift between the datasets. We show, however, that by identifying and accounting for the aforementioned bias, we can explain the vast majority of this accuracy drop. We conclude with concrete recommendations for recognizing and avoiding bias in dataset replication.
 dS^2LBI: Exploring Structural Sparsity on Deep Network via Differential Inclusion Paths 
 Over-parameterization is ubiquitous nowadays in training neural networks to benefit both optimization in seeking global optima and generalization in reducing prediction error. However, compressive networks are desired in many real world applications
and direct training of small networks may be trapped in local optima. In this paper, instead of pruning or distilling over-parameterized models to compressive ones, we propose a new approach based on differential inclusions of inverse scale spaces. Specifically, it generates a family of models from simple to complex ones that couples a pair of parameters to simultaneously train over-parameterized deep models and structural sparsity of which on weights of fully connected (fc) and convolutional layers. Such a differential inclusion scheme has a simple discretization, proposed as deep Structural Splitting Linearized Bregman Iteration (dS^2LBI), whose global convergence analysis in deep learning is established that from any initializations, algorithmic iterations converge to a critical point of empirical risks. Experimental evidence shows that gS^2LBI achieve comparable and even better performance than the competitive optimizers in exploring the structural sparsity of several widely used backbones on the benchmark datasets. Remarkably, with early stopping, gS2LBI unveils “winning tickets”, i.e., the effective sparse structure with comparable test accuracy to over-parameterized models after retraining.
 WaveFlow: A Compact Flow-based Model for Raw Audio 
 In this work, we propose WaveFlow, a small-footprint generative flow for raw audio, which is directly trained with maximum likelihood. It handles the long-range structure of 1-D waveform with a dilated 2-D convolutional architecture, while modeling the local variations using expressive autoregressive functions. WaveFlow provides a unified view of likelihood-based models for 1-D data, including WaveNet and WaveGlow as special cases. It generates high-fidelity speech as WaveNet, while synthesizing several orders of magnitude faster as it only requires a few sequential steps to generate very long waveforms with hundreds of thousands of time-steps. Furthermore, it can significantly reduce the likelihood gap that has existed between autoregressive models and flow-based models for efficient synthesis. Finally, our small-footprint WaveFlow has only 5.91M parameters, which is 15× smaller than WaveGlow. It can generate 22.05 kHz high-fidelity audio 42.6× faster than real-time (at a rate of 939.3 kHz) on a V100 GPU without engineered inference kernels.
 Stochastic Hamiltonian Gradient Methods for Smooth Games 
 The analysis of smooth games has attracted attention, motivated by the success of adversarial formulations. The Hamiltonian method is a lightweight second-order approach that recasts the problem in terms of a minimization objective. Consensus optimization can be seen as a generalization: it mixes a Hamiltonian term with the original game dynamics. This family of Hamiltonian methods has shown promise in literature. However, they come with no guarantees for stochastic games. Classic stochastic extragradient and mirror-prox methods require averaging over a compact domain to achieve convergence. Recent variance-reduced first-order schemes focus on unbounded domains, but stop short of proving last-iterate convergence for bilinear matrix games. We analyze the stochastic Hamiltonian method and a novel variance-reduced variant of it and provide the first set of last-iterate convergence guarantees for stochastic unbounded bilinear games. More generally, we provide convergence guarantees for a family of stochastic games, notably including some non-convex ones. We supplement our analysis with experiments on a stochastic bilinear game, where our theory is shown to be tight, and simple adversarial machine learning formulations.
 Learning Calibratable Policies using Programmatic Style-Consistency 
 We study the important and challenging problem of controllable generation of long-term sequential behaviors.  Solutions to this problem would impact many applications, such as calibrating behaviors of AI agents in games or predicting player trajectories in sports. In contrast to the well-studied areas of controllable generation of images, text,and speech, there are significant challenges that are unique to or exacerbated by generating long-term behaviors: how should we specify the factors of variation to control, and how can we ensure that the generated temporal behavior faithfully demonstrates diverse styles?  In this paper,  we leverage large amounts of raw behavioral data to learn policies that can be calibrated to generate a diverse range of behavior styles (e.g., aggressive versus passive play in sports). Inspired by recent work on leveraging programmatic labeling functions, we present a novel framework that combines imitation learning with data programming to learn style-calibratable policies. Our primary technical contribution is a formal notion of style-consistency as a learning objective, and its integration with conventional imitation learning approaches.  We evaluate our framework using demonstrations from professional basketball players and agents in the MuJoCo physics environment, and show that our learned policies can be accurately calibrated to generate interesting behavior styles in both domains.
 Concentration bounds for CVaR estimation: The cases of light-tailed and heavy-tailed distributions 
 Conditional Value-at-Risk (CVaR) is a widely used risk metric in applications such as finance. We derive concentration bounds for CVaR estimates, considering separately the cases of sub-Gaussian, light-tailed and heavy-tailed distributions. For the sub-Gaussian and light-tailed cases, we use a classical CVaR estimator based on the empirical distribution constructed from the samples. For heavy-tailed random variables, we assume a mild `bounded moment' condition, and derive a concentration bound for a truncation-based estimator. Our concentration bounds exhibit exponential decay in the sample size, and are tighter than those available in the literature for the above distribution classes. To demonstrate the applicability of our concentration results, we consider the CVaR optimization problem in a multi-armed bandit setting. Specifically, we address (i) the best CVaR-arm identification problem under a fixed budget; and (ii) CVaR-based regret minimization. Using our CVaR concentration bounds, we derive an upper-bound on the probability of incorrect identification for (i), and a regret guarantee for (ii).
 Optimal Sequential Maximization: One Interview is Enough! 
 Maximum selection under probabilistic queries
\emph{(probabilistic maximization)} is a fundamental algorithmic problem
arising in numerous theoretical and practical contexts. 
We derive the first query-optimal sequential algorithm for
probabilistic-maximization.
Departing from previous assumptions,
the algorithm and performance guarantees
apply even for infinitely many items, hence in particular do
not require a-priori knowledge of the number of items.
The algorithm has linear query complexity,
and is optimal also in the streaming setting.

To derive these results we consider a probabilistic setting where several candidates
for a position are asked multiple questions with the goal of
finding who has the highest probability of answering interview
questions correctly. Previous work minimized the total number
of questions asked by alternating back and forth between the
best performing candidates,
in a sense, inviting them to multiple interviews.  We
show that the same order-wise selection accuracy can be achieved by
querying the candidates sequentially, never returning to a previously
queried candidate. Hence one interview is enough!
 Discriminative Adversarial Search for Abstractive Summarization 
 We introduce a novel approach for sequence decoding, Discriminative Adversarial Search (DAS), which has the desirable properties of alleviating the effects of exposure bias without requiring external metrics. Inspired by Generative Adversarial Networks (GANs), wherein a discriminator is used to improve the generator, our method differs from GANs in that the generator parameters are not updated at training time and the discriminator is used to drive sequence generation at inference time. 

We investigate the effectiveness of the proposed approach on the task of Abstractive Summarization: the results obtained show that a naive application of DAS improves over the state-of-the-art methods, with further gains obtained via discriminator retraining. Moreover, we show how DAS can be effective for cross-domain adaptation. Finally, all results reported are obtained without additional rule-based filtering strategies, commonly used by the best performing systems available: this indicates that DAS can effectively be deployed without relying on post-hoc modifications of the generated outputs.
 Operation-Aware Soft Channel Pruning using Differentiable Masks 
 We propose a simple but effective data-driven channel pruning algorithm, which compresses deep neural networks effectively by exploiting the characteristics of operations in a differentiable way. The proposed approach makes a joint consideration of batch normalization (BN) and rectified linear unit (ReLU) for channel pruning; it estimates how likely each feature map is to be deactivated by the two successive operations and prunes the channels that have high probabilities. To this end, we learn differentiable masks for individual channels and make soft decisions throughout the optimization procedure, which allows to explore larger search space and train more stable networks. The proposed formulation combined with the training framework enables us to identify compressed models even without a separate procedure of fine-tuning. We perform extensive experiments and achieve outstanding performance in terms of the accuracy of output networks given the same amount of resources when compared with the state-of-the-art methods.
 Optimal transport mapping via input convex neural networks 
 In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework to estimate the optimal transport mapping as the gradient of a convex function that is trained via minimax optimization. Numerical experiments confirm the accuracy of the learned transport map. 

Our approach can be readily used to train a deep generative model. When trained between a simple distribution in the latent space and a target distribution, the learned optimal transport map acts as a deep generative model. Although scaling this to a large dataset is challenging, we demonstrate two important strengths over standard adversarial training: robustness and discontinuity. As we seek the optimal transport, the learned generative model provides the same mapping regardless of how we initialize the neural networks. Further, a gradient of a neural network can easily represent discontinuous mappings, unlike standard neural networks that are constrained to be continuous. This allows the learned transport map to match any target distribution with many discontinuous supports and achieve sharp boundaries. 
 Multigrid Neural Memory 
 We introduce a radical new approach to endowing neural networks with access to long-term and large-scale memory.  Architecting networks with internal multigrid structure and connectivity, while distributing memory cells alongside computation throughout this topology, we observe that coherent memory subsystems emerge as a result of training.  Our design both drastically differs from and is far simpler than prior efforts, such as the recently proposed Differentiable Neural Computer (DNC), which uses intricately crafted controllers to connect neural networks to external memory banks.  Our hierarchical spatial organization, parameterized convolutionally, permits efficient instantiation of large-capacity memories.  Our multigrid topology provides short internal routing pathways, allowing convolutional networks to efficiently approximate the behavior of fully connected networks.  Such networks have an implicit capacity for internal attention; augmented with memory, they learn to read and write specific memory locations in a dynamic data-dependent manner.  We demonstrate these capabilities on synthetic exploration and mapping tasks, where our network is able to self-organize and retain long-term memory for trajectories of thousands of time steps, outperforming the DNC. On tasks without any notion of spatial geometry: sorting, associative recall, question answering, our design functions as a truly generic memory and yields excellent results.
 Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation 
 This paper studies the statistical theory of batch data reinforcement learning with function approximation. Consider the off-policy evaluation problem, which is to estimate the cumulative value of a new target policy from logged history generated by unknown behavior policies. We study a regression-based fitted Q iteration method, and show that it is equivalent to a model-based method that estimates a conditional mean embedding of the transition operator. We prove that this method is information-theoretically optimal and has nearly minimal estimation error. In particular, by leveraging contraction property of Markov processes and martingale concentration, we establish a finite-sample instance-dependent error upper bound and a nearly-matching minimax lower bound. The policy evaluation error depends sharply on a restricted chi-square divergence over the function class between the long-term distribution of target policy and the distribution of past data. This restricted chi-square divergence is both instance-dependent and function-class-dependent. It characterizes the statistical limit of off-policy evaluation. Further, we provide an easily computable confidence bound for the policy evaluator, which may be useful for optimistic planning and safe policy improvement.
 More Information Supervised Probabilistic Deep Face Embedding Learning 
 Researches using margin based comparison loss demonstrate the effectiveness of penalizing the distance between face feature and their corresponding class centers.
Despite their popularity and excellent performance, they do not explicitly encourage the generic embedding learning for an open set recognition problem.
In this paper, we analyse margin based softmax loss in probability view.
With this perspective, we propose two general principles: 1) monotonically decreasing and 2) margin probability penalty, for designing new margin loss functions.
Unlike methods optimized with single comparison metric, we provide a new perspective to treat open set face recognition as a problem of information transmission. 
And the generalization capability for face embedding is gained with more clean information.
An auto-encoder architecture called Linear-Auto-TS-Encoder(LATSE) is proposed to corroborate this finding.
Extensive experiments on several benchmarks demonstrate that LATSE help face embedding to gain more generalization capability and it boost the single model performance with open training dataset to more than 99% on MegaFace test.
 Towards non-parametric drift detection via Dynamic Adapting Window Independence Drift Detection (DAWIDD) 
 The notion of concept drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time; as a consequence machine learning models may become inaccurate and need adjustment. Many online learning schemes include drift detection to actively detect and react to observed changes. Yet, reliable drift detection constitutes a challenging problem in particular in the context of high dimensional data, varying drift characteristics, and the absence of a parametric model such as a classification scheme which reflects the drift. In this paper we present a novel concept drift detection method, Dynamic Adapting Window Independence Drift Detection (DAWIDD), which aims for non-parametric drift detection of diverse drift characteristics. For this purpose, we establish a mathematical equivalence of the presence of drift to the dependency of specific random variables in an according drift process. This allows us to rely on independence tests rather than parametric models or the classification loss, resulting in a fairly robust scheme to universally detect different types of drift, as is also confirmed in experiments. 
 Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks Using PAC-Bayesian Analysis 
 The notion of flat minima has gained attention as a key metric of the generalization ability of deep learning models. However, current definitions of flatness are known to be sensitive to parameter rescaling. While some previous studies have proposed to rescale flatness metrics using parameter scales to avoid the scale dependence, the normalized metrics lose the direct theoretical connections between flat minima and generalization. In this paper, we first provide generalization error bounds using existing normalized flatness measures. Using the analysis, we then propose a novel normalized flatness metric. The proposed metric enjoys both direct theoretical connections and better empirical correlation to generalization error.
 Influenza Forecasting Framework based on Gaussian Processes 
 The seasonal epidemic of influenza costs thousands
of lives each year in the US. While influenza
epidemics occur every year, timing and size of the
epidemic vary strongly from season to season.
This complicates the public health efforts to adequately
respond to such epidemics. Forecasting
techniques to predict the development of seasonal
epidemics such as influenza, are of great help to
public health decision making. Therefore, the
US Center for Disease Control and Prevention
(CDC) has initiated a yearly challenge to forecast
influenza-like illness. Here, we propose a
new framework based on Gaussian process (GP)
for seasonal epidemics forecasting and demonstrate
its capability on the CDC reference data
on influenza like illness: our framework leads to
accurate forecasts with small but reliable uncertainty
estimation. We compare our framework
to several state of the art benchmarks and show
competitive performance. We, therefore, believe
that our GP based framework for seasonal epidemics
forecasting will play a key role for future
influenza forecasting and, lead to further research
in the area.
 Adaptive Reward-Poisoning Attacks against Reinforcement Learning 
 In reward-poisoning attacks against reinforcement learning (RL), an attacker can perturb the environment reward $r_t$ into $r_t+\delta_t$ at each step, with the goal of forcing the RL agent to learn a nefarious policy. 
We categorize such attacks by the infinity-norm constraint on $\delta_t$: We provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe; we provide a corresponding upper threshold above which the attack is feasible. 
Feasible attacks can be further categorized as non-adaptive where $\delta_t$ depends only on $(s_t,a_t, s_{t+1})$, or adaptive where $\delta_t$ depends further on the RL agent's learning process at time $t$. Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size $|S|$, whereas non-adaptive attacks require exponential steps.
We provide a constructive proof that a Fast Adaptive Attack strategy achieves the polynomial rate. Finally, we show that empirically an attacker can find effective reward-poisoning attacks using state-of-the-art deep RL techniques.
 Fractional Underdamped Langevin Dynamics: Retargeting SGD with Momentum under Heavy-Tailed Gradient Noise 
 Stochastic gradient descent with momentum (SGDm) is one of the most popular optimization algorithms in deep learning. While there is a rich theory of SGDm for convex problems, the theory is considerably less developed in the context of deep learning where the  problem is non-convex and the gradient noise might exhibit a heavy-tailed behavior, as empirically observed in recent studies. In this study, we consider a \emph{continuous-time} variant of SGDm, known as the underdamped Langevin dynamics (ULD), and investigate its asymptotic properties under heavy-tailed perturbations. Supported by recent studies from statistical physics, we argue both theoretically and empirically that the heavy-tails of such perturbations can result in a bias even when the step-size is small, in the sense that \emph{the optima of stationary distribution} of the dynamics might not match \emph{the optima of the cost function to be optimized}. As a remedy, we develop a novel framework, which we coin as \emph{fractional} ULD (FULD), and prove that FULD targets the so-called Gibbs distribution, whose optima exactly match the optima of the original cost. We observe that the Euler discretizatin of FULD has noteworthy algorithmic similarities with \emph{natural gradient} methods and \emph{gradient clipping}, bringing a new perspective on understanding their role in deep learning. We support our theory with experiments conducted on a synthetic model and neural networks.
 SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates 
 Uncertainty quantification is a fundamental yet unsolved problem for deep
learning. The Bayesian framework provides a principled way of uncertainty
estimation but is often not scalable to modern deep neural nets (DNNs) that
have a large number of parameters. Non-Bayesian methods are simple to implement
but often conflate different sources of uncertainties and require huge
computing resources.  We propose a new method for quantifying uncertainties of
DNNs from a dynamical system perspective.  The core of our method is to view
DNN transformations as state evolution of a stochastic dynamical system and
introduce a Brownian motion term for capturing epistemic uncertainty. Based on this
perspective, we propose a neural stochastic differential equation model
(SDE-Net) which consists of (1) a drift net that controls the system to fit the
predictive function; and (2) a diffusion net that captures epistemic uncertainty.
We theoretically analyze the existence and uniqueness of the solution to
SDE-Net.  Our experiments demonstrate that the SDE-Net model can outperform
existing uncertainty estimation methods across a series of tasks where
uncertainty plays a fundamental role.
 More Data Can Expand The Generalization Gap Between Adversarially Robust and Standard Models 
 Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under $\ell_\infty$ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.
 Neuro-Symbolic Visual Reasoning: Disentangling "Visual" from "Reasoning" 
 Visual reasoning tasks such as visual question answering (VQA) require an interplay of visual perception with reasoning about the question semantics grounded in perception. Challenges like VCR (Zellers et al., 2019) and GQA (Hudson& Manning, 2019) facilitate scientific progress from perception models to visual reasoning. However, recent advances on GQA are still primarily driven by perception improvements (e.g.  scene graph generation) rather than reasoning. Neuro-symbolic models such as MAC (Hudson& Manning, 2018) bring the benefits of compositional reasoning to VQA, but they are still entangled with visual representation learning, and thus neural reasoning is hard to improve and assess on its own.

To address this, we propose (1) a framework to isolate and evaluate the reasoning aspect of VQA separately from its perception, and (2) a novel top-down calibration technique that allows the model to answer reasoning questions even with imperfect perception. To this end, we introduce a differentiable first-order logic formalism for VQA that explicitly decouples question answering from visual perception. On the challenging GQA dataset, this approach is competitive with non-symbolic neural models while also interpretable by construction, composable with arbitrary pre-trained visual representation learning, and requires much fewer parameters.
 Revisiting Spatial Invariance with Low-Rank Local Connectivity 
 Convolutional neural networks are among the most successful architectures in deep learning. This success is at least partially attributable to the efficacy of spatial invariance as an inductive bias. Locally connected layers, which differ from convolutional layers only in their lack of spatial invariance, usually perform poorly in practice. However, these observations still leave open the possibility that some degree of relaxation of spatial invariance may yield a better inductive bias than either convolution or local connectivity. To test this hypothesis, we design a method to relax the spatial invariance of a network layer in a controlled manner. In particular, we create a \textit{low-rank} locally connected layer, where the kernel applied at each position is constructed as a linear combination of basis kernels with spatially varying combining weights. By varying the number of basis kernels, we can control the degree of relaxation of spatial invariance. In our experiments, we find that relaxing spatial invariance improves classification accuracy over both convolution and locally connected layers across MNIST, CIFAR-10, and CelebA datasets. These results suggest that spatial invariance may be an overly restrictive prior.
 Neural Contextual Bandits with UCB-based Exploration 
 We study the stochastic contextual bandit problem, where the reward is generated from an unknown bounded function with additive noise. We propose the NeuralUCB algorithm, which leverages the representation power of deep neural networks and uses a neural network-based random feature mapping to construct an upper confidence bound (UCB) of reward for efficient exploration. We prove that, under mild assumptions, NeuralUCB achieves $\tilde O(\sqrt{T})$ regret, 
where $T$ is the number of rounds. To the best of our knowledge, our algorithm is the first neural network-based contextual bandit algorithm with near-optimal regret guarantee. We also show the algorithm is empirically competitive against representative baselines in a number of benchmarks.
 Sparse Sinkhorn Attention 
 We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.
 Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data 
 Deep semi-supervised learning (SSL) has been shown very effectively. However, its performance is seriously hurt when the class distribution is mismatched, among which a common phenomenon is that unlabeled data contains the classes not seen in labeled data. Efforts on this aspect remain to be limited. This paper proposes a simple and effective safe deep SSL method to alleviate the performance harm caused by it. In theory, the result learned from the new method is never worse than learning from merely labeled data, and it is theoretically guaranteed that its generalization approaches the optimal in the order $O(\sqrt{d\ln(n)/n})$, even faster than the convergence rate in supervised learning associated with massive parameters. In the experiment of benchmark data, unlike the existing deep SSL methods which are no longer as good as supervised learning in 40\% of unseen-class unlabeled data, the new method can still achieve performance gain in more than 60\% of unseen-class unlabeled data. The proposal is suitable for any deep SSL algorithm and can be easily extended to handle other cases of class distribution mismatch.
 A Geometric Approach to Archetypal Analysis via Sparse Projections 
 Archetypal analysis (AA) aims to extract patterns using self-expressive decomposition of data as convex combinations of extremal points (on the convex hull) of the data. This work presents a computationally efficient greedy AA (GAA) algorithm. GAA leverages the underlying geometry and sparseness property of AA, is scalable to larger datasets, and has significantly faster convergence to existing methods. To achieve this, archetypes are learned via sparse projection of data in linearly transformed space. GAA employs an iterative subset selection approach to identify archetypes based on the sparsity of convex representations. The work further presents the use of GAA algorithm for extended AA models such as robust and kernel AA. Experimental results show that GAA is significantly faster while performing comparable to existing methods for tasks such as classification, data visualization/categorization.
 Model-Based Reinforcement Learning with Value-Targeted Regression 
 Reinforcement learning (RL) applies to control problems with large state and action spaces, hence it is natural to consider RL with a parametric model. In this paper we focus on finite-horizon episodic RL where the transition model admits a nonlinear parametrization $P_{\theta}$, a special case of which is the linear parameterization: $P_{\theta} = \sum_{i=1}^{d} (\theta)_{i}P_{i}$. 
We propose an upper confidence model-based RL algorithm with value-targeted model parameter estimation. The algorithm updates the estimate of $\theta$ by solving a nonlinear regression problem using the latest value estimate as the target. We demonstrate the efficiency of our algorithm by proving its expected regret bound which, in the special case of linear parameterization takes the form $\tilde{\mathcal{O}}(d\sqrt{H^{3}T})$, where $H, T, d$ are the horizon, total number of steps and dimension of $\theta$. This regret bound is independent of the total number of states or actions, and is close to a lower bound $\Omega(\sqrt{HdT})$. In the general nonlinear case, we handle the regret analysis by using the concept of Eluder dimension proposed by \citet{RuVR14}.
 Dissecting Non-Vacuous Generalization Bounds based on the Mean-Field Approximation 
 Explaining how overparametrized neural networks simultaneously achieve low risk and zero empirical risk on benchmark datasets is an open problem. PAC-Bayes bounds optimized using variational inference (VI) have been recently proposed as a promising direction in obtaining non-vacuous bounds. We show empirically that this approach gives negligible gains when modelling the posterior as a Gaussian with diagonal covariance---known as the mean-field approximation. We investigate common explanations, such as the failure of VI due to problems in optimization or choosing a suboptimal prior. Our results suggest that investigating richer posteriors is the most promising direction forward.
 Analyzing the effect of neural network architecture on training performance 
 In this paper we study how neural network architecture affects the speed of training. We introduce a simple concept called gradient confusion to help formally analyze this.  When confusion is high, stochastic gradients produced by different data samples may be negatively correlated, slowing down convergence. But when gradient confusion is low, data samples interact harmoniously, and training proceeds quickly. Through novel theoretical and experimental results, we show how the neural net architecture affects gradient confusion, and thus the efficiency of training. We show that for popular initialization techniques used in deep learning, increasing the width of neural networks leads to lower gradient confusion, and thus easier model training. On the other hand, increasing the depth of neural networks has the opposite effect. Finally, we observe that the combination of batch normalization and skip connections reduces gradient confusion, which helps reduce the training burden of very deep networks.
 Optimistic bounds for multi-output learning 
 We investigate the challenge of multi-output learning, where the goal is to learn a vector-valued function based on a supervised data set. This includes a range of important problems in Machine Learning including multi-target regression, multi-class classification and multi-label classification. We begin our analysis by introducing the self-bounding Lipschitz condition for multi-output loss functions, which interpolates continuously between a classical Lipschitz condition and a multi-dimensional analogue of a smoothness condition. We then show that the self-bounding Lipschitz condition gives rise to optimistic bounds for multi-output learning, which are minimax optimal up to logarithmic factors. The proof exploits local Rademacher complexity combined with a powerful minoration inequality due to Srebro, Sridharan and Tewari.  As an application we derive a state-of-the-art generalization bound for multi-class gradient boosting. 
 Mapping natural-language problems to formal-language solutions using structured neural representations 
 Generating formal-language programs represented by relational tuples, such as Lisp programs or mathematical operations, from natural-language problems is a challenging task because it requires explicitly capturing discrete symbolic structural information implicit in the input. However, most general neural sequence models do not explicitly capture such structural information, limiting their performance on these tasks. In this paper, we propose a new encoder-decoder model based on a structured neural representation, Tensor Product Representations (TPRs), for generating formal-language solutions from natural-language, called TP-N2F. The encoder of TP-N2F employs TPR `binding' to encode natural-language symbolic structure in vector space and the decoder uses TPR `unbinding' to generate, in symbolic space, a sequential program represented by relational tuples, each consisting of a relation (or operation) and a number of arguments. TP-N2F considerably outperforms LSTM-based seq2seq models on two benchmarks and creates new state-of-the-art results. Ablation studies show that improvements can be attributed to the use of structured TPRs explicitly in both the encoder and decoder. Analysis of the learned structures shows how TPRs enhance the interpretability of TP-N2F.
 Oracle Efficient Private Non-Convex Optimization 
 One of the most effective algorithms for differentially private learning and optimization is \emph{objective perturbation}. This technique augments a given optimization problem (e.g. deriving from an ERM problem) with a random linear term, and then exactly solves it. However, to date, analyses of this approach crucially rely on the convexity and smoothness of the objective function. We give two algorithms that extend this approach substantially. The first algorithm requires nothing except boundedness of the loss function, and operates over a discrete domain. Its privacy and accuracy guarantees hold even without assuming convexity. We are able to extend traditional analyses of objective perturbation by introducing a novel ``normalization`` step into the algorithm, which provides enough stability to be differentially private even without second-order conditions. The second algorithm operates over a continuous domain and requires only that the loss function be bounded and Lipschitz in its continuous parameter. Its privacy analysis does not even require convexity. Its accuracy analysis does require convexity, but does not require second order conditions like smoothness. We complement our theoretical results with an empirical evaluation of the non-convex case, in which we use an integer program solver as our optimization oracle. We find that for the problem of learning linear classifiers, directly optimizing for 0/1 loss using our approach can out-perform the more standard approach of privately optimizing a convex-surrogate loss function on the Adult dataset.

 Variance Reduction and Quasi-Newton for Particle-Based Variational Inference 
 Particle-based Variational Inference methods (ParVIs), like Stein Variational Gradient Descent, are nonparametric variational inference methods that optimize a set of particles to best approximate a target distribution. ParVIs have been proposed as efficient approximate inference algorithms and as potential alternatives to MCMC methods. However, to our knowledge, the quality of the posterior approximation of particles from ParVIs has not been examined before for challenging, large-scale Bayesian inference problems. In this paper, we find that existing ParVI approaches converge insufficiently fast under sample quality metrics, and we propose a novel variance reduction and quasi-Newton preconditioning framework for all ParVIs, by leveraging the Riemannian structure of the Wasserstein space and advanced Riemannian optimization algorithms. Experimental results demonstrate the accelerated convergence of ParVIs for accurate posterior inference in large-scale and ill-conditioned problems.
 Preference modelling with context-dependent salient features 
 We consider the problem of estimating a ranking on a set of items from noisy pairwise comparisons given item features. We address the observation that pairwise comparison data often reflects irrational choice, e.g. intransitivity. Our key observation is that two items compared in isolation from other items may be compared based on only a salient subset of features. Formalizing this framework, we propose the \textit{salient feature preference model} and prove a sample complexity result for learning the parameters of our model and the underlying ranking with maximum likelihood estimation. We also provide empirical results that support our theoretical bounds and illustrate how our model explains systematic intransitivity. Finally we demonstrate the strong performance of maximum likelihood estimation of our model on both synthetic data and two real data sets: the UT Zappos50K data set and comparison data about the compactness of legislative districts in the United States.
 Selective Dyna-style Planning Under Limited Model Capacity 
 In model-based reinforcement learning, planning with an imperfect model of the environment has the potential to harm learning progress.
But even when a model is imperfect, it may still contain information that is useful for planning.
In this paper, we investigate the idea of using an imperfect model selectively. 
The agent should plan in parts of the state space where the model would be helpful but refrain from using the model where it would be harmful.
An effective selective planning mechanism requires estimating predictive uncertainty, which arises out of aleatoric uncertainty and epistemic uncertainty.
Prior work has focused on parameter uncertainty, a particular kind of epistemic uncertainty, for selective planning.
In this work, we emphasize the importance of structural uncertainty, a distinct kind of epistemic uncertainty that signals the errors due to limited capacity or a misspecified model class. 
We show that heteroscedastic regression, under an isotropic Gaussian assumption, can signal structural uncertainty that is complementary to that which is detected by methods designed to detect parameter uncertainty, indicating that considering both parameter and structural uncertainty may be a more promising direction for effective selective planning than either in isolation.
 Adversarial Robustness for Code 
 Machine learning and deep learning in particular has been recently used to successfully address many tasks in the domain of code including -- finding and fixing bugs, code completion, decompilation, malware detection, type inference and many others. However, the issue of adversarial robustness of models for code has gone largely unnoticed. In this work we address this gap by: (i) developing adversarial attacks for code (a domain with discrete and highly structured inputs), (ii) showing that, similar to other domains, neural models for code are highly vulnerable to adversarial attacks, and (iii) developing a set of novel techniques that enable training robust and accurate models of code.
 Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems 
 We propose an efficient inference method for switching nonlinear dynamical systems. The key idea is to learn an inference network which can be used as a proposal distribution for the continuous latent variables, while performing exact marginalization of the discrete latent variables. This allows us to use the reparameterization trick, and apply end-to-end training with stochastic gradient descent. We show that the proposed method can successfully segment time series data, including videos and 3D human pose, into meaningful ``regimes'' by using the piece-wise nonlinear dynamics. 
 BINOCULARS for efficient, nonmyopic sequential experimental design 
 Finite-horizon sequential experimental design (SED) arises naturally in many contexts, including hyperparameter tuning in machine learning among more traditional settings. Computing the optimal policy for such problems requires solving Bellman equations, which are generally intractable. Most existing work resorts to severely myopic approximations by limiting the decision horizon to only a single time-step, which can underweight exploration in favor of exploitation. We present BINOCULARS: Batch-Informed NOnmyopic Choices, Using Long-horizons for Adaptive, Rapid SED, a general framework for deriving efficient, nonmyopic approximations to the optimal experimental policy. Our key idea is simple and surprisingly effective: we first compute a one-step optimal batch of experiments, then select a single point from this batch to evaluate. We realize BINOCULARS for Bayesian optimization and Bayesian quadrature -- two notable example problems with radically different objectives -- and demonstrate that BINOCULARS significantly outperforms significantly outperforms myopic alternatives in real-world scenarios.
 Sets Clustering 
 The input to the \emph{sets-$k$-means} problem is an integer $k\geq 1$ and a set $\mathcal{P}=\br{P_1,\cdots,P_n}$ of sets in $\mathbb{R}^d$. The goal is to compute a set $C$ of $k$ centers (points) in $\mathbb{R}^d$ that minimizes the sum $\sum_{P\in \mathcal{P}} \min_{p\in P, c\in C}\left\| p-c \right\|^2$ of squared distances to these sets.
An  \emph{$\eps$-core-set} for this problem is a weighted subset of $\mathcal{P}$ that approximates this sum up to $1\pm\varepsilon$ factor, for \emph{every} set $C$ of $k$ centers in $\mathbb{R}^d$.
We prove that such a core-set of $O(\log^2{n})$ sets always exists, and can be computed in $O(n\log{n})$ time, for every input $\mathcal{P}$ and every fixed $d,k\geq 1$ and $\varepsilon \in (0,1)$. The result easily generalized for any metric space, distances to the power of $z>0$, and M-estimators that handle outliers. Applying an inefficient but optimal algorithm on this coreset allows us to obtain the first PTAS ($1+\eps$ approximation) for the sets-$k$-means problem that takes time near linear in $n$.
This is the first result even for sets-mean on the plane ($k=1$, $d=2$). 
Open source code and experimental results for document classification and facility locations are also provided.
 The Intrinsic Robustness of Stochastic Bandits to Strategic Manipulation 
 Motivated by economic applications such as recommender systems, we study the behavior of  stochastic bandits algorithms under \emph{strategic behavior} conducted by rational actors, i.e.,  the arms. Each arm is a \emph{self-interested} strategic player who can modify its own reward whenever pulled, subject to a cross-period budget constraint, in order to maximize its own expected number of times of being pulled. We analyze the robustness of three popular  bandit algorithms: UCB, $\varepsilon$-Greedy, and Thompson Sampling. We prove that all three algorithms achieve a regret upper bound $\mathcal{O}(\max \{ B, K\ln T\})$  where $B$ is the total budget across arms, $K$ is the total number of arms and $T$ is the running time of the algorithms.  This regret guarantee holds for \emph{arbitrary  adaptive} manipulation strategy of  arms.  Our second set of main results shows that this regret bound is \emph{tight}--- in fact, for UCB, it is tight even when we restrict the arms' manipulation strategies to form a \emph{Nash equilibrium}. We do so by characterizing the Nash equilibrium of the game induced by arms' strategic manipulations and show a regret lower bound of $\Omega(\max \{ B, K\ln T\})$ at the equilibrium.   
 Rate-distortion optimization guided autoencoder for isometric embedding in Euclidean latent space 
 To analyze high-dimensional and complex data in the real world, generative model approach of machine learning aims to reduce the dimension and acquire a probabilistic model of the data. For this purpose, deep-autoencoder based generative models such as variational autoencoder (VAE) have been proposed. However, in previous works, the scale of metrics between the real and the reduced-dimensional space (latent space) is not well-controlled. Therefore, the quantitative impact of the latent variable on real data is unclear. In the end, the probability distribution function (PDF) in the real space cannot be estimated from that of the latent space accurately. To overcome this problem, we propose Rate-Distortion Optimization guided autoencoder. We show our method has the following properties theoretically and experimentally: (i) the columns of Jacobian matrix between two spaces is constantly-scaled orthonormal system and data can be embedded in a Euclidean space isometrically; (ii) the PDF of the latent space is proportional to that of the real space. 
Furthermore, to verify the usefulness in the practical application, we evaluate its performance in unsupervised anomaly detection and it outperforms current state-of-the-art methods.
 Meta Variance Transfer: Learning to Augment from the Others 
 Humans have the ability to robustly recognize objects with various factors of variations such as nonrigid transformation, background noise, and change in lighting conditions. However, deep learning frameworks generally require huge amount of data with instances under diverse variations, to train a robust model. To alleviate the need of collecting large data and better learn from scarce samples, we propose a novel meta-learning method which learns to transfer factors of variations from one class to another, such that it can improve the classification performance on unseen examples. Transferred variations generate virtual samples that augment the feature space of the target class during training, simulating upcoming query samples with similar variations. By sharing factors of variations across different classes, the model becomes more robust to variations in the unseen examples and tasks using small number of examples per class. We validate our model on multiple benchmark datasets for few-shot classification and face recognition, on which our model significantly improves the performance of the base model, outperforming relevant baselines.
 Performative Prediction 
 When predictions support decisions they may influence the outcome they aim to predict. We call such predictions performative; the prediction influences the target. Performativity is a well-studied phenomenon in policy-making that has so far been neglected in supervised learning. When ignored, performativity surfaces as undesirable distribution shift, routinely addressed with retraining.

We develop a risk minimization framework for performative prediction bringing together concepts from statistics, game theory, and causality. A conceptual novelty is an equilibrium notion we call performative stability. Performative stability implies that the predictions are calibrated not against past outcomes, but against the future outcomes that manifest from acting on the prediction. Our main results are necessary and sufficient conditions for the convergence of retraining to a performatively stable point of nearly minimal loss.

In full generality, performative prediction strictly subsumes the setting known as strategic classification. We thus also give the first sufficient conditions for retraining to overcome strategic feedback effects.
 The FAST Algorithm for Submodular Maximization 
 In this paper we describe a new parallel algorithm called Fast Adaptive Sequencing Technique (FAST) for maximizing a monotone submodular function under a cardinality constraint k. This algorithm achieves the optimal 1-1/e approximation guarantee and is orders of magnitude faster than the state-of-the-art on a variety of experiments over real-world data sets.

In the past two years, following the work by Balkanski and Singer there has been a great deal of work on algorithms whose theoretical parallel runtime is exponentially faster than algorithms used for submodular maximization over the past 40 years. Although these algorithms are fast in terms of asymptotic worst case guarantees, it is computationally infeasible to use them in practice. The reason is that the number of rounds and queries they require depends on very large constants as well as high-degree polynomials in terms of the precision and confidence, causing these algorithms to be impractical even on small data sets.  

The design principles behind the FAST algorithm we present here are a significant departure from those of theoretically fast algorithms that have been studied in the past two years. Rather than optimize for theoretical guarantees, the design of FAST introduces several new techniques that achieve remarkable practical and theoretical parallel runtimes. More specifically, the approximation guarantee obtained by FAST is arbitrarily close to 1 − 1/e, its theoretical parallel runtime (adaptivity) is O(log(n) log^2(log k)), and the total number of queries is O(n log log(k)). We show that FAST is orders of magnitude faster than any algorithm for submodular maximization we are aware of, including hyper-optimized parallel versions of state-of-the-art serial algorithms, by running experiments on large data sets.
 Restarted Bayesian Online Change-point Detector achieves Optimal Detection Delay 
 In this paper, we consider the problem of sequential change-point detection where both the change-points and the distributions before and after the change are assumed to be unknown. For this key problem in statistical and sequential learning theory,  we derive a variant of the Bayesian Online Change Point Detector proposed by \cite{adams2007bayesian} which is easier to analyze than the original version while keeping its powerful message-passing algorithm. 
	We provide a non-asymptotic analysis of the false-alarm rate and the detection delay that matches the existing lower-bound. We further provide the first explicit high-probability control of the detection delay for such approach. Experiments on synthetic and real-world data show that this proposal compares favorably with the state-of-art change-point detection strategy, namely the Improved Generalized Likelihood Ratio (Improved GLR) while outperforming the original Bayesian Online Change Point Detection strategy.
 Min-Max Optimization without Gradients: Convergence and Applications to Black-Box Evasion and Poisoning Attacks 
 In this paper, we study the problem of constrained min-max optimization in a black-box setting, where the desired optimizer cannot access the gradients of the objective function but may query its values. We present a principled optimization framework, integrating a zeroth-order (ZO) gradient estimator with an alternating projected stochastic gradient descent-ascent method, where the former only requires a small number of function queries and the later needs just one-step descent/ascent update. We show that the proposed framework, referred to as ZO-Min-Max, has a sub-linear convergence rate under mild conditions and scales gracefully with problem size. We also explore a promising connection between black-box min-max optimization and black-box evasion and poisoning attacks in adversarial machine learning (ML). Our empirical evaluations on these use cases demonstrate the effectiveness of our approach and its scalability to dimensions that prohibit using recent black-box solvers.
 On Semi-parametric Inference for BART 
 There has been a growing realization of the potential of Bayesian machine learning as a platform that can provide both flexible modeling, accurate predictions as well as coherent uncertainty statements. In particular, Bayesian Additive Regression Trees (BART) have emerged as one of today’s most effective general approaches to predictive modeling under minimal assumptions. Statistical theoretical developments for machine learning have been mostly concerned with approximability or rates of estimation when recovering infinite dimensional objects (curves or densities). Despite the impressive array of available theoretical results, the literature has been largely silent about uncertainty quantification. In this work, we continue the theoretical investigation of BART initiated recently by Rockova and van der Pas (2017). We focus on statistical inference questions. In particular, we study the Bernstein-von Mises (BvM) phenomenon (i.e. asymptotic normality) for smooth linear functionals of the regression surface within the framework of non-parametric regression with fixed covariates. Our semi-parametric BvM results show that, beyond rate-optimal estimation, BART can be also used for valid statistical inference. 
 Accelerating the diffusion-based ensemble sampling by non-reversible dynamics 
 Posterior distribution approximation is a central task in Bayesian inference. Stochastic gradient Langevin dynamics (SGLD) and its extensions have been widely used practically and studied theoretically. While SGLD updates a single particle at a time, ensemble methods that update multiple particles simultaneously have been recently gathering attention. Compared with the naive parallel-chain SGLD that updates multiple particles independently, ensemble methods update particles with their interactions. Thus, these methods are expected to be more particle-efficient than the naive parallel-chain SGLD because particles can be aware of other particles’ behavior through their interactions. Although ensemble methods demonstrated their superior performance numerically, no theoretical guarantee exists to assure such particle-efficiency and it is unclear whether those ensemble methods are really superior to the naive parallel-chain SGLD in the non-asymptotic settings. To cope with this problem, we propose a novel ensemble method that uses a non-reversible Markov chain for the interaction, and we present a non-asymptotic theoretical analysis for our method. Our analysis shows that, for the first time, the interaction causes a faster convergence rate than the naive parallel-chain SGLD in the non-asymptotic setting if the discretization error is appropriately controlled. Numerical experiments show that we can control the discretization error by tuning the interaction appropriately.
 Multilinear Latent Conditioning for Generating Unseen Attribute Combinations 
 Empirical studies have shown that deep generative models demonstrate inductive bias. Although this bias is crucial in problems with high dimensional data, like images, generative models lack the generalization ability that occurs naturally in human perception. For example, humans can visualize a woman smiling after only seeing a smiling man. On the contrary, the standard conditional variational auto-encoder (cVAE) is unable to generate unseen attribute combinations. To this end, we extend the cVAE by introducing a multilinear latent conditioning framework. We implement two variants of our model and demonstrate their efficacy on MNIST, Fashion-MNIST and CelebA. Altogether, we design a novel conditioning framework that can be used with any architecture to synthesize unseen attribute combinations.
 Privately detecting changes in unknown distributions 
 The change-point detection problem seeks to identify distributional changes in streams of data. Increasingly, tools for change-point detection are applied in settings where data may be highly sensitive and formal privacy guarantees are required, such as identifying disease outbreaks based on hospital records, or IoT devices detecting activity within a home. Differential privacy has emerged as a powerful technique for enabling data analysis while preventing information leakage about individuals. Much of the prior work on change-point detection---including the only private algorithms for this problem---requires complete knowledge of the pre-change and post-change distributions. However, this assumption is not realistic for many practical applications of interest. This work develops differentially private algorithms for solving the change-point problem when the data distributions are unknown. Additionally, the data may be sampled from distributions that change smoothly over time, rather than fixed pre-change and post-change distributions.  We apply our algorithms to detect changes in the linear trends of such data streams. We also provide experimental results to empirically validate the performance of our algorithms.
 A Unified Theory of Decentralized SGD with Changing Topology and Local Updates 
 Decentralized stochastic optimization methods have gained a lot of attention recently, mainly because of their cheap per iteration cost, data locality, and their communication-efficiency. In this paper we introduce a unified convergence analysis that covers a large variety of decentralized SGD methods which so far have required different intuitions, have different applications, and which have been developed separately in various communities. 

Our algorithmic framework covers local SGD updates and synchronous and pairwise gossip updates on adaptive network topology. We derive universal convergence rates for smooth (convex and non-convex) problems and the rates interpolate between the heterogeneous (non-identically distributed data) and iid-data settings, recovering linear convergence rates in many special cases, for instance for over-parametrized models. Our proofs rely on weak assumptions (typically improving over prior work in several aspects) and recover (and improve) the best known complexity results for a host of important scenarios, such as for instance coorperative SGD and federated averaging (local SGD).
 Learning Compound Tasks without Task-specific Knowledge via Imitation and Self-supervised Learning 
 Most real-world tasks are compound tasks that consist of multiple simpler sub-tasks. The main challenge of learning compound tasks is that we have no explicit supervision to learn the hierarchical structure of compound tasks. To address this challenge, previous imitation learning methods exploit task-specific knowledge, e.g., labeling demonstrations manually or specifying termination conditions for each sub-task. However, the need for task-specific knowledge makes it difficult to scale imitation learning to real-world tasks. In this paper, we propose an imitation learning method that can learn compound tasks without task-specific knowledge. The key idea behind our method is to leverage a self-supervised learning framework to learn the hierarchical structure of compound tasks. Our work also proposes a task-agnostic regularization technique to prevent unstable switching between sub-tasks, which has been a common degenerate case in previous works. We evaluate our method against several baselines on compound tasks. The results show that our method achieves state-of-the-art performance on compound tasks, outperforming prior imitation learning methods.
 Information-Theoretic Local Minima Characterization and Regularization 
 Recent advances in deep learning theory have evoked the study of generalizability across different local minima of deep neural networks (DNNs). While current work focused on either discovering properties of good local minima or developing regularization techniques to induce good local minima, no approach exists that can tackle both problems. We achieve these two goals successfully in a unified manner. Specifically, based on the observed Fisher information we propose a metric both strongly indicative of generalizability of local minima and effectively applied as a practical regularizer. We provide theoretical analysis including a generalization bound and empirically demonstrate the success of our approach in both capturing and improving the generalizability of DNNs. Experiments are performed on CIFAR-10 and CIFAR-100 for various network architectures.
 Adaptive Gradient Descent without Descent 
 We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on smoothness in a neighborhood of a solution. Given that the problem is convex, our method will converge even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.
 Minimax Weight and Q-Function Learning for Off-Policy Evaluation 
 We provide theoretical investigations into off-policy evaluation in reinforcement learning using function approximators for (marginalized) importance weights and value functions. Our contributions include: (1) A new estimator, MWL, that directly estimates importance ratios over the state-action distributions, removing the reliance on knowledge of the behavior policy as in prior work  (Liu et.al, 2018), (2) Another new estimator, MQL, obtained by swapping the roles of importance weights and value-functions in MWL. MQL has an intuitive interpretation of minimizing average Bellman errors and can be combined with MWL in a doubly robust manner, (3) Several additional results that offer further insights, including the sample complexities of MWL and MQL, their asymptotic optimality in the tabular setting, how the learned importance weights depend the choice of the discriminator class, and how our methods provide a unified view of some old and new algorithms in RL.
 Random extrapolation for primal-dual coordinate descent 
 We introduce a randomly extrapolated primal-dual coordinate descent method that automatically adapts to the sparsity of the data matrix as well as the favorable structures of the objective function in optimization.
Our method can update only a subset of primal and dual variables with sparse data, and it can provably use large step sizes with dense data, retaining the benefits of the specific methods designed for each case.
In addition to key adaptivity to the sparsity, our method attains fast convergence guarantees in favorable cases \textit{without any modifications}. In particular, we prove linear convergence under metric subregularity, which applies to strongly convex-strongly concave problems, linear programs and piecewise linear quadratic functions.
We also show almost sure convergence of the sequence and optimal sublinear convergence rates for the primal-dual gap and objective values in the worst case.
Numerical evidence demonstrates the state-of-the-art empirical performance of our method in sparse and dense settings, matching and improving the existing methods over different applications with real data.
 Class-Weighted Classification: Trade-offs and Robust Approaches 
 We consider imbalanced classification, the problem in which a label may have low marginal probability relative to other labels, by weighting losses according to the correct class. 
First, we examine the convergence rates of the expected excess weighted risk of plug-in classifiers where the weighting for the plug-in classifier and the risk may be different.
This leads to irreducible errors that do not converge to the weighted Bayes risk, which motivates our consideration of robust risks.
We define a robust risk that minimizes risk over a set of weightings and show excess risk bounds for this problem. 
Finally, we show that particular choices of the weighting set leads to a special instance of conditional value at risk (CVaR) from stochastic programming, which we call label conditional value at risk (LCVaR).
Additionally, we generalize this weighting to derive a new robust risk problem that we call label heterogeneous conditional value at risk (LHCVaR).
Finally, we empirically demonstrate the efficacy of LCVaR and LHCVaR on improving class conditional risks.
 Error Estimation for Sketched SVD 
 In order to compute fast approximations to the singular value decompositions (SVD) of very large matrices, randomized sketching algorithms have become a leading approach. However, a key practical difficulty of sketching an SVD is that the user does not know how far the sketched singular vectors/values are from the exact ones. Indeed, the user may be forced to rely on analytical worst-case error bounds, which do not account for the unique structure of a given problem. As a result, the lack of tools for error estimation often leads to much more computation than is really necessary. To overcome these challenges, this paper develops a fully data-driven bootstrap method that numerically estimates the actual error of sketched singular vectors/values. In particular, this approach allows the user to inspect the quality of a rough initial SVD, and then adaptively predict how much extra work is needed to reach a given error tolerance. Meanwhile, from a computational standpoint, the proposed method incurs only minor cost, because it operates on the (small) output of a sketching algorithm, and it requires no passes over the (large) matrix being factored. Lastly, the proposed method is supported by theoretical guarantees and a very encouraging set of experimental results.
 Sparse Shrunk Additive Models 
 Most existing feature selection methods in literature are linear models, so that the nonlinear relations between features and response variables are not considered. Meanwhile, in these feature selection models, the interactions between features are often ignored or just discussed under prior structure information. To address these challenging issues, we consider the problem of sparse additive models for high-dimensional nonparametric regression with the allowance of the flexible interactions between features. A new method, called as sparse shrunk additive models (SSAM), is proposed to explore the structure information among features. This method bridges sparse kernel regression and sparse feature selection. Theoretical results on the convergence rate and sparsity characteristics of SSAM are established by the novel analysis techniques with integral operator and concentration estimate. In particular, our algorithm and theoretical analysis only require the component functions to be continuous and bounded, which are not necessary to be in reproducing kernel Hilbert spaces. Experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed approach.
 Adversarial Risk via Optimal Transport and Optimal Couplings 
 The accuracy of modern machine learning algorithms deteriorates severely on adversarially manipulated test data. Optimal adversarial risk quantifies the best error rate of any classifier in the presence of adversaries, and optimal adversarial classifiers are sought that minimize adversarial risk. In this paper, we investigate the optimal adversarial risk and optimal adversarial classifiers from an optimal transport perspective. We present a new and simple approach to show that the optimal adversarial risk for binary classification with 0 − 1 loss function is completely characterized by an optimal transport cost between the probability distributions of the two classes, for a suitably defined cost function. We propose a novel coupling strategy that achieves the optimal transport cost for several univariate distributions like Gaussian, uniform and triangular. Using the optimal couplings, we obtain the optimal adversarial classifiers in these settings and show how they differ from optimal classifiers in the absence of adversaries. Based on our analysis, we evaluate algorithm-independent fundamental limits on adversarial risk for CIFAR-10, MNIST, Fashion-MNIST and SVHN datasets, and Gaussian mixtures based on them.
 An EM Approach to Non-autoregressive Conditional Sequence Generation 
 Autoregressive (AR) models have been the dominating approach to conditional sequence generation, but are suffering from the issue of high inference latency.  Non-autoregressive (NAR) models have been recently proposed to reduce the latency by generating all output tokens in parallel but could only achieve inferior accuracy compared to their autoregressive counterparts, primarily due to a difficulty in dealing with the multi-modality in sequence generation.  This paper proposes a new approach that jointly optimizes both AR and NAR models in a unified Expectation-Maximization (EM) framework. In the E-step, an AR model learns to approximate the regularized posterior of the NAR model. In the M-step, the NAR model is updated on the new posterior and selects the training examples for the next AR model. This iterative process can effectively guide the system to remove the multi-modality in the output sequences and remedy the multi-modality problem. To our knowledge, this is the first EM approach to NAR sequence generation. We evaluate our method on the task of machine translation. Experimental results on benchmark data sets show that the proposed approach achieves competitive, if not better, performance with existing NAR models and significantly reduces the inference latency.
 Computational and Statistical Tradeoffs in Inferring Combinatorial Structures of Ising Model 
 We study the computational and statistical tradeoffs in inferring combinatorial structures of high dimensional simple zero-field ferromagnetic Ising model. Under the framework of oracle computational model where an algorithm interacts with an oracle that discourses a randomized version of truth, we characterize the computational lower bounds of learning combinatorial structures in polynomial time, under which no algorithms within polynomial-time can distinguish between graphs with and without certain structures. This hardness of learning with limited computational budget is shown to be characterized by a novel quantity called vertex overlap ratio. Such quantity is universally valid for many specific graph structures including cliques and nearest neighbors. On the other side, we attain the optimal rates for testing these structures against empty graph by proposing the quadratic testing statistics to match the lower bounds. We also investigate the relationship between computational bounds and information-theoretic bounds for such problems, and found gaps between the two boundaries in inferring some particular structures, especially for those with dense edges.
 On Implicit Regularization in $\beta$-VAEs 
 While the impact of variational inference (VI) on posterior inference in a fixed generative model is well-characterized, its role in regularizing a learned generative model when used in variational autoencoders (VAEs) is poorly understood. We study the regularizing effects of variational distributions on learning in generative models from two perspectives. First, we analyze the role that the choice of variational family plays in imparting uniqueness to the learned model by restricting the set of optimal generative models. Second, we study the regularization effect of the variational family on the local geometry of the decoding model. This analysis uncovers the regularizer implicit in the $\beta$-VAE objective, and leads to an approximation consisting of a deterministic autoencoding objective plus analytic regularizers that depend on the Hessian or Jacobian of the decoding model, unifying VAEs with recent heuristics proposed for training regularized autoencoders. We empirically verify these findings, observing that the proposed deterministic objective indeed exhibits similar behavior to the $\beta$-VAE in terms of objective value and sample quality on CelebA and MNIST. 

 Bayesian Learning from Sequential Data using Gaussian Processes with Signature Covariances 
 We develop a Bayesian approach to learning from sequential data by using Gaussian processes (GPs) with so-called signature kernels as covariance functions. This allows to make sequences of different length comparable and to rely on strong theoretical results from stochastic analysis.
Signatures capture sequential structure with tensors that can scale unfavourably in sequence length and state space dimension. To deal with this, we introduce a sparse variational approach with inducing tensors.
We then combine the resulting GP with LSTMs and GRUs to build larger models that leverage the strengths of each of these approaches and benchmark the resulting GPs on multivariate time series (TS) classification datasets.
 Streaming Coresets for Symmetric Tensor Factorization 
 Factorizing tensors has recently become an important optimization module in a number of machine learning pipelines, especially in latent variable models. We show how to do this efficiently in the streaming setting. Given a set of $n$ vectors, each in $\~R^d$, we present algorithms to select a sublinear number of these vectors as coreset, while guaranteeing that the CP decomposition of the $p$-moment tensor of the coreset approximates the corresponding decomposition of the $p$-moment tensor computed from the full data. We introduce two novel algorithmic techniques: online filtering and kernelization. Using these two, we  
present four algorithms that achieve different tradeoffs of coreset size, update time and working space, beating or matching various state of the art algorithms. In case of matrices (2-ordered tensor) our online row sampling algorithm guarantees $(1 \pm \epsilon)$ relative error spectral approximation. We show applications of our algorithms in learning single topic modeling. 
 Online mirror descent and dual averaging: keeping pace in the dynamic case 
 Online mirror descent (OMD) and dual averaging (DA) are two fundamental algorithms for online convex optimization. They are known to have very similar (or even identical) performance guarantees in most scenarios when a \emph{fixed} learning rate is used. However, for \emph{dynamic} learning rates OMD is provably inferior to DA. It is known that, with a dynamic learning rate, OMD can suffer linear regret, even in common settings such as prediction with expert advice. This hints that the relationship between OMD and DA is not fully understood at present.

In this paper, we modify the OMD algorithm by a simple technique that we call stabilization. We give essentially the same abstract regret bound for stabilized OMD and DA by modifying the classical OMD convergence analysis in a careful and modular way, yielding proofs that we believe to be clean and flexible. Simple corollaries of these bounds show that OMD with stabilization and DA enjoy the same performance guarantees in many applications even under dynamic learning rates. We also shed some light on the similarities between OMD and DA and show simple conditions under which stabilized OMD and DA generate the same iterates.
 Minimax Rate for Learning From Pairwise Comparisons in the BTL Model 
 We consider the problem of learning the qualities w_1, ... , w_n of a collection of items by performing noisy comparisons among them. We assume there is a fixed ``comparison graph'' and every neighboring pair of items is compared k times. We will study the popular Bradley-Terry-Luce model,  where the probability that item i wins a  comparison against j equals w_i/(w_i + w_j).  We are interested in how the expected error in estimating the vector w = (w_1, ... , w_n) behaves in the regime when the number of comparisons k is large.

Our contribution is the determination of the minimax rate up to a constant factor. We   show that this rate is achieved by a simple algorithm based on weighted least squares, with weights determined from the empirical outcomes of the comparisons. This algorithm can be implemented  in nearly linear time in the total number of comparisons.
 Learning with Feature and Distribution Evolvable Streams 
 In many real-world applications, data are often collected in the form of a stream, and thus the feature space of streaming data can evolve over time. For example, in the environmental monitoring task, features can be dynamically vanished or augmented due to the existence of expired old sensors and deployed new sensors. Besides the feature space evolving, it is noteworthy that the data distribution often changes in streaming data. When both feature space and data distribution are evolvable, it is quite challenging to design algorithms with guarantees, particularly the theoretical understanding of generalization ability. To address this difficulty, we propose a novel discrepancy measure for evolving feature space and data distribution named the evolving discrepancy, based on which we provide the generalization error analysis. The theory motivates the design of a learning algorithm, which is further implemented by deep neural networks. We present empirical studies on synthetic data to verify the rationale of the proposed discrepancy measure. Extensive experiments on real-world tasks validate the effectiveness of our algorithm.
 Parameterized Rate-Distortion Stochastic Encoder 
 We propose a novel gradient-based tractable approach for the Blahut-Arimoto (BA) algorithm to compute the rate-distortion function where the BA algorithm is fully parameterized. This results in a rich and flexible framework to learn a new class of stochastic encoders, termed PArameterized RAte-DIstortion Stochastic Encoder (PARADISE). The framework can be applied to a wide range of settings from semi-supervised, multi-task to supervised and robust learning. We show that the training objective of PARADISE can be seen as a form of regularization that helps improve generalization. With an emphasis on robust learning we further develop a novel posterior matching objective to encourage smoothness on the loss function and show that PARADISE can significantly improve interpretability as well as robustness to adversarial attacks on the CIFAR-10 and ImageNet datasets. In particular, on the CIFAR-10 dataset, our model reduces standard and adversarial error rates in comparison to the state-of-the-art by 50% and 41%, respectively without the expensive computational cost of adversarial training.
 Black-box Certification and Learning under Adversarial Perturbations 
 We formally study the problem of classification under adversarial perturbations, both from the learner's perspective, and from the viewpoint of a third-party who aims at certifying the robustness of a given black-box classifier. 
We further introduce and study a new setting of black-box certification under limited query budget. We analyze this for various classes of predictors and types of perturbation. 
We also consider the viewpoint of a black-box adversary that aims at finding adversarial examples, showing that the existence of an adversary with polynomial query complexity implies the existence of a robust learner with small sample complexity.

 Decision Trees for Decision-Making under the Predict-then-Optimize Framework 
 We consider the use of decision trees for decision-making problems under the predict-then-optimize framework. That is, we would like to first use a decision tree to predict unknown input parameters of an optimization problem, and then make decisions by solving the optimization problem using the predicted parameters. A natural loss function in this framework is to measure the suboptimality of the decisions induced by the predicted input parameters, as opposed to measuring loss using input parameter prediction error. This natural loss function is known in the literature as the Smart Predict-then-Optimize (SPO) loss, and we propose a tractable methodology called SPO Trees (SPOTs) for training decision trees under this loss. SPOTs benefit from the interpretability of decision trees, providing an interpretable segmentation of contextual features into groups with distinct optimal solutions to the optimization problem of interest. We conduct several numerical experiments on synthetic and real data including the prediction of travel times for shortest path problems and predicting click probabilities for news article recommendation. We demonstrate on these datasets that SPOTs simultaneously provide higher quality decisions and significantly lower model complexity than other machine learning approaches (e.g., CART) trained to minimize prediction error.
 Visual Grounding of Learned Physical Models 
 Humans can intuitively recognize objects’ physical properties and predict their future motion, even when the objects are engaged in complicated interactions with each other. The ability to perform physical reasoning and adapt to new environments, while intrinsic to humans, remains challenging to state-of-the-art computational models. In this work, we present a neural model that simultaneously reasons about physics and make future predictions based on visual and dynamics priors. The visual prior predicts a particle-based representation of the system from visual observations. An inference module operates on those particles, predicting and refining estimates of particle locations, object states, and physical parameters, subject to the constraints imposed by the dynamics prior, which we refer to as visual grounding. We demonstrate the effectiveness of our method in environments involving rigid objects, deformable materials, and fluids. Experiments show that our model can infer the physical properties within a few observations, which allows the model to quickly adapt to unseen scenarios and make accurate predictions into the future.
  Topic Modeling via Full Dependence Mixtures 
 In this paper we introduce a new approach to topic modelling that scales to 
large datasets by  using  a compact representation of the data and by
leveraging the GPU architecture.   
In this approach, topics are learned directly from the  
co-occurrence data of the corpus. In particular, we introduce a novel
mixture model which we term the Full Dependence Mixture (FDM) model.
FDMs model  second moment under  general generative 
assumptions on the data. While there is previous work on topic 
modeling using second moments,  we develop a direct stochastic 
optimization procedure for fitting an FDM with a single Kullback 
Leibler objective. Moment methods in general have the benefit that 
an iteration no longer needs to scale with the size of the corpus. 
Our approach allows us to leverage standard 
optimizers and GPUs for the problem of topic modeling. In 
particular, we evaluate the approach on two large datasets, 
NeurIPS papers and a Twitter corpus, with a large number of 
topics, and show that the approach performs comparably or better than the standard benchmarks.
 Laplacian Regularized Few-Shot Learning 
 Few-shot learning attempts to generalize to unlabeled query samples of new classes, which are unseen during training, given just a few labeled examples of those classes. It has received substantial research interest recently, with a large body of works based on complex meta-learning strategies and architecture 
choices. We propose a Laplacian-regularization objective for few-shot tasks, which 
integrates two types of potentials: (1) unary potentials assigning query samples to the nearest class prototype and (2) pairwise Laplacian potentials encouraging nearby query samples to have consistent predictions.We optimize a tight upper bound of a concave-convex relaxation of our objective, thereby guaranteeing convergence, while computing independent updates for each query sample. Following the standard experimental setting for few-shot learning, our LaplacianShot technique outperforms state-of-the-art methods significantly, while using simple cross-entropy training on the base classes. In the 1-shot setting on the standard miniImageNet and tieredImageNet benchmarks, and on the recent meta-iNat benchmark, across various networks, LaplacianShot consistently pro-vides 3 − 4% improvement in accuracy over the best-performing state-of-the-art method.
 Evaluating Machine Accuracy on ImageNet 
 We perform an in-depth evaluation of human accuracy on the ImageNet dataset. First, three expert labelers re-annotated 30,000 images from the original ImageNet validation set and the ImageNetV2 replication experiment with multi-label annotations to enable a semantically coherent accuracy measurement. Then we evaluated five trained humans on both datasets. The median of the five labelers outperforms the best publicly released ImageNet model by 1.5% on the original validation set and by 6.2% on ImageNetV2. Moreover, the human labelers see a substantially smaller drop in accuracy between the two datasets compared to the best available model (less than 1% vs 5.4%). Our results put claims of superhuman performance on ImageNet in context and show that robustly classifying ImageNet at human-level performance is still an open problem.

 Extra-gradient with player sampling for faster convergence in n-player games 
 Data-driven modeling increasingly requires to find a Nash equilibrium in multi-player games, e.g. when training GANs. In this paper, we analyse a new extra-gradient method for Nash equilibrium finding, that performs gradient extrapolations and updates on a random subset of players at each iteration. This approach provably exhibits a better rate of convergence than full extra-gradient for non-smooth convex games with noisy gradient oracle. We propose an additional variance reduction mechanism to obtain speed-ups in smooth convex games. Our approach makes extrapolation amenable to massive multiplayer settings, and brings empirical speed-ups, in particular when using a heuristic cyclic sampling scheme. Most importantly, it allows to train faster and better GANs and mixtures of GANs.
 Causal Structure Discovery from Distributions Arising from Mixtures of DAGs 
 We consider distributions arising from a mixture of causal models, where each model is represented by a directed acyclic graph (DAG). We provide a graphical representation of such mixture distributions and prove that this representation encodes the conditional independence relations of the mixture distribution. We  then  consider  the  problem  of  structure  learning  based  on  samples  from  such distributions. Since the mixing variable is latent, we consider causal structure discovery algorithms such as FCI that can deal with latent variables. We show that such algorithms recover a “union” of the component DAGs and can identify variables whose conditional distribution across the component DAGs vary. We demonstrate our results on synthetic and real data showing that the inferred graph identifies nodes that vary between the different mixture components. As an immediate application, we demonstrate how retrieval of this causal information can be used to cluster samples according to each mixture component.
 Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label Rates 
 We propose a new framework, called Poisson learning, for graph based semi-supervised learning at very low label rates. Poisson learning is motivated by the need to address the degeneracy of Laplacian semi-supervised learning at very low label rates. The method replaces the assignment of label values at training points with the placement of sources and sinks, and solves the resulting Poisson equation on the graph. The outcomes are provably more stable and informative than those of Laplacian learning. Poisson learning is fast and efficient to implement, and we present numerical experiments showing the method is superior to other recent approaches to semi-supervised learning at low label rates on the MNIST, FashionMNIST, and the WebKb datasets. We also propose a graph-cut version of Poisson learning, called Poisson MBO, that gives higher accuracy and can incorporate prior knowledge of relative class sizes.
 Dual Mirror Descent for Online Allocation Problems 
 We consider online allocation problems with concave revenue functions and resource constraints, which are central problems in revenue management and online advertising. In these settings, requests arrive sequentially during a finite horizon and, for each request, a decision maker needs to choose an action that consumes a certain amount of resources and generates revenue. The revenue function and resource consumption of each request are drawn independently and at random from a probability distribution that is unknown to the decision maker. The objective is to maximize cumulative revenues subject to a constraint on the total consumption of resources. 
    
We design a general class of algorithms that achieve sub-linear expected regret compared to the hindsight optimal allocation. Our algorithms operate in the Lagrangian dual space: they maintain a dual multiplier for each resource that is updated using online mirror descent. By choosing the reference function accordingly, we recover dual sub-gradient descent and dual exponential weights algorithm. The resulting algorithms are simple, efficient, and shown to attain the optimal order of regret when the length of the horizon and the initial number of resources are scaled proportionally. We discuss applications to online bidding in repeated auctions with budget constraints and online proportional matching with high entropy.
 Off-Policy Actor-Critic with Shared Experience Replay 
 We investigate the combination of actor-critic reinforcement learning algorithms with a uniform large-scale experience replay and propose solutions for two ensuing challenges: (a) efficient actor-critic learning with experience replay (b) the stability of off-policy learning where agents learn from other agents behaviour. 

To this end we analyze the bias-variance tradeoffs in V-trace, a form of importance sampling for actor-critic methods. Based on our analysis, we then argue for mixing experience sampled from replay with on-policy experience, and propose a new trust region scheme that scales effectively to data distributions where V-trace becomes unstable.

We provide extensive empirical validation of the proposed solutions on DMLab-30. We further show the benefits of this setup in two training regimes for Atari: (1) a single agent is trained up until 200M environment frames per game (2) a population of agents is trained up until 200M environment frames each and may share experience. While (1) is a standard regime, (2) reflects the use case of concurrently executed hyper-parameter sweeps. We demonstrate state-of-the-art data efficiency among model-free agents in both regimes.
 Understanding the Curse of Horizon in Off-Policy Evaluation via Conditional Importance Sampling 
 Off-policy policy estimators that use importance sampling (IS) can suffer from high variance in long-horizon domains, and there has been particular excitement over new IS methods that leverage the structure of Markov decision processes. We analyze the variance of the most popular approaches through the viewpoint of conditional Monte Carlo. Surprisingly, we find that in finite horizon MDPs there is no strict variance reduction of per-decision importance sampling or stationary importance sampling, comparing with vanilla importance sampling. We then provide sufficient conditions under which the per-decision or stationary estimators will provably reduce the variance over importance sampling with finite horizons. For the asymptotic (in terms of horizon $T$) case, we develop upper and lower bounds on the variance of those estimators which yields sufficient conditions under which there exists an exponential v.s. polynomial gap between the variance of importance sampling and that of the per-decision or stationary estimators. These results help advance our understanding of if and when new types of IS estimators will improve the accuracy of off-policy estimation.
 Explicit Gradient Learning for Black-Box Optimization 
 Black-Box Optimization (BBO) methods can find optimal policies for systems that interact with complex environments with no analytical representation. As such, they are of interest in many Artificial Intelligence (AI) domains. Yet classical BBO methods fall short in high-dimensional non-convex problems. They are thus often overlooked in real-world AI tasks. Here we present a BBO method, termed Explicit Gradient Learning (EGL), that is designed to optimize high-dimensional ill-behaved functions. We derive EGL by finding weak spots in methods that fit the objective function with a parametric Neural Network (NN) model and obtain the gradient signal by calculating the parametric gradient. Instead of fitting the function, EGL trains a NN to estimate the objective gradient directly. We prove the convergence of EGL to a stationary point and its robustness in the optimization of integrable functions. We evaluate EGL and achieve state-of-the-art results in two challenging problems: (1) the COCO test suite against an assortment of standard BBO methods; and (2) in a high-dimensional non-convex image generation task.
 Domain Aggregation Networks for Multi-Source Domain Adaptation 
 In many real-world applications, we want to exploit multiple source datasets to build a model for a different but related target dataset. Despite the recent empirical success, most existing research has used ad-hoc methods to combine multiple sources, leading to a gap between theory and practice. In this paper, we develop a finite-sample generalization bound based on domain discrepancy and accordingly propose a theoretically justified optimization procedure. Our algorithm, Domain AggRegation Network (DARN), can automatically and dynamically balance between including more data to increase effective sample size and excluding irrelevant data to avoid negative effects during training. We find that DARN can significantly outperform the state-of-the-art alternatives on multiple real-world tasks, including digit/object recognition and sentiment analysis.
 A Swiss Army Knife for Minimax Optimal Transport 
 The Optimal transport (OT) problem and its associated Wasserstein distance have recently become a topic of great interest in the machine learning community. However, the underlying optimization problem is known to have two major restrictions: (i) it largely depends on the choice of the cost function and (ii) its sample complexity scales exponentially with the dimension. In this paper, we propose a general formulation of a minimax OT problem that can tackle these restrictions by jointly optimizing the cost matrix and the transport plan, allowing us to define a robust distance between distributions. We propose to use a cutting-set method to solve this general problem and show its links and advantages compared to other existing minimax OT approaches. Additionally, we use this method to define a notion of stability allowing us to select the most robust cost matrix. Finally, we provide an experimental study highlighting the efficiency of our approach.
  Recht-Re Noncommutative Arithmetic-Geometric Mean Conjecture is False 
 Stochastic optimization algorithms have become indispensable in machine learning. An unresolved foundational question in this area is the difference between with-replacement sampling and without-replacement sampling --- does the latter have superior convergence rate compared to the former? A groundbreaking result of Recht and Re reduces the problem to a noncommutative analogue of the arithmetic-geometric mean inequality where positive numbers are replaced by n positive definite matrices. If this inequality holds for all n, then without-replacement sampling indeed outperforms with-replacement sampling. The conjectured Recht--Re inequality has so far only been established for n = 2 and a special case of n = 3. We will show that the Recht--Re conjecture is false for general n. Our approach relies on the noncommutative positivstellensatz, which allows us to reduce the conjectured inequality to a semidefinite program and the validity of the conjecture to certain bounds for the optimum values, which we show are false as soon as n = 5.
 Learning From Strategic Agents: Accuracy, Improvement, and Causality 
 In many predictive decision-making scenarios, such as credit scoring and academic testing, a decision-maker must construct a model that accounts for agents' incentives to ``game'' their features in order to receive better decisions. Whereas the strategic classification literature generally assumes that agents' outcomes are not causally dependent on their features (and thus strategic behavior is a form of lying), we join concurrent work in modeling agents' outcomes as a function of their changeable attributes. Our formulation is the first to incorporate a crucial phenomenon: when agents act to change observable features, they may as a side effect perturb unobserved features that causally affect their true outcomes. We consider three distinct desiderata for a decision-maker's model: accurately predicting agents' post-gaming outcomes (accuracy), incentivizing agents to improve these outcomes (improvement), and, in the linear setting, estimating the visible coefficients of the true causal model (causal precision). As our main contribution, we provide the first algorithms for learning accuracy-optimizing, improvement-optimizing, and causal-precision-optimizing linear regression models directly from data, without prior knowledge of agents' possible actions. These algorithms circumvent the hardness result of Miller et al. (2019) by allowing the decision maker to observe agents' responses to a sequence of decision rules, in effect inducing agents to perform causal interventions for free.
 Invariant Rationalization 
 Selective rationalization improves neural network interpretability by identifying a small subset of input features — the rationale — that best explains or supports the prediction. A typical rationalization criterion, i.e. maximum mutual information (MMI), finds the rationale that maximizes the prediction performance based only on the rationale. However, MMI can be problematic because it picks up spurious correlations between the input features and the output.  Instead, we introduce a game-theoretic invariant rationalization criterion where the rationales are constrained to enable the same predictor to be optimal across different environments. We show both theoretically and empirically that the proposed rationales can rule out spurious correlations and generalize better to different test scenarios. The resulting explanations also align better with human judgments. 
 Learning Human Objectives by Evaluating Hypothetical Behavior 
 We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. We propose an algorithm that safely and efficiently learns a model of the user's reward function by posing 'what if?' questions about hypothetical agent behavior. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a state-based 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.
 Deep Molecular Programming: A Natural Implementation of Binary-Weight ReLU Neural Networks 
 Embedding computation in molecular contexts incompatible with traditional electronics is expected to have wide ranging impact in synthetic biology, medicine, nanofabrication and other fields.
A key remaining challenge lies in developing programming paradigms for molecular computation that are well-aligned with the underlying chemical hardware and do not attempt to shoehorn ill-fitting electronics paradigms.
We discover a surprisingly tight connection between a popular class of neural networks (Binary-weight ReLU aka BinaryConnect) and a class of coupled chemical reactions that are absolutely robust to reaction rates.
The robustness of rate-independent chemical computation makes it a promising target for bioengineering implementation.
We show how a BinaryConnect neural network trained in silico using well-founded deep learning optimization techniques, can be compiled to an equivalent chemical reaction network, providing a novel molecular programming paradigm.
We illustrate such translation on widely used IRIS and MNIST datasets.
Our work sets the stage for rich knowledge transfer between neural network and molecular programming communities.
 A Pairwise Fair and Community-preserving Approach to k-Center Clustering 
 Clustering is a foundational problem in machine learning with numerous applications. As machine learning increases in ubiquity as a backend for automated systems, concerns about fairness arise. Much of the current literature on fairness deals with discrimination against protected classes in supervised learning (group fairness). We define a different notion of fair clustering wherein the probability that two points (or a community of points) become separated is bounded by an increasing function of their pairwise distance (or community diameter). We capture the situation where data points represent people who gain some benefit from being clustered together. Unfairness arises when certain points are deterministically separated, either arbitrarily or by someone who intends to harm them as in the case of gerrymandering election districts. In response, we formally define two new types of fairness in the clustering setting, pairwise fairness and community preservation. To explore the practicality of our fairness goals, we devise an approach for extending existing k-center algorithms to satisfy these fairness constraints. In doing so, we show that reasonable approximations can be achieved while maintaining fairness. In experiments, we compare the effectiveness of our approach to classical k-center algorithms/heuristics and explore the tradeoff between optimal clustering and fairness.
 Provable Representation Learning for Imitation Learning via Bi-level Optimization 
 A common strategy in modern learning systems is to learn a representation that is useful for many tasks, a.k.a. representation learning. We study this strategy in the imitation learning setting for Markov decision processes (MDPs) where multiple experts' trajectories are available. We formulate representation learning  as a bi-level optimization problem where the ``outer" optimization tries to learn the joint representation and the ``inner" optimization encodes the imitation learning setup and tries to learn task-specific parameters. We instantiate this framework for the imitation learning settings of behavior cloning and observation-alone. Theoretically, we show using our framework that representation learning can provide sample complexity benefits for imitation learning in both settings. We also provide proof-of-concept experiments to verify our theory.
 Meta-Learning with Shared Amortized Variational Inference 
 In the context of an empirical Bayes model for meta-learning where a subset of model parameters is treated as latent variables, we propose a novel scheme for amortized variational inference. This approach is based on the conditional variational autoencoder framework, which allows to learn the conditional prior distribution over model parameters given limited training data. In our model, we share the same amortized inference network between the prior and posterior distributions over the model parameters. While the posterior inference leverages both the test and the train data, including the labels, the prior inference is based on the train data only. 
We show that in earlier approaches based on Monte-Carlo approximation the prior collapses to a Dirac delta function. In contrast, our variational approach prevents this collapse and preserves uncertainty over the model parameters. We evaluate our approach on standard benchmark datasets, including miniImageNet, and obtain results demonstrating the advantage of our approach over previous work.
 Predictive Multiplicity in Classification 
 Prediction problems often admit competing models that perform almost equally well. This effect – called the multiplicity of good models – challenges how we build and deploy predictive models. In this paper, we study a specific notion of model multiplicity – predictive multiplicity – where competing models assign conflicting predictions. Predictive multiplicity signals irreconcilable differences in the predictions of competing models. In applications such as recidivism prediction and credit scoring, evidence of predictive multiplicity challenges model selection and downstream processes that depend on it. We propose measures to evaluate the severity of predictive multiplicity in classification, and develop integer programming methods to compute them efficiently. We apply our methods to evaluate predictive multiplicity in recidivism prediction problems. Our results show that real-world datasets may admit competing models that assign wildly conflicting predictions, and support the need to measure and report predictive multiplicity in model development.
 Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention 
 Transformers achieve remarkable performance in several tasks but due to their
quadratic complexity, with respect to the input's length, they are
prohibitively slow for very long sequences. To address this limitation, we
express the self-attention as a linear dot-product of kernel feature maps and
make use of the associativity property of matrix products to reduce the
complexity from $\bigO{N^2}$ to $\bigO{N}$, where $N$ is the sequence length.
We show that this formulation permits an iterative implementation that
dramatically accelerates autoregressive transformers and reveals their
relationship to recurrent neural networks. Our \textit{Linear Transformers}
achieve similar performance to vanilla Transformers and they are up to 4000x
faster on autoregressive prediction of very long sequences.
 Latent Variable Modelling with Hyperbolic Normalizing Flows 
 The choice of approximate posterior distributions plays a central role in stochastic variational inference (SVI). One effective solution is the use of normalizing flows \cut{defined on Euclidean spaces} to construct flexible posterior distributions. 
However, one key limitation of existing normalizing flows is that they are restricted to the Euclidean space and are ill-equipped to model data with an underlying hierarchical structure.
To address this fundamental limitation, we present the first extension of normalizing flows to hyperbolic spaces. 
We first elevate normalizing flows to hyperbolic spaces using coupling transforms defined on the tangent bundle, termed Tangent Coupling ($\mathcal{TC}$). 
We further introduce Wrapped Hyperboloid Coupling ($\mathcal{W}\mathbb{H}C$), a fully invertible and learnable transformation that explicitly utilizes the geometric structure of hyperbolic spaces, allowing for expressive posteriors while being efficient to sample from. We demonstrate the efficacy of our novel normalizing flow over hyperbolic VAEs and Euclidean normalizing flows. 
Our approach achieves improved performance on density estimation, as well as reconstruction of real-world graph data, which exhibit a hierarchical structure. 
Finally, we show that our approach can be used to power a generative model over hierarchical data using hyperbolic latent variables. 
 Private Counting from Anonymous Messages: Near-Optimal Accuracy with Vanishing Communication Overhead 
 Differential privacy (DP) is a formal notion for quantifying the privacy loss of algorithms.  Algorithms in the central model of DP achieve high accuracy but make the strongest trust assumptions whereas those in the local DP model make the weakest trust assumptions but incur substantial accuracy loss. The shuffled DP model [Bittau et al 2017, Erlingsson et al 2019, Cheu et al 19] has recently emerged as a feasible middle ground between the central and local models, providing stronger trust assumptions than the former while promising higher accuracies than the latter.

In this paper, we obtain practical communication-efficient algorithms in the shuffled DP model for two basic aggregation primitives: 1) binary summation, and 2) histograms over a moderate number of buckets.  Our algorithms achieve accuracy that is arbitrarily close to that of central DP algorithms with an expected communication per user essentially matching what is needed without any privacy constraints!

We demonstrate the practicality of our algorithms by experimentally evaluating them and comparing their performance to several widely-used protocols such as Randomized Response [Warner 1965] and RAPPOR [Erlingsson et al. 2014].
 Boosted Histogram Transform for Regression 
 In this paper, we propose a boosting algorithm for regression problems called \textit{boosted histogram transform for regression} (BHTR) based on histogram transforms composed of random rotations, stretchings, and translations. From the theoretical perspective, we first prove fast convergence rates for BHTR under the assumption that the target function lies in the spaces $C^{0,\alpha}$. Moreover, if the target function resides in the subspace $C^{1,\alpha}$, by establishing the upper bound of the convergence rate for the boosted regressor, i.e. BHTR, and the lower bound for base regressors, i.e. histogram transform regressors (HTR), we manage to explain the benefits of the boosting procedure. In the experiments, compared with other state-of-the-art algorithms such as gradient boosted regression tree (GBRT), Breiman's forest, and kernel-based methods, our BHTR algorithm shows promising performance on both synthetic and real datasets.
 Supervised learning: no loss no cry 
 Supervised learning requires the specification of a loss function to minimise.
While the theory of admissible losses from both a computational and statistical perspective is well-developed,
these offer a panoply of different choices.
In practice, this choice is typically made in an \emph{ad hoc} manner.
In hopes of making this procedure more principled,
the problem of \emph{learning the loss function} for a downstream task (e.g., classification) has garnered recent interest.
However, works in this area have been generally empirical in nature.

In this paper, 
we revisit the {\sc SLIsotron} algorithm of Kakade et al. (2011) through a novel lens, 
derive a generalisation based on Bregman divergences,
and show how it provides a principled procedure for learning the loss.
In detail, 
we cast
{\sc SLIsotron}
as learning a loss from a family of composite square losses.
By interpreting this through the lens of \emph{proper losses},
we derive a generalisation of {\sc SLIsotron} based on Bregman divergences.
The resulting {\sc BregmanTron} algorithm
jointly learns the loss along with the classifier. 
It comes equipped with a simple guarantee of convergence for the loss it learns, and its set of possible outputs comes with a guarantee of agnostic approximability of Bayes rule.
Experiments indicate that the {\sc BregmanTron} significantly outperforms the {\sc SLIsotron}, and that the loss it learns can be minimized by other algorithms for different tasks, thereby opening the interesting problem of \textit{loss transfer} between domains.
 Harmonic Decompositions of Convolutional Networks 
 We present a description of function spaces and smoothness classes associated with convolutional networks from a reproducing kernel Hilbert space viewpoint. We establish harmonic decompositions of convolutional networks, that is expansions into sums of elementary functions of feature-representation maps implemented by convolutional networks. The elementary functions are related to the spherical harmonics, a fundamental class of special functions on spheres. These harmonic decompositions allow us to characterize the integral operators associated with convolutional networks, and obtain as a result risk bounds for convolutional networks which highlight their behavior in high dimensions.
 Improving the Sample and Communication Complexity for Decentralized Non-Convex Optimization: Joint Gradient Estimation and Tracking 
 Many modern large-scale machine learning problems benefit from decentralized and stochastic optimization. Recent works have shown that utilizing both decentralized computing and local stochastic gradient estimates can outperform state-of-the-art centralized algorithms, in applications involving highly non-convex problems, such as training deep neural networks. 	
	
In this work, we propose a decentralized stochastic algorithm to deal with certain smooth non-convex problems where there are $m$ nodes in the system, and each node has a large number of samples (denoted as $n$). Differently from the majority of the existing decentralized learning algorithms for either stochastic or finite-sum problems, our focus is given to {\it both} reducing the total communication rounds among the nodes, while accessing the minimum number of local data samples. In particular, we propose an algorithm named D-GET (decentralized gradient estimation and tracking), which jointly performs decentralized gradient estimation (which estimates the local gradient using a subset of local samples) {\it and} gradient tracking (which tracks the global full gradient using local estimates). We show that, to achieve certain $\epsilon$  stationary solution of the deterministic finite sum problem, the proposed algorithm achieves an $\mathcal{O}(mn^{1/2}\epsilon^{-1})$ sample complexity and an $\mathcal{O}(\epsilon^{-1})$ communication complexity. These bounds significantly improve upon the best existing bounds of $\mathcal{O}(mn\epsilon^{-1})$ and $\mathcal{O}(\epsilon^{-1})$, respectively. Similarly, for online problems, the proposed method achieves an $\mathcal{O}(m \epsilon^{-3/2})$ sample complexity and  an $\mathcal{O}(\epsilon^{-1})$ communication complexity, while the best existing bounds are  $\mathcal{O}(m\epsilon^{-2})$ and $\mathcal{O}(\epsilon^{-2})$.
 Preselection Bandits 
 In this paper, we introduce the Preselection Bandit problem, in which the learner preselects a subset of arms (choice alternatives) for a user, which then chooses the final arm from this subset. The learner is not aware of the user's preferences, but can learn them from observed choices. In our concrete setting, we allow these choices to be stochastic and model the user's actions by means of the Plackett-Luce model. The learner's main task is to preselect subsets that eventually lead to highly preferred choices. To formalize this goal, we introduce a reasonable notion of regret and derive lower bounds on the expected regret. Moreover, we propose algorithms for which the upper bound on expected regret matches the lower bound up to a logarithmic term of the time horizon. 
 Overparameterization hurts worst-group accuracy with spurious correlations 
 Increasing model capacity well beyond the point of zero training error has been observed to improve average test accuracy. However, such overparameterized models have been recently shown to obtain low worst-group accuracy --- i.e., low accuracy on atypical groups of test examples --- when there are spurious correlations that hold for the majority of training examples. We show on two image datasets that in contrast to average accuracy, overparameterization hurts worst-group accuracy in the presence of spurious correlations. We replicate this surprising phenomenon in a synthetic example and identify properties of the data distribution that induce the detrimental effect of overparameterization on worst-group accuracy. Our analysis leads us to show that a counter-intuitive approach of subsampling the majority group yields high worst-group accuracy in the overparameterized regime, whereas upweighting the minority does not. Our results suggest that when it comes to achieving high worst-group accuracy, there is a tension between using overparameterized models vs. using all of the training data.
 Few-shot Relation Extraction via Bayesian Meta-learning on Task Graphs 
 This paper studies few-shot relation extraction, which aims at predicting the relation for a pair of entities in a sentence by training with a few labeled examples in each relation. To more effectively generalize to new relations/tasks, in this paper we study the relationships between different tasks and propose to leverage a global task graph. We propose a novel Bayesian meta-learning approach to effectively learn the posterior distributions of the prototype vectors of tasks, where the initial prior of the prototype vectors is parameterized with a graph neural network on the global task graph. Moreover, to effectively optimize the posterior distributions of the prototype vectors, we propose to use the stochastic gradient Langevin dynamic, which can be related to the MAML algorithm but is able to handle the uncertainty of the prototype vectors. The whole framework can be effectively and efficiently optimized in an end-to-end fashion. Experiments on two benchmark datasets prove the effectiveness of our proposed approach against competitive baselines in both the few-shot and zero-shot settings. 
 Improving generalization by controlling label-noise information in neural network weights 
 In the presence of noisy or incorrect labels, neural networks have the undesirable tendency to memorize information about the noise.
Standard regularization techniques such as dropout, weight decay or data augmentation sometimes help, but do not prevent this behavior.
If one considers neural network weights as random variables that depend on the data and stochasticity of training, the amount of memorized information can be quantified with the Shannon mutual information between weights and the vector of all training labels given inputs, $I(w : \Y \mid \X)$.
We show that for any training algorithm, low values of this term correspond to reduction in memorization of label-noise and better generalization bounds.
To obtain these low values, we propose training algorithms that employ an auxiliary network that predicts gradients in the final layers of a classifier without accessing labels.
We illustrate the effectiveness of our approach on versions of MNIST, CIFAR-10, and CIFAR-100 corrupted with various noise models, and on a large-scale dataset Clothing1M that has noisy labels.
 Boosting for Control of Dynamical Systems 
 We study the question of how to aggregate controllers for dynamical systems in order to improve their performance. To this end, we propose a framework of boosting for online control. Our main result is an efficient boosting algorithm that combines weak controllers into a provably more accurate one. Empirical evaluation on a host of control settings supports our theoretical findings. 

 Adaptive Adversarial Multi-task Representation Learning 
 Adversarial Multi-task Representation Learning (AMTRL) methods are able to boost the performance of Multi-task Representation Learning (MTRL) models. However, the theoretical mechanism behind AMTRL is less investigated. To fill this gap, we study the generalization error bound of AMTRL through the lens of Lagrangian duality . Based on the duality, we proposed an novel adaptive AMTRL algorithm which improves the performance of original AMTRL methods. The extensive experiments back up our theoretical analysis and validate the superiority of our proposed algorithm.
 Scalable Identification of Partially Observed Systems with Certainty-Equivalent EM 
 System identification is a key step for model-based control, estimator design, and output prediction. This work considers the offline identification of partially observed nonlinear systems. We empirically show that the certainty-equivalent approximation to expectation-maximization can be a reliable and scalable approach for high-dimensional deterministic systems, which are common in robotics. We formulate certainty-equivalent expectation-maximization as block coordinate-ascent, and provide an efficient implementation. The algorithm is tested on a simulated system of coupled Lorenz attractors, demonstrating its ability to identify high-dimensional systems that can be intractable for particle-based approaches. Our approach is also used to identify the dynamics of an aerobatic helicopter. By augmenting the state with unobserved fluid states, a model is learned that predicts the acceleration of the helicopter better than state-of-the-art approaches. The codebase for this work is available at https://github.com/sisl/CEEM.
 From Sets to Multisets: Provable Variational  Inference for Probabilistic Integer Submodular Models 
 Submodular functions have been studied extensively in machine learning and data mining. In particular, the optimization of submodular functions over  the integer lattice has recently attracted much interest, because this domain relates naturally to many practical problem settings, such as multilabel graph cut, budget allocation and revenue maximization with discrete assignments. In contrast, the use of these functions for probabilistic modeling has received surprisingly little attention so far. 
In this work, we firstly propose the Generalized Multilinear Extension, a continuous DR-Submodular extension for integer submodular functions. We study central properties of this extension and formulate a new probabilistic model which is defined through integer submodular functions. Then, we introduce a method to perform approximate inference for those class of models. Finally, we demonstrate its effectiveness and viability on several real-world social connection graph datasets with integer submodular objectives.
 Linear bandits with Stochastic Delayed Feedback 
 Stochastic linear bandits are a natural and well-studied model for structured exploration/exploitation problems and are widely used in applications such as on-line marketing and recommendation.
One of the main challenges faced by practitioners hoping to apply existing algorithms is that usually the feedback is randomly delayed and delays are only partially observable.
For example, while a purchase is usually observable some time after the display, the decision of not buying is never explicitly sent to the system.
In other words, the learner only observes delayed positive events.
We formalize this problem as a novel stochastic delayed linear bandit and propose OTFLinUCB and OTFLinTS, two computationally efficient algorithms able to integrate new information as it
becomes available and to deal with the permanently censored feedback. We prove optimal O(d\sqrt{T}) bounds on the regret of the first algorithm and study the dependency on delay-dependent parameters.
Our model, assumptions and results are validated by experiments on simulated and real data.
 Explaining Groups of Points in Low-Dimensional Representations 
 A common workflow in data exploration is to learn a low-dimensional representation of the data, identify groups of points in that representation, and examine the differences between the groups to determine what they represent.  We treat this as an interpretable machine learning problem by leveraging the model that learned the low-dimensional representation to help identify the key differences between the groups.  To solve this problem, we introduce a new type of explanation, a Global Counterfactual Explanation (GCE), and our algorithm, Transitive Global Translations (TGT), for computing GCEs.   TGT identifies the differences between each pair of groups using compressed sensing but constrains those pairwise differences to be consistent among all of the groups.  Empirically, we demonstrate that TGT is able to identify explanations that accurately explain the model while being relatively sparse, and that these explanations match real patterns in the data. 
 Differentiable Likelihoods for Fast Inversion of 'Likelihood-Free' Dynamical Systems 
 Likelihood-free (a.k.a. simulation-based) inference problems are inverse problems with expensive, or intractable, forward models. ODE inverse problems are commonly treated as likelihood-free, as their forward map has to be numerically approximated by an ODE solver. This, however, is not a fundamental constraint but just a lack of functionality in classic ODE solvers, which do not return a likelihood but a point estimate. To address this shortcoming, we employ Gaussian ODE filtering (a probabilistic numerical method for ODEs) to construct a local Gaussian approximation to the likelihood. This approximation yields tractable estimators for the gradient and Hessian of the (log-)likelihood. Insertion of these estimators into existing gradient-based optimization and sampling methods engenders new solvers for ODE inverse problems. We demonstrate that these methods outperform standard likelihood-free approaches on three benchmark-systems.
 Learning Structured Latent Factors from Dependent Data:A Generative Model Framework from Information-Theoretic Perspective 
 Learning controllable and generalizable representation of multivariate data with desired structural properties remains a fundamental problem in machine learning.
In this paper, we present a novel framework for learning generative models with various underlying structures in the latent space.
We represent the inductive bias in the form of mask variables to model the dependency structure in the graphical model and extend the theory of multivariate information bottleneck to enforce it.
Our model provides a principled approach to learn a set of semantically meaningful latent factors that reflect various types of desired structures like capturing correlation or encoding invariance, while also offering the flexibility to automatically estimate the dependency structure from data.
We show that our framework unifies many existing generative models and can be applied to a variety of tasks including multi-modal data modeling, algorithmic fairness, and invariant risk minimization.
 Gradient-free Online Learning in Continuous Games with Delayed Rewards 
 Motivated by applications to online advertising and recommender systems, we consider a game-theoretic model with delayed rewards and asynchronous, payoff-based feedback. In contrast to previous work on delayed multi-armed bandits, we focus on games with continuous action spaces, and we examine the long-run behavior of strategic agents that follow a no-regret learning policy (but are otherwise oblivious to the game being played, the objectives of their opponents, etc.). To account for the lack of a consistent stream of information (for instance, rewards can arrive out of order and with an a priori unbounded delay), we introduce a gradient-free learning policy where payoff information is placed in a priority queue as it arrives. Somewhat surprisingly, we find that under a standard diagonal concavity assumption, the induced sequence of play converges to Nash Equilibrium (NE) with probability 1, even if the delay between choosing an action and receiving the corresponding reward is unbounded.
 Dynamics of Deep Neural Networks and  Neural Tangent Hierarchy 
 The evolution of a deep neural network trained by the gradient descent can be described by its neural tangent kernel (NTK) as introduced in \cite{jacot2018neural}, where it was proven that in the infinite width limit the NTK converges to an explicit limiting kernel and it stays constant during training. The NTK was also implicit in some other recent papers \cite{du2018gradient1,du2018gradient2,arora2019fine}. In the overparametrization regime, a fully-trained deep neural network is indeed equivalent to the kernel regression predictor using the limiting NTK. And the gradient descent achieves zero training loss for a deep overparameterized neural network. However, it was observed in \cite{arora2019exact} that there is a performance gap between the kernel regression using the limiting NTK and the deep neural networks. This performance gap  is likely to  originate   from the change of the NTK along training due to the finite width effect. The change of the NTK along the  training is central to describe the generalization features of deep neural networks. 

In the current paper, we study the dynamic of the NTK for finite width deep fully-connected neural networks. We derive an  infinite hierarchy of ordinary differential equations, the neural tangent hierarchy (NTH) which captures the gradient descent  dynamic of the deep neural network. Moreover, under certain conditions 
on the neural network width and the data set dimension,  we prove  that the truncated hierarchy of NTH approximates the dynamic of the NTK up to arbitrary precision. This description makes it possible to directly study the change of the NTK for deep neural networks, and sheds light on the observation that deep neural networks outperform kernel regressions using the corresponding limiting NTK. 
 Projection-free Distributed Online Convex Optimization with $O(\sqrt{T})$ Communication Complexity 
 To deal with complicated constraints via locally light computation in distributed online learning, recent study has presented a projection-free algorithm called distributed online conditional gradient (D-OCG), and achieved an $O(T^{3/4})$ regret bound, where $T$ is the number of prediction rounds. However, in each round, the local learners of D-OCG need to communicate with their neighbors to share the local gradients, which results in a high communication complexity of $O(T)$. In this paper, we first propose an improved variant of D-OCG, namely D-BOCG, which enjoys an $O(T^{3/4})$ regret bound with only $O(\sqrt{T})$ communication complexity. The key idea is to divide the total prediction rounds into $\sqrt{T}$ equally-sized blocks, and only update the local learners in the beginning of each block by performing iterative linear optimization steps. Furthermore, to handle the more challenging bandit setting, in which only the loss value is available, we incorporate the classical one-point gradient estimator into D-BOCG, and obtain similar theoretical guarantees.
 Automated Synthetic-to-Real Generalization 
 Models trained on synthetic images often face degraded generalization to real data. To remedy such domain gaps, synthetic training often starts with ImageNet pretrained models in domain generalization and adaptation as they contain the representation from real images. However, the role of ImageNet representation is seldom discussed despite common practices that leverage this knowledge implicitly to maintain generalization ability. An example is the careful hand tuning of learning rates across different network layers which can be laborious and non-scalable. We treat this as a learning without forgetting problem and propose a learning-to-optimize (L2O) method to automate layer-wise learning rates. With comprehensive experiments, we demonstrate that the proposed method can significantly improve the synthetic-to-real generalization performance without seeing and training on real data, while benefiting downstream tasks such as domain adaptation.
 Linear Convergence of Randomized Primal-Dual Coordinate Method for Large-scale Linear Constrained Convex Programming 
 Linear constrained convex programming (LCCP) has many practical applications, including support vector machine (SVM) and machine learning portfolio (MLP) problems. We propose the randomized primal-dual coordinate (RPDC) method, a randomized coordinate extension of the first-order primal-dual method by Cohen and Zhu, 1984 and Zhao and Zhu, 2019, to solve LCCP. We randomly choose a block of variables based on the uniform distribution and apply linearization and a Bregman-like function (core function) to the selected block to obtain simple parallel primal-dual decomposition for LCCP. We establish almost surely convergence and expected O(1/t) convergence rate. Under global strong metric subregularity, we establish the linear convergence of RPDC. Both SVM and MLP problems satisfy the global strong metric subregularity condition under some reasonable conditions. Finally, we discuss the implementation details of RPDC and present numerical experiments on SVM and MLP problems to verify the linear convergence.
 XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning 
 Learning novel concepts while preserving prior knowledge is a long-standing challenge in machine learning. The challenge gets greater when a novel task is given with only a few labeled examples, a problem known as incremental few-shot learning. We propose XtarNet, which learns to extract task-adaptive representation (TAR) for facilitating incremental few-shot learning. The method utilizes a backbone network pretrained on a set of base categories while also employing additional modules that are meta-trained across episodes. Given a new task, the novel feature extracted from the meta-trained modules is mixed with the base feature obtained from the pretrained model. The process of combining two different features provides TAR and is also controlled by meta-trained modules. The TAR contains effective information for classifying both novel and base categories. The base and novel classifiers quickly adapt to a given task by utilizing the TAR. Experiments on standard image datasets indicate that XtarNet achieves state-of-the-art incremental few-shot learning performance. The concept of TAR can also be used in conjunction with existing incremental few-shot learning methods; extensive simulation results in fact show that applying TAR enhances the known methods significantly.
 p-Norm Flow Diffusion for Local Graph Clustering 
 Local graph clustering and the closely related seed set expansion problem are primitives on graphs that are central to a wide range of analytic and learning tasks such as local clustering, community detection, nodes ranking and feature inference. Prior work on local graph clustering mostly falls into two categories with numerical and combinatorial roots respectively. 
In this work, we draw inspiration from both fields and propose a family of convex optimization formulations based on the idea of diffusion with $p$-norm network flow for $p\in (1,\infty)$. 

In the context of local clustering, we characterize the optimal solutions for these optimization problems and show their usefulness in finding low conductance cuts around input seed set. In particular, we achieve quadratic approximation of conductance in the case of $p=2$ similar to the Cheeger-type bounds of spectral methods, constant factor approximation when $p\rightarrow\infty$ similar to max-flow based methods, and a smooth transition for general $p$ values in between. Thus, our optimization formulation can be viewed as bridging the numerical and combinatorial approaches, and we can achieve the best of both worlds in terms of speed and noise robustness. 

We show that the proposed problem can be solved in strongly local running time for $p\ge 2$ and conduct empirical evaluations on both synthetic and real-world graphs to illustrate our approach compares favorably with existing methods.
 Time-aware Large Kernel Convolutions 
 To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.
 On Efficient Constructions of Checkpoints 
 Efficient construction of checkpoints/snapshots is a critical tool for training and diagnosing deep learning models. In this paper, we propose a lossy compression scheme for checkpoint constructions (called LC-Checkpoint). LC-Checkpoint simultaneously maximizes the compression rate and optimizes the recovery speed, under the assumption that SGD is used to train the model. LC-Checkpoint uses quantization and priority promotion to store the most crucial information for SGD to recover, and then uses a Huffman coding to leverage the non-uniform distribution of the gradient scales. Our extensive experiments show that LC-Checkpoint achieves a compression rate up to 28× and recovery speedup up to 5.77× over a state-of-the-art algorithm (SCAR).
 Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills 
 Acquiring abilities in the absence of a task-oriented reward function is at the frontier of reinforcement learning research. This problem has been studied through the lens of \textit{empowerment}, which draws a connection between option discovery and information theory. Information-theoretic skill discovery methods have garnered much interest from the community, but little research has been conducted in understanding their limitations.  Through theoretical analysis and empirical evidence, we show that existing algorithms suffer from a common limitation -- they discover options that provide a poor coverage of the state space. In light of this, we propose \textit{Explore, Discover and Learn} (EDL), an alternative approach to information-theoretic skill discovery. Crucially, EDL optimizes the same information-theoretic objective derived from the empowerment literature, but addresses the optimization problem using different machinery. We perform an extensive evaluation of skill discovery methods on controlled environments and show that EDL offers significant advantages, such as overcoming the coverage problem, reducing the dependence of learned skills on the initial state, and allowing the user to define a prior over which behaviors should be learned.
 Efficient proximal mapping of the path-norm regularizer of shallow networks 
 We demonstrate two new important properties of the path-norm regularizer for shallow neural networks. First, despite its non-smoothness and non-convexity it allows a closed form proximal operator which can be efficiently computed, allowing the use of stochastic proximal-gradient-type methods for regularized empirical risk minimization. Second, it provides an upper bound on the Lipschitz constant of the network, which is tighter than the trivial layer-wise product of Lipschitz constants, motivating its use for training networks robust to adversarial perturbations. Finally, in practical experiments we show that it provides a better robustness-accuracy trade-off when compared to $\ell_1$-norm regularization or training with a layer-wise constrain of the Lipschitz constant.
 Concept Bottleneck Models 
 We seek to learn models that support interventions on high-level concepts: would the model predict severe arthritis if it thought there was a bone spur in the x-ray? State-of-the-art models today do not typically support manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts (provided at training time), and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") and bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.
 CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information 
 Mutual information (MI) minimization has gained considerable interests in various machine learning tasks. However, estimating and minimizing MI in high-dimensional spaces remains a challenging problem, especially when only samples, rather than distribution forms, are accessible. Previous works mainly focus on MI lower bound approximations, which is not applicable to MI minimization problems. In this paper, we propose a novel Contrastive Log-ratio Upper Bound (CLUB) of mutual information. We provide theoretical analysis to the properties of CLUB and its variational approximation. Based on this upper bound, we introduce an accelerated MI minimization training scheme, which bridges MI minimization with contrastive learning and negative sampling. Simulation studies on Gaussian and Bernoulli distributions show the reliable estimation ability of CLUB. Real-world MI minimization experiments, including domain adaptation and information bottleneck, further demonstrate the effectiveness of the proposed method.
 Non-convex Learning via Replica Exchange Stochastic Gradient MCMC 
 Replica exchange method (RE), also known as parallel tempering, is an important technique for accelerating the convergence of the conventional Markov Chain Monte Carlo (MCMC) algorithms. However, such a method requires the evaluation of the energy function based on the full dataset and is not scalable to big data. The na\"ive implementation of RE in mini-batch settings introduces large biases, which cannot be directly extended to the stochastic gradient MCMC (SG-MCMC), the standard sampling method for simulating from deep neural networks (DNNs). In this paper, we propose an adaptive replica exchange SG-MCMC (reSG-MCMC) to automatically correct the bias and study the corresponding properties. The analysis implies an acceleration-accuracy trade-off in the numerical discretization of a Markov jump process in a stochastic environment. Empirically, we test the algorithm through extensive experiments on various setups and obtain the state-of-the-art results on CIFAR10, CIFAR100, and SVHN in both supervised learning and semi-supervised learning tasks.
 Layered Sampling for Robust Optimization Problems 
 In real world,  our datasets often contain outliers. Moreover, the outliers can seriously affect the final machine learning result. Most existing algorithms for handling outliers take high time complexities ({\em e.g.} quadratic or cubic complexity). {\em Coreset} is a popular approach for compressing data so as to speed up the optimization algorithms. However, the current coreset methods cannot be easily extended to handle the case with outliers. In this paper, we propose a new variant of coreset technique,  {\em layered sampling}, to deal with two fundamental robust optimization problems: {\em $k$-median/means clustering with outliers} and {\em linear regression with outliers}. This new coreset method is in particular suitable to speed up the iterative algorithms (which often improve the solution within a local range) for those robust optimization problems. Moreover, our method is easy to be implemented in practice. We expect that our framework of layered sampling will be applicable to  other robust optimization problems.
 Distributed Online Optimization over a Heterogeneous Network 
 In distributed online optimization over a computing network with heterogeneous nodes, slow nodes can adversely affect the progress of fast nodes, leading to drastic slowdown of the overall convergence process. To address this issue, we consider a new algorithm termed Distributed Any-Batch Mirror Descent (DABMD), which is based on distributed Mirror Descent but uses a fixed per-round computing time to limit the waiting by fast nodes to receive information updates from slow nodes. DABMD is characterized by varying minibatch sizes across nodes. It is applicable to a broader range of problems compared with existing distributed online optimization methods such as those based on dual averaging, and it accommodates time-varying network topology. We study two versions of DABMD, depending on whether the computing nodes average their primal variables via single or multiple consensus iterations. We show that both versions provide strong theoretical performance guarantee, by deriving upperbounds on their expected dynamic regret, which capture the variability in minibatch sizes. Our experimental results show substantial reduction in cost and acceleration in convergence compared with the known best alternative.
 Real-Time Optimisation for Online Learning in Auctions 
 In display advertising, a small group of sellers and bidders face each other in up to 10^{12} auctions a day. In this context, revenue maximisation via monopoly price learning is a high-value problem for sellers. By nature, these auctions are online and produce a very high frequency stream of data. This results in a computational strain that requires algorithms be real-time. Unfortunately, existing methods, inherited from the batch setting, suffer O(\sqrt(t)) time/memory complexity at each update, prohibiting their use. In this paper, we provide the first algorithm for online learning of monopoly prices in online auctions whose update is constant in time and memory.

 Evaluating Lossy Compression Rates of Deep Generative Models 
 Deep generative models have achieved remarkable progress in recent years. Despite this progress, quantitative evaluation and comparison of generative models remains as one of the important challenges. One of the most popular metrics for evaluating generative models is the log-likelihood. While the direct computation of log-likelihood can be intractable, it has been recently shown that the log-likelihood of some of the most interesting generative models such as variational autoencoders (VAE) or generative adversarial networks (GAN) can be efficiently estimated using annealed importance sampling (AIS). In this work, we argue that the log-likelihood metric by itself cannot represent all the different performance characteristics of generative models, and propose to use rate distortion curves to evaluate and compare deep generative models. We show that we can approximate the entire rate distortion curve using one single run of AIS for roughly the same computational cost as a single log-likelihood estimate. We evaluate lossy compression rates of different deep generative models such as VAEs, GANs (and its variants) and adversarial autoencoders (AAE) on MNIST and CIFAR10, and arrive at a number of insights not obtainable from log-likelihoods alone.
 Approximating Stacked and Bidirectional Recurrent Architectures with the Delayed Recurrent Neural Network 
 Recent work has shown that topological enhancements to recurrent neural networks (RNNs) can increase their expressiveness and representational capacity. Two popular enhancements are stacked RNNs, which increases the capacity for learning non-linear functions, and bidirectional processing, which exploits acausal information in a sequence. In this work, we explore the delayed-RNN, which is a single-layer RNN that has a delay between the input and output. We prove that a weight-constrained version of the delayed-RNN is equivalent to a stacked-RNN. We also show that the delay gives rise to partial acausality, much like bidirectional networks. Synthetic experiments confirm that the delayed-RNN can mimic bidirectional networks, solving some acausal tasks similarly, and outperforming them in others. Moreover, we show similar performance to bidirectional networks in a real-world natural language processing task. These results suggest that delayed-RNNs can approximate topologies including stacked RNNs, bidirectional RNNs, and stacked bidirectional RNNs -- but with equivalent or faster runtimes for the delayed-RNNs.
 Online Control of the False Coverage Rate and False Sign Rate 
 The reproducibility debate has caused a renewed interest in changing how one reports uncertainty, from $p$-value for testing a null hypothesis to a confidence interval (CI) for the corresponding parameter. When CIs for multiple selected parameters are being reported, the analog of the false discovery rate (FDR) is the false coverage rate (FCR), which is the expected ratio of number of reported CIs failing to cover their respective parameters to the total number of reported CIs. 
Here, we consider the general problem of FCR control in the online setting, where there is an infinite sequence of fixed unknown parameters ordered by time. 
While much progress has been made in online testing, a procedure controlling the FDR does not automatically translate to a (nontrivial) procedure that controls the FCR. Therefore, the problem of online FCR control needs to be treated separately. 
We propose a novel solution to the problem which only requires the scientist to be able to construct a marginal CI at any given level. If so desired, our framework also yields online FDR control as a special case, or even online sign-classification procedures that control the false sign rate (FSR). Last, all of our methodology applies equally well to prediction intervals, having particular implications for selective conformal inference.
 Convolutional Kernel Networks for Graph-Structured Data 
 We introduce a family of multilayer graph kernels and establish new links between graph convolutional neural networks and kernel methods. Our approach generalizes convolutional kernel networks to graph-structured data, by representing graphs as a sequence of feature maps, where each node carries information about local graph substructures. On the one hand, the kernel point of view offers an unsupervised, expressive, and easy-to-regularize data representation, which is useful when limited samples are available. On the other hand, our model can also be trained end-to-end on large-scale data, leading to new types of graph convolutional neural networks.  We show that our method achieves state-of-the-art performance on several graph classification benchmarks, while offering simple model interpretation.

 DRWR: A Differentiable Renderer without Rendering for Unsupervised 3D Structure Learning from Silhouette Images 
 Differentiable renderers have been used successfully for unsupervised 3D structure learning from 2D images because they can bridge the gap between 3D and 2D. To optimize 3D shape parameters, current renderers rely on pixel-wise losses between rendered images of 3D reconstructions and ground truth images from corresponding viewpoints. Hence they require interpolation of the recovered 3D structure at each pixel, visibility handling, and optionally evaluating a shading
model. In contrast, here we propose a Differentiable Renderer Without Rendering (DRWR) that omits these steps. DRWR only relies on a simple but effective loss that evaluates how well the projections of reconstructed 3D point clouds cover the ground truth object silhouette. Specifically, DRWR employs a smooth silhouette loss to pull the projection of each individual 3D point inside the object silhouette, and a structure-aware repulsion loss to push each pair of projections that fall inside the silhouette far away from each other. Although we omit surface interpolation, visibility handling, and shading, our results demonstrate that DRWR achieves state-of-the-art accuracies under widely used benchmarks, outperforming previous methods both qualitatively and quantitatively. In addition, our training times are significantly lower due to the simplicity of DRWR.
 Skew-Fit: State-Covering Self-Supervised Reinforcement Learning 
 Autonomous agents that must exhibit flexible and broad capabilities will need to be equipped with large repertoires of skills. Defining each skill with a manually-designed reward function limits this repertoire and imposes a manual engineering burden. Self-supervised agents that set their own goals can automate this process, but designing appropriate goal setting objectives can be difficult, and often involves heuristic design decisions. In this paper, we propose a formal exploration objective for goal-reaching policies that maximizes state coverage. We show that this objective is equivalent to maximizing goal reaching performance together with the entropy of the goal distribution, where goals correspond to full state observations. To instantiate this principle, we present an algorithm called Skew-Fit for learning a maximum-entropy goal distributions. We prove that, under regularity conditions, Skew-Fit converges to a uniform distribution over the set of valid states, even when we do not know this set beforehand. Our experiments show that combining Skew-Fit for learning goal distributions with existing goal-reaching methods outperforms a variety of prior methods on open-sourced visual goal-reaching tasks. Moreover, we demonstrate that Skew-Fit enables a real-world robot to learn to open a door, entirely from scratch, from pixels, and without any manually-designed reward function.
 Optimizing for the Future in Non-Stationary MDPs 
 Most reinforcement learning methods are based upon the key assumption that the transition dynamics and reward functions are fixed, that is, the underlying Markov decision process (MDP) is stationary. However, in many practical real-world applications, this assumption is clearly violated. We discuss how current methods can have inherent limitations for non-stationary MDPs, and therefore searching a policy that is good for the future, unknown MDP, requires rethinking the optimization paradigm. To address this problem, we develop a method that builds upon ideas from both counter-factual reasoning and curve-fitting to proactively search for a good future policy, without ever modeling the underlying non-stationarity. The effectiveness of the proposed method is demonstrated on problems motivated by real-world applications.
 Reinforcement Learning for Molecular Design Guided by Quantum Mechanics 
 Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. A limitation of existing approaches is that they work with molecular graphs and thus ignore the location of atoms in space, which restricts them to (1) generating single organic molecules and (2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in 3D space, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical calculations. To enable progress towards designing molecules in 3D space, we introduce MolGym, an RL environment comprising several molecular design tasks alongside with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space.
 A Game Theoretic Perspective on Model-Based Reinforcement Learning 
 We illustrate how game theory is a good framework to understand model-based reinforcement learning (MBRL). We point out that a large class of MBRL algorithms can be viewed as a game between two players: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. Their goals need not be aligned, and are often conflicting. We show that stable algorithms for MBRL can be derived by considering a Stackelberg game between the two players. This formulation gives rise to two natural schools of MBRL algorithms based on which player is chosen as the leader in the Stackelberg game, and together encapsulate many existing MBRL algorithms. Through experiments on a suite of continuous control tasks, we validate that algorithms based on our framework lead to stable and sample efficient learning.
 Concise Explanations of Neural Networks using Adversarial Training 
 We show new connections between adversarial learning and explainability for deep neural networks (DNNs). One form of explanation of the output of a neural network model in terms of its input features, is a vector of feature-attributions, which can be generated by various techniques such as Integrated Gradients (IG), DeepSHAP, LIME, and CXPlain. Two desirable characteristics of an attribution-based explanation are: (1) \textit{sparseness}: the attributions of irrelevant or weakly relevant features should be negligible, thus resulting in \textit{concise} explanations in terms of the significant features, and (2) \textit{stability}: it should not vary significantly within a small local neighborhood of the input. Our first contribution is a theoretical exploration of how these two properties (when using IG-based attributions) are related to adversarial training, for a class of 1-layer networks (which includes logistic regression models for binary and multi-class classification); for these networks we show that (a) adversarial training using an $\ell_\infty$-bounded adversary produces models with sparse attribution vectors, and (b) natural model-training while encouraging stable explanations (via an extra term in the loss function), is equivalent to adversarial training.  Our second contribution is an empirical verification of phenomenon (a), which we show, somewhat surprisingly, occurs \textit{not only in 1-layer networks, but also DNNs trained on standard image datasets}, and extends beyond IG-based attributions,  to those based on DeepSHAP:  adversarial training with $\linf$-bounded perturbations yields significantly sparser attribution vectors, with little degradation in performance on natural test data, compared to natural training. Moreover, the sparseness of the attribution vectors is significantly better than that achievable via $\ell_1$-regularized natural training.

 Randomized Smoothing of All Shapes and Sizes 
 Randomized smoothing is the current state-of-the-art defense with provable robustness against $\ell_2$ adversarial attacks. Many works have devised new randomized smoothing schemes for other metrics, such as $\ell_1$ or $\ell_\infty$; however, substantial effort was needed to derive such new guarantees. This begs the question: can we find a general theory for randomized smoothing?

We propose a novel framework for devising and analyzing randomized smoothing schemes, and validate its effectiveness in practice. Our theoretical contributions are: (1) we show that for an appropriate notion of "optimal", the optimal smoothing distributions for any "nice" norms have level sets given by the norm's *Wulff Crystal*; (2) we propose two novel and complementary methods for deriving provably robust radii for any smoothing distribution; and, (3) we show fundamental limits to current randomized smoothing techniques via the theory of *Banach space cotypes*. By combining (1) and (2), we significantly improve the state-of-the-art certified accuracy in $\ell_1$ on standard datasets. Meanwhile, we show using (3) that with only label statistics under random input perturbations, randomized smoothing cannot achieve nontrivial certified accuracy against perturbations of $\ell_p$-norm $\Omega(\min(1, d^{\frac{1}{p} - \frac{1}{2}}))$, when the input dimension $d$ is large. We provide code in github.com/tonyduan/rs4a.
 MoNet3D: Towards Accurate Monocular 3D Object Localization in Real Time 
 Monocular multi-object detection and localization in 3D space has been proven to be a challenging task. The MoNet3D algorithm is a novel and effective framework that can predict the 3D position of each object in a monocular image, and draw a 3D bounding box on each object. The MoNet3D method incorporates the prior knowledge of spatial geometric correlation of neighboring objects into the deep neural network training process, in order to improve the accuracy of 3D object localization. Experiments over the KITTI data set show that the accuracy of predicting the depth and horizontal coordinate of the object in 3D space can reach 96.25% and 94.74%,  respectively. Meanwhile, the method can realize the real-time image processing capability of 27.85 FPS. Our demo and code will be published on GitHub when the paper is accepted.
 Scaling up Hybrid Probabilistic Inference with Logical and Arithmetic Constraints via Message Passing 
 Weighted model integration (WMI) is a very appealing framework for probabilistic inference: it allows to express the complex dependencies of real-world scenarios where variables are both continuous and discrete, via the language of Satisfiability Modulo Theories (SMT), as well as to compute probabilistic queries with complex logical and arithmetic constraints. Yet, existing WMI solvers are not ready to scale to these problems. They either ignore the intrinsic dependency structure of the problem at all, or they are limited to too restrictive structures. To narrow this gap, we derive a factorized formalism of WMI enabling us to devise a scalable WMI solver based on message passing, MP-WMI. Namely MP-WMI is the first WMI solver which allows to: 1) perform exact inference on the full class of tree-structured WMI problems; 2) compute all the marginal densities in linear time; 3) amortize inference for any query conforming to the problem structure. Experimental results show that our solver dramatically outperforms the existing WMI solvers on a large set of benchmarks.
 Acceleration through spectral density estimation 
 We develop a framework for designing optimal optimization methods in terms of their average-case runtime. This yields a new class of methods that achieve acceleration through a model of the Hessian's expected spectral density. We develop explicit algorithms for the uniform, Marchenko-Pastur and exponential distribution. These methods are momentum-based gradient algorithms whose hyper-parameters can be estimated cheaply using only the norm and the trace of the Hessian, in stark contrast with classical accelerated methods like Nesterov acceleration and Polyak momentum that require knowledge of the Hessian's largest and smallest singular value. Empirical results on quadratic, logistic regression and neural network show the proposed methods always match and in many cases significantly improve upon classical accelerated methods.

 Forecasting sequential data using Consistent Koopman Autoencoders 
 Neural networks are widely used for processing time series data, yet such models often ignore the underlying physical structures in the input measurements. Recently Koopman-based models have been suggested, as a promising alternative to recurrent neural networks,  for forecasting complex high-dimensional dynamical systems. We propose a novel Consistent Koopman Autoencoder that exploits the forward and backward dynamics to achieve long time predictions. Key to our approach is a new analysis where we unravel the interplay between invertible dynamics and their associated Koopman operators. Our architecture and loss function are interpretable from a physical viewpoint, and the computational requirements are comparable to other baselines. We evaluate the proposed algorithm on a wide range of high-dimensional problems, from simple canonical systems such as linear and nonlinear oscillators, to complex ocean dynamics and fluid flows on a curved domain. Overall, our results show that our model yields accurate estimates for significant prediction horizons, while being robust to noise in the input data.
 Thompson Sampling Algorithms for Mean-Variance Bandits 
 The multi-armed bandit (MAB) problem is a classical learning task that exemplifies the exploration-exploitation tradeoff.  However, standard formulations do not take into account risk. In online decision making systems, risk is a primary concern. In this regard, the mean-variance risk measure is one of the most common objective functions. Existing algorithms for mean-variance optimization in the context of MAB problems have unrealistic assumptions on the reward distributions. We develop Thompson Sampling-style algorithms for mean-variance MAB and provide comprehensive regret analyses for Gaussian and Bernoulli bandits with fewer assumptions. Our algorithms achieve the best known regret bounds for mean-variance MABs and also attain the information-theoretic bounds in some parameter regimes. Empirical simulations show that our algorithms significantly outperform existing LCB-based algorithms for all risk tolerances.
 Training Neural Networks for and by Interpolation 
 In modern supervised learning, many deep neural networks are able to interpolate the data: the empirical loss can be driven to near zero on all samples simultaneously. In this work, we explicitly exploit this interpolation property for the design of a new optimization algorithm for deep learning, which we term Adaptive Learning-rates for Interpolation with Gradients (ALI-G). ALI-G retains the two main advantages of Stochastic Gradient Descent (SGD), which are (i) a low computational cost per iteration and (ii) good generalization performance in practice. At each iteration, ALI-G exploits the interpolation property to compute an adaptive learning-rate in closed form. In addition, ALI-G clips the learning-rate to a maximal value, which we prove to be helpful for non-convex problems. Crucially, in contrast to the learning-rate of SGD, the maximal learning-rate of ALI-G does not require a decay schedule. This makes ALI-G considerably easier to tune than SGD. We prove the convergence of ALI-G in various stochastic settings. Notably, we tackle the realistic case where the interpolation property is satisfied up to some tolerance. We also provide experiments on a variety of deep learning architectures and tasks: (i) learning a differentiable neural computer; (ii) training a wide residual network on the SVHN data set; (iii) training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art results among adaptive methods, and even yields comparable performance with SGD, which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple to implement in any standard deep learning framework and can be used as a drop-in replacement in existing code.
 Predictive Coding for Locally-Linear Control 
 High-dimensional observations and unknown dynamics are major challenges when applying optimal control to many real-world decision making tasks. The Learning Controllable Embedding (LCE) framework addresses these challenges by embedding the observations into a lower dimensional latent space, estimating the latent dynamics, and then performing control directly in the latent space. To ensure the learned latent dynamics are predictive of next-observations, all existing LCE approaches decode back into the observation space and explicitly perform next-observation prediction---a challenging high-dimensional task that furthermore introduces a large number of nuisance parameters (i.e., the decoder) which are discarded during control. In this paper, we propose a novel information-theoretic LCE approach and show theoretically that explicit next-observation prediction can be replaced with predictive coding. We then use predictive coding to develop a decoder-free LCE model whose latent dynamics are amenable to locally-linear control. Extensive experiments on benchmark tasks show that our model reliably learns a controllable latent space that leads to superior performance when compared with state-of-the-art LCE baselines.
 Lifted Disjoint Paths with Application in Multiple Object Tracking 
 We present an extension to the disjoint paths problem in which additional lifted edges are introduced to provide path connectivity priors. We call the resulting optimization problem the lifted disjoint paths problem. We show that this problem is NP-hard by reduction from multicommodity flow and 3-SAT. To enable practical global optimization, we propose several classes of linear inequalities that produce a high-quality LP-relaxation. Additionally, we propose efficient cutting plane algorithms for separating the proposed linear inequalities. The lifted disjoint path problem is a natural model for multiple object tracking and allows an elegant mathematical formulation for long-range temporal interactions. Lifted edges help to prevent id switches and to re-identify persons. Our lifted disjoint paths tracker leads on all three main benchmarks of the MOT challenge, improving significantly over state-of-the-art.
 Scalable Gaussian Process Regression for Kernels with a Non-Stationary Phase 
 The application of Gaussian processes (GPs) to large data sets is limited due to heavy memory and computational requirements. A variety of methods has been proposed to enable scalability, one of which is to exploit structure in the kernel matrix. Previous methods, however, cannot easily deal with non-stationary processes. This paper investigates an efficient GP framework, that extends structured kernel interpolation methods to GPs with a non-stationary phase. We particularly treat mixtures of non-stationary processes, which are commonly used in the context of separation problems e.g. in biomedical signal processing. Our approach employs multiple sets of non-equidistant inducing points to account for the non-stationarity and retrieve Toeplitz and Kronecker structure in the kernel matrix allowing for efficient inference and kernel learning. The approach is demonstrated on numerical examples and large biomedical datasets.
 Inductive Relation Prediction by Subgraph Reasoning 
 The dominant paradigm for relation prediction in knowledge graphs involves learning and operating on latent representations (i.e., embeddings) of entities and relations. However, these embedding-based methods do not explicitly capture the compositional logical rules underlying the knowledge graph,  and they are limited to the transductive setting,  where the full set of entities must be known during training. Here, we propose a graph neural network based relation prediction framework,  GraIL, that reasons over local subgraph structures and has a strong inductive bias to learn entity-independent relational semantics.  Unlike embedding-based models, GraIL is naturally inductive and can generalize to unseen entities and graphs after training. We provide theoretical proof and strong empirical evidence that GraIL can rep-resent a useful subset of first-order logic and show that GraIL outperforms existing rule-induction baselines in the inductive setting. We also demonstrate significant gains obtained by ensembling GraIL with various knowledge graph embedding methods in the transductive setting, highlighting the complementary inductive bias of our method.
 Strategyproof Mean Estimation from Multiple-Choice Questions 
 Given n values possessed by n agents, we study the problem of estimating the mean by truthfully eliciting agents' answers to multiple-choice questions about their values. We consider two natural candidates for estimation error: mean squared error (MSE) and mean absolute error (MAE). We design a randomized estimator which is asymptotically optimal for both measures in the worst case. In the case where prior distributions over the agents' values are known, we give an optimal, polynomial-time algorithm for MSE, and show that the task of computing an optimal estimate for MAE is #P-hard. Finally, we demonstrate empirically that knowledge of prior distributions gives a significant edge.
 Fair k-Centers via Maximum Matching 
 The field of algorithms has seen a push for fairness, or the removal of inherent bias, in recent history. In data summarization, where a much smaller subset of a data set is chosen to represent the whole of the data, fairness can be introduced by guaranteeing each "demographic group" a specific portion of the representative subset. Specifically, this paper examines this fair variant of the k-centers problem, where a subset of the data with cardinality k is chosen to minimize distance to the rest of the data. Previous papers working on this problem presented both a 3-approximation algorithm with a super-linear runtime and a linear-time algorithm whose approximation factor is exponential in the number of demographic groups. This paper combines the best parts of each algorithm , by presenting a linear-time algorithm with a guaranteed 3-approximation factor, and provides empirical evidence of both the algorithm's runtime and effectiveness.
 Sparse Subspace Clustering with Entropy-Norm 
 Sparse subspace clustering (SSC) and spectral clustering (SC) are both state-of-the-art methods to identify complex clusters in high-dimensional input space. However, there are few researches to discuss the relation between them. Therefore, in this paper, we provide an explicit theoretical connection between them from the respective of learning a data similarity matrix. We show that spectral clustering with Gaussian kernel can be viewed as sparse subspace clustering with entropy-norm (SSC+E). Compared to existing SSC algorithms, the SSC+E algorithm can obtain a sparse, analytical, symmetrical and nonnegative similarity matrix. Besides, it makes use of Gaussian kernel to compute the sparse similarity matrix of objects, which can avoid the complex computation of the sparse optimization program of SSC. Finally, we provide the experimental analysis to compare the efficiency and effectiveness of sparse subspace clustering and spectral clustering on ten benchmark data sets. The theoretical and experimental analysis can well help users for the selection of high-dimensional data clustering algorithms.
 Q-value Path Decomposition for Deep Multiagent Reinforcement Learning 
 Recently, deep multiagent reinforcement learning (MARL) has become a highly active research area as many real-world problems can be inherently viewed as multiagent systems. A particularly interesting and widely applicable class of problems is the partially observable cooperative multiagent setting, in which a team of agents learns to coordinate their behaviors conditioning on their private observations and commonly shared global reward signals. One natural solution is to resort to the centralized training and decentralized execution paradigm. During centralized training, one key challenge is the multiagent credit assignment: how to allocate the global rewards for individual agent policies for better coordination towards maximizing system-level's benefits. In this paper, we propose a new method called Q-value Path Decomposition (QPD) to decompose the system's global Q-values into individual agents' Q-values. Unlike previous works which restrict the representation relation of the individual Q-values and the global one, we leverage the integrated gradient attribution technique into deep MARL to directly decompose global Q-values along trajectory paths to assign credits for agents. We evaluate QPD on the challenging StarCraft II micromanagement tasks and show that QPD achieves the state-of-the-art performance in both homogeneous and heterogeneous multiagent scenarios compared with existing cooperative MARL algorithms.
 Improving Transformer Optimization Through Better Initialization  
 The Transformer architecture has achieved considerable success in areas such as language modeling and machine translation. The key component of the Transformer is the attention layer that enables the model to focus on important regions within the input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence.  As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these models. In this work our contributions are two-fold. We first investigate and empirically validate the source of optimization problems in encoder-decoder Transformer architecture.We then propose a new weight initialization scheme with theoretical justification, which enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers without difficulty. Full code for this work will be released with the final version of this draft.
 Deep Reinforcement Learning with Smooth Policy 
 Deep neural networks have been widely adopted in modern reinforcement learning (RL) algorithms with great empirical successes in various domains. 
However, the large search space of training a neural network requires a significant amount of data, which makes the current RL algorithms not sample efficient. Motivated by the fact that many environments with continuous state space have smooth transitions, we propose to learn a smooth policy that behaves smoothly with respect to the state. In contrast to policy parameterized by linear/reproducing kernel functions, where simple regularization techniques suffice to control smoothness, for neural network based reinforcement learning algorithms, there is no readily available solution to learn a smooth policy. In this paper, we develop a new training framework --- \textbf{S}mooth \textbf{R}egularized \textbf{R}einforcement \textbf{L}earning ($\textbf{SR}^2\textbf{L}$), where the policy is trained with smoothness-inducing regularization. Such regularization effectively constrains the search space of the learning algorithms and enforces smoothness in the learned policy. We apply the proposed framework to both on-policy (TRPO) and off-policy algorithm (DDPG). Through extensive experiments, we demonstrate that our method achieves improved sample efficiency.

 Batch Reinforcement Learning with Hyperparameter  Gradients 
 We consider the batch reinforcement learning problem where the agent needs to learn only from a fixed batch of data, without further interaction with the environment. In such a scenario, we want to prevent the optimized policy from deviating too much from the data collection policy since the estimation becomes highly unstable otherwise due to the off-policy nature of the problem. However, imposing this requirement too strongly will result in a policy that merely follows the data collection policy. Unlike prior work where this trade-off is controlled by hand-tuned hyperparameters, we propose a novel batch reinforcement learning approach, batch optimization of policy and hyperparameter (BOPAH), that uses a gradient-based optimization of the hyperparameter using held-out data. We show that BOPAH outperforms other batch reinforcement learning algorithms in tabular and continuous control tasks, by finding a good balance to the trade-off between adhering to the data collection policy and pursuing the possible policy improvement.
 Expectation Maximization with Bias-Corrected Calibration is Hard-To-Beat at Label Shift Adaptation 
 Label shift refers to the phenomenon where the prior class probability p(y) changes between the training and test distributions, while the conditional probability p(x|y) stays fixed. Label shift arises in settings like medical diagnosis, where a classifier trained to predict disease given symptoms must be adapted to scenarios where the baseline prevalence of the disease is different. Given estimates of p(y|x) from a predictive model, Saerens et al. proposed an efficient EM algorithm to correct for label shift that does not require model retraining. A limiting assumption of this algorithm is that p(y|x) is calibrated, which is not true of modern neural networks. Recently, Black Box Shift Learning (BBSL) and Regularized Learning under Label Shifts (RLLS) have emerged as state-of-the-art techniques to cope with label shift when a classifier does not output calibrated probabilities. However, both BBSL and RLLS require model retraining with importance weights, which poses challenges in practice, and neither has been benchmarked against EM. We show that by combining EM with a type of calibration we call bias-corrected calibration, we outperform both BBSL and RLLS across diverse datasets and distribution shifts. We further show that the EM objective is concave and bounded, and introduce a theoretically principled strategy for estimating source-domain priors that improves robustness to poor calibration. This work demonstrates that EM with appropriate calibration is a formidable and efficient baseline that future work in label shift adaptation should be compared with.

Colab notebooks reproducing experiments are available at (anonymized link): https://github.com/blindauth/labelshiftexperiments
 Unsupervised Transfer Learning for Spatiotemporal Predictive Networks 
 This paper explores a new research problem of unsupervised transfer learning across multiple spatiotemporal prediction tasks. Unlike most existing transfer learning methods that focus on fixing the discrepancy between supervised tasks, we study how to transfer knowledge from a zoo of unsupervisedly learned models towards another predictive network. Our motivation is that models from different sources are expected to understand the complex spatiotemporal dynamics from different perspectives, and thus provide an effective supplement to the new task, even if this task already has sufficient training data. Technically, we propose a differentiable framework named transferable memory. It adaptively distills knowledge from a bank of memory states of predictive networks, and then applies it to the target network with a novel recurrent structure called transferable memory unit (TMU). Compared with finetuning, our approach yields significant improvements on three benchmarks for spatiotemporal prediction, and benefits the target task even from less relevant pretext tasks.
 Bayesian Optimisation over Multiple Continuous and Categorical Inputs 
 Efficient optimisation of black-box problems that comprise both continuous and categorical inputs is important, yet poses significant challenges. Current approaches, like one-hot encoding, severely increase the dimension of the search space, while separate modelling of category-specific data is sample-inefficient. Both frameworks are not scalable to practical applications involving multiple categorical variables, each with multiple possible values. We propose a new approach, Continuous and Categorical Bayesian Optimisation (CoCaBO), which combines the strengths of multi-armed bandits and Bayesian optimisation to select values for both categorical and continuous inputs. We model this mixed-type space using a Gaussian Process kernel, designed to allow sharing of information across multiple categorical variables; this allows CoCaBO to leverage all available data efficiently. We extend our method to the batch setting and propose an efficient selection procedure that dynamically balances exploration and exploitation whilst encouraging batch diversity. 
We demonstrate empirically that our method outperforms existing approaches on both synthetic and real-world optimisation tasks with continuous and categorical inputs.
 Confidence Sets and Hypothesis Testing in a Likelihood-Free Inference Setting 
 Parameter estimation, statistical tests and confidence sets are the cornerstones of classical statistics that  allow  scientists to make inferences about the underlying process that generated the observed data. A key question is whether one can still construct hypothesis tests and confidence sets with proper coverage and high power in a so-called likelihood-free inference (LFI) setting, where the likelihood is not explicitly known but one can forward-simulate observable data according to a stochastic model.
In this paper, we present ACORE (Approximate Computation via Odds Ratio Estimation), a frequentist approach to LFI that first formulates the classical likelihood ratio test (LRT) as a parametrized classification problem, and then uses the equivalence of tests and confidence sets to build confidence regions for parameters of interest. We also present a goodness-of-fit test for checking whether the constructed tests and confidence regions are valid. ACORE is based on the key observation that the LRT statistic, the rejection probability of the test, and the coverage of the confidence set are  conditional distribution functions which often vary smoothly as a function of the the parameters of interest. Hence, instead of relying solely on  samples simulated at fixed parameter settings (as is the convention in standard Monte Carlo solutions), one can leverage machine learning tools and data simulated in the neighborhood of a parameter to improve estimates of quantities of interest. We demonstrate the efficacy of ACORE with both theoretical and empirical results.
 Multi-Agent Routing Value Iteration Network 
 Multi-agent coordination and routing is a complex problem and has a wide range of applications in areas from vehicle fleet coordination to autonomous mapping. Whereas traditional methods are not designed for realistic environments such as sparse connectivity and unknown traffics and are often slow in runtime; in this paper, we propose a graph neural network based model that is able to perform multiagent routing in a sparsely connected graph with dynamically changing traffic conditions, outperforming existing methods.

Our learned communication module in the proposed model enables the agents to coordinate online and adapt to changes to their environment. We also show that our model trained with only two agents on graphs with a maximum of twenty-five nodes can easily generalize to five agents with a hundred nodes.
 Extreme Multi-label Classification from Aggregated Labels 
 Extreme multi-label classification (XMC) is the problem of finding the relevant labels for an input, from a very large universe of possible labels. We consider XMC in the setting where labels are available only for groups of samples - but not for individual ones. Current XMC approaches are not built for such multi-instance multi-label (MIML) training data, and MIML approaches do not scale to XMC sizes. We develop a new and scalable algorithm to impute individual-sample labels from the group labels; this can be paired with any existing XMC method to solve the aggregated label problem. We characterize the statistical properties of our algorithm under mild assumptions, and provide a new end-to-end framework for MIML as an extension. Experiments on both aggregated label XMC and MIML tasks show the advantages over existing approaches.
 k-means++:  few more steps yield constant approximation 
 The k-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is a state-of-the-art algorithm for solving the k-means clustering problem and is known to give an O(log k) approximation. Recently, Lattanzi and Sohler (ICML 2019) proposed augmenting k-means++ with O(k log log k) local search steps to yield a constant approximation (in expectation) to the k-means clustering problem. In this paper, we improve their analysis to show that, for any arbitrarily small constant epsilon > 0, with only epsilon * k additional local search steps, one can achieve a constant approximation guarantee (with high probability in k), resolving an open problem in their paper.
 Fully Parallel Hyperparameter Search: Reshaped Space-Filling 
 Space-filling designs such as Low Discrepancy Sequence (LDS), Latin Hypercube Sampling (LHS) and Jittered Sampling (JS) were proposed for fully parallel hyperparameter search, and were shown to be more effective than random and grid search. We prove that LHS and JS outperform random search only by a constant factor. Consequently, we introduce a new sampling approach based on the reshaping of the search distribution, and we show both theoretically and numerically that it leads to significant gains over random search. Two methods are proposed for the reshaping: Recentering (when the distribution of the optimum is known), and Cauchy transformation (when the distribution of the optimum is unknown). The proposed methods are first validated on artificial experiments and simple real-world tests on clustering and Salmon mappings. Then we demonstrate that they drive performance improvement in a wide range of expensive artificial intelligence tasks, namely attend/infer/repeat, video next frame segmentation forecasting and progressive generative adversarial networks.
 Robust Multi-Agent Decision-Making with Heavy-Tailed Payoffs 
 We study the heavy-tailed stochastic bandit problem in the cooperative multiagent setting, where a group of agents interact with a common bandit problem, while communicating on a network with delays. Existing algorithms for the stochastic bandit in this setting utilize confidence intervals arising from an averaging-based communication protocol known as~\textit{running consensus}, that does not lend itself to robust estimation for heavy-tailed settings. We propose \textsc{MP-UCB}, a decentralized multi-agent algorithm for the cooperative stochastic bandit that incorporates robust estimation with a message-passing protocol. We prove optimal regret bounds for \textsc{MP-UCB} for several problem settings, and also demonstrate its superiority to existing methods. Furthermore, we establish the first lower bounds for the cooperative bandit problem, in addition to providing efficient algorithms for robust bandit estimation of location.
 Automatic Shortcut Removal for Self-Supervised Representation Learning 
 In self-supervised visual representation learning, a feature extractor is trained on a "pretext task" for which labels can be generated cheaply. A central challenge in this approach is that the feature extractor quickly learns to exploit low-level visual features such as color aberrations or watermarks and then fails to learn useful semantic representations. Much work has gone into identifying such "shortcut" features and hand-designing schemes to reduce their effect. Here, we propose a general framework for removing shortcut features automatically. Our key assumption is that those features which are the first to be exploited for solving the pretext task may also be the most vulnerable to an adversary trained to make the task harder. We show that this assumption holds across common pretext tasks and datasets by training a "lens" network to make small image changes that maximally reduce performance in the pretext task. Representations learned with the modified images outperform those learned without in all tested cases. Additionally, the modifications made by the lens reveal how the choice of pretext task and dataset affects the features learned by self-supervision.
 Identifying the Reward Function by Anchor Actions 
 We propose a reward function estimation framework for inverse reinforcement learning with deep energy-based policies. Our method sequentially estimates the policy, the $Q$-function, and the reward. We refer to it as the PQR method. This method does not require the assumption that the reward depends on the state only, but instead allows also for dependency on the choice of action. Moreover, the method allows for the state transitions to be stochastic. To accomplish this, we assume the existence of one anchor action whose reward is known, typically the action of doing nothing, yielding no reward. We present both estimators and algorithms for the PQR method. When the environment transition is known, we prove that the reward estimator of PQR uniquely recovers the true reward.
With unknown transitions, convergence analysis is presented for the PQR method.
Finally, we apply PQR to both synthetic and real-world datasets, demonstrating superior performance in terms of reward estimation compared to competing methods. 
 Nested Subspace Arrangement for Representation of Relational Data 
 Studies of acquiring appropriate continuous representations of a discrete objects such as graph and knowledge based data have been conducted by many researches in the field of machine learning.
In this paper, we introduce Nested SubSpace arrangement (NSS arrangement), a comprehensive framework for representation learning.
We show that existing embedding techniques can be regarded as a member of NSS arrangement.
Based on the concept of the NSS arrangement, we implemented Disk-ANChor ARrangement (DANCAR), a representation learning method specializing to reproduce general graphs.
Numerical experiments have shown that DANCAR has successfully embedded WordNet in ${\mathbb R}^{20}$ with the F1 score of 99.3\% in the reconstruction task.
DANCAR is also suitable for visualization to understand the characteristics of graph.
 LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments 
 The causal relationships among a set of random variables are commonly represented by a Directed Acyclic Graph (DAG), where there is a directed edge from variable $X$ to variable $Y$ if $X$ is a direct cause of $Y$. From the purely observational data, the true causal graph can be identified up to a Markov Equivalence Class (MEC), which is a set of DAGs with the same conditional independencies between the variables. The size of an MEC is a measure of complexity for recovering the true causal graph by performing interventions. We propose a method for efficient iteration over possible MECs given intervention results. We utilize the proposed method for computing MEC sizes and experiment design in active and passive learning settings. Compared to previous work for computing the size of MEC, our proposed algorithm reduces the time complexity by a factor of $O(n)$ for sparse graphs where $n$ is the number of variables in the system. Additionally, integrating our approach with dynamic programming, we design an optimal algorithm for passive experiment design. Experimental results show that our proposed algorithms for both computing the size of MEC and experiment design outperform the state of the art.

 Towards Understanding the Dynamics of the First-Order Adversaries 
 An acknowledged weakness of neural networks is their vulnerability to adversarial perturbations to the inputs. To improve the robustness of these models, one of the most popular defense mechanisms is to alternatively maximize the loss over the constrained perturbations (or called adversaries) on the inputs using projected gradient ascent and minimize over weights. In this paper, we analyze the dynamics of the maximization step towards understanding the experimentally observed effectiveness of this defense mechanism. Specifically, we investigate the landscape of the adversaries for a two-layer neural network with a quadratic loss. Our main result proves that projected gradient ascent finds a local maximum of this non-concave problem in a polynomial number of iterations with high probability.  To our knowledge, this is the first work that provides a convergence analysis of the first-order adversaries. Moreover, our analysis demonstrates that, in the initial phase of adversarial training, the scale of the inputs matters in the sense that a smaller input scale leads to faster convergence of adversarial training and a ``more regular'' landscape. Finally, we show that these theoretical findings are in excellent agreement with a series of experiments.
 Channel Equilibrium Networks for Learning Deep Representation 
 Convolutional Neural Networks (CNNs) are typically constructed by stacking multiple building blocks, each of which contains a normalization layer such as batch normalization (BN) and a rectified linear function such as ReLU. 
However, this work shows that the combination of normalization and rectified linear function leads to inhibited channels, which have small magnitude and contribute little to the learned feature representation, impeding the generalization ability of CNNs. Unlike prior arts that simply removed the inhibited channels, we propose to ``wake them up'' during training by designing a novel neural building block, termed Channel Equilibrium (CE) block, which enables channels at the same layer to contribute equally to the learned representation. We show that CE is able to prevent inhibited channels both empirically and theoretically.
CE has several appealing benefits. (1) It can be integrated into many advanced CNN architectures such as ResNet and MobileNet, outperforming their original networks. (2) CE has an interesting connection with the Nash Equilibrium, a well-known solution of a non-cooperative game. (3) Extensive experiments show that CE achieves state-of-the-art performance on various challenging benchmarks such as ImageNet and COCO. 
 Scalable Nearest Neighbor Search for Optimal Transport 
 The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly popular similarity measure for rich data domains, such as images or text documents.  This raises the necessity for fast nearest neighbor search algorithms according to this distance, which poses a substantial computational bottleneck on massive datasets.

In this work we introduce Flowtree, a fast and accurate approximation algorithm for the Wasserstein-1 distance. We formally analyze its approximation factor and running time.  We perform extensive experimental evaluation of nearest neighbor search algorithms in the W_1 distance on real-world dataset.  Our results show that compared to previous state of the art, Flowtree achieves up to 7.4 times faster running time.
 What Can Learned Intrinsic Rewards Capture? 
 The objective of a reinforcement learning agent is to behave so as to maximise the sum of a suitable scalar function of state: the reward. These rewards are typically given and immutable. In this paper, we instead consider the proposition that the reward function itself can be a good locus of learned knowledge. To investigate this, we propose a scalable meta-gradient framework for learning useful intrinsic reward functions across multiple lifetimes of experience. Through several proof-of-concept experiments, we show that it is feasible to learn and capture knowledge about long-term exploration and exploitation into a reward function. Furthermore, we show that unlike policy transfer methods that capture ``how'' the agent should behave, the learned reward functions can generalise to other kinds of agents and to changes in the dynamics of the environment by capturing ``what'' the agent should strive to do.
 Learning Opinions in Social Networks 
 We study the problem of learning opinions in social networks.  The learner observes the states of some sample nodes from a social network, and tries to infer the states of other nodes, based on the structure of the network.  We show that sample-efficient learning is impossible when the network exhibits strong noise, and give a polynomial-time algorithm for the problem with nearly optimal sample complexity when the network is sufficiently stable.
 Learning Discrete Structured Representations by Adversarially Maximizing Mutual Information 
 We propose learning discrete structured representations from unlabeled data by maximizing the mutual information between a structured latent variable and a target variable. Calculating mutual information is intractable in this setting. Our key technical contribution is an adversarial objective that can be used to tractably estimate mutual information assuming only the feasibility of cross entropy calculation. We develop a concrete realization of this general formulation with Markov distributions over binary encodings. We report critical and unexpected findings on practical aspects of the objective such as the choice of variational priors. We apply our model on document hashing and show that it outperforms current best baselines based on straight-through estimators and vector quantization. It also yields highly compressed interpretable representations.
 Modulating Surrogates for Bayesian Optimization 
 Bayesian optimization (BO) methods often rely on the assumption that the objective function is well-behaved,
but in practice, this is seldom true for real-world objectives even if noise-free observations can be collected.
Common approaches, which try to model the objective as precisely as possible, often fail to make progress by spending too many evaluations modeling irrelevant details.
We address this issue by proposing surrogate models that focus on the well-behaved structure in the objective function, which is informative for search, while ignoring detrimental structure that is challenging to model from few observations.
First, we demonstrate that surrogate models with appropriate noise distributions can absorb challenging structures in the objective function by treating them as irreducible uncertainty.
Secondly, we show that a latent Gaussian process is an excellent surrogate for this purpose, comparing with Gaussian processes with standard noise distributions.
We perform numerous experiments on a range of BO benchmarks and find that our approach improves reliability and performance when faced with challenging objective functions.
 Rank Aggregation from Pairwise Comparisons in the Presence of Adversarial Corruptions 
 Rank aggregation from pairwise preferences has widespread applications in recommendation systems and information retrieval. Given the enormous economic and societal impact of these applications, and the consequent incentives for malicious players to manipulate ranking outcomes in their favor, an important challenge is to make rank aggregation algorithms robust to adversarial manipulations in data. In this paper, we initiate the study of robustness in rank aggregation under the popular Bradley-Terry-Luce (BTL) model for pairwise comparisons. We consider a setting where pairwise comparisons are initially generated according to a BTL model, but a fraction of these comparisons are corrupted by an adversary prior to being reported to us. We consider a strong contamination model, where an adversary having complete knowledge of the initial truthful data and the underlying true BTL parameters, can subsequently corrupt the truthful data by inserting, deleting, or changing data points. The goal is to estimate the true score/weight of each item under the BTL model, even in the presence of these corruptions. We characterize the extent of adversarial corruption under which the true BTL parameters are uniquely identifiable. We also provide a novel pruning algorithm that provably cleans the data of adversarial corruption under reasonable conditions on data generation and corruption. We corroborate our theory with experiments on synthetic data showing that previous algorithms are vulnerable to even small amounts of corruption, whereas our algorithm can clean a reasonably high amount of corruption.
 RIFLE: Backpropagation in Depth for Deep Transfer Learning through Re-Initializing the Fully-connected LayEr 
 Fine-tuning the deep convolution neural network (CNN) using a pre-trained model helps transfer knowledge learned from larger datasets to the target task. While the accuracy could be largely improved even when the training dataset is small, the transfer learning outcome is similar with the pre-trained one with closed CNN weights[17], as the backpropagation here brings less updates to deeper CNN layers. In this work, we propose RIFLE - a simple yet effective strategy that deepens backpropagation in transfer learning settings, through periodically ReInitializing the Fully-connected LayEr with random scratch during the fine-tuning procedure. RIFLE brings significant perturbation to the backpropagation process and leads to deep CNN weights update, while the affects of perturbation can be easily converged throughout the overall learning procedure. The experiments show that the use of RIFLE significantly improves deep transfer learning accuracy on a wide range of datasets. It outperforms known tricks for the similar purpose, such as dropout, dropconnect, stochastic depth, and cyclic learning rate, under the same settings with 0.5%-2% higher testing accuracy. Empirical cases and ablation studies further indicate RIFLE brings meaningful updates to deep CNN layers with accuracy improved.
 Adaptive Sampling for Estimating Probability Distributions 
 We consider the problem of allocating a fixed budget of samples to a finite set of discrete distributions to learn them uniformly well (minimizing the maximum error) in terms of four common distance measures: $\ell_2^2$, $\ell_1$, $f$-divergence, and separation distance. To present a unified treatment of these distances, we first propose a general \emph{optimistic tracking algorithm} and analyze its sample allocation performance w.r.t.~an oracle. We then instantiate this algorithm for the four distance measures and derive bounds on their regret. We also show that the allocation performance of the proposed algorithm cannot, in general, be improved, by deriving lower-bounds on the expected deviation from the oracle allocation for any adaptive scheme. We verify our theoretical findings through some experiments. Finally, we show that the techniques developed in the paper can be easily extended to learn some classes of continuous distributions as well as to the related setting of minimizing the average error (in terms of the four distances) in learning a set of distributions. 
 Feature Quantization Improves GAN Training 
 The instability in GANs' training has been a long-standing problem despite remarkable research efforts. We identify that instability issues stem from difficulties of performing feature matching with mini-batch statistics, due to a fragile balance between the fixed target distribution and the progressively generated distribution. In this work, we propose feature quantizatoin (FQ) for the discriminator, to embed both true and fake data samples into a shared discrete space. The quantized values of FQ are constructed as an evolving dictionary, which is consistent with feature statistics of the recent distribution history. Hence, FQ implicitly enables robust feature matching in a compact space.  Our method can be easily plugged into existing GAN models, with little computational overhead in training. Extensive experimental results show that the proposed FQ-GAN can improve the FID scores of baseline methods by a large margin on a variety of tasks, including three representative GAN models on 10 benchmarks, achieving new state-of-the-art performance.
 Adversarial Robustness Against the Union of Multiple Threat Models 
 Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers.  While most work has defended against a single type of attack, recent work has looked at defending against multiple threat models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual threat models, resulting in a sub-optimal worst-case loss over the combined threat model.  In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple threat models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different threat models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against l_∞, l_2, and l_1 attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 46.1% against the union of (l_∞,l_2,l_1) perturbations with radius= (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6% accuracy.
 Global Decision-Making via Local Economic Transactions 
 This paper seeks to establish a mechanism for directing a collection of simple, specialized, self-interested agents to solve what traditionally are posed as monolithic single-agent sequential decision problems with a central global objective. What makes it challenging to use a decentralized approach to collectively optimize a central objective is the difficulty in characterizing the equilibrium strategy profile of non-cooperative games. To overcome this challenge, we design a mechanism for defining the learning environment of each primitive agent for which we know that the optimal solution for the global objective coincides with a Nash equilibrium strategy profile of the agents optimizing their own local objectives. We then derive a learning algorithm for the system and empirically test to what extent the desired equilibrium is achieved. The system functions as an economy of agents that learn the credit assignment process itself by buying and selling to each other the right to operate on the environment state. We also show that redundancy not only enforces credit conservation but also improves robustness against suboptimal equilibria.
 Reward-Free Exploration for Reinforcement Learning 
 Exploration is widely regarded as one of the most challenging aspects of reinforcement learning (RL), with many naive approaches succumbing to exponential sample complexity. To isolate the challenges of exploration, we propose the following ``reward-free RL'' framework. In the exploration phase, the agent first collects trajectories from an MDP $M$ without a pre-specified reward function. After exploration, it is tasked with computing a near-policies under the transitions of $\mathcal{M}$ for a collection of given reward functions.  This framework is particularly suitable where there are many reward functions of interest, or where the reward function is shaped by an external agent to elicit desired behavior. 

We give an efficient algorithm that conducts  $\widetilde{O}(S^2A\mathrm{poly}(H)/\epsilon^2)$ episodes of exploration, and returns $\epsilon$-suboptimal policies for an arbitrary number of reward functions. We achieve this by finding exploratory policies that jointly visit each ``significant'' state with probability proportional to its maximum visitation probability under any possible policy. Moreover, our planning procedure can be instantiated by any black-box approximate planner, such as value iteration or natural policy gradient. Finally, we give a nearly-matching $\Omega(S^2AH^2/\epsilon^2)$ lower bound, demonstrating the near-optimality of our algorithm in this setting. 
 PoKED: A Semi-Supervised System for Word Sense Disambiguation 
 Word Sense Disambiguation (WSD) is an open problem in Natural Language Processing, which is challenging and useful in both supervised and unsupervised settings where all the words in any given text need to be disambiguated without sufficient labeled data. Typically, Most WSD systems use the sentence or a small window of words around the target word as the context for disambiguation, as their computational complexity scales exponentially with the size of the context. In this paper, we propose a semi-supervised neural system, Position-wise Orthogonal Knowledge-Enhanced Disambiguator (PoKED), which allows attention-driven, long-range dependency modeling for word sense disambiguation tasks. The proposed PoKED incorporates position-wise encoding into an orthogonal framework and applies a knowledge-based attentive neural model to solve the WSD problem. Our proposed unsupervised language model is trained over unlabelled corpus, and then the pre-trained language model is capable of abstracting the surrounding context of polyseme instances in labeled corpus into context embeddings. We further utilize the semantic relations in the WordNet, by extracting semantic level inter-word connections from each document-sentence pair in the WSD dataset, and allows us to control the amount of the extraction results by setting a hyperparameter. Our experimental results from standard benchmarks show that our proposed system, PoKED, can achieve competitive performance compared with state-of-the-art knowledge-based WSD systems.
 Discount Factor as a Regularizer in Reinforcement Learning  
 Specifying a Reinforcement Learning (RL) task involves choosing a suitable planning horizon, which is typically modeled by an evaluation discount factor. It is known that applying RL algorithms with a discount set lower than the evaluation discount factor can act as a regularizer, improving performance in the limited data regime. Yet the exact nature of this regularizer has not been investigated. In this work, we fill in this gap. For TD learning and expected SARSA, we show an explicit equivalence between using a reduced discount factor and adding an explicit regularization term to the algorithm loss.
For a fixed policy, we argue that chains with a uniform stationary distribution and a fast mixing rate are amenable to regularization with a reduced discount. We validate this conclusion with extensive experiments in discrete and continuous domains, using tabular and functional representations.
 Quadratically Regularized Subgradient Methods for Weakly Convex Optimization with Weakly Convex Constraints 
 Optimization models with non-convex constraints arise in many tasks in machine learning, e.g., learning with fairness constraints or Neyman-Pearson classification with non-convex loss. Although many efficient methods have been developed with theoretical convergence guarantees for non-convex unconstrained problems, it remains a challenge to design provably efficient algorithms for problems with non-convex functional constraints. This paper proposes a class of subgradient methods for constrained optimization where the objective function and the constraint functions are weakly convex and nonsmooth. Our methods solve a sequence of strongly convex subproblems, where a quadratic regularization term is added to both the objective function and each constraint function. Each subproblem can be solved by various algorithms for strongly convex optimization. Under a uniform Slater’s condition, we establish the computation complexities of our methods for finding a nearly stationary point.
 A Nearly-Linear Time Algorithm for Exact Community Recovery in Stochastic Block Model 
 Learning community structures in graphs that are randomly generated by stochastic block models (SBMs) has received much attention lately. In this paper, we focus on the problem of exactly recovering the communities in a binary symmetric SBM, where a graph of $n$ vertices is partitioned into two equal-sized communities and the vertices are connected with probability $p = \alpha\log(n)/n$ within communities and $q = \beta\log(n)/n$ across communities for some $\alpha>\beta>0$. We propose a two-stage iterative algorithm for solving this problem, which employs the power method with a random starting point in the first-stage and turns to a generalized power method that can identify the communities in a finite number of iterations in the second-stage. It is shown that for any fixed $\alpha$ and $\beta$ such that $\sqrt{\alpha} - \sqrt{\beta} > \sqrt{2}$, which is known to be the information-theoretical limit for exact recovery, the proposed algorithm exactly identifies the underlying communities in $\tilde{O}(n)$ running time with probability tending to one as $n\rightarrow\infty$. As far as we know, this is the first algorithm with nearly-linear running time that achieves exact recovery at the information-theoretical limit. We also present numerical results of the proposed algorithm to support and complement our theoretical development.
 Fast computation of Nash Equilibria in Imperfect Information Games 
 We introduce and analyze a class of algorithms, called Mirror Ascent against an Improved Opponent (MAIO), for computing Nash equilibria in two-player zero-sum games, both in normal form and in sequential imperfect information form. These algorithms update the policy of each player with a mirror-descent step to minimize the loss of playing against an improved opponent. We establish a convergence result to the set of Nash equilibria where the speed of convergence depends on the amount of improvement of the opponent policies. In addition, if the improved opponent is a best response, then an exponential convergence rate is achieved. 
 Constrained Markov Decision Processes via Backward Value Functions 
 Although Reinforcement Learning (RL) algorithms have found tremendous success in simulated domains, they often cannot directly be applied to physical systems, especially in cases where there are hard constraints to satisfy (e.g. on safety or resources). In standard RL, the agent is incentivized to explore any behavior as long as it maximizes rewards, but in the real world, undesired behavior can damage either the system or the agent in a way that breaks the learning process itself. In this work, we model the problem of learning with constraints as a Constrained Markov Decision Process and provide a new on-policy formulation for solving it. A key contribution of our approach is to translate cumulative cost constraints into state-based constraints. Through this, we define a safe policy improvement method which maximizes returns while ensuring that the constraints are satisfied at every step. We provide theoretical guarantees under which the agent converges while ensuring safety over the course of training. We also highlight the computational advantages of this approach. The effectiveness of our approach is demonstrated on safe navigation tasks and in safety-constrained versions of MuJoCo environments, with deep neural networks. 
 Federated Learning with Only Positive Labels 
 We consider learning a multi-class classification model in the federated setting, where each user has access to the positive data associated with only a single class. As a result, during each federated learning round, the users need to locally update the classifier without having access to the features and the model parameters for the negative labels. Since the loss function at a user is independent of the negative labels, naively employing conventional decentralized learning such as the distributed SGD or Federated Averaging may lead to trivial or extremely poor classifiers. In particular, for the embedding based classifiers, all the class embeddings might collapse to a single point.

To address this problem, we propose a generic framework for training with only positive labels, namely Federated Averaging with Spreadout (FedAwS), where the server imposes a geometric regularizer after each round to encourage classes spread out in the embedding space. We show, both theoretically and empirically, that FedAwS can almost match the performance of conventional learning where users have access to negative labels. We further extend the proposed method to the settings with large output spaces, such as the extreme multi-class classification. 
 Learning Attentive Meta-Transfer 
 Meta-transfer learning seeks to improve the efficiency of learning a new task via both meta-learning and transfer-learning in a setting with a stream of evolving tasks. While standard attention has been effective in a variety of settings, we question its effectiveness in improving meta-transfer learning since the tasks being learned are dynamic, and the amount of context information can be substantially small. In this paper, using a recently proposed meta-transfer learning model, Sequential Neural Processes (SNP), we first empirically show that it suffers a similar underfitting problem observed in the functions inferred by Neural Processes. However, we further demonstrate that unlike the meta-learning setting, standard attention mechanisms are ineffective in meta-transfer learning.~To resolve, we propose a new attention mechanism, Recurrent Memory Reconstruction (RMR), and demonstrate that providing an imaginary context that is recurrently updated and reconstructed with interaction is crucial in achieving effective attention for meta-transfer learning. Furthermore, incorporating RMR into SNP, we propose Attentive Sequential Neural Processes (ASNP) and demonstrate in various tasks that ASNP significantly outperforms the baselines. 
 Nonparametric Score Estimators 
 Estimating the score, i.e., the gradient of log density function, from a set of samples generated by an unknown distribution is a fundamental task in inference and learning of probabilistic models that involve flexible yet intractable densities. Kernel estimators based on Stein's methods or score matching have shown promise, however their theoretical properties and relationships have not been fully-understood. We provide a unifying view of these estimators under the framework of regularized nonparametric regression. It allows us to analyse existing estimators and construct new ones with desirable properties by choosing different hypothesis spaces and regularizers. A unified convergence analysis is provided for such estimators. Finally, we propose score estimators based on iterative regularization that enjoy computational benefits from curl-free kernels and fast convergence.

 SoftSort: A Differantiable Continuous Relaxation of the argsort Operator 
 Sorting is an important procedure in computer science. However, the argsort operator - which takes as input a vector and returns its sorting per-mutation - has a discrete image and thus zero gradients almost everywhere. This prohibits end-to-end, gradient-based learning of models that rely on the argsort operator. A natural way to overcome this problem is to replace the argsort operator with a continuous relaxation. Recent work has shown a number of ways to do this. However, the relaxations proposed so far are computationally complex. In this work we propose a simple continuous relaxation for the argsort operator. Unlike previous works, our relaxation is straight-forward: it can be implemented in three lines of code, achieves state-of-the-art performance, is easy to reason about mathematically - substantially simplifying proofs - and is up to six times faster than competing approaches. We open-source the code to reproduce all of the experiments
 From Chaos to Order: Symmetry and Conservation Laws in Game Dynamics 
 Games are an increasingly useful tool for training and testing learning algorithms. Recent examples include GANs, AlphaZero and the AlphaStar league. However, multi-agent learning can be extremely difficult to predict and control. Even simple games learning dynamics can yield chaotic behavior.  In this paper, we present basic \emph{mechanism design} tools for constructing games with predictable and controllable dynamics. We show that arbitrarily large and complex network games, encoding both cooperation (team play) and competition (zero-sum interaction), exhibit conservation laws when agents use the standard regret-minimizing dynamics known as Follow-the-Regularized-Leader. These laws persist even if different agents use different dynamics and encode long-range correlations between agents' behavior even though the agents may not interact directly. Moreover, we provide sufficient conditions under which the dynamics have multiple, linearly independent, conservation laws. Increasing the number of conservation laws results in more predictable dynamics, eventually making chaotic behavior in some cases even formally impossible.
 GradientDICE: Rethinking Generalized Offline Estimation of Stationary Values 
 We present GradientDICE for estimating the density ratio between the state distribution of the target policy and the sampling distribution in off-policy reinforcement learning.
GradientDICE fixes several problems of GenDICE (Zhang et al., 2020), the current state-of-the-art for estimating such density ratios. 
Namely, the optimization problem in GenDICE is not a convex-concave saddle-point problem once nonlinearity in optimization variable parameterization is introduced to ensure positivity, 
so primal-dual algorithms are not guaranteed to find the desired solution. 
However, such nonlinearity is essential to ensure the consistency of GenDICE even with a tabular representation.
This is a fundamental contradiction,
resulting from GenDICE's original formulation of the optimization problem.
In GradientDICE, we optimize a different objective from GenDICE
by using the Perron-Frobenius theorem and eliminating GenDICE's use of divergence,
such that nonlinearity in parameterization is not necessary for GradientDICE, 
which is provably convergent under linear function approximation.
 Test-Time Training for Generalization under Distribution Shifts 
 We introduce a general approach, called test-time training, for improving the performance of predictive models when training and test data come from different distributions. Test-time training turns a single unlabeled test instance into a self-supervised learning problem, on which we update the model parameters before making a prediction. We show that this simple idea leads to surprising improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts. Theoretical investigations on a convex model reveal helpful intuitions for when we can expect our approach to help.
 Complexity of Finding Stationary Points of Nonconvex Nonsmooth Functions 
 We provide the first non-asymptotic analysis for finding stationary points of nonsmooth, nonconvex functions. In particular, we study the class of Hadamard semi-differentiable functions, perhaps the largest class of nonsmooth functions for
which the chain rule of calculus holds. This class contains important examples such as ReLU neural networks and others with non-differentiable activation functions. First, we show that finding an epsilon-stationary point with first-order methods is
impossible in finite time. Therefore, we introduce the notion of (delta, epsilon)-stationarity, a generalization that allows for a point to be within distance delta of an epsilon-stationary point and reduces to epsilon-stationarity for smooth functions. We propose a series of randomized first-order methods and analyze their complexity
of finding a (delta, epsilon)-stationary point. Furthermore, we provide a lower bound and show that our stochastic algorithm has min-max optimal dependence on delta. Empirically, our methods perform well for training ReLU neural networks.
 Extrapolation for Large-batch Training in Deep Learning 
 Deep learning networks are typically trained by Stochastic Gradient Descent (SGD) methods that iteratively improve the model parameters by estimating a gradient on a very small fraction of the training data. A major roadblock faced when increasing the batch size to a substantial fraction of the training data for reducing training time is the persistent degradation in performance (generalization gap). To address this issue, recent work propose to add small perturbations to the model parameters when computing the stochastic gradients and report improved generalization performance due to smoothing effects. However, this approach is poorly understood; it requires often model-specific noise and fine-tuning.
To alleviate these drawbacks, we propose to use instead computationally efficient extrapolation (extragradient) to stabilize the optimization trajectory while still benefiting from smoothing to avoid sharp minima. This principled approach is well grounded from an optimization perspective and we show that a host of variations can be covered in a unified framework that we propose. We prove the convergence of this novel scheme and rigorously evaluate its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that in a variety of experiments the scheme allows scaling to much larger batch sizes than before whilst reaching or surpassing SOTA accuracy.
 Universal Equivariant Multilayer Perceptrons 
 Group invariant and equivariant Multilayer Perceptrons (MLP), also known as Equivariant Networks, have achieved remarkable success in learning on a variety of data structures, such as sequences, images, sets, and graphs. Using tools from group theory, this paper proves the universality of a broad class of equivariant MLPs with a single hidden layer.  In particular, it is shown that having a hidden layer on which the group acts regularly is sufficient for universal equivariance (invariance). A corollary is unconditional universality of equivariant MLPs for Abelian groups, such as CNNs with a single hidden layer. A second corollary is the universality of equivariant MLPs with a high-order hidden layer, where we give both group-agnostic bounds and means for calculating group-specific bounds on the order of hidden layer that guarantees universal equivariance (invariance). 

 On the Global Optimality of Model-Agnostic Meta-Learning 
 Model-agnostic meta-learning (MAML) formulates the meta-learning as a bilevel optimization problem where the inner level solves each subtask based on a shared prior, while the outer level searches for the optimal shared prior based on its aggregated perfor- mance over the subtasks. Despite its empirical success, MAML remains less understood theoretically, especially in terms of its global optimality due to the nonconvexity of the meta-objective (outer-level objective). To bridge such a gap between theory and practice, we characterize the optimality gap of the stationary points attained by MAML for both reinforcement learning and supervised learning, where both the inner- and outer-level problems are solved via first-order optimization methods. In particular, our characterization connects the optimality gap of such stationary points with (i) the functional geometry of the inner-level objective and (ii) the representation power of function approximators, including both linear models and neural networks. To the best of our knowledge, our analysis establishes the global optimality of MAML with the nonconvex meta-objective for the first time.
 Convolutional dictionary learning based auto-encoders for natural exponential-family distributions 
 We introduce a class of auto-encoder neural networks tailored to data from the natural exponential family (e.g., count data). The architectures are inspired by the problem of learning the filters in a convolutional generative model with sparsity constraints, often referred to as convolutional dictionary learning (CDL). Our work is the first to merge the ideas from convolutional generative models and deep learning for data that are naturally modeled with non-Gaussian distribution (e.g., binomial and Poisson). This perspective provides us with a scalable and flexible framework that can be re-purposed for a wide range of tasks and assumptions on the generative model. Specifically, the iterative optimization procedure for solving CDL, an unsupervised task, is mapped to an unfolded and constrained neural network, with iterative adjustments to the inputs to account for the generative distribution. We also show that the framework can easily be extended for discriminative training, appropriate for a supervised task. We demonstrate 1) that fitting the generative model to learn, in an unsupervised fashion, the latent stimulus that underlies neural spiking data leads to better goodness-of-fit compared to other baselines, 2) competitive performance compared to state-of-the-art algorithms for supervised Poisson image denoising, with significantly fewer parameters, and 3) gradient dynamics of shallow binomial auto-encoder.
 Efficient Optimistic Exploration in Linear-Quadratic Regulators via Lagrangian Relaxation 
 We study the exploration-exploitation dilemma in the linear quadratic regulator (LQR) setting. Inspired by the extended value iteration algorithm used in optimistic algorithms for finite MDPs, we propose to relax the optimistic optimization of \ofulq and cast it into a constrained \textit{extended} LQR problem, where an additional control variable implicitly selects the system dynamics within a confidence interval. We then move to the corresponding Lagrangian formulation for which we prove strong duality. As a result, we show that an $\epsilon$- optimistic controller can be computed efficiently by solving at most $O\big(\log(1/\epsilon)\big)$ Riccati equations. Finally, we prove that relaxing the original \ofu problem does not impact the learning performance, thus recovering the $\wt O(\sqrt{T})$ regret of \ofulq.
 Closing the convergence gap of SGD without replacement 
 Stochastic gradient descent without replacement sampling is widely used in practice for model training. However, the vast majority of SGD analyses assumes data sampled with replacement, and when the function minimized is strongly convex, an $\mathcal{O}\left(\frac{1}{T}\right)$ rate can be established when SGD is run for $T$ iterations.
A recent line of breakthrough work on SGD without replacement (SGDo) established an $\mathcal{O}\left(\frac{n}{T^2}\right)$ convergence rate when the function minimized is strongly convex and is a sum of $n$ smooth functions, and an $\mathcal{O}\left(\frac{1}{T^2}+\frac{n^3}{T^3}\right)$ rate for sums of quadratics. On the other hand, the tightest known lower bound postulates an $\Omega\left(\frac{1}{T^2}+\frac{n^2}{T^3}\right)$ rate, leaving open the possibility of better SGDo convergence rates in the general case.
In this paper, we close this gap and show that SGD without replacement achieves a rate of $\mathcal{O}\left(\frac{1}{T^2}+\frac{n^2}{T^3}\right)$ when the sum of the functions is a quadratic, and offer a new lower bound of $\Omega\left(\frac{n}{T^2}\right)$ for strongly convex functions that are sums of smooth functions.
 An Accelerated DFO Algorithm for Finite-sum Convex Functions 
 Derivative-free optimization (DFO) has recently gained a lot of momentum in machine learning, spawning interest in the community to design faster methods for problems where gradients are not accessible. While some attention has been given to the concept of acceleration in the DFO literature, there exists no algorithm with a provably accelerated rate of convergence for objective functions with a finite-sum structure. Stochastic algorithms that use acceleration in such a setting are prone to instabilities, making it difficult to reach convergence. In this work, we exploit the finite-sum structure of the objective to design a variance-reduced DFO algorithm that probably yields an accelerated rate of convergence. We prove rates of convergence for both smooth convex and strongly-convex finite-sum objective functions. Finally, we validate our theoretical results empirically on several datasets.
 Graph-based, Self-Supervised Program Repair from Diagnostic Feedback 
 We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small.
In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 68.2\% full repair rate on DeepFix (+22.9\% over the prior best), and 48.4\% synthesis success rate on SPoC (+3.7\% over the prior best).
 On Coresets for Regularized Regression 
 We study the effect of norm based regularization on the size of coresets for the regularized regression problems. Specifically, given a matrix $ \mathbf{A} \in {\mathbf{R}}^{n \M{x}d}$ with $n\gg d$ and a vector $\mathbf{b} \in \mathbf{R} ^ n $ and $\lambda > 0$, we analyze the size of coresets for regularized versions of regression of the form $||\M{Ax}-\M{b}||_p^r + \lambda||{\M{x}}||_q^s$ . It has been shown for the case of ridge regression ($p,q,r,s=2$) that we can obtain a coreset smaller than the coreset for its unregularized counterpart i.e. least squares regression\cite{avron2017sharper}. However we show that when $r \neq s$, no coreset for some regularized regression can have size smaller than the optimal coreset of the unregularized version. The well known LASSO problem falls under this category and hence does not allow a coreset smaller than the one for least squares regression. We propose a modified version of the LASSO problem and obtain for it a coreset of size smaller than the least square regression. We empirically show that the modified version of LASSO also induces sparsity in solution like the LASSO. We also obtain smaller coresets for $\ell_p$-regression with $\ell_p$-regularization. We extend our methods to multi response regularized regression. Finally, we empirically demonstrate the coreset performance for the modified LASSO and the $\ell_1$-regression with $\ell_1$- regularization.
 From PAC to Instance-Optimal Sample Complexity in the Plackett-Luce Model 
 We consider PAC learning a good item from $k$-subsetwise feedback sampled from a Plackett-Luce probability model, with instance-dependent sample complexity performance. In the setting where subsets of a fixed size can be tested and top-ranked feedback is made available to the learner, we give an optimal instance-dependent algorithm with a  sample complexity bound for PAC best arm identification algorithm of 
$O\bigg(\frac{\Theta_{[k]}}{k}\sum_{i = 2}^n\max\Big(1,\frac{1}{\Delta_i^2}\Big) \ln\frac{k}{\delta}\Big(\ln \frac{1}{\Delta_i}\Big)\bigg)$, $\Delta_i$ being the Plackett-Luce parameter gap between the best and the $i^{th}$ best item, and $\Theta_{[k]}$ is the sum of the Plackett-Luce parameters for top-$k$ items. The algorithm is based on a wrapper around a PAC winner-finding algorithm with weaker performance guarantees to adapt to the hardness of the input instance. The sample complexity is also shown to be multiplicatively better depending on the length of rank-ordered feedback available in each subset-wise play. We show optimality of our algorithms with matching sample complexity lower bounds. We next address the winner-finding problem in Plackett-Luce models in the fixed-budget setting with instance dependent upper and lower bounds on the misidentification probability, of $\Omega\left(\exp(-2 \tilde \Delta Q) \right)$ for a given budget $Q$, where $\tilde \Delta$ is an explicit instance-dependent problem complexity parameter. Numerical performance results are also reported for the algorithms. 
 AdaScale SGD: A User-Friendly Algorithm for Distributed Training 
 When using large-batch training to speed up stochastic gradient descent, learning rates must adapt to new batch sizes in order to maximize speed-ups and preserve model quality.  Re-tuning learning rates is resource intensive, while fixed scaling rules often degrade model quality.  We propose AdaScale SGD, an algorithm that reliably adapts learning rates to large-batch training.  By continually adapting to the gradient's variance, AdaScale automatically achieves speed-ups for a wide range of batch sizes.  We formally describe this quality with AdaScale’s convergence bound, which maintains final objective values, even as batch sizes grow large and the number of iterations decreases.  In empirical comparisons, AdaScale trains well beyond the batch size limits of popular “linear learning rate scaling” rules.  This includes large-batch training with no model degradation for machine translation, image classification, object detection, and speech recognition tasks.  AdaScale's qualitative behavior is similar to that of "warm-up" heuristics, but unlike warm-up, this behavior emerges naturally from a principled mechanism.
The algorithm introduces negligible computational overhead and no new hyperparameters, making AdaScale an attractive choice for large-scale training in practice.
 Multi-Precision Policy Enforced Training (MuPPET) : A Precision-Switching Strategy for Quantised Fixed-Point Training of CNNs 
 Large-scale convolutional neural networks (CNNs) suffer from very long training times, spanning from hours to weeks, limiting the productivity and experimentation of deep learning practitioners. As networks grow in size and complexity, training time can be reduced through low-precision data representations and
computations. However, in doing so the final accuracy suffers due to the problem of vanishing gradients. Existing state-of-the-art methods combat this issue by means of a mixed-precision approach utilising two different precision levels, FP32 (32-bit floating-point) and FP16/FP8 (16-/8-bit floating-point), leveraging the hardware support of recent GPU architectures for FP16 operations to obtain performance gains. This work pushes the boundary of quantised training by employing a multilevel optimisation approach
that utilises multiple precisions including low-precision fixed-point representations. The novel training strategy, MuPPET, combines the use of multiple number representation regimes together with a precision-switching mechanism that decides at run time the transition point between precision regimes. Overall, the proposed strategy tailors the training process to the hardware-level capabilities of the target hardware architecture and yields improvements in training time and energy efficiency compared to state-of-the-art
approaches. Applying MuPPET on the training of AlexNet, ResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA Turing GPU, MuPPET achieves the same accuracy as standard full-precision training with training-time speedup of up to 1.84× and an average speedup of 1.58× across the networks.
 On the Relation between Quality-Diversity Evaluation and Distribution-Fitting Goal in Text Generation 
 The goal of text generation models is to fit the
underlying real probability distribution of text.
For performance evaluation, quality and diversity
metrics are usually applied. However, it is still
not clear to what extend can the quality-diversity
evaluation reflect the distribution-fitting goal. In
this paper, we try to reveal such relation in a
theoretical approach. We prove that under certain
conditions, a linear combination of quality and
diversity constitutes a divergence metric between
the generated distribution and the real distribution.
We also show that the commonly used BLEU/Self-BLEU metric pair fails to match any divergence
metric, thus propose CR/NRR as a substitute for
quality/diversity metric pair.
 Curvature-corrected learning dynamics in deep neural networks 
 Deep neural networks exhibit highly non-convex loss landscape, which results in complex learning dynamics under steepest gradient descent. Second order optimization methods, such as natural gradient descent, can facilitate learning by compensating for ill-conditioned curvature. However, the exact nature of such curvature-corrected learning process remains largely unknown. Here, we derive exact solutions to curvature-corrected learning rules for the restricted case of deep linear neural networks. Our analysis reveals that natural gradient descent follows the same path as gradient descent, only adjusting the temporal dynamics along the path. This preserves the implicit bias of gradient-based learning, such as weight balance across layers. However, block-diagonal approximations of natural gradient, which are widely used in most second order methods (e.g. K-FAC), significantly distort the dynamics to follow highly divergent paths, destroying weight balance across layers. We introduce partially curvature-corrected learning rule, which provides most of the benefit of full curvature correction in terms of convergence speed with superior numerical stability while preserving the core property of gradient descent under block-diagonal approximations.
 Learning Deep Kernels for Non-Parametric Two-Sample Tests 
 We propose a class of kernel-based two-sample tests, which aim to determine whether two sets of samples are drawn from the same distribution. Our tests are constructed from kernels parameterized by deep neural nets, trained to maximize test power. These tests adapt to variations in distribution smoothness and shape over space, and are especially suited to high dimensions and complex data. By contrast, the simpler kernels used in prior kernel testing work are spatially homogeneous, and adaptive only in lengthscale. We explain how this scheme includes popular classifier-based two-sample tests as a special case, but improves on them in general. We provide the first proof of consistency for the proposed adaptation method, which applies both to kernels on deep features and to simpler radial basis kernels or multiple kernel learning. In experiments, we establish the superior performance of our deep kernels in hypothesis testing on benchmark and real-world data.
 Learning and Evaluating Contextual Embedding of Source Code 
 Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained con-textual embeddings such as BERT. These can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple tasks simultaneously.

In this paper, we alleviate this gap by curating a code-understanding benchmark and evaluating a learned contextual embedding of source code. More specifically, we curate a massive, deduplicated corpus of Python code from GitHub and train a BERT model, which we call B4C. We also create a benchmark comprising five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. For comparison, we train different variants of Word2Vec token embeddings, and BiLSTM and Transformer models. For the repair task, we also compare to SOTA models. We show that fine-tuned B4C models give better results, even with shorter training or fewer examples. Future work on source-code embedding could benefit from reusing our benchmark and comparing against B4C as a strong baseline.
 Influence Diagram Bandits 
 We propose a novel framework for structured bandits, which we call influence diagram bandit. Our framework captures complicated statistical dependencies between actions, latent variables, and observations; and unifies and extends many existing models, such as combinatorial semi-bandits, cascading bandits, and low-rank bandits. We develop novel online learning algorithms that allow us to act efficiently in our models. The key idea is to track a structured posterior distribution of model parameters, either exactly or approximately. To act, we sample model parameters from their posterior and then use the structure of the influence diagram to find the most optimistic actions under the sampled parameters. We experiment with three structured bandit problems: cascading bandits, online learning to rank in the position-based model, and rank-1 bandits. Our algorithms achieve up to about 3 times higher cumulative reward than baselines.
 Training Binary Neural Networks using the Bayesian Learning Rule 
 Neural networks with binary weights are computation-efficient and hardware-friendly, but their training is challenging because it involves a discrete optimization problem. Surprisingly, ignoring the discrete nature of the problem and using gradient-based methods, such as Straight-Through Estimator, still works well in practice. This raises the question: are there principled approaches which justify such methods? In this paper, we propose such an approach using the Bayesian learning rule. The rule, when applied to estimate a Bernoulli distribution over the binary weights, results in an algorithm which justifies some of the algorithmic choices made by the previous approaches. The algorithm not only obtains state-of-the-art performance, but also enables uncertainty estimation and continual learning to avoid catastrophic forgetting. Our work provides a principled approach for training binary neural networks which also justifies and extends existing approaches. 
 Meta-learning for Mixed Linear Regression 
 In modern supervised learning, there are a large number of tasks, but many of them are associated with only a small amount of labelled data. These include data from medical image processing and robotic interaction. Even though each individual task cannot be meaningfully trained in isolation, one seeks to meta-learn across the tasks from past experiences by exploiting some similarities. We study a fundamental question of interest: When can abundant tasks with small data compensate for lack of tasks with big data? We focus on a canonical scenario where each task is drawn from a mixture of $k$ linear regressions, and identify sufficient conditions for such a graceful exchange to hold; there is little loss in sample complexity even when we only have access to small data tasks. To this end, we introduce a novel spectral approach and show that we can efficiently utilize small data tasks with the help of $\tilde\Omega(k^{3/2})$ medium data tasks each with  $\tilde\Omega(k^{1/2})$ examples.
 Variational Autoencoders with Riemannian Brownian Motion Priors 
 Variational Autoencoders (VAEs) represent the given data in a low-dimensional latent space, which is generally assumed to be Euclidean. This assumption naturally leads to the common choice of a standard Gaussian prior over continuous latent variables. Recent work has, however, shown that this prior has a detrimental effect on model capacity, leading to subpar performance. We propose that the Euclidean assumption lies at the heart of this failure mode. To counter this, we assume a Riemannian structure over the latent space, which constitutes a more principled geometric view of the latent codes, and replace the standard Gaussian prior with a Riemannian Brownian motion prior. We propose an efficient inference scheme that does not rely on the unknown normalizing factor of this prior. Finally, we demonstrate that this prior significantly increases model capacity using only one additional scalar parameter.
 Active World Model Learning in Agent-rich Environments with Progress Curiosity 
 World models are a family of predictive models that solve self-supervised problems on how the world evolves. Humans learn world models by curiously exploring their environment, in the process acquiring compact abstractions of high bandwidth sensory inputs, the ability to plan across long temporal horizons, and an understanding of the behavioral patterns of other agents. In this work, we study how to design such a curiosity-driven Active World Model Learning (AWML) system. To do so, we simulate a curious agent building world models while visually exploring a 3D physical environment rich with distillations of representative real-world stimuli. We propose an AWML system driven by $\gamma$-Progress: a scalable and effective learning progress-based curiosity signal. We show that $\gamma$-Progress is robust to "white noise" and naturally gives rise to an exploration policy that allocates attention in a balanced manner, with a preference towards agents displaying complex yet learnable behaviors. As a result, our $\gamma$-Progress driven controller achieves significantly higher AWML performance than baseline controllers equipped with state-of-the-art exploration strategies such as Random Network Distillation and Model Disagreement. 
 Dynamic Knapsack Optimization Towards Efficient Multi-Channel Sequential Advertising 
 In E-commerce, advertising is essential for merchants to reach their target users. The typical objective is to maximize the advertiser's cumulative revenue over a period of time under a budget constraint. In real applications, an advertisement (ad) usually needs to be exposed to the same user multiple times until the user finally contributes revenue (e.g., places an order). However, existing advertising systems mainly focus on the immediate revenue with single ad exposures, ignoring the contribution of each exposure to the final conversion, thus usually falls into suboptimal solutions. In this paper, we formulate the sequential advertising strategy optimization as a dynamic knapsack problem. We propose a theoretically guaranteed bilevel optimization framework, which significantly reduces the solution space of the original optimization space while ensuring the solution quality. To improve the exploration efficiency of reinforcement learning, we also devise an effective action space reduction approach. Extensive offline and online experiments show the superior performance of our approaches over state-of-the-art baselines in terms of cumulative revenue.
 Bayesian Differential Privacy for Machine Learning 
 Traditional differential privacy is independent of the data distribution. However, this is not well-matched with the modern machine learning context, where models are trained on specific data. As a result, achieving meaningful privacy guarantees in ML often excessively reduces accuracy. We propose Bayesian differential privacy (BDP), which takes into account the data distribution to provide more practical privacy guarantees. We derive a general privacy accounting method under BDP and show that it is a generalisation of the well-known moments accountant. Our experiments demonstrate that in-distribution samples in classic machine learning datasets, such as MNIST and CIFAR-10, enjoy significantly stronger privacy guarantees than postulated by DP, while models maintain high classification accuracy.
 Simultaneous Inference for Massive Data: Distributed Bootstrap 
 In this paper, we propose a bootstrap method applied to massive data processed distributedly in a large number of machines. This new method is computationally efficient in that we bootstrap on the master machine without over-resampling, typically required by existing methods \cite{kleiner2014scalable,sengupta2016subsampled}, while provably achieving optimal statistical efficiency with minimal communication. Our method does not require repeatedly re-fitting the model but only applies multiplier bootstrap in the master machine on the gradients received from the worker machines. Simulations validate our theory.
 Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks 
 The point estimates of ReLU classification networks---arguably the most widely used neural network architecture---have been shown to yield arbitrarily high confidence far away from the training data. This architecture, in conjunction with a maximum a posteriori estimation scheme, is thus not calibrated nor robust. Approximate Bayesian inference has been empirically demonstrated to improve predictive uncertainty in neural networks, although the theoretical analysis of such Bayesian approximations is limited. We theoretically analyze approximate Gaussian posterior distributions on the weights of ReLU networks and show that they fix the overconfidence problem. Furthermore, we show that even a simplistic, thus cheap, Bayesian approximation, also fixes these issues. This indicates that a sufficient condition for a calibrated uncertainty on a ReLU network is ``to be a bit Bayesian''. These theoretical results validate the usage of last-layer Bayesian approximation and motivate a range of a fidelity-cost trade-off. We further validate these findings empirically via various standard experiments using common deep ReLU networks and Laplace approximations.

 Bootstrap Latent-Predictive Representations for Multitask Reinforcement Learning 
 Learning a good representation is an essential component for deep reinforcement learning (RL). 
Representation learning is especially important in multitask and partially observable settings where building a representation of the unknown environment is crucial to solve the tasks. 
Here we introduce Predictions of Bootstrapped Latents (PBL), a simple and flexible self-supervised representation learning algorithm for multitask deep RL.
PBL builds on multistep predictive representations of future observations, and focuses on capturing structured information about environment dynamics.
Specifically, PBL trains its representation by predicting latent embeddings of future observations. These latent embeddings are themselves trained to be predictive of the aforementioned representations. These predictions form a bootstrapping effect, allowing the agent to learn more about the key aspects of the environment dynamics.
In addition, by defining prediction tasks completely in latent space, PBL provides the flexibility of using multimodal observations involving pixel images, language instructions, rewards and more. 
We show in our experiments that PBL delivers across-the-board improved performance over state of the art deep RL agents in the DMLab-30 multitask setting. 
 Estimating Model Uncertainty of Neural Network in Sparse Information Form 
 We present a sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. The key insight of our work is that the information matrix, i.e. the inverse of the covariance matrix tends to be sparse in its spectrum. Therefore, dimensionality reduction techniques such as low rank approximations can be effectively exploited. To achieve this, we develop a novel sparsification algorithm and derive a cost-effective analytical sampler. As a result, we show that the information form of MND can be scalably applied to represent model uncertainty in MND. Our exhaustive theoretical analysis and empirical evaluations on various benchmarks show the competitiveness of our approach over the current methods.
 Randomization matters How to defend against strong adversarial attacks 
 \emph{Is there a classifier that ensures optimal robustness against all adversarial attacks?}
This paper answers this question by adopting a game-theoretic point of view. We show that adversarial attacks and defenses form an \emph{infinite} zero-sum game where classical results (e.g. Nash or Sion theorems) do not apply. We demonstrate the non-existence of a Nash equilibrium in our game when the classifier and the adversary are both deterministic, hence giving a negative answer to the above question in the deterministic regime. Nonetheless, the question remains open in the randomized regime. We tackle this problem by showing that, under mild conditions on the dataset distribution, any deterministic classifier can be outperformed by a randomized one. This gives arguments for using randomization, and leads us to a new algorithm for building randomized classifiers that are robust to \emph{strong} adversarial attacks. Empirical results validate our theoretical analysis, and show that our defense method considerably outperforms Adversarial Training against state-of-the-art attacks.
 Optimal Continual Learning has Perfect Memory and is NP-hard 
 Continual Learning (CL) algorithms incrementally learn a predictor or representation across multiple sequentially observed tasks. Designing CL algorithms that perform reliably and avoid so-called catastrophic forgetting has proven a persistent challenge. The current paper develops a theoretical approach that explains why. In particular, we derive the computational properties which CL algorithms would have to possess in order to avoid catastrophic forgetting. Our main finding is that such optimal CL algorithms generally solve an NP-hard problem and will require perfect memory to do so. The findings are of theoretical interest, but also explain the excellent performance of CL algorithms using experience replay, episodic memory and core sets relative to regularization-based approaches.
 Parameter-free, Dynamic, and Strongly-Adaptive Online Learning 
 We provide a new online learning algorithm that for the first time combines several disparate notions of adaptivity. First, our algorithm obtains a ``parameter-free'' regret bound that adapts to the norm of the comparator and the squared norm of the size of the gradients it observes. Second, it obtains a ``strongly-adaptive'' regret bound, so that for any given interval of length $N$, the regret over the interval is $\tilde O(\sqrt{N})$. Finally, our algorithm obtains an optimal ``dynamic'' regret bound: for any sequence of comparators with path-length $P$, our algorithm obtains regret $\tilde O(\sqrt{PN})$ over intervals of length $N$. Our primary technique for achieving these goals is a new method of combining constrained online learning regret bounds that does not rely on an expert meta-algorithm to aggregate learners.
 Learning Reasoning Strategies in End-to-End Differentiable Proving 
 Attempts to render deep learning models interpretable, data-efficient, and robust has seen some success through hybridisation with rule-based systems like Neural Theorem Provers (NTPs). These neuro-symbolic reasoning models can induce interpretable rules and learn representations from data via back-propagation, while providing logical explanations for their predictions.  However, they are restricted by their computational complexity, as they need to consider all possible proof paths for explaining a goal, thus rendering them unfit for large-scale applications. We present Conditional Theorem Provers (CTPs), an extension to NTPs that learns an optimal rule selection strategy via gradient-based optimisation.  We show that CTPs are scalable and yield state-of-the-art results on the CLUTRR dataset, which tests systematic generalisation of neural models. CTPs generalise better than a wide range of baselines when tested on larger graphs than trained on, while producing competitive results on standard link prediction benchmarks.
 Feature-map-level Online Adversarial Knowledge Distillation 
 Feature maps contain rich information about image intensity and spatial correlation. However, previous online knowledge distillation methods only utilize the class probabilities. Thus in this paper, we propose an online knowledge distillation method that transfers not only the knowledge of the class probabilities but also that of the feature map using the adversarial training framework. We train multiple networks simultaneously by employing discriminators to distinguish the feature map distributions of different networks. Each network has its corresponding discriminator which discriminates the feature map from its own as fake while classifying that of the other network as real. By training a network to fool the corresponding discriminator, it can learn the other network’s feature map distribution. We show that our method performs better than the conventional direct alignment method such as L1 and is more suitable for online distillation. Also, we propose a novel cyclic learning scheme for training more than two networks together. We have applied our method to various network architectures on the classification task and discovered a significant improvement of performance especially in the case of training a pair of a small network and a large one.
 Compressive sensing with un-trained neural networks: Gradient descent finds a smooth approximation 
 Un-trained convolutional neural networks have emerged as highly successful tools for image recovery and restoration. They are capable of solving standard inverse problems such as denoising and compressive sensing with excellent results by simply fitting a neural network model to measurements from a single image or signal without the need for any additional training data. For some applications, this critically requires additional regularization in the form of early stopping the optimization. For signal recovery from a few measurements, however, un-trained convolutional networks have an intriguing self-regularizing property: Even though the network can perfectly fit any image, the network recovers a natural image from few measurements when trained with gradient descent until convergence. In this paper, we demonstrate this property numerically and study it theoretically. We show that---without any further regularization---an un-trained convolutional neural network can approximately reconstruct signals and images that are sufficiently structured, from a near minimal number of random measurements.
 Entropy Minimization In Emergent Languages 
 There is a growing interest in studying the languages emerging when neural agents are jointly trained to solve tasks requiring communication through a discrete channel.  We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.
 Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks 
 Meta-learning algorithms produce feature extractors which achieve state-of-the-art performance on few-shot classification.  While the literature is rich with meta-learning methods, little is known about why the resulting feature extractors perform so well.  We develop a better understanding of the underlying mechanics of meta-learning and the difference between models trained using meta-learning and models which are trained classically.  In doing so, we develop several hypotheses for why meta-learned models perform better.  In addition to visualizations, we design several regularizers inspired by our hypotheses which improve performance on few-shot classification.
 Being Bayesian about Categorical Probability 
 Neural networks utilize the softmax as a building block in classification tasks, which contains an overconfidence problem and lacks an uncertainty representation ability. As a Bayesian alternative to the softmax, we consider a random variable of a categorical probability over class labels. In this framework, the prior distribution explicitly models the presumed noise inherent in the observed label, which provides consistent gains in generalization performance in multiple challenging tasks. The proposed method inherits advantages of Bayesian approaches that achieve better uncertainty estimation and model calibration. Our method can be implemented as a plug-and-play loss function with negligible computational overhead compared to the softmax with the cross-entropy loss function.
 Certified Data Removal from Machine Learning Models 
 Good data stewardship requires removal of data at the request of the data's owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it possible to ``remove'' data from a machine-learning model? We study this problem by defining certified removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certified-removal mechanism for linear classifiers and empirically study learning settings in which this mechanism is practical.
 Deep Isometric Learning for Visual Recognition 
 Initialization, residual learning, and normalization are believed to be three indispensable techniques for training very deep convolutional neural networks and obtaining state-of-the-art performance. This paper shows that deep vanilla ConvNets without normalization nor residual structure can also be trained to achieve surprisingly good performance on standard image recognition benchmarks (ImageNet, and COCO). This is achieved by enforcing the convolution kernels to be near isometric during initialization and training, as well as by using a variant of ReLU that is shifted towards being isometric. Further experiments show that if combined with residual structure, such near isometric networks can achieve performances on par with the standard ResNet, even without normalization at all. 
 On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems 
 We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in \mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding light on its superior practical performance in training generative adversarial networks (GANs) and other real applications. 
 Regularized Optimal Transport is Ground Cost Adversarial 
 Regularizing Wasserstein distances has proved to be the key in the recent advances of optimal transport (OT) in machine learning. Most prominent is the entropic regularization of OT, which not only allows for fast computations and differentiation using Sinkhorn algorithm, but also improves stability with respect to data and accuracy in many numerical experiments. Theoretical understanding of these benefits remains unclear, although recent statistical works have shown that entropy-regularized OT mitigates classical OT's curse of dimensionality. In this paper, we adopt a more geometrical point of view, and show using Fenchel duality that any convex regularization of OT can be interpreted as ground cost adversarial. This incidentally gives access to a robust dissimilarity measure on the ground space, which can in turn be used in other applications. We propose algorithms to compute this robust cost, and illustrate the interest of this approach empirically.
 Towards Accurate Post-training Network Quantization via Bit-Split and Stitching 
 Network quantization is essential for deploying deep models to IoT devices due to the high efficiency, no matter on special hardware like TPU or general hardware like CPU and GPU. Most existing quantization approaches rely on retraining to retain accuracy, which is referred to as quantization-aware training. However, this quantization scheme assumes the access to the training data, which is not always the case. Moreover, retraining is a tedious and time-consuming procedure, which hinders the application of quantization to a wider range of tasks. Post-training quantization, on the other hand, does not have these problems. However, it has only been shown effective for 8-bit quantization due to the simple optimization strategy. In this paper, we propose a Bit-Split and Stitching framework for lower-bit post-training quantization with minimal accuracy degradation. The proposed framework are validated on a variety of computer vision tasks, including image classification, object detection, instance segmentation, with various network architectures.
 Second-Order Provable Defenses against Adversarial Attacks 
 A robustness certificate against adversarial examples is the minimum distance of a given input to the decision boundary of the classifier (or its lower bound). For {\it any} perturbation of the input with a magnitude smaller than the certificate value, the classification output will provably remain unchanged. Computing exact robustness certificates for neural networks is difficult in general since it requires solving a non-convex optimization. In this paper, we provide computationally-efficient robustness certificates for neural networks with differentiable activation functions in two steps. First, we show that if the eigenvalues of the Hessian of the network (curvatures of the network) are bounded (globally or locally), we can compute a robustness certificate in the $l_2$ norm efficiently using convex optimization. Second, we derive a computationally-efficient differentiable upper bound on the curvature of a deep network. We also use the curvature bound as a regularization term during the training of the network to boost its certified robustness. Putting these results together leads to our proposed {\bf C}urvature-based {\bf R}obustness {\bf C}ertificate (CRC) and {\bf C}urvature-based {\bf R}obust {\bf T}raining (CRT). Our numerical results show that CRT leads to significantly higher certified robust accuracy compared to interval-bound propagation based training.
 FedBoost: A Communication-Efficient Algorithm for Federated Learning 
 Communication cost is often a bottleneck in federated learning and other client-based distributed learning scenarios. To overcome this, several gradient compression and model compression algorithms have been proposed. In this work, we propose an alternative approach whereby an ensemble of pre-trained base predictors is trained via federated learning. This method allows for training a model which may otherwise surpass the communication bandwidth and storage capacity of the clients to be learned with on-device data through federated learning. Motivated by language modeling, we prove the optimality of ensemble methods for density estimation for standard empirical risk minimization and agnostic risk minimization. We provide communication-efficient ensemble algorithms for federated learning, where per-round communication cost is independent of the size of the ensemble. Furthermore, unlike works on gradient compression, our proposed approach reduces the communication cost of both server-to-client and client-to-server communication.
 Reinforcement Learning for Integer Programming: Learning to Cut 
 Integer programming is a general optimization framework with a wide variety of applications, e.g., in scheduling, production planning, and graph optimization. As Integer Programs (IPs) model many provably hard to solve problems, modern IP solvers rely on heuristics. These heuristics are often human-designed, and tuned over time using experience and data.  The goal of this work is to show that the performance of those solvers can be greatly enhanced using reinforcement learning (RL). In particular, we investigate a specific methodology for solving IPs, known as the Cutting Plane Method.  This method is employed as a subroutine by all modern IP solvers. We present a deep RL formulation, network architecture, and algorithms for intelligent adaptive selection of cutting planes (aka cuts). Across a wide range of IP tasks, we show that our trained RL agent significantly outperforms human-designed heuristics, and effectively generalizes to larger instances and across IP problem classes. The trained agent is also demonstrated to benefit the popular downstream application of cutting plane methods in Branch-and-Cut algorithm, which is the backbone of state-of-the-art commercial IP solvers.
 A quantile-based approach for hyperparameter transfer learning 
 Bayesian optimization (BO) is a popular methodology to tune the hyperparameters of expensive black-box functions. Traditionally, BO focuses on a single task at a time and is not designed to leverage information from related functions, such as tuning performance objectives of the same algorithm across multiple datasets. In this work, we introduce a novel approach to achieve transfer learning across different datasets as well as different objectives. The main idea is to regress the mapping from hyperparameter to objective quantiles with a semi-parametric Gaussian Copula distribution, which provides robustness against different scales or outliers that can occur in different tasks. We introduce two methods to leverage this estimation: a Thompson sampling strategy as well as a Gaussian Copula process using such quantile estimate as a prior. We show that these strategies can combine the estimation of multiple objectives such as latency and accuracy, steering the optimization toward faster predictions for the same level of accuracy. Experiments on an extensive set of hyperparameter tuning tasks demonstrate significant improvements over state-of-the-art methods for both hyperparameter optimization and neural architecture search.

 Normalizing Flows on Tori and Spheres 
 Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.
 A Mean Field Analysis Of Deep ResNet And Beyond: Towards  Provably Optimization Via Overparameterization From Depth 
 Training deep neural networks with stochastic gradient descent (SGD) can often achieve zero training loss on real-world tasks although the optimization landscape is known to be highly non-convex. To understand the success of SGD for training deep neural networks, this work presents a mean-field analysis of deep residual networks, based on a line of works which interpret the continuum limit of the deep residual network as an ordinary differential equation as the the network capacity tends to infinity. Specifically, we propose a \textbf{new continuum limit} of deep residual networks, which enjoys a good landscape in the sense that \textbf{every local minimizer is global}. 
This characterization enables us to derive the first global convergence result for multilayer neural networks in the mean-field regime. Furthermore, our proof does not rely on the convexity of the loss landscape, but instead, an assumption on the global minimizer should achieve zero loss which can be achieved when the model shares a universal approximation property. Key to our result is the observation that a deep residual network resembles a shallow network ensemble~\cite{veit2016residual}, \emph{i.e.} a two-layer network. We bound the difference between the shallow network and our ResNet model via the adjoint sensitivity method, which enables us to transfer previous mean-field analysis of two-layer networks to deep networks. Furthermore, we propose several novel training schemes based on our new continuous model, among which one new training procedure introduces the operation of switching the order of the residual blocks and results in strong empirical performance on benchmark datasets. 
 GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation 
 This paper presents a new Graph Neural Network (GNN) type using feature-wise linear modulation (FiLM).
Many standard GNN variants propagate information along the edges of a graph by computing messages based only on the representation of the source of each edge.
In GNN-FiLM, the representation of the target node of an edge is used to compute a transformation that can be applied to all incoming messages, allowing feature-wise modulation of the passed information.

Different GNN architectures are compared in extensive experiments on three tasks from the literature, using re-implementations of many baseline methods.
Hyperparameters for all methods were found using extensive search, yielding somewhat surprising results: differences between state of the art models are much smaller than reported in the literature and well-known simple baselines that are often not compared to perform better than recently proposed GNN variants.
Nonetheless, GNN-FiLM outperforms these methods on a regression task on molecular graphs and performs competitively on other tasks.
 Understanding and Mitigating the Tradeoff between Robustness and Accuracy 
 Adversarial training augments the training set with perturbations to improve the robust error (over worst-case perturbations), but it often leads to an increase in the standard error (on unperturbed test inputs). Previous explanations for this tradeoff rely on the assumption that no predictor in the hypothesis class has low standard and robust error. In this work, we precisely characterize the effect of augmentation on the standard error in linear regression when the optimal linear predictor has zero standard and robust error. In particular, we show that the standard error could increase even when the augmented perturbations have noiseless observations from the optimal linear predictor. We then prove that the recently proposed robust self-training (RST) estimator improves robust error without sacrificing standard error for noiseless linear regression. Empirically, for neural networks, we find that RST with different adversarial training methods improves both standard and robust error for random and adversarial rotations and adversarial l_infty perturbations in CIFAR-10.
 Data preprocessing to mitigate bias: A maximum entropy based approach 
 Data containing human or social features may over- or under-represent groups with respect to salient social attributes such as gender or race, which can lead to biases in downstream applications. Prior approaches towards preprocessing data to mitigate such biases either reweigh the points in the dataset or set up a constrained optimization problem on the domain to minimize a metric of bias. However, the former do not learn a distribution over the entire domain and the latter do not scale well with the domain size. This paper presents an optimization framework that can be used as a data preprocessing method towards mitigating bias: It can learn distributions over large domains and controllably adjust the representation rates of protected groups and/or achieve target fairness metrics such as statistical parity, yet remains close to the empirical distribution induced by the given dataset. Our approach appeals to the principle of maximum entropy, which states that amongst all distributions satisfying a given set of constraints, we should choose the one closest in KL-divergence to a given prior. While maximum entropy distributions can succinctly encode distributions over large domains, they can be difficult to compute. Our main technical contribution is an instantiation of the maximum entropy framework for our set of constraints and priors, which encode our bias mitigation goals, that runs in time polynomial in the dimension of the data. Empirically, we observe that samples from the learned distribution have desired representation rates and statistical rates, and when used for training a classifier incurs only a slight loss in accuracy while maintaining fairness properties.
 Online Convex Optimization in the Random Order Model 
 Online Convex Optimization (OCO) is a powerful framework for sequential prediction, portraying the natural uncertainty inherent in data-streams as though the data were generated by an almost limitless adversary. However, this view, which is often too pessimistic for real-world data, comes with a price. The complexity of solving many important online tasks in this adversarial framework becomes much worse than that of their offline counterparts.

In this work we consider a natural random-order version of the OCO model, in which the adversary can choose the set of loss functions, but does not get to choose the order in which they are supplied to the learner; Instead, they are observed in uniformly random order. While such a model is clearly not suitable for temporal data, which inherently depends on time, it is very much plausible in distributed settings, in which data is generated by multiple independent sources, or streamed without particular order.

Focusing on two important families of online tasks, one which generalizes online linear and logistic regression, and the other being online PCA, we show that under standard well-conditioned-data assumptions (that are often being made in the corresponding offline settings), standard online gradient descent (OGD) methods become much more efficient in the random-order model. In particular, for the first group of tasks which includes linear regression, we show that OGD guarantees polylogarithmic regret (while the only method to achieve comparable regret in the fully-adversarial setting is the Online-Newton Step method which requires quadratic memory and at least quadratic runtime). This result holds even without assuming the convexity of individual loss functions. In the case of online k-PCA, we show that OGD minimizes regret using only a rank-k SVD on each iteration and requires only linear memory (instead of nearly quadratic memory and/or potentially high-rank SVDs required by algorithms for the fully-adversarial setting).
 The role of regularization in classification of high-dimensional noisy Gaussian mixture 
 We consider a high-dimensional mixture of two Gaussians in the noisy regime where even an oracle knowing the centers of the clusters misclassifies a small but finite fraction of the points. 
We provide a rigorous analysis of the generalization error of regularized convex classifiers, including ridge, hinge and logistic regression, in the high-dimensional limit where the number $n$ of samples and their dimension $d$ goes to infinity while their ratio is fixed to $\alpha=n/d$.  
We discuss surprising effects of the regularization that in some cases allows to reach the Bayes-optimal performances, we illustrate the interpolation peak at low regularization, and analyze the role of the respective sizes of the two clusters. 
 Frustratingly Simple Few-Shot Object Detection 
 Detecting rare objects with a few examples is an emerging problem. Prior works show meta-learning is a promising approach. But, fine-tuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by a large margin on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: Pascal VOC, COCO and LVIS. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks.
 Statistically Efficient Off-Policy Policy Gradients 
 Policy gradient methods in reinforcement learning update policy parameters by taking steps in the direction of an estimated gradient of policy value. In this paper, we consider the efficient estimation of policy gradients from off-policy data, where the estimation is particularly non-trivial. We derive the asymptotic lower bound on the feasible mean-squared error in both Markov and non-Markov decision processes and show that existing estimators fail to achieve it in general settings. We propose a meta-algorithm that achieves the lower bound without any parametric assumptions and exhibits a unique 4-way double robustness property. We discuss how to estimate nuisances that the algorithm relies on. Finally, we establish guarantees at the rate at which we approach a stationary point when we take steps in the direction of our new estimated policy gradient.
 Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks 
 The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and  user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 40 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\%$, identifying several broken defenses.
 ECLIPSE: An Extreme-Scale Linear Program Solver for Web-Applications 
 Key problems arising in web applications (with millions of users and thousands of items) can be formulated as Linear Programs (LP) involving billions to trillions of decision variables and constraints. Despite the appeal of LP formulations, solving problems at these scales is well beyond the capabilities of existing LP solvers. Often ad-hoc decomposition rules are used to approximately solve these LPs, which have limited optimality guarantees and lead to sub-optimal performance in practice. In this work, we propose a distributed solver that solves a perturbation of the LP problems at scale. We propose a gradient-based algorithm on the smooth dual of the perturbed LP with computational guarantees. The main workhorses of our algorithm are distributed matrix-vector multiplications (with load balancing) and efficient projection operations on distributed machines. Experiments on real-world data show that our proposed LP solver, ECLIPSE, can solve problems with $10^{12}$ decision variables -- well beyond the capabilities of current solvers.
 The Buckley-Osthus model and the block preferential attachment model: statistical analysis and application 
 This paper is concerned with statistical estimation of two preferential attachment models: the Buckley-Osthus model and the block preferential attachment model. We prove that the maximum likelihood estimates for both models are consistent. We perform simulation studies to corroborate our theoretical findings. We also apply both models to study the evolution of a real-world network. A list of open problems are presented.
 Evolving Machine Learning Algorithms From Scratch 
 Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.
 Source Separation with Deep Generative Priors 
 Despite substantial progress in signal source separation, results for richly structured data continue to contain perceptible artifacts. In contrast, recent deep generative models can produce authentic samples in a variety of domains that are indistinguishable from samples of the data distribution. This paper introduces a Bayesian approach to source separation that uses deep generative models as priors over the components of a mixture of sources, and Langevin dynamics to sample from the posterior distribution of sources given a mixture. This decouples the source separation problem from generative modeling, enabling us to directly use cutting-edge generative models as priors. The method achieves state-of-the-art performance for MNIST digit separation. We introduce new methodology for evaluating separation quality on richer datasets, providing quantitative evaluation and qualitative discussion of results for CIFAR-10 image separation.
 Involutive MCMC: One Way to Derive Them All 
 Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental problems such as inference, integration, optimization, and simulation. The field has developed a broad spectrum of algorithms, varying in the way they are motivated, the way they are applied and how efficiently they sample. Despite all the differences, many of them share the same core principle, which we unify as the Involutive MCMC (iMCMC) framework. Building upon this, we describe a wide range of MCMC algorithms in terms of iMCMC, and formulate a number of "tricks" which one can use as design principles for developing new MCMC algorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms, which facilitates the derivation of powerful extensions. We demonstrate the latter with two examples where we transform known reversible MCMC algorithms into more efficient irreversible ones.
 PolyGen: An Autoregressive Generative Model of 3D Meshes 
 Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is fully probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task.  We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task. 
 Fast Learning of Graph Neural Networks with Guaranteed Generalizability: One-hidden-layer Case 
 Although graph neural networks (GNNs) have made great progress recently on learning from graph-structured data in practice, their theoretical guarantee on generalizability remains elusive in the literature. In this paper, we provide a theoretically-grounded generalizability analysis of GNNs with one hidden layer for both regression and binary classification problems. Under the assumption that there exists a ground-truth GNN model (with zero generalization error), the objective of GNN learning is to estimate the ground-truth GNN parameters from the training data. To achieve this objective, we propose a learning algorithm that is built on tensor initialization and accelerated gradient descent. We then show that the proposed learning algorithm converges to the ground-truth GNN model for the regression problem, and to a model sufficiently close to the ground-truth for the binary classification problem. Moreover, for both cases, the convergence rate of the proposed learning algorithm is proven to be linear and faster than the vanilla gradient descent algorithm. We further explore the relationship between the sample complexity of GNNs and their underlying graph properties. Lastly, we provide numerical experiments to demonstrate the validity of our analysis and the effectiveness of the proposed learning algorithm for GNNs.
 Sample Amplification: Increasing Dataset Size even when Learning is Impossible 
 Given data drawn from an unknown distribution, D, to what extent is it possible to ``amplify'' this dataset and faithfully output an even larger set of samples that appear to have been drawn from D? We formalize this question as follows: an (n,m) amplification procedure takes as input n independent draws from an unknown distribution D, and outputs a set of m > n ``samples'' which must be indistinguishable from m samples drawn iid from D. We consider this sample amplification problem in two fundamental settings: the case where D is an arbitrary discrete distribution supported on k elements, and the case where D is a d-dimensional Gaussian with unknown mean, and fixed covariance matrix. Perhaps surprisingly, we show a valid amplification procedure exists for both of these settings, even in the regime where the size of the input dataset, n, is significantly less than what would be necessary to learn distribution D to non-trivial accuracy. We also show that our procedures are optimal up to constant factors.  Beyond these results, we describe potential applications of such data amplification, and formalize a number of curious directions for future research along this vein. 
 Reinforcement Learning with Differential Privacy 
 Motivated by high-stakes decision-making domains like personalized
medicine where user information is inherently sensitive, we design
privacy preserving exploration policies for episodic reinforcement
learning (RL). We first provide a meaningful privacy formulation using
the notion of joint differential privacy (JDP)--a strong variant of
differential privacy for settings where each user receives their own
sets of output (e.g., policy recommendations). We then develop a
private optimism-based learning algorithm that simultaneously achieves
strong PAC and regret bounds, and enjoys a JDP guarantee. Our
algorithm only pays for a moderate privacy cost on exploration: in
comparison to the non-private bounds, the privacy parameter only
appears in lower-order terms.  Finally, we present lower bounds on
sample complexity and regret for reinforcement learning subject to
JDP.
 A new regret analysis for Adam-type algorithms 
 In this paper, we focus on a theory-practice gap for Adam and its variants (AMSgrad, AdamNC, etc.). In practice, these algorithms are used with a constant first-order moment parameter $\beta_{1}$ (typically between $0.9$ and $0.99$). In theory, regret guarantees for online convex optimization require a rapidly decaying $\beta_{1}\to0$ schedule. We show that this is an artifact of the standard analysis, and we propose a novel framework that allows us to derive optimal, data-dependent regret bounds with a constant $\beta_{1}$, without further assumptions. We also demonstrate the flexibility of our analysis on a wide range of different algorithms and settings.
 Which Tasks Should Be Learned Together in Multi-task Learning? 
 Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.
 Incremental Sampling Without Replacement for Sequence Models 
 Sampling is a fundamental technique, and sampling without replacement is often desirable when duplicate samples are not beneficial. Within machine learning, sampling is useful for generating diverse outputs from a trained model. We present an elegant procedure for sampling without replacement from a broad class of randomized programs, including generative neural models that construct outputs sequentially. Our procedure is efficient even for exponentially-large output spaces. Unlike prior work, our approach is incremental, i.e., samples can be drawn one at a time, allowing for increased flexibility. We also present a new estimator for computing expectations from samples drawn without replacement. We show that incremental sampling without replacement is applicable to many domains, e.g., program synthesis and combinatorial optimization.
 Student-Teacher Curriculum Learning via Reinforcement Learning: Predicting Hospital Inpatient Admission Location 
 Accurate and reliable prediction of hospital admission location is important due to resource-constraints and space availability in a clinical setting, particularly when dealing with patients who come from the emergency department. In this work we propose a student-teacher network via reinforcement learning to deal with this specific problem. A representation of the weights of the student network is treated as the state and is fed as an input to the teacher network. The teacher network's action is to select the most appropriate batch of data to train the student network on from a training set sorted according to entropy. By validating on three datasets, not only do we show that our approach outperforms state-of-the-art methods on tabular data and performs competitively on image recognition, but also that novel curricula are learned by the teacher network. We demonstrate experimentally that the teacher network can actively learn about the student network and guide it to achieve better performance than if trained alone.
 (Individual) Fairness for k-Clustering 
 We give a local search based algorithm for $k$-median ($k$-means) clustering from the perspective of individual fairness. 
More precisely, for a point $x$ in a point set $P$ of size $n$, let $r(x)$ be the minimum radius such that the ball of radius $r(x)$ centered at $x$ has at least $n/k$ points from $P$. Intuitively, if a set of $k$ random points are chosen from $P$ as centers, every point $x\in P$ expects to have a center within radius $r(x)$. An individually fair clustering provides such a guarantee for every point $x\in P$. This notion of fairness was introduced in [Jung et al., 2019] where they showed how to get an approximately feasible $k$-clustering with respect to this fairness condition.

In this work, we show how to get an approximately \emph{optimal} such fair $k$-clustering. The $k$-median ($k$-means) cost of our solution is within a constant factor of the cost of an optimal fair $k$-clustering, and our solution approximately satisfies the fairness condition (also within a constant factor). 

 Task Understanding from Confusing Multi-task Data 
 Beyond machine learning's success in the specific tasks, research for learning multiple tasks simultaneously is referred to as multi-task learning. However, existing multi-task learning needs manual definition of tasks and manual task annotation. A crucial problem for advanced intelligence is how to understand the human task concept using basic input-output pairs. Without task definition, samples from multiple tasks are mixed together and result in a confusing mapping challenge. We propose Confusing Supervised Learning (CSL) that takes these confusing samples and extracts task concepts by differentiating between these samples. We theoretically proved the feasibility of the CSL framework and designed an iterative algorithm to distinguish between tasks. The experiments demonstrate that our CSL methods could achieve a human-like task understanding without task labeling in multi-function regression problems and multi-task recognition problems.

 Time-Consistent Self-Supervision for Semi-Supervised Learning 
 Semi-supervised learning (SSL) leverages unlabeled data when training a model with insufficient labeled data. A common strategy for SSL is to enforce the consistency of model outputs between similar samples, e.g., neighbors or data augmentations of the same sample. However, model outputs can vary dramatically on unlabeled data over different training stages, e.g., when using large learning rates. This can introduce harmful noises and inconsistent objectives over time that may lead to concept drift and catastrophic forgetting. In this paper, we study the dynamics of neural net outputs in SSL and show that selecting and using first the unlabeled samples with more consistent outputs over the course of training (i.e., "time-consistency") can improve the final test accuracy and save computation. Under the time-consistent data selection, we design an SSL objective composed of two self-supervised losses, i.e., a consistency loss between a sample and its augmentation, and a contrastive loss encouraging different samples to have different outputs. Our approach achieves SOTA on several SSL benchmarks with much fewer computations.
 An Explicitly Relational Neural Network Architecture 
 With a view to bridging the gap between deep learning and symbolic AI, we present a novel end-to-end neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. In order to evaluate and analyse the architecture, we introduce a family of simple visual relational reasoning tasks of varying complexity. We show that the proposed architecture, when pre-trained on a curriculum of such tasks, learns to generate reusable representations that better facilitate subsequent learning on previously unseen tasks when compared to a number of baseline architectures. The workings of a successfully trained model are visualised to shed some light on how the architecture functions.
 Sharp Composition Bounds for Gaussian Differential Privacy via Edgeworth Expansion 
 Datasets containing sensitive information are often sequentially analyzed by many algorithms and, accordingly, a fundamental question in differential privacy is concerned with how the overall privacy bound degrades under composition. To address this question, we introduce a family of analytical and sharp privacy bounds under composition using the Edgeworth expansion in the framework of the recently proposed $f$-differential privacy. In short, whereas the existing composition theorem, for example, relies on the central limit theorem, our new privacy bounds under composition gain improved tightness by leveraging the refined approximation accuracy of the Edgeworth expansion. Our approach is easy to implement and computationally efficient for any number of compositions. The superiority of these new bounds is confirmed by an asymptotic error analysis and an application to quantifying the overall privacy guarantees of noisy stochastic gradient descent used in training private deep neural networks.
 Feature Selection using Stochastic Gates 
 Feature selection problems have been extensively studied in the setting of linear estimation, for instance LASSO, but less emphasis has been placed on feature selection for neural networks. 
In this study, we propose a method for feature selection in non-linear function estimation problems. The new procedure is based on directly penalizing the $\ell_0$ norm of features, or the count of the number of selected features. Our $\ell_0$ based regularization relies on a continuous relaxation of the Bernoulli distribution, which allows our model to learn the parameters of the approximate Bernoulli distributions via gradient descent. The proposed framework simultaneously learns a non-linear regression or classification function while selecting a small subset of features. We provide an information-theoretic justification for incorporating Bernoulli distribution for feature selection. Furthermore, we evaluate our method using synthetic and real-life data and demonstrate that our approach outperforms other commonly used methods in terms of predictive performance and feature selection.
 LTF: A Label Transformation Framework for Correcting Label Shift 
 Distribution shift is a major obstacle to the deployment of current deep learning models on real-world problems. Let $Y$ be the class label and $X$ the features. We focus on one type of distribution shift, \textit{ label shift}, where the label marginal distribution $P_Y$ changes but the conditional distribution $P_{X|Y}$ does not. Most existing methods estimate the density ratio between the source- and target-domain label distributions by density matching. However, these methods are either computationally infeasible for large-scale data or restricted to shift correction for discrete labels. In this paper, we propose an end-to-end Label Transformation Framework (LTF) for correcting label shift, which implicitly models the shift of $P_Y$ and the conditional distribution $P_{X|Y}$ using neural networks. Thanks to the flexibility of deep networks, our framework can handle continuous, discrete, and even multi-dimensional labels in a unified way and is scalable to large data. Moreover, for high dimensional $X$, such as images, we find that the redundant information in $X$ severely degrades the estimation accuracy. To remedy this issue, we propose to match the distribution implied by our generative model and the target-domain distribution in a low-dimensional feature space that discards information irrelevant to $Y$. Both theoretical and empirical studies demonstrate the superiority of our method over previous approaches.  
 Discriminative Jackknife: Quantifying Uncertainty in Deep Learning via Higher-Order Influence Functions 
 Deep learning models achieve high predictive accuracy in a broad spectrum of tasks, but rigorously quantifying their predictive uncertainty remains challenging. Usable estimates of predictive uncertainty should (1) cover the true prediction target with a high probability, and (2) discriminate between high- and low-confidence prediction instances. State-of-the-art methods for uncertainty quantification are based predominantly on Bayesian neural networks. However, Bayesian methods may fall short of (1) and (2) — i.e., Bayesian credible intervals do not guarantee frequentist coverage, and approximate posterior inference may undermine discriminative accuracy. To this end, this paper develops the discriminative jackknife (DJ), a frequentist procedure that uses higher-order influence functions (HOIFs) of a trained model parameters to construct a jackknife (leave-one-out) estimator of predictive confidence intervals. The DJ satisfies (1) and (2), is applicable to  a wide range of deep learning models, is easy to implement, and can be applied in a post-hoc fashion without compromising model accuracy. Experiments demonstrate that DJ performs competitively compared to existing Bayesian and non-Bayesian baselines. 
 TrajectoryNet: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics 
 It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present {\em TrajectoryNet}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.
 Outsourced Bayesian Optimization 
 This paper presents the outsourced-Gaussian process-upper confidence bound (O-GP-UCB) algorithm, which is the first algorithm for privacy-preserving Bayesian optimization (BO) in the outsourced setting with a provable performance guarantee. We consider the outsourced setting where the entity holding the dataset and the entity performing BO are represented by different parties, and the dataset cannot be released non-privately. For example, a hospital holds a dataset of sensitive medical records and outsources the BO task on this dataset to an industrial AI company. The key idea of our approach is to make the BO performance of our algorithm similar to that of non-private GP-UCB run using the original dataset, which is achieved by using a random projection-based transformation that preserves both privacy and the pairwise distances between inputs. Our main theoretical contribution is to show that a regret bound similar to that of the standard GP-UCB algorithm can be established for our O-GP-UCB algorithm. We empirically evaluate the performance of our O-GP-UCB algorithm with synthetic and real-world datasets.
 Non-Autoregressive Neural Text-to-Speech 
 In this work, we propose ParaNet, a non-autoregressive seq2seq model that converts text to spectrogram. It is fully convolutional and brings 46.7 times speed-up over its autoregressive counterpart at synthesis, while obtaining reasonably good speech quality.  ParaNet also produces stable alignment between text and speech on the challenging test sentences by iteratively improving the attention in a layer-by-layer manner.  Furthermore, we build the parallel text-to-speech system by applying various parallel neural vocoders, which can synthesize speech from text through a single feed-forward pass.  We also explore a novel approach to train the IAF-based vocoder from scratch, which avoids the need for distillation from a separately trained WaveNet.
 Hybrid Stochastic-Deterministic Minibatch Proximal Gradient: Less-Than-Single-Pass Optimization with Nearly Optimal Generalization 
 Stochastic variance-reduced gradient (SVRG) algorithms have been shown to work favorably in solving large-scale learning problems. Despite the remarkable success, the stochastic gradient complexity of SVRG-type algorithms usually scales linearly with data size and thus could still be expensive for huge data. To address this deficiency, we propose a hybrid stochastic-deterministic minibatch proximal gradient (HSDMPG) algorithm for strongly-convex problems that enjoys provably improved data-size-independent complexity guarantees. More precisely, for quadratic loss $F(\wm)$ of $n$ components, we prove that HSDMPG can attain an $\epsilon$-optimization-error  $E[F(\theta)-F(\theta^*)] \leq \epsilon$ within $\mathcal{O}\Big(\!\frac{\kappa^{1.5}}{\epsilon^{0.25}}\!  \log^{\!1.5}\!\!\big(\frac{1}{\epsilon}\big) \wedge   \Big(\!\kappa \sqrt{n}  \log^2\!\!\big(\frac{1}{\epsilon}\big) \!+\! \frac{\kappa^3}{n^{1.5}\epsilon} \!\Big)\!\Big)$ stochastic gradient evaluations, where $\kappa$ is condition number. For generic strongly convex loss functions, we prove a nearly identical complexity bound though at the cost of slightly increased logarithmic factors. For large-scale learning problems, our complexity bounds are superior to those of the prior state-of-the-art SVRG algorithms with or without dependence on data size. Particularly, in the case of $\epsilon\!=\!\mathcal{O}\big(1/\sqrt{n}\big)$ which is at the order of intrinsic excess error bound of a learning model and thus sufficient for generalization,  the stochastic gradient complexity bounds of HSDMPG~for quadratic and generic loss functions are respectively $\mathcal{O} (n^{0.875}\log^{1.5}(n))$ and $\mathcal{O} (n^{0.875}\log^{2.25}(n))$, which to our best knowledge, for the first time achieve optimal generalization in less than a single pass over data. Extensive numerical results demonstrate the computational advantages of our algorithm over the prior ones.
 Fair Generative Modeling via Weak Supervision 
 Real-world datasets are often biased with respect to key demographic factors such as race and gender. Due to the latent nature of the underlying factors, detecting and mitigating bias is especially challenging for unsupervised machine learning. We present a weakly supervised algorithm for overcoming dataset bias for deep generative models. Our approach requires access to an additional small, unlabeled but unbiased dataset as the supervision signal, thus sidestepping the need for explicit labels on the underlying bias factors. Using this supplementary dataset, we detect the bias in existing datasets via a density ratio technique and learn generative models which efficiently achieve the twin goals of: 1) data efficiency by using training examples from both biased and unbiased datasets for learning, 2) unbiased data generation at test time. Empirically, we demonstrate the efficacy of our approach which reduces bias w.r.t. latent factors by 37.2% on average over baselines for comparable image generation using generative adversarial networks.
 On Validation and Planning of An Optimal Decision Rule with Application in Healthcare Studies 
 In the current era of personalized recommendation, one major interest is to develop an optimal individualized decision rule that assigns individuals with the best treatment option according to their covariates. Estimation of optimal decision rules (ODR) has been extensively investigated recently, however, at present, no testing procedure is proposed to verify whether these ODRs are significantly better than the naive decision rule that always assigning individuals to a fixed treatment option. In this paper, we propose a testing procedure for detecting the existence of an ODR that is better than the naive decision rule under the randomized trials. We construct the proposed test based on the difference of estimated value functions using the augmented inverse probability weighted method. The asymptotic distributions of the proposed test statistic under the null and local alternative hypotheses are established. Based on the established asymptotic distributions, we further develop a sample size calculation formula for testing the existence of an ODR in designing A/B tests. Extensive simulations and a real data application to a schizophrenia clinical trial data are conducted to demonstrate the empirical validity of the proposed methods.
 Cautious Adaptation For Reinforcement Learning in Safety-Critical Settings 
 We study the problem of safe adaptation: given a model trained on a variety of past experiences for some task, can this model learn to perform that task in a new situation while avoiding catastrophic failure? This problem setting occurs frequently in real-world reinforcement learning scenarios such as a vehicle adapting to drive in a new city, or a robotic drone adapting a policy trained only in simulation. While learning without catastrophic failures is exceptionally difficult, prior experience can allow us to learn models that make this much easier. These models might not directly transfer to new settings, but can enable cautious adaptation that is substantially safer than na\"{i}ve adaptation as well as learning from scratch. Building on this intuition, we propose risk-averse domain adaptation (RADA). RADA works in two steps: it first trains probabilistic model-based RL agents in a population of source domains to gain experience and capture epistemic uncertainty about the environment dynamics. Then, when dropped into a new environment, it employs a pessimistic exploration policy, selecting actions that have the best worst-case performance as forecasted by the probabilistic model. We show that this simple maximin policy accelerates domain adaptation in a safety-critical driving environment with varying vehicle sizes. We compare our approach against other approaches for adapting to new environments.
 Constant Curvature Graph Convolutional Networks 
 Interest has been rising lately towards methods representing data in non-Euclidean spaces (e.g. hyperbolic or elliptical)  that provide specific inductive biases useful for certain real-world data properties, e.g. scale-free, hierarchical or cyclical. However, the popular graph neural networks are currently limited in modeling data only via Euclidean geometry and associated vector space operations. Here, we bridge this gap by proposing mathematically grounded generalizations of graph convolutional networks (GCN) to (products of) constant curvature spaces. We do this by i) introducing a unified gyrovector space formalism that can interpolate smoothly between all geometries of constant curvature irrespective of their sign, ii) leveraging gyro-barycentric coordinates that generalize the classic Euclidean concept of the center of mass. Our class of models smoothly recover their Euclidean counterparts when the curvature goes to zero from either side. Empirically, we outperform Euclidean GCNs in the tasks of node classification and distortion minimization for symbolic data exhibiting non-Euclidean behavior, according to their discrete curvature.
 Approximation Guarantees of Local Search Algorithms via Localizability of Set Functions 
 This paper proposes a new framework for providing approximation guarantees of local search algorithms. Local search is a basic algorithm design technique and is widely used for various combinatorial optimization problems. To analyze local search algorithms for set function maximization, we propose a new notion called \textit{localizability} of set functions, which measures how effective local improvement is. Moreover, we provide approximation guarantees of standard local search algorithms under various combinatorial constraints in terms of localizability. The main application of our framework is sparse optimization, for which we show that restricted strong concavity and restricted smoothness of the objective function imply localizability, and further develop accelerated versions of local search algorithms. We conduct experiments in sparse regression and structure learning of graphical models to confirm the practical efficiency of the proposed local search algorithms.
 Proving the Lottery Ticket Hypothesis: Pruning is All You Need 
 The lottery ticket hypothesis (Frankle and Carbin, 2018), states that a randomly-initialized network contains a small subnetwork such that, when trained in isolation, can compete with the performance of the original network. 
    We prove an even stronger hypothesis (as was also conjectured in Ramanujan et al., 2019), showing that for every bounded distribution and every target network with bounded weights, a sufficiently over-parameterized neural network with random weights contains a subnetwork with roughly the same accuracy as the target network, without any further training. 
 (Locally) Differentially Private Combinatorial Semi-Bandits 
 In this paper, we study (locally) differentially private Combinatorial Semi-Bandits (CSB). Compared with private Multi-Armed Bandits (MAB), since the server receives more information from the user, it usually leads to additional dependence over the dimension of feedback, which is a notorious problem in private learning. 
Somewhat surprisingly, we show that it is possible to remove this side-effect caused by privacy protection and nearly match corresponding non-private best results. In detail, for general CSB with $B$-bounded smooth reward function in the sense of Chen et al. 2016, we propose a novel algorithm that achieves regret bound $\tilde{\mc{O}}(mB^2\log T / \epsilon)$ over $T$ rounds under $\epsilon$-local differential privacy, where $m$ is the number of base arms. However, for Linear CSB, $B$ equals $K$, where $K$ is the maximum number of feedback in each round, and above bound has an additional $K$ compared with non-private optimal result. We then propose a different algorithm with nearly optimal regret bound $\tilde{\mc{O}}(mK\log T / \epsilon)$ if one cares about $\epsilon$-differential privacy rather than $\epsilon$-local differential privacy. Besides, we also prove some  lower bounds in each setting.
 Population-Based Black-Box Optimization for Biological Sequence Design 
 The use of black-box optimization for the design of new biological sequences is an emerging research area with potentially revolutionary impact. The cost and latency of wet-lab experiments requires methods that find good sequences in few experimental rounds of large batches of sequences --- a setting that off-the-shelf black-box optimization methods are ill-equipped to handle. We find that the performance of existing methods varies drastically across optimization tasks, posing a significant obstacle to real-world applications. To improve robustness, we propose population-based optimization (P3BO), which generates batches of sequences by sampling from an ensemble of methods. The number of sequences sampled from any method is proportional to the quality of sequences it previously proposed, allowing P3BO to combine the strengths of individual methods while hedging against their innate brittleness. Adapting the hyper-parameters of each of the methods online using evolutionary optimization further improves performance. Through extensive experiments on in-silico optimization tasks, we show that P3BO outperforms any single method in its population, proposing  higher quality sequences as well as more diverse batches. As such, P3BO and Adaptive-P3BO are a crucial step towards deploying ML to real-world sequence design.
 Median Matrix Completion: from Embarrassment to Optimality 
 In this paper, we consider matrix completion with absolute deviation loss and obtain an estimator of the median matrix. Despite several appealing properties of median, the non-smooth absolute deviation loss leads to computational challenge for large-scale data sets which are increasingly common among matrix completion problems. A simple solution to large-scale problems is parallel computing. However, embarrassingly parallel fashion often leads to inefficient estimators. Based on the idea of pseudo data, we propose a novel refinement step, which turns such inefficient estimators into a rate (near-)optimal matrix completion procedure. The refined estimator is an approximation of a regularized least median estimator, and therefore not an ordinary regularized empirical risk estimator. This leads to a non-standard analysis of asymptotic behaviors. Empirical results are also provided to confirm the effectiveness of the proposed method.
 Simple and sharp analysis of k-means|| 
 We present a truly simple analysis of k-means|| (Bahmani et al., PVLDB 2012) -- a distributed variant of the k-means++ algorithm (Arthur and Vassilvitskii, SODA 2007) -- and improve its round complexity from O(log (Var X)), where Var X is the variance of the input data set, to O(log (Var X) / log log (Var X)), which we show to be tight. 

 Sub-linear Memory Sketches for Near Neighbor Search on Streaming Data with RACE 
 We present the first sublinear memory sketch that can be queried to find the nearest neighbors in a dataset. Our online sketching algorithm compresses an N element dataset to a sketch of size O(N^b log^3 N) in O(N^(b+1) log^3 N) time, where b < 1. This sketch can correctly report the nearest neighbors of any query that satisfies a stability condition parameterized by b. We achieve sublinear memory performance on stable queries by combining recent advances in locality sensitive hash (LSH)-based estimators, online kernel density estimation, and compressed sensing. Our theoretical results shed new light on the memory-accuracy tradeoff for nearest neighbor search, and our sketch, which consists entirely of short integer arrays, has a variety of attractive features in practice. We provide rigorous theoretical guarantees and present a thorough evaluation of the memory-recall tradeoff of our method on a friend recommendation task in social media networks, including the Google plus graph. We find that RACE provides orders of magnitude better compression than the random projection based alternative while retaining the ability to report the nearest neighbors of practical queries. We expect that our theory will lead to new insight on geometric sketching problems while our methods will enable new possibilities in high-speed data mining and IoT settings. 
 CAUSE: Learning Granger Causality from Event Sequences using Attribution Methods 
 We study the problem of learning Granger causality between event types from asynchronous, interdependent, multi-type event sequences. Existing work suffers from either limited model flexibility or poor model explainability and thus fails to uncover Granger causality across a wide variety of event sequences with diverse event interdependency. To address these weaknesses, we propose CAUSE (Causality from AttribUtions on Sequence of Events), a novel framework for the studied task. The key idea of CAUSE is to first implicitly capture the underlying event interdependency by fitting a neural point process, and then extract from the process a Granger causality statistic using an axiomatic attribution method. Across multiple datasets riddled with diverse event interdependency, we demonstrate that CAUSE achieves superior performance on correctly inferring the inter-type Granger causality over a range of state-of-the-art methods.
 Error-Bounded Correction of Noisy Labels 
 To collect large scale annotated data, it is inevitable to introduce label noise, i.e., incorrect class labels. A major challenge is to develop robust deep learning models that achieve high test performance despite training set label noise. We introduce a novel approach that directly cleans labels in order to train a high quality model. Our method leverages statistical principles to correct data labels and has a theoretical guarantee of the correctness. In particular, we use a likelihood ratio test to flip the labels of training data. We prove that the corrected labels are consistent with the true Bayesian optimal classifier with high probability. We incorporate our label correction algorithm into the training of deep neural networks and train models that achieve superior testing performance on multiple public datasets. 
 Scalable Differentiable Physics for Learning and Control 
 Differentiable physics is a powerful approach to learning and control problems that involve physical objects and environments. While notable progress has been made, the capabilities of differentiable physics solvers remain limited. We develop a scalable framework for differentiable physics that can support a large number of objects and their interactions. To accommodate objects with arbitrary geometry and topology, we adopt meshes as our representation and leverage the sparsity of contacts for scalable differentiable collision handling. Collisions are resolved in localized regions to minimize the number of optimization variables even when the number of simulated objects is high. We further accelerate implicit differentiation of optimization with nonlinear constraints. Experiments demonstrate that the presented framework requires up to two orders of magnitude less memory and computation in comparison to recent particle-based methods. We further validate the approach on inverse problems and control scenarios, where it outperforms derivative-free and model-free baselines by at least an order of magnitude.
 On the Power of Compressed Sensing with Generative Models  
 The goal of compressed sensing is to learn a structured signal $x$ from a limited number of noisy linear measurements $y \approx Ax$.  In traditional compressed sensing, ``structure'' is represented by sparsity in some known basis.  Inspired by the success of deep learning in modeling images, recent work starting with Bora et.al has instead considered structure to come from a generative model $G: \R^k \to \R^n$. In this paper, we prove results that (i)establish the difficulty of this task and show that existing bounds are tight and (ii) demonstrate that the latter task is a generalization of the former.

First, we provide a lower bound matching the upper bound of Bora et.al. for compressed sensing from $L$-Lipschitz generative models $G$.  In particular, there exists such a function that requires roughly $\Omega(k \log L)$ linear measurements for sparse recovery to be possible. This holds even for the more relaxed goal of \emph{nonuniform} recovery.

Second, we show that generative models generalize sparsity as a representation of structure. In particular, we construct a ReLU-based neural network $G: \R^{k} \to \R^n$ with $O(1)$ layers and $O(n)$ activations per layer, such that the range of $G$ contains all $k$-sparse vectors.
 Flexible and Efficient Long-Range Planning Through Curious Exploration 
 Identifying algorithms that flexibly and efficiently discover temporally-extended multi-phase plans is an essential step for the advancement of robotics and model-based reinforcement learning. The core problem of long-range planning is finding an efficient way to search through the tree of possible action sequences. Existing non-learned planning solutions from the Task and Motion Planning (TAMP) literature rely on the existence of logical descriptions for the effects and preconditions for actions. This constraint allows TAMP methods to efficiently reduce the tree search problem but limits their ability to generalize to unseen and complex physical environments. In contrast, deep reinforcement learning (DRL) methods use flexible neural-network-based function approximators to discover policies that generalize naturally to unseen circumstances. However, DRL methods struggle to handle the very sparse reward landscapes inherent to long-range multi-step planning situations. Here, we propose the Curious Sample Planner (CSP), which fuses elements of TAMP and DRL by combining a curiosity-guided sampling strategy with imitation learning to accelerate planning. We show that CSP can efficiently discover interesting and complex temporally-extended plans for solving a wide range of physically realistic 3D tasks. In contrast, standard planning and learning methods often fail to solve these tasks at all or do so only with a huge and highly variable number of training samples. We explore the use of a variety of curiosity metrics with CSP and analyze the types of solutions that CSP discovers. Finally, we show that CSP supports task transfer so that the exploration policies learned during experience with one task can help improve efficiency on related tasks.
 Eliminating the Invariance on the Loss Landscape of Linear Autoencoders 
 In this paper, we propose a new loss function for linear autoencoders (LAEs) and then analytically identify the structure of the loss surface. Optimizing the conventional Mean Square Error (MSE) loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This shortcoming originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e., the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we characterize the full structure of the loss landscape in the following sense: we establish analytical expression for the set of all critical points, show that it is a subset of critical points of MSE, and that all local minima are still global. However, the invariant global minima under MSE become saddle points under the new loss. Moreover, we show that the order of computational complexity of the loss and its gradients are the same as MSE and, hence, the new loss is not only of theoretical importance but is of practical value, e.g., for low-rank approximation. 
 How recurrent networks implement contextual processing in sentiment analysis 
 Neural networks have a remarkable capacity for contextual processing—using recent or nearby inputs to modify processing of current input. For example, in natural language, contextual processing is necessary to correctly interpret negation (e.g. phrases such as "not bad"). However, our ability to understand how networks process context is limited. Here, we propose general methods for reverse engineering recurrent neural networks (RNNs) to identify and elucidate contextual processing. We apply these methods to understand RNNs trained on sentiment classification. This analysis reveals inputs that induce contextual effects, quantifies the strength and timescale of these effects, and identifies sets of these inputs with similar properties. Additionally, we analyze contextual effects related to differential processing of the beginning and end of documents. Using the insights learned from the RNNs we improve baseline Bag-of-Words models with simple extensions that incorporate contextual modification, recovering greater than 90% of the RNN's performance increase over the baseline. This work yields a new understanding of how RNNs process contextual information, and provides tools that should provide similar insight more broadly.
 Soft Threshold Weight Reparameterization for Learnable Sparsity 
 Sparsity in Deep Neural Networks (DNNs) is studied extensively with the focus of maximizing prediction accuracy given an overall parameter budget. Existing methods rely on uniform or heuristic non-uniform sparsity budgets which have sub-optimal layer-wise parameter allocation resulting in a) lower prediction accuracy or b) higher inference cost (FLOPs). This work proposes Soft Threshold Reparameterization (STR), a novel use of the soft-threshold operator on DNN weights. STR smoothly induces sparsity while learning pruning thresholds thereby obtaining a non-uniform sparsity budget. Our method achieves state-of-the-art accuracy for unstructured sparsity in CNNs (ResNet50 and MobileNetV1 on ImageNet-1K), and, additionally, learns non-uniform budgets that empirically reduce the FLOPs by up to 50%. Notably, STR boosts the accuracy over existing results by up to 10% in the ultra sparse (99%) regime and can also be used to induce low-rank (structured sparsity) in RNNs. In short, STR is a simple mechanism which learns effective sparsity budgets that contrast with popular heuristics. Code, pretrained models and sparsity budgets are at https://github.com/RAIVNLab/STR.
 Unique Properties of Wide Minima in Deep Networks 
 It is well known that (stochastic) gradient descent has an implicit bias towards wide minima. In deep neural network training, this mechanism serves to screen out minima. However, the precise effect that this has on the trained network is not yet fully understood. In this paper, we characterize the wide minima in linear neural networks trained with a quadratic loss. First, we show that linear ResNets with zero initialization necessarily converge to the widest of all minima. We then prove that these minima correspond to nearly balanced networks whereby the gain from the input to any intermediate representation does not change drastically from one layer to the next. Finally, we show that consecutive layers in wide minima solutions are coupled. That is, one of the left singular vectors of each weight matrix, equals one of the right singular vectors of the next matrix. This forms a distinct path from input to output, that, as we show, is dedicated to the signal that experiences the largest gain end-to-end. Experiments indicate that these properties are characteristic of both linear and nonlinear models trained in practice.
 CURL: Contrastive Unsupervised Representation Learning for Reinforcement Learning 
 Reinforcement Learning for control tasks where the agent learns from raw high dimensional pixels has proven to be difficult and sample-inefficient.  Operating on high-dimensional observational input poses a challenging credit assignment problem, which hinders the agent’s ability to learn optimal policies quickly. One promising approach to improve the sample efficiency of image-based RL algorithms is to learn low-dimensional representations from the raw input using unsupervised learning. To that end, we propose a new model: Contrastive Unsupervised Representation Learning for Reinforcement Learning (CURL). CURL extracts high level features from raw pixels using a contrastive learning objective and performs off-policy control on top of the extracted features. CURL achieves state-of-the-art performance and is the first image based algorithm across both model-free and model-based settings to nearly match the sample-efficiency and performance of state-based features on five out of the six DeepMind control benchmarks.
 Why bigger is not always better: on finite and infinite neural networks 
 Recent work has argued that neural networks can be understood theoretically by taking the number of channels to infinity, at which point the outputs become Gaussian process (GP) distributed. However, we note that infinite Bayesian neural networks lack a key facet of the behaviour of real neural networks: the fixed kernel, determined only by network hyperparameters, implies that they cannot do any form of representation learning. The lack of representation or equivalently kernel learning leads to less flexibility and hence worse performance, giving a potential explanation for the inferior performance of infinite networks observed in the literature (e.g. Novak et al. 2019). We give analytic results characterising the prior over representations and representation learning in finite deep linear networks. We show empirically that the representations in SOTA architectures such as ResNets trained with SGD are much closer to those suggested by our deep linear results than by the corresponding infinite network. This motivates the introduction of a new class of network: infinite networks with bottlenecks, which inherit the theoretical tractability of infinite networks while at the same time allowing representation learning.
 Collaborative Machine Learning with Incentive-Aware Model Rewards 
 Collaborative machine learning (ML) is an appealing paradigm to build high-quality ML models by training on the aggregated data from many parties. However, these parties are only willing to share their data when given enough incentives, such as a guaranteed fair reward based on their contributions. This motivates the need for measuring a party's contribution and designing an incentive-aware reward scheme accordingly. This paper proposes to value a party's contribution based on Shapley value and information gain on model parameters given its data. Subsequently, we give each party a model as a reward. To formally incentivize the collaboration, we define some desirable properties (e.g., fairness and stability) which are inspired by cooperative game theory but adapted for our model reward that is uniquely freely replicable. Then, we propose a novel model reward scheme to satisfy minimum fairness and trade off between the desirable properties via an adjustable parameter. The value of each party's model reward determined by our scheme is attained by injecting Gaussian noise to the aggregated training data with an optimized noise variance. We empirically demonstrate interesting properties of our scheme and evaluate its performance using synthetic and real-world datasets.
 Revisiting Training Strategies and Generalization Performance in Deep Metric Learning 
 Deep Metric Learning (DML) is arguably one of the most influential lines of research for learning visual similarities with many proposed approaches every year. Although the field benefits from the rapid progress, the divergence in training protocols, architectures, and parameter choices make an unbiased comparison difficult. To provide a consistent reference point, we revisit the most widely used DML objective functions and conduct a study of the crucial parameter choices as well as the commonly neglected mini-batch sampling process. Under consistent comparison, DML objectives show much higher saturation than indicated by literature. Further based on our analysis, we uncover a correlation between the embedding space density and compression to the generalization performance of DML models. Exploiting these insights, we propose a simple, yet effective, training regularization to reliably boost the performance of ranking-based DML models on various standard benchmark datasets; code and a publicly accessible WandB-repo are available at https://github.com/Confusezius/Revisiting_Deep_Metric_Learning_PyTorch.
 Adaptive Region-Based Active Learning 
 We present a new active learning algorithm that adaptively partitions the input space into a finite number of regions, and subsequently seeks a distinct predictor for each region, while actively requesting labels. We prove theoretical guarantees for both the generalization error and the label complexity of our algorithm, and analyze the number of regions defined by the algorithm under some mild assumptions. We also report the results of an extensive suite of experiments on several real-world datasets demonstrating substantial empirical benefits over existing single-region and non-adaptive region-based active learning baselines.
 On the Theoretical Properties of the Network Jackknife 
 We study the properties of a leave-node-out jackknife procedure for network data.  Under the sparse graphon model, we prove an Efron-Stein-type inequality, showing that the network jackknife leads to conservative estimates of the variance (in expectation) for any network functional that is invariant to node permutation.  For a general class of count functionals, we also establish consistency of the network jackknife.  We complement our theoretical analysis with a range of simulated and real-data examples and show that the network jackknife offers competitive performance in cases where other resampling methods are known to be valid. In fact, for several network statistics, we see that the jackknife provides more accurate inferences compared to related methods such as subsampling.
 Too Relaxed to Be Fair 
 We address the problem of classification under fairness constraints. Given a notion of fairness, the goal is to learn a classifier that is not discriminatory against a group of individuals. In the literature, this problem is often formulated as a constrained optimization problem and solved using relaxations of the fairness constraints. We show that many existing relaxations are unsatisfactory: even if a model satisfies the relaxed constraint, it can be surprisingly unfair. We propose a principled framework to solve this problem. This new approach uses a strongly convex formulation and comes with theoretical guarantees on the fairness of its solution. In practice, we show that this method gives promising results on real data.
 Learnable Group Transform For Time-Series 
 We propose a novel approach to filter bank learning for time-series by considering spectral decompositions of signals defined as a Group Transform. This framework allows us to generalize classical time-frequency transformations such as the Wavelet Transform, and  to efficiently learn the representation of signals. While the creation of the wavelet transform filter-bank relies on affine transformations of a mother filter, our approach allows for non-linear transformations. The transformations induced by such maps enable us to span a larger class of signal representations, from wavelet to chirplet-like filters. We propose a parameterization of such a non-linear map such that its sampling can be optimized for a specific task and signal. The Learnable Group Transform can be cast into a Deep Neural Network. The experiments on diverse time-series datasets demonstrate the expressivity of this framework, which competes with state-of-the-art performances.
 Deep Gaussian Markov Random Fields 
 Gaussian Markov random fields (GMRFs) are probabilistic graphical models widely used in spatial statistics and related fields to model dependencies over spatial structures. We establish a formal connection between GMRFs and convolutional neural networks (CNNs). Common GMRFs are special cases of a generative model where the inverse mapping from data to latent variables is given by a 1-layer linear CNN. This connection allows us to generalize GMRFs to multi-layer CNN architectures, effectively increasing the order of the corresponding GMRF in a way which has favorable computational scaling. We describe how well-established tools, such as autodiff and variational inference, can be used for simple and efficient inference and learning of the deep GMRF. We demonstrate the flexibility of the proposed model and show that it outperforms the state-of-the-art on a dataset of satellite temperatures, in terms of prediction and predictive uncertainty.
 SGD Learns One-Layer Networks in WGANs 
 Generative adversarial networks (GANs) are a widely used framework for learning generative models. Wasserstein GANs (WGANs), one of the most successful variants of GANs, require solving a minmax optimization problem to global optimality, but are in practice successfully trained using stochastic gradient descent-ascent. In this paper, we show that, when the generator is a one-layer network, stochastic gradient descent-ascent converges to a global solution with polynomial time and sample complexity.
 Transparency Promotion with Model-Agnostic Linear Competitors 
 We propose a novel type of hybrid model for multi-class classification, which utilizes competing linear models to collaborate with an existing black-box model, promoting transparency in the decision-making process. Our proposed hybrid model, Model-Agnostic Linear Competitors (MALC), brings together the interpretable power of linear models and the good predictive performance of the state-of-the-art black-box models. We formulate the training of a MALC model as a convex optimization problem, which optimizes the predictive accuracy and transparency (defined as the percentage of data captured by the linear models) in the objective function. Experiments show that MALC offers more model flexibility for users to balance transparency and accuracy, in contrast to the currently available choice of either a pure black-box model or a pure interpretable model. The human evaluation also shows that more users are likely to choose MALC for this model flexibility when comparing with interpretable models and black-box models.
 Near Input Sparsity Time Kernel Embeddings via Adaptive Sampling 
 To accelerate kernel methods, we propose a near input sparsity time method for sampling the high-dimensional space implicitly defined by a kernel transformation. Our main contribution is an importance sampling method for subsampling the feature space of a degree $q$ tensoring of data points in almost input sparsity time, improving the recent oblivious sketching of (Ahle et al., 2020) by a factor of $q^{5/2}/\epsilon^2$. This leads to a subspace embedding for the polynomial kernel as well as the Gaussian kernel with a target dimension that is only linearly dependent on the statistical dimension of the kernel and in time which is only linearly dependent on the sparsity of the input dataset. We show how our spectral matrix approximation bounds imply new statistical guarantees for kernel ridge regression. Furthermore, we empirically show that in large-scale regression tasks, our algorithm outperforms state-of-the-art kernel approximation methods.
 Online Learning with Imperfect Hints 
 We consider a variant of the classical online linear optimization problem in which at every step, the online player receives a ``hint'' vector before choosing the action for that round. Rather surprisingly, it was shown that if the hint vector is guaranteed to have a positive correlation with the cost vector, then the online player can achieve a regret of $O(\log T)$, thus significantly improving over the $O(\sqrt{T})$ regret in the general setting. However, the result and analysis require the correlation property at \emph{all} time steps, thus raising the natural question: can we design online learning algorithms that are resilient to bad hints? 

In this paper we develop algorithms and nearly matching lower bounds for online learning with imperfect hints.  Our algorithms are oblivious to the quality of the hints, and the regret bounds interpolate between the always-correlated hints case and the no-hints case.  Our results also generalize, simplify, and improve upon previous results on optimistic regret bounds, which can be viewed as an additive version of hints.
 The many Shapley values for model explanation 
 The Shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing ~\cite{Shapley53} showing that it is the \emph{unique} method that satisfies certain good properties (\emph{axioms}). 

There are, however, a multiplicity of ways in which the Shapley value is operationalized in the attribution problem. These differ in how they reference the model, the training data, and the explanation context. These give very different results, rendering the uniqueness result meaningless. Furthermore, we find that previously proposed approaches can produce counterintuitive attributions in theory and in practice---for instance, they can assign non-zero attributions to features that are not even referenced by the model. 

In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution, and propose a technique called Baseline Shapley (BShap) that is backed by a proper uniqueness result. We also contrast BShap with Integrated Gradients, another extension of Shapley value to the continuous setting.

 Fast and Consistent Learning of Hidden Markov Models by Incorporating Non-Consecutive Correlations 
 Can the parameters of a hidden Markov model (HMM) be estimated from a single sweep through the observations -- and additionally, without being trapped at a local optimum in the likelihood surface? That is the premise of recent method of moments algorithms devised for HMMs. In these, correlations between consecutive pair- or triplet-wise observations are empirically estimated and used to compute estimates of the HMM parameters. Albeit computationally very attractive, the main drawback is that by restricting to only low-order correlations in the data, information is being neglected which results in a loss of accuracy (compared to standard maximum likelihood schemes). In this paper, we propose extending these methods (both pair- and triplet-based) by also including non-consecutive correlations in a way which does not significantly increase the computational cost (which scales linearly with the number of additional lags included). We prove strong consistency of the new methods, and demonstrate an improved performance in numerical simulations on both synthetic and real-world financial time-series datasets.
 Logarithmic Regret for Learning Linear Quadratic Regulators Efficiently 
 We consider the problem of learning in Linear Quadratic Control systems whose transition parameters are initially unknown. Recent results in this setting have demonstrated efficient learning algorithms with regret growing with the square root of the number of decision steps.  We present new efficient algorithms that achieve, perhaps surprisingly,regret that scales only (poly-)logarithmically with the number of steps, in two scenarios: when only the state transition matrix A is unknown, and when only the state-action transition matrix B is unknown and the optimal policy satisfies a certain non-degeneracy condition.  On the other hand, we give a lower bound which shows that when the latter condition is violated, square root regret is unavoidable.
 Learning What to Defer for Maximum Independent Sets 
 Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the size of the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme where the agent adaptively shrinks or stretch the number of stages by learning to defer the determination of the solution at each stage. We apply the proposed framework, coined Learning what to Defer (LwD), to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget. 
 Mix-n-Match : Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning 
 This paper studies the problem of post-hoc calibration of machine learning classifiers. We introduce the following desiderata for uncertainty calibration: (a) accuracy-preserving, (b) data-efficient, and (c) high expressive power. We show that none of the existing methods satisfy all three requirements, and demonstrate how Mix-n-Match calibration strategies (i.e., ensemble and composition) can help achieve remarkably better data-efficiency and expressive power while provably preserving classification accuracy of the original classifier. We also show that existing calibration error estimators (e.g., histogram-based ECE) are unreliable especially in small-data regime. Therefore, we propose an alternative data-efficient kernel density-based estimator for a reliable evaluation of the calibration performance and prove its asymptotically unbiasedness and consistency.
 Piecewise Linear Regression via a Difference of Convex Functions 
 We present a new piecewise linear regression methodology that utilises fitting a \emph{difference of convex} functions (DC functions) to the data. These are functions $f$ that may be represented as the difference $\phi_1 - \phi_2$ for a choice of \emph{convex} functions $\phi_1, \phi_2$. The method proceeds by estimating piecewise-liner convex functions, in a manner similar to max-affine regression, whose difference approximates the data. The choice of the function is regularised by a new seminorm over the class of DC functions that controls the $\ell_\infty$ Lipschitz constant of the estimate. The resulting methodology can be efficiently implemented via Quadratic programming \emph{even in high dimensions}, and is shown to have close to minimax statistical risk. We empirically validate the method, showing it to be practically implementable, and to outperform existing regression methods in accuracy on real-world datasets. 
 Learning Task-Agnostic Embedding of Multiple Black-Box Experts for Multi-Task Model Fusion 
 Model fusion is an emerging study in collective learning where heterogeneous experts with private data and learning architectures need to combine their black-box knowledge for better performance. Existing literature achieves this via a local knowledge distillation scheme that transfuses the predictive patterns of each pre-trained expert onto a white-box imitator model, which can be incorporated efficiently into a global model. This scheme however does not extend to multi-task scenarios where different experts were trained to solve different tasks and only part of their distilled knowledge is relevant to a new task. To address this multi-task challenge, we develop a new fusion paradigm that represents each expert as a distribution over a spectrum of predictive prototypes, which are isolated from task-specific information encoded within the prototype distribution. The task-agnostic prototypes can then be reintegrated to generate a new model that solves a new task encoded with a different prototype distribution. The fusion and adaptation performance of the proposed framework is demonstrated empirically on several real-world benchmark datasets.
 Learning Adversarial Markov Decision Processes with Bandit Feedback and Unknown Transition 
 We consider the task of learning in episodic finite-horizon Markov decision processes with an unknown transition function, bandit feedback, and adversarial losses. We propose an efficient algorithm that achieves  O(√L|X|AT ) regret with high probability, where L is the horizon, |X| the number of states, |A| the number of actions, and T the number of episodes. To our knowledge, our algorithm is the first to ensure O(√T) regret in this challenging setting; in fact, it achieves the same regret as (Rosenberg & Mansour, 2019a) who consider the easier setting with full-information. Our key contributions are two-fold: a tighter confidence set for the transition function; and an optimistic loss estimator that is inversely weighted by an "upper occupancy bound". 
 Logarithmic Regret for Online Control with Adversarial Noise 
 We consider the problem of online control in a known linear dynamical system subject to adversarial noise. Existing regret bounds for this setting scale as $\sqrt{T}$ unless strong stochastic assumptions are imposed on the noise. We give the first algorithm with logarithmic regret for arbitrary adversarial noise sequences, provided that the state and control costs are given by fixed quadratic functions. We propose a novel analysis that combines a new variant of the performance difference lemma with techniques from optimal control, allowing us to reduce online control to online prediction with delayed feedback. Unlike prior work, which leverages the so-called online convex optimization with memory framework, our analysis \emph{does not} need to bound movement costs of the iterates, leading to logarithmic regret. Our performance difference lemma-based analysis may be of broader interest beyond linear control.
 Learning Factorized Weight Matrix for Joint Image Filtering 
 Joint image filtering is a fundamental problem in computer vision with applications in many different areas. Most existing algorithms solve this problem with a weighted averaging process to aggregate input pixels. However, the weight matrix of this process is often empirically designed and not robust to complex input. In this work, we propose to learn the weight matrix for joint image filtering. This is a challenging problem, as directly learning a large weight matrix is computationally intractable. To address this issue, we introduce the correlation of deep features to approximate the aggregation weights. However, this strategy only uses inner product for the weight matrix estimation, which limits the performance of the proposed algorithm. Therefore, we further propose to learn a nonlinear function to predict sparse residuals of the feature correlation matrix. Note that the proposed method essentially factorizes the weight matrix into a low-rank and a sparse matrix and then learn both of them simultaneously with deep neural networks. Extensive experiments show that the proposed algorithm compares favorably against the state-of-the-art approaches on a wide variety of joint image filtering tasks.
 Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation 
 Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled domain. Prior UDA methods typically require to access the source data when learning to adapt the model, making them risky and inefficient for decentralized private data. In this work we tackle a novel setting where only a trained source model is available and investigate how we can effectively utilize such a model without source data to solve UDA problems. To this end, we propose a simple yet generic representation learning framework, named \emph{Source HypOthesis Transfer} (SHOT). Specifically, SHOT freezes the classifier module (hypothesis) of the source model and learns the target-specific feature extraction module by exploiting both information maximization and self-supervised pseudo-labeling to implicitly align representations from the target domains to the source hypothesis. In this way, the learned target model can directly predict the labels of target data. We further investigate several techniques to refine the network architecture to parameterize the source model for better transfer performance. To verify its versatility, we evaluate SHOT in a variety of adaptation cases including closed-set, partial-set, and open-set domain adaptation. Experiments indicate that SHOT yields state-of-the-art results among multiple domain adaptation benchmarks.
 Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning 
 Model-based reinforcement learning (RL) enjoys several benefits, such as data-efficiency and planning, by learning a model of the environment's dynamics. However, learning a global model that can generalize across different dynamics remains a challenge. To tackle this problem, we decompose the task of learning a global dynamics model into two stages: (a) learning a context latent vector that captures the local dynamics, then (b) predicting the next state conditioned on it. In order to encode dynamics-specific information into the context latent vector, we introduce a novel loss function that encourages the context latent vector to be useful for predicting both forward and backward dynamics. The proposed method achieves superior generalization ability across various simulated robotics and control tasks, compared to existing RL schemes.
 Adversarial Robustness via Runtime Masking and Cleansing 
 Deep neural networks are shown to be vulnerable to adversarial attacks. This motivates robust learning techniques, such as the adversarial training, whose goal is to learn a network that is robust against adversarial attacks.  However, the sample complexity of robust learning can be significantly larger than that of “standard” learning. In this paper, we propose improving the adversarial robustness of a network by leveraging the potentially large test data seen at runtime. We devise a new defense method, called runtime masking and cleansing (RMC), that adapts the network at runtime before making a prediction to dynamically mask network gradients and cleanse the model of the non-robust features inevitably learned during the training process due to the size limit of the training set. We conduct experiments on real-world datasets and the results demonstrate the effectiveness of RMC empirically.
 NetGAN without GAN: From Random Walks to Low-Rank Approximations 
 A graph generative model takes a graph as input and is supposed to generate new graphs that ``look like'' the input graph. While most classical models focus on few, hand-selected graph statistics and are too simplistic to reproduce real-world graphs, NetGAN recently emerged as an attractive alternative: by training a GAN to learn the random walk distribution of the input graph, the algorithm is able to reproduce a large number of important network patterns simultaneously, without explicitly specifying any of them. In this paper, we investigate the implicit bias of NetGAN. We find that the root of its generalization properties does not lie in the GAN architecture, but in an inconspicuous low-rank approximation of the logits random walk transition matrix. Step by step we can strip NetGAN of all unnecessary parts, including the GAN, and obtain a highly simplified reformulation that achieves comparable generalization results, but is orders of magnitudes faster and easier to adapt. Being much simpler on the conceptual side, we reveal the implicit inductive bias of the algorithm  --- an important step towards increasing the interpretability, transparency and acceptance of machine learning systems.
 On Differentially Private Stochastic Convex Optimization  with Heavy-tailed Data 
 In this paper, we consider the problem of designing 
Differentially Private (DP) algorithms for Stochastic Convex Optimization (SCO) on heavy-tailed data. The irregularity of such data violates some key 
assumptions used in almost all existing DP-SCO and DP-ERM methods, resulting in failure to provide the DP guarantees. To better understand this type of challenges, we provide in this paper a comprehensive study of DP-SCO under various settings. First, we consider the case where the loss function is strongly convex and smooth. For this case, we propose a method based on the sample-and-aggregate framework, which has an excess population risk of 
$\tilde{O}(\frac{d^3}{n\epsilon^4})$ (after omitting other factors), where $n$ is the sample size and $d$ is the dimensionality of the data. Then, we show that with some additional assumptions on the loss functions, it is possible to reduce the 
\textit{expected} excess population risk to $\tilde{O}(\frac{ d^2}{ n\epsilon^2 })$. To lift these additional conditions, we also  
 provide a gradient smoothing and trimming based scheme to achieve  excess population risks of   
$\tilde{O}(\frac{ d^2}{n\epsilon^2})$ and $\tilde{O}(\frac{d^\frac{2}{3}}{(n\epsilon^2)^\frac{1}{3}})$ for strongly convex and general convex loss functions, respectively, \textit{with high probability}.
Experiments on both synthetic and real-world datasets suggest that our algorithms can effectively deal with the challenges caused by data irregularity.

 Quantized Decentralized Stochastic Learning over Directed Graphs 
 We consider a decentralized stochastic learning problem where data points are distributed among computing nodes communicating over a directed graph. As the model size gets large, decentralized learning faces a major bottleneck that is the heavy communication load due to each node transmitting large messages (model updates) to its neighbors. To tackle this bottleneck, we propose the quantized decentralized stochastic learning algorithm over directed graphs that is based on the push-sum algorithm in decentralized consensus optimization. More importantly, we prove that our algorithm achieves the same convergence rates of the decentralized stochastic learning algorithm with exact-communication for both convex and non-convex losses. A key technical challenge of the work is to prove exact convergence of the proposed decentralized learning algorithm in the presence of quantization noise with unbounded variance over directed graphs. We provide numerical evaluations that corroborate our main theoretical results and illustrate significant speed-up compared to the exact-communication methods.
 Stochastic Coordinate Minimization with Progressive Precision for Stochastic Convex Optimization 
 A framework based on iterative coordinate minimization (CM) is developed for stochastic convex optimization. Given that exact coordinate minimization is impossible due to the unknown stochastic nature of the objective function, the crux of the proposed optimization algorithm is an optimal control of the minimization precision in each iteration.  We establish the optimal precision control and the resulting order-optimal regret performance for strongly convex and separably nonsmooth functions.  An interesting finding is that the optimal progression of precision across iterations is independent of the low-dimension CM routine employed, suggesting a general framework for extending low-dimensional optimization routines to high-dimensional problems. The proposed algorithm is amenable to online implementation and inherits the scalability and parallelizability  properties of CM for large-scale optimization. Requiring only a sublinear order of message exchanges,  it also lends itself well to distributed computing as compared with the alternative approach of coordinate gradient descent.
 Automatic Reparameterisation of Probabilistic Programs 
 Probabilistic programming has emerged as a powerful paradigm in statistics, applied science, and machine learning: by decoupling modelling from inference, it promises to allow modellers to directly reason about the processes generating data. However, the performance of inference algorithms can be dramatically affected by the parameterisation used to express a model, requiring users to transform their programs in non-intuitive ways. We argue for automating these transformations, and demonstrate that mechanisms available in recent modelling frameworks can implement non-centring and related reparameterisations. This enables new inference algorithms, and we propose two: a simple approach using interleaved sampling and a novel variational formulation that searches over a continuous space of parameterisations. We show that these approaches enable robust inference across a range of models, and can yield more efficient samplers than the best fixed parameterisation.
 Continuously Indexed Domain Adaptation 
 Existing domain adaptation focuses on transferring knowledge between domains with categorical indices (e.g., between datasets A and B). However, many tasks involve continuously indexed domains. For example, in medical applications, one often needs to transfer disease analysis and prediction across patients of different ages, where age acts as a continuous domain index. Such tasks are challenging for prior domain adaptation methods since they ignore the underlying relation among domains.

In this paper, we propose the first method for continuously indexed domain adaptation. Our approach combines traditional adversarial adaptation with a novel discriminator that models the encoding-conditioned domain index distribution. Our theoretical analysis demonstrates the value of leveraging the domain index to generate invariant features across a continuous range of domains. Our empirical results show that our approach outperforms the state-of-the-art domain adaption methods on both synthetic and real-world medical datasets.
 Manifold Identification for Ultimately Communication-Efficient Distributed Optimization 
 The expensive inter-machine communication is the bottleneck of
distributed optimization.
Existing study tackles this problem by shortening the communication
rounds, but the reduction of per-round communication cost is not
well-studied.
This work proposes a progressive manifold identification approach with 
sound theoretical justifications to greatly reduce both the
communication rounds and the bytes communicated per round for partly
smooth regularized problems, which include many large-scale machine
learning tasks such as the training of $\ell_1$- and
group-LASSO-regularized models.
Our method uses an inexact proximal quasi-Newton method to iteratively
identify a sequence of low-dimensional smooth manifolds in which the
final solution lies, and restricts the model update within the current
manifold to lower significantly the per-round communication cost.
After identifying the final manifold within which the problem is
smooth, we take superlinear-convergent truncated semismooth
Newton steps obtained through preconditioned conjugate gradient to
largely reduce the communication rounds.
Experiments show that when compared with the state of the art,
the communication cost of our method is significantly lower
and the running time is up to $10$ times faster.
 Efficient non-conjugate Gaussian process factor models for spike countdata using polynomial approximations 
 Gaussian Process Factor Analysis (GPFA) hasbeen broadly applied to the problem of identi-fying smooth, low-dimensional temporal struc-ture underlying large-scale neural recordings.However, spike trains are non-Gaussian, whichmotivates combining GPFA with discrete ob-servation models for binned spike count data.The drawback to this approach is that GPFApriors are not conjugate to count model like-lihoods, which makes inference challenging.Here we address this obstacle by introduc-ing a fast, approximate inference method fornon-conjugate GPFA models. Our approachuses orthogonal second-order polynomials toapproximate the nonlinear terms in the non-conjugate log-likelihood, resulting in a methodwe refer to aspolynomial approximate log-likelihood(PAL) estimators. This approxima-tion allows for accurate closed-form evalua-tion of marginal likelihoods and fast numericaloptimization for parameters and hyperparam-eters. We derive PAL estimators for GPFAmodels with binomial, Poisson, and negativebinomial observations. We find the PAL esti-mation achieves faster convergence times andhigh accuracy compared to existing state-of-the-art inference methods. We also find thatPAL hyperparameters can provide sensible ini-tialization for black box variational inference(BBVI), which will improve BBVI accuracy.We apply these methods to data from mousevisual cortex and primate higher-order visualand parietal cortices. We demonstrate thatPreliminary work. Under review by AISTATS 2020. Do notdistribute.PAL estimators achieve fast and accurate ex-traction of latent structure from multi-neuronspike train data.
 Imputer: Sequence Modelling via Imputation and Dynamic Programming 
 This paper presents the Imputer, a neural sequence model that generates output sequences iteratively via imputations. The Imputer is an iterative generation model, requiring only a constant number of generation steps independent of the number of input or output tokens. The Imputer can be trained to approximately marginalize over all possible alignments between the input and output sequences, and all possible generation orders. We present a tractable dynamic programming training algorithm, which yields a lower bound on the log marginal likelihood. When applied to end-to-end speech recognition, the Imputer outperforms prior non-autoregressive models and achieves competitive results to autoregressive models. On LibriSpeech test-other, the Imputer achieves 11.1 WER, outperforming CTC at 13.0 WER and seq2seq at 12.5 WER.
 Generative Pretraining From Pixels 
 Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear probe of our features.
 Deep Divergence Learning 
 Classical linear metric learning methods have recently been extended along two distinct lines: deep metric learning methods for learning embeddings of the data using neural networks, and Bregman divergence learning approaches for extending learning Euclidean distances to more general divergence measures such as divergences over distributions.  In this paper, we introduce deep Bregman divergences, which are based on learning and parameterizing functional Bregman divergences using neural networks, and which unify and extend these existing lines of work.  We show in particular how deep metric learning formulations, kernel metric learning, Mahalanobis metric learning, and moment-matching functions for comparing distributions arise as special cases of these divergences in the symmetric setting.  We then describe a deep learning framework for learning general functional Bregman divergences, and show in experiments that this method yields superior performance on benchmark datasets as compared to existing deep metric learning approaches.  We also discuss novel applications, including a semi-supervised distributional clustering problem, and a new loss function for unsupervised data generation.
 Learning Portable Representations for High-Level Planning 
 We present a framework for autonomously learning a portable representation that describes a collection of low-level continuous environments. We show that these abstract representations can be learned in a task-independent egocentric space \textit{specific to the agent} that, when grounded with problem-specific information, are provably sufficient for planning. We demonstrate transfer in two different domains, where an agent learns a portable, task-independent symbolic vocabulary, as well as rules expressed in that vocabulary, and then learns to instantiate those rules on a per-task basis. This reduces the samples required to learn a representation of a new task.
 AutoGAN-Distiller: Searching to Compress Generative Adversarial Networks 
 The compression of Generative Adversarial Networks (GANs) has lately drawn attention, due to the increasing demand for deploying GANs into mobile devices for numerous applications such as image translation, enhancement and editing. However, compared to the substantial efforts to compressing other deep models, the research on compressing GANs (usually the generators) remains at its infancy stage. Existing GAN compression algorithms are limited to handling specific GAN architectures and losses. Inspired by the recent success of AutoML in deep compression, we introduce AutoML to GAN compression and develop an AutoGAN-Distiller (AGD) framework. Starting with a specifically designed efficient search space, AGD performs an end-to-end discovery for new efficient generators, given the target computational resource constraints. The search is guided by the original GAN model via knowledge distillation, therefore fulfilling the compression. AGD is fully automatic, standalone (i.e., needing no trained discriminators), and generically applicable to various GAN models. We evaluate AGD in two representative GAN tasks: image translation and super resolution. Without bells and whistles, AGD yields remarkably lightweight yet more competitive compressed models, that largely outperform existing alternatives. Our codes and pretrained models are available at: https://github.com/TAMU-VITA/AGD.
 Stochastic Optimization for Regularized Wasserstein Estimators 
 Optimal transport is a foundational problem in optimization, that allows to compare probability distributions while taking into account geometric aspects. Its optimal objective value, the Wasserstein distance, provides an important loss between distributions that has been used in many applications throughout machine learning and statistics. Recent algorithmic progress on this problem and its regularized versions have made these tools increasingly popular. However, existing techniques require solving an optimization problem to obtain a single gradient of the loss, thus slowing down first-order methods to minimize the sum of losses, that require many such gradient computations. In this work, we introduce an algorithm to solve a regularized version of this problem of Wasserstein estimators, with a time per step which is sublinear in the natural dimensions of the problem. We introduce a dual formulation, and optimize it with stochastic gradient steps that can be computed directly from samples, without solving additional optimization problems at each step. Doing so, the estimation and computation tasks are performed jointly. We show that this algorithm can be extended to other tasks, including estimation of Wasserstein barycenters. We provide theoretical guarantees and illustrate the performance of our algorithm with experiments on synthetic data.
 Growing Action Spaces 
 In complex tasks, such as those with large combinatorial action spaces, random exploration may be too inefficient to achieve meaningful learning progress.
In this work, we use a curriculum of progressively growing action spaces to accelerate learning.
We assume the environment is out of our control, but that the agent may set an internal curriculum by initially restricting its action space.
Our approach uses off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfers data,  value estimates, and state representations from restricted action spaces to the full task.
We show the efficacy of our approach in proof-of-concept control tasks and on challenging large-scale StarCraft micromanagement tasks with large, multi-agent action spaces.
 On Learning Sets of Symmetric Elements 
 Learning from unordered sets is a fundamental learning setup, which is attracting increasing attention. Research in this area has focused on the case where elements of the set are represented by feature vectors, and far less emphasis has been given to the common case where set elements themselves adhere to certain symmetries. That case is relevant to numerous applications, from deblurring image bursts to multi-view 3D shape recognition and reconstruction. In this paper, we present a principled approach to learning sets of general symmetric elements. We first characterize the space of linear layers that are equivariant both to element reordering and to the inherent symmetries of elements, like translation in the case of images. We further show that networks that are composed of these layers, called Deep Sets for Symmetric elements layers (DSS), are universal approximators of both invariant and equivariant functions. DSS layers are also straightforward to implement. Finally, we show that they improve over existing set-learning architectures in a series of experiments with images, graphs, and point-clouds.
 No-Regret and Incentive-Compatible Online Learning 
  We study online learning settings in which experts act strategically to maximize their influence on the learning algorithm's predictions by potentially misreporting their beliefs about a sequence of binary events. Our goal is twofold. First, we want the learning algorithm to be no-regret with respect to the best fixed expert in hindsight. Second, we want incentive compatibility, a guarantee that each expert's best strategy is to report his true beliefs about the realization of each event.  To achieve this goal, we build on the literature on wagering mechanisms, a type of multi-agent scoring rule. We provide algorithms that achieve no regret and incentive compatibility for myopic experts for both the full and partial information settings.  In experiments on datasets from FiveThirtyEight, our algorithms have regret comparable to classic no-regret algorithms, which are not incentive-compatible. Finally, we identify an incentive-compatible algorithm for forward-looking strategic agents that exhibits diminishing regret in practice.
 Convex Calibrated Surrogates for the Multi-Label F-Measure 
 The F-measure is a widely used performance measure for multi-label classification, where multiple labels can be active in an instance simultaneously (e.g. in image tagging, multiple tags can be active in any image). In particular, the F-measure explicitly balances recall (fraction of active labels predicted to be active) and precision (fraction of labels predicted to be active that are actually so), both of which are important in evaluating the overall performance of a multi-label classifier.  As with most discrete prediction problems, however, directly optimizing the F-measure is computationally hard. In this paper, we explore the question of designing convex surrogate losses that are \emph{calibrated} for the F-measure -- specifically, that have the property that minimizing the surrogate loss yields (in the limit of sufficient data) a Bayes optimal multi-label classifier for the F-measure. We show that the F-measure for an $s$-label problem, when viewed as a $2^s \times 2^s$ loss matrix, has rank at most $s^2+1$, and apply a result of Ramaswamy et al. (2014) to design a family of convex calibrated surrogates for the F-measure. The resulting surrogate risk minimization algorithms can be viewed as decomposing the multi-label F-measure learning problem into $s^2+1$ binary class probability estimation problems. We also provide a quantitative regret transfer bound for our surrogates, which allows any regret guarantees for the binary problems to be transferred to regret guarantees for the overall F-measure problem, and discuss a connection with the algorithm of Dembczynski et al. (2013). Our experiments confirm our theoretical findings.  
 InfoGAN-CR: Disentangling Generative Adversarial Networks with Contrastive Regularizers 
 Standard deep generative models have latent codes that can be arbitrarily rotated, and a specific coordinate has no meaning. For manipulation and exploration of the samples, we seek a disentangled latent code where each coordinate is associated with a distinct property of the target distribution. Recent advances has been dominated by Variational Autoencoder (VAE)-based methods, while training disentangled generative adversarial networks (GANs) remains challenging. To this end, we make two contributions: a novel approach for training disentangled GANs and a novel approach for selecting the best disentangled model. First, we propose a regularizer that achieves higher disentanglement scores than state-of-the-art VAE- and GAN-based approaches. This contrastive regularizer is inspired by a natural notion of disentanglement: latent traversal. Latent traversal refers to generating images by varying one latent code while fixing the rest. We turn this intuition into a regularizer by adding a discriminator that detects how the latent codes are coupled together, in paired examples. Next, one major weakness of all disentanglement benchmark tests is that all reported scores are  based on hyperparameters  tuned with a predefined disentangled representations on synthetic datasets. This is neither fair nor realistic, as one can arbitrarily improve the performance with more hyperparameter tuning and real datasets do not come with such supervision. We propose an unsupervised model selection scheme based on medoids. Numerical experiments confirm that  thus selected models improve upon the state-of-the-art models selected with supervised hyperparameter tuning. 

 Naive Exploration is Optimal for Online LQR 
 We consider the problem of online adaptive control of the linear quadratic regulator, where the true system parameters are unknown. We prove new upper and lower bounds demonstrating that the optimal regret scales as $\widetilde{\Theta}({\sqrt{d_{\mathbf{u}}^2 d_{\mathbf{x}} T}})$, where $T$ is the number of time steps, $d_{\mathbf{u}}$ is the dimension of the input space, and $d_{\mathbf{x}}$ is the dimension of the system state.  Notably, our lower bounds rule out the possibility of a $\mathrm{poly}(\log{}T)$-regret algorithm, which has been conjectured due to the apparent strong convexity of the problem. Our upper bounds are attained by a simple variant of \emph{certainty equivalence control}, where the learner selects control inputs according to  the optimal controller for their estimate of the system while injecting exploratory random noise (Mania et al. 2019).

Central to our upper and lower bounds is a new approach for controlling perturbations of Riccati equations, which we call the \emph{self-bounding ODE method}. The approach enables regret upper bounds which hold for \emph{any stabilizable instance}, require no foreknowledge of the system except for a single stabilizing controller, and scale with natural control-theoretic quantities.


 Improved Communication Cost in Distributed PageRank Computation – A Theoretical Study 
 PageRank is a widely used approach for measuring the importance of a node in a graph. Computing PageRank is a fundamental task in numerous applications including web search, machine learning and recommendation systems. The importance of computing PageRanks in a distributed environment has been recognized due to the rapid growth of the graph size in real world. However, only a few previous works can provide a provable complexity and accuracy for distributed PageRank computation. Given a constant $d>0$ and a graph of $n$ nodes and under the well-known congested-clique distributed model, the state-of-the-art approach, Radar-Push, uses $O(\log\log{n}+\log{d})$ communication rounds to approximate the PageRanks within a relative error $O(\frac{1}{\log^d{n}})$. However, Radar-Push entails as large as $O(\log^{2d+3}{n})$ bits of bandwidth (e.g., the communication cost between a pair of nodes per round) in the worst case. In this paper, we provide a new algorithm that uses asymptotically the same communication rounds while significantly improves the bandwidth from $O(\log^{2d+3}{n})$ bits to $O(d\log^3{n})$ bits. To the best of our knowledge, our distributed PageRank algorithm is the first to achieve $o(d\log{n})$ communication rounds with $O(d\log^3{n})$ bits of bandwidth in approximating PageRanks with relative error $O(\frac{1}{\log^d{n}})$ under the congested-clique model. 
 The Performance Analysis of Generalized Margin Maximizers on Separable Data 
 Logistic models are commonly used for binary classification tasks. The success of such models has often been attributed to their connection to maximum-likelihood estimators. It has been shown that SGD algorithms , when applied on the logistic loss, converge to the max-margin classifier (a.k.a. hard-margin SVM). The performance of hard-margin SVM has been recently analyzed in~\cite{montanari2019generalization, deng2019model}. Inspired by these results, in this paper, we present and study a more general setting, where the underlying parameters of the logistic model possess certain structures (sparse, block-sparse, low-rank, etc.) and introduce a more general framework (which is referred to as “Generalized Margin Maximizer”, GMM). While classical max-margin classifiers minimize the 2-norm of the parameter vector subject to linearly separating the data, GMM minimizes any arbitrary convex function of the parameter vector. We provide a precise performance analysis of the generalization error of such methods and show improvement over the max-margin method which does not take into account the structure of the model. In future work we show that mirror descent algorithms, with a properly tuned step size, can be exploited to achieve GMM classifiers. Our theoretical results are validated by extensive simulation results across a range of parameter values, problem instances, and model structures.
 Learning From Irregularly-Sampled Time Series: A Missing Data Perspective 
 Irregularly-sampled time series occur in many domains including healthcare. They can be challenging to model because they do not naturally yield a fixed-dimensional representation as required by many standard machine learning models. In this paper, we consider irregular sampling from the perspective of missing data. We model observed irregularly sampled time series data as a sequence of index-value pairs sampled from a continuous but unobserved function. We introduce an encoder-decoder framework for learning from such generic indexed sequences. We propose learning methods for this framework based on variational autoencoders and generative adversarial networks. We focus on the continuous-time case and introduce continuous convolutional layers that can interface with existing neural network architectures. We investigate two applications of this framework: interpolation and time series classification. Experiments show that our models are able to achieve competitive or better classification results on irregularly sampled multivariate time series classification tasks compared to recent RNN models while offering significantly faster training times.
 Stochastic bandits with arm-dependent delays 
 Significant work has been recently dedicated to the stochastic delayed bandit setting because of its relevance in applications. The applicability of existing algorithms is however restricted by the fact that strong assumptions are often made on the delay distributions, such as full observability, restrictive shape constraints, or uniformity over arms. In this work, we weaken them significantly and only assume that there is a bound on the tail of the delay. In particular, we cover the important case where the delay distributions vary across arms, and the case where the delays are heavy-tailed. Addressing these difficulties, we propose a simple but efficient UCB-based algorithm called the PATIENTBANDITS. We provide both problem-dependent and problem-independent bounds on
the regret as well as performance lower bounds.
 Doubly robust off-policy evaluation with shrinkage  
 We propose a new framework for designing estimators for off-policy evaluation in contextual bandits. Our approach is based on the asymptotically optimal doubly robust estimator, but we shrink the importance weights to minimize a bound on the mean squared error, which results in a better bias-variance tradeoff in finite samples. We use this optimization-based framework to obtain three estimators: (a) a weight-clipping estimator, (b) a new weight-shrinkage estimator, and (c) the first shrinkage-based estimator for combinatorial action sets. Extensive experiments in both standard and combinatorial bandit benchmark problems show that our estimators are highly adaptive and typically outperform state-of-the-art methods.
 On the Sample Complexity of Adversarial Multi-Source PAC Learning 
 We study the problem of learning from multiple untrusted data sources,  a scenario of increasing practical relevance given the recent emergence of crowdsourcing and collaborative learning paradigms. Specifically, we analyze the situation in which a learning system obtains datasets from multiple sources, some of which might be biased or even adversarially perturbed. It is known that in the single-source case, an adversary with the power to corrupt a fixed fraction of the training data can prevent PAC-learnability, that is, even in the limit of infinitely much training data, no learning system can approach the optimal test error. In this work we show that, surprisingly, the same is not true in the multi-source setting, where the adversary can arbitrarily corrupt a fixed fraction of the data sources. Our main results are a generalization bound that provides finite-sample guarantees for this learning setting, as well as corresponding lower bounds. Besides establishing PAC-learnability our results also show that in a cooperative learning setting sharing data with other parties has provable benefits, even if some participants are malicious. 
 Online Dense Subgraph Discovery via Blurred-Graph Feedback 
 \emph{Dense subgraph discovery} aims to find a dense component in edge-weighted graphs. This is a fundamental graph-mining task with a variety of applications and thus has received much attention recently. Although most existing methods assume that each individual edge weight is easily obtained, such an assumption is not necessarily valid in practice. In this paper, we introduce a novel learning problem for dense subgraph discovery in which a learner queries edge subsets rather than only single edges and observes a noisy sum of edge weights in a queried subset. For this problem, we first propose a polynomial-time algorithm that obtains a nearly-optimal solution with high probability. Moreover, to deal with large-sized graphs, we design a more scalable algorithm with a theoretical guarantee. Computational experiments using real-world graphs demonstrate the effectiveness of our algorithms.
 Momentum-Based Policy Gradient Methods 
 Policy gradient methods are a class of powerful algorithms in reinforcement learning (RL). More recently, some variance reduced policy gradient methods have been developed to improve sample efficiency
and obtain a near-optimal sample complexity $O(\epsilon^{-3})$ for finding an
$\epsilon$-stationary point of non-concave performance function in model-free RL.
However, the practical performances of these variance reduced policy gradient methods are not consistent with their near-optimal sample complexity,
because these methods require large batches and
strict learning rates to achieve this optimal complexity.
In the paper, thus, we propose a class of efficient momentum-based policy gradient methods, which use adaptive learning rates and do not require large batches.
Specifically, we propose a fast important-sampling momentum-based policy gradient (IS-MBPG) method by using the important sampling technique.
Meanwhile, we also propose a fast hessian-aided momentum-based policy gradient (HA-MBPG) method via using the semi-hessian information.
In theoretical analysis, we prove that our algorithms also have the sample complexity $O(\epsilon^{-3})$, as the existing best policy gradient methods.
In the experiments, we use some benchmark tasks to demonstrate the effectiveness of algorithms.
 Private Query Release Assisted by Public Data 
 We study the problem of differentially private query release assisted by public data. Given a class $H$ of queries $h:X \rightarrow \{-1, 1\}$, and data set of i.i.d. samples from an unknown distribution $D$, a query-release algorithm is required to output a data structure $G: H \rightarrow [-1, 1]$ such that for any query $h\in H,$ $G(h)$ approximates $E_{x\sim D}[h(x)]$ up to some error $\alpha$. In this problem, the input data set consists of two types of samples: private and public.  The algorithm is required to satisfy differential privacy only with respect to the private samples. We study the limits of this task in terms of private and public sample complexities. First, we show that this task is achievable for any query class of finite VC-dimension using only (roughly) $d/\alpha$ public samples and $\sqrt{p}d^{3/2}/\alpha^2$ private samples, where $d$ is the VC-dimension of the class, and $p$ is the dual VC-dimension. When there are no public samples, there are known examples of classes of VC-dimension one for which this task is impossible under differential privacy (e.g., the class of threshold functions over $R$). Moreover, our upper bound on the public sample complexity is non-trivial since, without private samples, it is known that this task is equivalent to uniform convergence over $H$ which requires at least $d/\alpha^2$ public samples. Next, we give lower bounds on private and public sample complexities with tight dependence on $p$ and $\alpha$. In particular, for the class of decision stumps, we give a lower bound of $\sqrt{p}/\alpha$ on the private sample complexity whenever the number of public samples is $<1/\alpha^2$. Given our upper bound, this shows that the dependence on $\sqrt{p}$ in the private sample complexity is necessary (in the non-trivial regime where the public samples are insufficient to solve the problem on its own). We also give a tight lower bound of $1/\alpha$ on the public sample complexity for a broad family of query classes.
 Learning Algebraic Multigrid Using Graph Neural Networks 
 Efficient numerical solvers for sparse linear systems are crucial in science and engineering. One of the fastest methods for solving large-scale sparse linear systems is algebraic multigrid (AMG). The main challenge in the construction of AMG algorithms is the selection of the prolongation operator---a problem-dependent sparse matrix which governs the multiscale hierarchy of the solver and is critical to its efficiency. Over many years, numerous methods have been developed for this task, and yet there is no known single right answer except in very special cases. Here we propose a framework for learning AMG prolongation operators for linear systems with sparse symmetric positive (semi-) definite matrices. We train a single graph neural network to learn a mapping from an entire class of such matrices to prolongation operators, using an efficient unsupervised loss function.  Experiments on a broad class of problems demonstrate improved convergence rates compared to classical AMG, demonstrating the potential utility of neural networks for developing sparse system solvers.
 Fast Adaptation to New Environments via Policy-Dynamics Value Functions 
 Standard RL algorithms assume fixed environment dynamics and require a significant amount of interaction to adapt to new environments. We introduce Policy-Dynamics Value Functions (PD-VF), a novel approach for rapidly adapting to dynamics different from those previously seen in training. PD-VF explicitly estimates the cumulative reward in a space of policies and environments. An ensemble of conventional RL policies is used to gather experience on training environments, from which embeddings of both policies and environments can be learned. Then, a value function conditioned on both embeddings is trained. At test time, a few actions are sufficient to infer the environment embedding, enabling a policy to be selected by maximizing the learned value function (which requires no additional environment interaction). We show that our method can rapidly adapt to new dynamics on a set of MuJoCo domains. 
 Estimating Q(s,s') with Deterministic Dynamics Gradients 
 In this paper, we introduce a novel form of a value function, $Q(s, s')$, that expresses the utility of transitioning from a state $s$ to a neighboring state $s'$ and then acting optimally thereafter. In order to derive an optimal policy, we develop a novel forward dynamics model that learns to make next-state predictions that maximize $Q(s,s')$. This formulation decouples actions from values while still learning off-policy. We highlight the benefits of this approach in terms of value function transfer, learning within redundant action spaces, and learning off-policy from state observations generated by sub-optimal or completely random policies.
 Dispersed EM-VAEs for Interpretable Text Generation 
 Interpretability is important in text generation for guiding the generation with interpretable attributes.
Variational auto-encoder (VAE) with Gaussian distribution as prior has been successfully applied in text generation, but it is hard to interpret the meaning of the latent variable.
To enhance the controllability and interpretability, one can replace the Gaussian prior with a mixture of Gaussian distributions (GM-VAE), whose mixture components could be related to some latent attributes of data.
Unfortunately, straightforward variational training of GM-VAE leads the mode-collapse problem.
In this paper, we find that mode-collapse is a general problem for VAEs with exponential family mixture priors. 
We propose DEM-VAE, which introduces an extra dispersion term to induce a well-structured latent space.
Experimental results show that our approach does obtain a well structured latent space, with which our method outperforms strong baselines in interpretable text generation benchmarks.
 Relaxing Bijectivity Constraints with Continuously Indexed Normalising Flows 
 We show that the bijectivity of normalising flows means they are misspecified for modelling target densities whose support has a different topology from the prior. In this case, we prove that the flow must become arbitrarily close to noninvertible in order even to approximate the target closely. This result has implications for all flow-based models, and particularly residual flows (ResFlows), which explicitly control the Lipschitz constant of the bijection used. To address this, we propose continuously indexed flows (CIFs), which replace the single bijection used by normalising flows with a continuously indexed family of bijections, and which intuitively allow rerouting mass that would be misplaced by a single bijection. We prove that CIFs can exactly match the support of the target even when its topology differs from the prior, and obtain empirically better performance for a variety of models on a variety of benchmarks.
 Generalization via Derandomization 
 We propose to study the generalization error of a learned predictor h^ in terms of that of a surrogate (potentially randomized) classifier that is coupled to h^ and designed to trade empirical risk for control of generalization error. In the case where h^ interpolates the data, it is interesting to consider theoretical surrogate classifiers that are partially derandomized or rerandomized, e.g., fit to the training data but with modified label noise. We show that replacing h^ by its conditional distribution with respect to an arbitrary sigma-field is a viable method to derandomize. We give an example, inspired by the work of Nagarajan and Kolter (2019), where the learned classifier h^ interpolates the training data with high probability, has small risk, and, yet, does not belong to a nonrandom class with a tight uniform bound on two-sided generalization error. At the same time, we bound the risk of h^ in terms of a surrogate that is constructed by conditioning and shown to belong to a nonrandom class with uniformly small generalization error. 
 Composing Molecules with Multiple Property Constraints 
 Drug discovery aims to find novel compounds with specified chemical property profiles. In terms of generative modeling, the goal is to learn to sample molecules in the intersection of multiple property constraints. This task becomes increasingly challenging when there are many property constraints. We propose to offset this complexity by composing molecules from a vocabulary of substructures that we call molecular rationales. These rationales are identified from molecules as substructures that are likely responsible for each property of interest. We then learn to expand rationales into a full molecule using graph generative models. Our final generative model composes molecules as mixtures of multiple rationale completions, and this mixture is fine-tuned to preserve the properties of interest. We evaluate our model on various drug design tasks and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy, diversity, and novelty of generated compounds.
 Perceptual Generative Autoencoders 
 Modern generative models are usually designed to match target distributions directly in the data space, where the intrinsic dimension of data can be much lower than the ambient dimension. We argue that this discrepancy may contribute to the difficulties in training generative models. We therefore propose to map both the generated and target distributions to the latent space using the encoder of a standard autoencoder, and train the generator (or decoder) to match the target distribution in the latent space. Specifically, we enforce the consistency in both the data space and the latent space with theoretically justified data and latent reconstruction losses. The resulting generative model, which we call a perceptual generative autoencoder (PGA), is then trained with a maximum likelihood or variational autoencoder (VAE) objective. With maximum likelihood, PGAs generalize the idea of reversible generative models to unrestricted neural network architectures and arbitrary number of latent dimensions. When combined with VAEs, PGAs substantially improve over the baseline VAEs in terms of sample quality. Compared to other autoencoder-based generative models using simple priors, PGAs achieve state-of-the-art FID scores on CIFAR-10 and CelebA.
 Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences 
 Bayesian reward learning from demonstrations enables rigorous safety and uncertainty analysis when performing imitation learning. However, Bayesian reward learning methods are typically computationally intractable for complex control problems. We propose a highly efficient Bayesian reward learning algorithm that scales to high-dimensional imitation learning problems by first pre-training a low-dimensional feature encoding via self-supervised tasks and then leveraging preferences over demonstrations to perform fast Bayesian inference via sampling. We evaluate our proposed approach on the task of learning to play Atari games from demonstrations, without access to the game score, and achieve state-of-the-art imitation learning performance. Furthermore, we also demonstrate that our approach enables efficient high-confidence performance bounds for any evaluation policy. We show that these high-confidence performance bounds can be used to accurately rank the performance and risk of a variety of different evaluation policies, despite not having samples of the true reward function. 
 Neural Architecture Search in a Proxy Validation Loss Landscape 
 This paper searches for the optimal neural architecture  by minimizing a proxy of validation loss. Existing neural architecture search (NAS) methods used to discover the optimal neural architecture that best fits the validation examples given the up-to-date network weights. However, back propagation with a number of validation examples could be time consuming, especially when it needs to be repeated many times in NAS. Though these  intermediate validation results are invaluable, they would be wasted if we cannot use them to predict the future from the past. In this paper, we propose to approximate the validation loss landscape by learning a mapping from neural architectures to their corresponding validate losses. The optimal neural architecture thus can be easily identified as the minimum of this proxy validation loss landscape. A novel sampling strategy is further developed for an efficient approximation of the loss landscape. Theoretical analysis indicates that our sampling strategy can reach a lower error rate and a lower label complexity compared with a uniform sampling. Experimental results on benchmarks demonstrate that the architecture searched by the proposed algorithm can achieve a satisfactory accuracy with less time cost.
 On Unbalanced Optimal Transport: An Analysis of Sinkhorn Algorithm 
 We provide a computational complexity analysis for the Sinkhorn algorithm that solves the entropic regularized Unbalanced Optimal Transport (UOT) problem between two measures of possibly different masses with at most $n$ components.  We show that the complexity of the Sinkhorn algorithm for finding an $\varepsilon$-approximate solution to the UOT problem is of order  $\widetilde{\mathcal{O}}(n^2/ \varepsilon)$, which is near-linear time. To the best of our knowledge, this complexity is better than the complexity of the Sinkhorn algorithm for solving the Optimal Transport (OT) problem, which is of order $\widetilde{\mathcal{O}}(n^2/\varepsilon^2)$. Our proof technique is based on the geometric convergence of the Sinkhorn updates to the optimal dual solution of the entropic regularized UOT problem and some properties of the primal solution. It is also different from the proof for the complexity of the Sinkhorn algorithm for approximating the OT problem since the UOT solution does not have to meet the marginal constraints.
 Quantile Causal Discovery 
 Causal inference using observational data is challenging, especially in the bivariate case.
Through the minimum description length principle, we link the postulate of independence between the generating mechanisms of the cause and of the effect given the cause to quantile regression.
Based on this theory, we develop Quantile Causal Discovery (QCD), a new method to uncover causal relationships.
Because it uses multiple quantile levels instead of the conditional mean only, QCD is adaptive not only to additive, but also to multiplicative or even location-scale generating mechanisms.
To illustrate the empirical effectiveness of our approach, we perform an extensive empirical comparison on both synthetic and real datasets.
This study shows that QCD is robust across different implementations of the method (i.e., the quantile regression algorithm), computationally efficient, and compares favorably to state-of-the-art methods.
 Model-Agnostic Characterization of Fairness Trade-offs 
 There exist several inherent trade-offs while designing a fair model, such as those between the model’s predictive accuracy and fairness, or even among different  notions of fairness. In practice, exploring these trade-offs requires significant human and computational resources. 
We propose a diagnostic to enable practitioners to explore these trade-offs without training a single model. Our work hinges on the observation that many widely-used fairness definitions can be expressed via the fairness-confusion tensor, an object obtained by splitting the traditional confusion matrix according to protected data attributes. Our diagnostic optimizes accuracy and fairness objectives directly over the elements in this tensor in a data-dependent, yet model-agnostic fashion. We further leverage our tensor-based perspective to generalize existing theoretical impossibility results to a wider range of fairness definitions. Finally, we demonstrate the usefulness of the proposed diagnostic on synthetic and real datasets.
 How Good is the Bayes Posterior in Deep Neural Networks Really? 
 During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks.  However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice.  In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions when compared to simpler methods including point estimates obtained from SGD.  Furthermore, we demonstrate that predictive performance is improved significantly through the use of a ``cold posterior'' that overcounts evidence.  Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers.  We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments.  Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations?  Instead, we argue that it is timely to focus on understanding the origin of cold posteriors.
 Optimal Differential Privacy Composition for Exponential Mechanisms 
 Composition is one of the most important properties of differential privacy (DP), as it allows algorithm designers to build complex private algorithms from DP primitives. We consider precise composition bounds of the overall privacy loss for exponential mechanisms, one of the fundamental classes of mechanisms in DP. Exponential mechanism has also become a fundamental building block in private machine learning, e.g. private PCA and hyper-parameter selection. We give explicit formulations of the optimal privacy loss for both the adaptive and non-adaptive composition of exponential mechanism. For the non-adaptive setting in which each mechanism has the same privacy parameter, we give an efficiently computable formulation of the optimal privacy loss. In the adaptive case, we derive a recursive formula and an efficiently computable upper bound. These precise understandings about the problem lead to a 40\% saving of the privacy budget in a practical application. Furthermore, the algorithm-specific analysis shows a difference in privacy parameters of adaptive and non-adaptive composition, which was widely believed to not exist based on the evidence from general analysis.
 Equivariant Neural Rendering 
 We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.
 A Generic First-Order Algorithmic Framework for Bi-Level Programming Beyond Lower-Level Singleton 
 Recently, gradient-based methods have been developed for Bi-Level Programmings (BLPs) in learning and vision fields. The successes of these methods heavily rely on the simplification that for each fixed upper-level variable, the lower-level solution is a singleton (i.e., Lower-Level Singleton, LLS). However, LLS is usually too restrictive to be satisfied in real-world complex scenarios. This paper first presents a counter-example to illustrate the invalidation of those existing gradient-based bi-level schemes in the absence of the LLS condition. To address this critical issue, a new method, named Bi-level Descent Aggregation (BDA) is proposed, aiming to broaden the application horizon of first-order schemes for BLPs. In particular, by investigating BLPs from the view point of optimistic bi-level, BDA establishes a generic algorithmic framework. In our strategy, the aggregation of hierarchical objective information helps to produce flexible bi-level iteration schemes. Our theoretical investigations prove the strict convergence of BDA for general BLPs without the LLS condition. We also show that BDA is indeed compatible to a verify of particular first-order computation modules. Additionally, as an interesting byproduct, we improve those conventional first-order bi-level schemes under the LLS simplification. Particularly, we establish their convergences with weaker assumptions. Extensive experiments justify our theoretical results and demonstrate the superiority of the proposed BDA for different tasks, including hyper-parameter optimization and meta learning.
 Differentially Private Set Union 
 We study the basic operation of set union in the global model of differential privacy.  In this problem, we are given a universe $U$ of items, possibly of infinite size, and a database $D$ of users. Each user $i$ contributes a subset $W_i \subseteq U$ of items. We want an ($\epsilon$,$\delta$)-differentially private Algorithm  which outputs a subset $S \subset \cup_i W_i$ such that the size of $S$ is as large as possible. The problem arises in countless real world applications, and is particularly important and ubiquitous  in natural language processing (NLP) problems. For example, discovering words, sentences,  $n$-grams etc., from private text data belonging to users is an instance of the set union problem.
 
Known algorithms for this problem proceed by collecting a (weighted) subset of items from each user, taking the union of such  subsets, and disclosing the items whose noisy counts fall above a certain cutoff threshold. Crucially, in the above process, the contribution of each individual user is always independent from identity of items held by other users, resulting in a wasteful aggregation process, where some items’ counts happen to be very large – far above the cutoff threshold. We deviate from the above paradigm, by allowing users to contribute their items in a {\em dependent fashion}, guided by a policy. In this new setting ensuring privacy is  significantly delicate.  We prove that any policy which has certain {\em contractive} properties would result in a differentially private algorithm. We design two new algorithms, one using Laplace Noise and other Gaussian noise, as specific instances of policies satisfying the contractive properties. Our experiments show that the new algorithms significantly outperform previously known mechanisms for the problem.

 Interpolation between CNNs and ResNets 
 Although ordinary differential equations (ODEs) provide insights for designing networks architectures, its relationship with the non-residual convolutional neural networks (CNNs) is still unclear. In this paper, we present a novel ODE model by adding a damping term. It can be shown that the proposed model can recover both a ResNet and a CNN by adjusting an interpolation coefficient. Therefore, the damped ODE model provides a unified framework for the interpretation of CNNs and ResNets. The Lyapunov analysis reveals better stability of the proposed model, and thus yields robustness improvement of the learned networks. Experiments on a number of image classification benchmarks show that the proposed model substantially improves the accuracy of ResNet and ResNeXt over the perturbed inputs from both stochastic noise and adversarial attack methods. Moreover, the loss landscape analysis demonstrates the improved robustness of our method along the attack direction.
 A Markov Decision Process Model for Socio-Economic Systems Impacted by Climate Change 
 Coastal communities are at high risk of natural hazards due to unremitting global warming and sea level rise. Both the catastrophic impacts, e.g., tidal flooding and storm surges, and the long-term impacts, e.g., beach erosion, inundation of low lying areas, and saltwater intrusion into aquifers, cause economic, social, and ecological losses. Creating policies through appropriate modeling of the responses of stakeholders, such as government, businesses, and residents, to climate change and sea level rise scenarios can help to reduce these losses. In this work, we propose a Markov decision process (MDP) formulation for an agent (government) which interacts with the environment (nature and residents) to deal with the impacts of climate change, in particular sea level rise. Through theoretical analysis we show that a reasonable government's policy on infrastructure development ought to be proactive and based on detected sea levels in order to minimize the expected total cost, as opposed to a straightforward government that reacts to observed costs from nature. We also provide a deep reinforcement learning-based scenario planning tool considering different government and resident types in terms of cooperation, and different sea level rise projections by the National Oceanic and Atmospheric Administration (NOAA). 
 Optimal approximation for unconstrained non-submodular minimization 
   Submodular function minimization is well studied, and existing algorithms solve it exactly or up to arbitrary accuracy. However, in many applications, such as structured sparse learning, and batch Bayesian optimization, the objective function is not exactly submodular, but close. In this case, no theoretical guarantees exist. Indeed, submodular minimization algorithms rely on intricate connections between submodularity and convexity. We show how these relations can be extended to obtain approximation guarantees for minimizing non-submodular functions, characterized by how close the function is to submodular. We also extend this result to noisy function evaluations. Our approximation results are the first for minimizing non-submodular functions, and are optimal, as established by our matching lower bound.
 Learning to Score Behaviors for Guided Policy Optimization 
 We introduce a new approach for comparing reinforcement learning policies, using Wasserstein distances (WDs) in a newly defined latent behavioral space. We show that by utilizing the dual formulation of the WD, we can learn score functions over policy behaviors that can in turn be used to lead policy optimization towards (or away from) (un)desired behaviors. Combined with smoothed WDs, the dual formulation allows us to devise efficient algorithms that take stochastic gradient descent steps through WD regularizers. We incorporate these regularizers into two novel on-policy algorithms, Behavior-Guided Policy Gradient and Behavior-Guided Evolution Strategies, which we demonstrate can outperform existing methods in a variety of challenging environments. We also provide an open source demo.
 Curse of Dimensionality on Randomized Smoothing for Certifiable Robustness 
 Randomized smoothing, using just a simple isotropic Gaussian distribution, has been shown to produce good robustness guarantees against $\ell_2$-norm bounded adversaries. In this work, we show that extending the smoothing technique to defend against other attack models can be challenging, especially in the high-dimensional regime.  In particular, for a vast class of i.i.d.~smoothing distributions, we prove that the largest $\ell_p$-radius that can be certified decreases as $O(1/d^{\frac{1}{2} - \frac{1}{p}})$ with dimension $d$ for $p > 2$. Notably, for $p \geq 2$, this dependence on $d$ is no better than that of the $\ell_p$-radius that can be certified using isotropic Gaussian smoothing, essentially putting a matching lower bound on the robustness radius.
When restricted to {\it generalized} Gaussian smoothing, these two bounds can be shown to be within a constant factor of each other in an asymptotic sense, establishing that Gaussian smoothing provides the best possible results, up to a constant factor, when $p \geq 2$. We present experimental results on CIFAR to validate our theory.
For other smoothing distributions, such as, a uniform distribution within an $\ell_1$ or an $\ell_\infty$-norm ball, we show upper bounds of the form $O(1 / d)$ and $O(1 / d^{1 - \frac{1}{p}})$ respectively, which have an even worse dependence on $d$. 

 Single Point Transductive Prediction 
 Standard methods in supervised learning separate training and prediction: the model is fit independently of any test points it may encounter. However, can knowledge of the next test point $\mathbf{x}_{\star}$ be exploited to improve prediction accuracy? We address this question in the context of linear prediction, showing how  techniques from semi-parametric inference can be used transductively to combat regularization bias. We first lower bound the $\mathbf{x}_{\star}$ prediction error of ridge regression and the Lasso, showing that they must incur significant bias in certain test directions. We then provide non-asymptotic upper bounds on the $\mathbf{x}_{\star}$ prediction error of two transductive prediction rules. We conclude by showing the efficacy of our methods on both synthetic and real data, highlighting the improvements single point transductive
    prediction can provide in settings with distribution shift.
 Implicit Euler Skip Connections: Enhancing Adversarial Robustness via Numerical Stability 
 Deep neural networks have achieved great success in various areas. However, recent works have found that neural networks are vulnerable to adversarial attacks, which leads to a hot topic nowadays. Although many approaches have been proposed to enhance the robustness of neural networks, few of them explored robust architectures for neural networks. On this account, we try to address such an issue from the perspective of dynamic system in this work. By viewing ResNet as an explicit Euler discretization of an ordinary differential equation (ODE), for the first time, we find that the adversarial robustness of ResNet is connected to the numerical stability of the corresponding dynamic system. Namely, more stable numerical schemes may correspond to more robust deep networks. Furthermore, inspired by the implicit Euler method for solving numerical ODE problems, we propose Implicit Euler skip connections (IE-Skips) by modifying the original skip connection in ResNet or its variants. Then we theoretically prove its advantages under the adversarial attack. Experimental results show that our ResNet with IE-Skips can largely improve the robustness and the generalization ability under adversarial attacks when compared with the vanilla ResNet of the same parameter size.
 Robust Pricing in Dynamic Mechanism Design 
 Motivated by the repeated sale of online ads via auctions, optimal pricing in repeated auctions has attracted a large body of research. While dynamic mechanisms offer powerful techniques to improve on both revenue and efficiency by optimizing auctions across different items, their reliance on exact distributional information of buyers' valuations (present and future) limits their use in practice. In this paper, we propose robust dynamic mechanism design. We develop a new framework to design dynamic mechanisms that are robust to both estimation errors in value distributions and strategic behavior. We apply the framework in learning environments, leading to the first policy that achieves provably low regret against the optimal dynamic mechanism in contextual auctions, where the dynamic benchmark has full and accurate distributional information.
 Bayesian Graph Neural Networks with Adaptive Connection Sampling 
 We propose a unified framework for adaptive connection sampling in graph neural  networks (GNNs) that generalizes existing stochastic regularization methods for  training GNNs. The proposed framework not only alleviates over-smoothing and  over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning themas model hyperparameters in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training BayesianGNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boost the performance of GNNs in semi-supervised node classification, less prone to over-smoothing and over-fitting with more robust prediction.
 Interpreting Robust Optimization via Adversarial Influence Functions 
 Robust optimization has been widely used in nowadays data science, especially in adversarial training. However, little research has been done to quantify how robust optimization changes the optimizers and the prediction losses comparing to standard training.  In this paper, inspired by the influence function in robust statistics, we introduce the Adversarial Influence Function (AIF) as a tool to investigate the solution produced by robust optimization. The proposed AIF enjoys a closed-form and can be calculated efficiently. To illustrate the usage of AIF, we apply it to study model sensitivity -- a quantity defined to capture the change of prediction losses on the natural data after implementing robust optimization. We use AIF to analyze how model complexity and randomized smoothing affect the model sensitivity with respect to specific models.  We further derive AIF for kernel regressions, with a particular application to neural tangent kernels, and experimentally demonstrate the effectiveness of the proposed AIF. Lastly, the theories of AIF will be extended to distributional robust optimization.
 A Tree-Structured Decoder for Image-to-Markup Generation 
 Recent encoder-decoder approaches typically employ string decoders to convert images into serialized strings for image-to-markup. However, for tree-structured representational markup, string representations can hardly cope with the structural complexity. In this work, we first show via a set of toy problems that string decoders struggle to decode tree structures, especially as structural complexity increases. We then propose a tree-structured decoder that specifically aims at generating a tree-structured markup. Our decoders works sequentially, where at each step a child node and its parent node are simultaneously generated to form a sub-tree. This sub-tree is consequently used to construct the final tree structure in a recurrent manner. Key to the success of our tree decoder is twofold, (i) it strictly respects the parent-child relationship of trees, and (ii) it explicitly outputs trees as oppose to a linear string. Evaluated on both math formula recognition and chemical formula recognition, the proposed tree decoder is shown to greatly outperform strong string decoder baselines.
 Differentiable Product Quantization for Learning Compact Embedding Layers 
 Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints.
In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238x) at negligible or no performance cost on 10 datasets across three different language tasks.
 Learning to Simulate Complex Physics with Graph Networks 
 Here we present a general framework for learning simulation, and provide a single model implementation that yields state-of-the-art performance across a variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators"" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework is the most accurate general-purpose learned physics simulator to date, and holds promise for solving a wide range of complex forward and inverse problems.
 Model Fusion with Kullback--Leibler Divergence 
 We propose a method to fuse posterior distributions learned from heterogeneous datasets. Our algorithm relies on a mean field assumption for both the fused model and the individual dataset posteriors, and proceeds in a simple assign-and-average approach. The components of the dataset posteriors are assigned to the proposed global model components by solving a regularized variant of the assignment problem. The global components are then updated based on these assignments by their mean under a KL divergence. For exponential family variational distributions, our formulation leads to an efficient non-parametric algorithm for computing the fused model. Our algorithm is easy to describe and implement, efficient, and performs competitive with state-of-the-art when tested on motion capture analysis, topic modeling, and federated learning of Bayesian neural networks.
 Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism 
 We consider un-discounted reinforcement learning (RL) in  Markov decision processes (MDPs) under drifting non-stationarity, \ie, both the reward and state transition distributions are allowed to evolve over time, as long as their respective total variations, quantified by suitable metrics, do not exceed certain \emph{variation budgets}. We first develop the Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence Widening (\texttt{SWUCRL2-CW}) algorithm, and establish its dynamic regret bound when the variation budgets are known. In addition, we propose the Bandit-over-Reinforcement Learning (\texttt{BORL}) algorithm to adaptively tune the \sw~to achieve the same dynamic regret bound, but  in a \emph{parameter-free} manner, \ie, without knowing the variation budgets. Notably, learning drifting MDPs via conventional optimistic exploration presents a unique challenge absent in existing (non-stationary) bandit learning settings. We overcome the challenge by a novel confidence widening technique that incorporates additional optimism.
 Retro*: Learning Retrosynthetic Planning with Neural Guided A* Search 
 Retrosynthetic planning is a critical task in organic chemistry which identifies a series of reactions that can lead to the synthesis of a target product.  The vast number of possible chemical transformations makes the size of the search space very big, and retrosynthetic planning is challenging even for experienced chemists. However, existing methods either require expensive return estimation by rollout with high variance, or optimize for search speed rather than the quality.  In this paper, we propose Retro*, a neural-based A*-like algorithm that finds high-quality synthetic routes efficiently. It maintains the search as an AND-OR tree, and learns a neural search bias with off-policy data. Then guided by this neural network, it performs best first search efficiently during new planning episode. Experiments on  benchmark USPTO datasets show that, our proposed method outperforms existing state-of-the-art with respect to both the success rate and solution quality, while being more efficient at the same time.
 Conditional gradient methods for stochastically constrained convex minimization 
 We propose two novel conditional gradient-based methods for solving structured stochastic convex optimization problems with a large number of linear constraints. Instances of this template naturally arise from SDP-relaxations of combinatorial problems, which involve a number of constraints that is polynomial in the problem dimension. The most important feature of our framework is that only a subset of the constraints is processed at each iteration, thus gaining a computational advantage over prior works that require full passes. Our algorithms rely on variance reduction and smoothing used in conjunction with conditional gradient steps, and are accompanied by rigorous convergence guarantees. Preliminary numerical experiments are provided for illustrating the practical performance of the methods.
 Rethinking Bias-Variance Trade-off for Generalization of Neural Networks 
 The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation of this by measuring the bias and variance of neural networks: while the bias is {\em monotonically decreasing} as in the classical theory, the variance is {\em unimodal} or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent in the recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.
 Searching to Exploit Memorization Effect in Learning with Noisy Labels 
 Sample selection approaches are popular in robust learning from noisy labels. However, how to properly control the selection process so that deep networks can benefit from the memorization effect is a hard problem. In this paper, motivated by the success of automated machine learning (AutoML), we model this issue as a function approximation problem. 
Specifically, we design a domain-specific search space based on general patterns of the memorization effect and propose a novel Newton algorithm to solve the bi-level optimization problem efficiently.  We further provide a theoretical analysis of the algorithm, which ensures a good approximation to critical points. Experiments are performed on both benchmark and real-world data sets. Results demonstrate that the proposed method is much better than the state-of-the-art noisy-label-learning approaches, and also much more efficient than existing AutoML algorithms.
 Stochastic Gradient and Langevin Processes 
 We prove quantitative convergence rates at which discrete Langevin-like processes converge to the invariant distribution of a related stochastic differential equation. We study the setup where the additive noise can be non-Gaussian and state-dependent and the potential function can be non-convex. We show that the key properties of these processes depend on the potential function and the second moment of the additive noise. We apply our theoretical findings to studying the convergence of Stochastic Gradient Descent (SGD) for non-convex problems and corroborate them with experiments using SGD to train deep neural networks on the CIFAR-10 dataset.
 PowerNorm: Rethinking Batch Normalization in Transformers 
 The standard normalization method for neural network (NN) models used in Natural Language Processing (NLP) is layer normalization (LN).This is different than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred use of LN in NLP is principally due to the empirical observation that a (naive/vanilla) use of BN leads to significant performance degradation for NLP tasks; however, a thorough understanding of the underlying reasons for this is not always evident.
In this paper, we perform a systematic study of NLP transformer models to understand why BN has a poor performance, as compared to LN. We find that the statistics of NLP data across the batch dimension exhibit large fluctuations throughout training. This results in instability, if BN is naively implemented.
To address this, we propose Power Normalization (PN), a novel normalization scheme that resolves this issue by (i) relaxing zero-mean normalization in BN,
(ii) incorporating a running quadratic mean instead of per batch statistics to stabilize
fluctuations, and (iii) using an approximate backpropagation for incorporating the running statistics in the forward pass. We show theoretically, under mild assumptions, that PN leads to a smaller Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the approximate backpropagation scheme leads to bounded gradients. We extensively test PN for transformers on a range of NLP tasks, and we show that it significantly outperforms both LN and BN. 
In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14 and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at https://github.com/sIncerass/powernorm.
 Streaming Submodular Maximization under a k-Set System Constraint 
 In this paper, we propose a novel framework that converts streaming algorithms for monotone submodular maximization into streaming algorithms for non-monotone submodular maximization. This reduction readily leads to the currently tightest deterministic approximation ratio for submodular maximization subject to a $k$-matchoid constraint. Moreover, we propose the first streaming algorithms for monotone submodular maximization subject to $k$-extendible and $k$-system constraints. Together with our proposed reduction, we obtain $O(k\log k)$ and $O(k^2\log k)$ approximation ratio for submodular maximization subject to the above constraints, respectively. We extensively evaluate the empirical performance of our algorithm against the existing work in a series of experiments including finding the maximum independent set in randomly generated graphs, maximizing linear functions over social networks, movie recommendation, Yelp location summarization, and Twitter data summarization.
 SCAFFOLD: Stochastic Controlled Averaging for Federated Learning 
 Federated learning is a key scenario in modern large-scale machine learning where the data remains distributed over a large number of clients and the task is to learn a centralized model without transmitting the client data. The standard optimization algorithm used in this setting is Federated Averaging (FedAvg) due to its low communication cost. We obtain a tight characterization of the convergence of FedAvg and prove that heterogeneity (non-iid-ness) in the client's data results in a `drift' in the local updates resulting in poor performance.
    
As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the `client drift'. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client's data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.
 Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning 
 Increasing the scale of reinforcement learning experiments has allowed researchers to achieve unprecedented results in both training sophisticated agents for video games, and in sim-to-real transfer for robotics. Typically such experiments rely on large distributed systems and require expensive hardware setups, limiting wider access to this exciting area of research. In this work we aim to solve this problem by optimizing the efficiency and resource utilization of reinforcement learning algorithms instead of relying on distributed computation. We present the "Sample Factory", a high-throughput training system optimized for a single-machine setting. Our architecture combines a highly efficient, asynchronous, GPU-based sampler with off-policy correction techniques, allowing us to achieve throughput higher than $10^5$ environment frames/second on non-trivial control problems in 3D without sacrificing sample efficiency. We extend Sample Factory to support self-play and population-based training and apply these techniques to train highly capable agents for a multiplayer first-person shooter game. Github: https://github.com/alex-petrenko/sample-factory
 Polynomial Tensor Sketch for Element-wise Function of Low-Rank Matrix 
 This paper studies how to sketch element-wise functions of low-rank matrices. Formally, given low-rank matrix A = [Aij] and scalar non-linear function f, we aim for finding an approximated low-rank representation of the (possibly high-rank) matrix [f(Aij)]. To this end, we propose an efficient sketching-based algorithm whose complexity is significantly lower than the number of entries of A, i.e., it runs without accessing all entries of [f(Aij)] explicitly. The main idea underlying our method is to combine a polynomial approximation of f with the existing tensor sketch scheme for approximating monomials of entries of A. To balance the errors of the two approximation components in an optimal manner, we propose a novel regression formula to find polynomial coefficients given A and f. In particular, we utilize a coreset-based regression with a rigorous approximation guarantee. Finally, we demonstrate the applicability and superiority of the proposed scheme under various machine learning tasks.
 Beyond Signal Propagation: Is Feature Diversity Necessary in Deep Neural Network Initialization? 
 Deep neural networks are typically initialized with random weights, with variances chosen to facilitate signal propagation and stable gradients. It is also believed that diversity of features is an important property of these initializations. We construct a deep convolutional network with identical features by initializing almost all the weights to $0$. The architecture also enables perfect signal propagation and stable gradients, and achieves high accuracy on standard benchmarks. This indicates that random, diverse initializations are \textit{not} necessary for training neural networks. An essential element in training this network is a mechanism of symmetry breaking; we study this phenomenon and find that standard GPU operations, which are non-deterministic, can serve as a sufficient source of symmetry breaking to enable training.
 Confidence-Aware Learning for Deep Neural Networks 
 Despite the power of deep neural networks for a wide range of tasks, an overconfident prediction issue has limited their practical use in many safety-critical applications. Many recent works have been proposed to mitigate this issue, but most of them require either additional computational costs in training and/or inference phases or customized architectures to output confidence estimates separately. In this paper, we propose a method of training deep neural networks with a novel loss function, named Correctness Ranking Loss, which regularizes class probabilities explicitly to be better confidence estimates in terms of ordinal ranking according to confidence. The proposed method is easy to implement and can be applied to the existing architectures without any modification. Also, it has almost the same computational costs for training as conventional deep classifiers and outputs reliable predictions by a single inference. Extensive experimental results on classification benchmark datasets indicate that the proposed method helps networks to produce well-ranked confidence estimates. We also demonstrate that it is effective for the tasks closely related to confidence estimation, out-of-distribution detection and active learning.
 Training Deep Energy-Based Models with f-Divergence Minimization 
 Deep energy-based models (EBMs) are very flexible in distribution parametrization but computationally challenging because of the intractable partition function. They are typically trained via maximum likelihood, using contrastive divergence to approximate the gradient of the KL divergence between data and model distribution. While KL divergence has many desirable properties, other f-divergences have shown advantages in training implicit density generative models such as generative adversarial networks. In this paper, we propose a general variational framework termed f-EBM to train EBMs using any desired f-divergence. We introduce a corresponding optimization algorithm and prove its local convergence property with non-linear dynamical systems theory. Experimental results demonstrate the superiority of f-EBM over contrastive divergence, as well as the benefits of training EBMs using f-divergences other than KL.
 Efficiently Learning Adversarially Robust Halfspaces with Noise 
 We study the problem of learning adversarially robust halfspaces in the distribution-independent setting. We give the first computationally efficient algorithm for this problem in the realizable setting and in the presence of random label noise with respect to any $\ell_p$-perturbation (and, more generally, perturbations with respect to any norm). 
 Bio-Inspired Hashing for Unsupervised Similarity Search 
 The fruit fly Drosophila's olfactory circuit has inspired a new locality sensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH algorithms that produce low dimensional hash codes, FlyHash produces sparse high-dimensional hash codes and has also been shown to have superior empirical performance compared to classical LSH algorithms in similarity search. However, FlyHash uses random projections and cannot learn from data. Building on inspiration from FlyHash and the ubiquity of sparse expansive representations in neurobiology, our work proposes a novel hashing algorithm BioHash that produces sparse high dimensional hash codes in a data-driven manner. We show that BioHash outperforms previously published benchmarks for various hashing methods. Since our learning algorithm is based on a local and biologically plausible synaptic plasticity rule, our work provides evidence for the proposal that LSH might be a computational reason for the abundance of sparse expansive motifs in a variety of biological systems. We also propose a convolutional variant BioConvHash that further improves performance. From the perspective of computer science, BioHash and BioConvHash are fast, scalable and yield compressed binary representations that are useful for similarity search.
 Projective Preferential Bayesian Optimization 
 Bayesian optimization is an effective method for finding extrema of a black-box function. We propose a new type of Bayesian optimization for learning user preferences in high-dimensional spaces. The central assumption is that the underlying objective function cannot be evaluated directly, but instead a minimizer along a projection can be queried, which we call a projective preferential query. The form of the query allows for feedback that is natural for a human to give, and which enables interaction. This is demonstrated in a user experiment in which the user feedback comes in the form of optimal position and orientation of a molecule adsorbing to a surface. We demonstrate that our framework is able to find a global minimum of a high-dimensional black-box function, which is an infeasible task for existing preferential Bayesian optimization frameworks that are based on pairwise comparisons.
 Causal Modeling for Fairness In Dynamical Systems 
 In many applications areas---lending, education, and online recommenders, for example---fairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and long-term effects for individuals and demographic groups. We discuss causal directed acyclic graphs (DAGs) as a unifying framework for the recent literature on fairness in such dynamical systems. We show that this formulation affords several new directions of inquiry to the modeler, where sound causal assumptions can be expressed and manipulated. We emphasize the importance of computing interventional quantities in the dynamical fairness setting, and show how causal assumptions enable simulation (when environment dynamics are known) and estimation by adjustment (when dynamics are unknown) of intervention on short- and long-term outcomes, at both the group and individual levels.

 On Thompson Sampling with Langevin Algorithms 
 Thompson sampling has been demonstrated both theoretically and empirically to enjoy favorable performance in tackling multi-armed bandits problems.
Despite its successes, however, one key obstacle to its use in a much broader range of scenarios is the need for perfect samples from posterior distributions at every iteration, which is oftentimes not feasible in practice.
We propose a Markov Chain Monte Carlo (MCMC) method tailored to Thompson sampling to address this issue. We construct a fast converging Langevin algorithm to generate approximate samples with accuracy guarantees. We then leverage novel posterior concentration rates to analyze the statistical risk of the overall Thompson sampling method.  Finally, we specify the necessary hyperparameters and
the required computational resources for the MCMC procedure to match the optimal risk. The resulting algorithm enjoys both optimal instance-dependent frequentist regret and appealing computation complexity.
 High-dimensional Robust Mean Estimation via Gradient Descent 
 We study the problem of high-dimensional robust mean estimation in the presence of a constant fraction of adversarial outliers. A recent line of work has provided sophisticated polynomial-time algorithms for this problem with dimension-independent error guarantees for a range of natural distribution families. In this work, we show that a natural non-convex formulation of the problem can be solved directly by gradient descent. Our approach leverages a novel structural lemma, roughly showing that any approximate stationary point of our non-convex objective gives a near-optimal solution to the underlying robust estimation task. Our work establishes an intriguing connection between algorithmic high-dimensional robust statistics and non-convex optimization, which may have broader applications to other robust estimation tasks.
 DeepMatch: Balancing Deep Covariate Representations for Causal Inference Using Adversarial Training 
 We study optimal covariate balance for causal inferences from observational data when rich covariates and complex relationships necessitate flexible modeling with neural networks. Standard approaches such as propensity weighting and matching/balancing fail in such settings due to miscalibrated propensity nets and inappropriate covariate representations, respectively. We propose a new method based on adversarial training of a weighting and a discriminator network that effectively addresses this methodological gap. This is demonstrated through new theoretical characterizations and empirical results on both synthetic and clinical data showing how causal analyses can be salvaged in such challenging settings.
 Supervised Quantile Normalization for Low Rank Matrix Factorization 
 Low rank matrix factorization is a fundamental building block in machine learning, used for instance to summarize gene expression profile data or word-document counts. To be robust to outliers and differences in scale across features, a matrix factorization step is usually preceded by ad-hoc feature normalization steps, such as tf-idf scaling or data whitening. We propose in this work to learn these normalization operators jointly with the factorization itself. More precisely, given a $d\times n$ matrix $X$ of $d$ features measured on $n$ individuals, we propose to learn the parameters of quantile normalization operators that can operate row-wise on the values of $X$ and/or of its factorization $UV$  to improve the quality of the low-rank representation of $X$ itself. This optimization is facilitated by the introduction of differentiable quantile normalization operators derived using regularized optimal transport algorithms.
 On the Global Convergence Rates of Softmax Policy Gradient Methods 
 We make three contributions toward better understanding policy gradient methods.
First, we show that with the true gradient, policy gradient with a softmax parametrization converges at a $O(1/t)$ rate, with constants depending on the problem and initialization.
This result significantly improves recent asymptotic convergence results. 
The analysis relies on two findings:
that the softmax policy gradient satisfies a \L{}ojasiewicz inequality, and the minimum probability of an optimal action during optimization can be bounded in terms of its initial value.
Second, we analyze entropy regularized policy gradient and show that in the one state (bandit) case it enjoys a linear convergence rate $O(e^{-t})$, 
while for general MDPs we prove that it converges at a $O(1/t)$ rate.
This result resolves an open question in the recent literature.
A key insight is that the entropy regularized gradient update behaves similarly to the contraction operator in value learning,
with contraction factor depending on current policy.
Finally, combining the above two results and additional lower bound results, we explain how entropy regularization improves policy optimization, even with the true gradient, from the perspective of convergence rate. 
These results provide a theoretical understanding of the impact of entropy and corroborate existing empirical studies.
 Fast Deterministic CUR Matrix Decomposition with Accuracy Assurance 
 The deterministic CUR matrix decomposition is a low-rank approximation method to analyze a data matrix.
It has attracted considerable attention due to its high interpretability, which results from the fact that the decomposed matrices consist of subsets of the original columns and rows of the data matrix.
The subset is obtained by optimizing an objective function with sparsity-inducing norms via coordinate descent.
However, the existing algorithms for optimization incur high computation costs.
This is because coordinate descent iteratively updates all the parameters in the objective until convergence.
This paper proposes a fast deterministic CUR matrix decomposition.
Our algorithm safely skips unnecessary updates by efficiently evaluating the optimality conditions for the parameters to be zeros.
In addition, we preferentially update the parameters that must be nonzeros.
Theoretically, our approach guarantees the same result as the original approach.
Experiments demonstrate that our algorithm speeds up the deterministic CUR while achieving the same accuracy.
 Deep Graph Random Process for Relational-Thinking-Based  Speech Recognition 
 Both relational thinking and relational reasoning lie at the core of human intelligence. While relational reasoning has inspired many perspectives in artificial intelligence, relational thinking is relatively unexplored in solving machine learning problems. It is characterized by initially relying on innumerable unconscious percepts pertaining to relations between new sensory signals and prior knowledge, consequently becoming a recognizable concept or object through coupling of these percepts. Such mental processes are difficult to model in real-world problems such as in conversational automatic speech recognition (ASR), as the percepts (e.g. unconscious mental impressions formed while hearing sounds) are supposed to be innumerable and not directly observable. And yet the dialogue history of the conversation might still reflect such underlying processes, allowing an indirect way of modeling. We present a framework that models a percept as weak relations between a current utterance and its history. We assume the probability of the existence of such a relation to be close to zero due to the unconsciousness of the percept. Given an utterance and its history, our method can generate an infinite number of probabilistic graphs representing percepts and further analytically combine them into a new graph representing strong relations among utterances. This new graph can be further transformed to be task-specific and provide an informative representation for acoustic modeling. Our approach is able to successfully infer relations among utterances without using any relational data during training. Experimental evaluations on ASR tasks including CHiME-2, SWB-30k and CHiME-5 demonstrate the effectiveness and benefits of our method.
 On Breaking Deep Generative Model-based Defenses and Beyond 
 Deep neural networks have been proven to be vulnerable to the so-called adversarial attacks. Recently there have been efforts to defend such attacks with deep generative models. These defenses often involve an inversion phase that they first seek the latent representation that best matches with the input, then use this representation for prediction. Such defenses are often difficult to attack due to the non-analytical gradients. In this work, we develop a new gradient approximation attack to break these defenses. The idea is to view the inversion phase as a dynamical system, through which we extract the gradient with respect to the input by tracing its recent trajectory. An amortized strategy is further developed to accelerate the attack. Experiments show that our attack outperforms state-of-the-art approaches (e.g Backward Pass Differential Approximation) with unprecedented low distortions. Additionally, our empirical results reveal a key defect of current deep generative model-based defenses that it may not realize the on-manifold conjecture expectedly.
 Stochastic Subspace Cubic Newton Method 
 In this paper, we propose a new randomized second-order optimization algorithm---Stochastic Subspace Cubic Newton (SSCN)---for minimizing a high dimensional convex function $f$. Our method can be seen both as a {\em stochastic} extension of the cubically-regularized Newton method of Nesterov and Polyak (2006), and a {\em second-order} enhancement of stochastic subspace descent of Kozak et al. (2019). We prove that as we vary the minibatch size, the global convergence rate of SSCN interpolates between the rate of stochastic coordinate descent (CD) and the rate of cubic regularized Newton, thus giving new insights into the connection between first and second-order methods. Remarkably, the local convergence rate of SSCN matches the rate of stochastic subspace descent applied to the problem of minimizing the quadratic function $\frac12 (x-x^*)^\top \nabla^2f(x^*)(x-x^*)$, where $x^*$ is the minimizer of $f$, and hence depends on the properties of $f$ at the optimum only. Our numerical experiments show that SSCN outperforms non-accelerated first-order CD algorithms while being competitive to their accelerated variants.
 On the Unreasonable Effectiveness of the Greedy Algorithm: Greedy Adapts to Sharpness 
 Submodular maximization has been widely studied over the past decades, mostly because of its numerous applications in real-world problems. It is well known that the standard greedy algorithm guarantees a worst-case approximation factor of 1 − 1/e when maximizing a monotone submodular function under a cardinality constraint. However, empirical studies show that its performance is substantially better in practice. This raises a natural question of explaining this improved performance of the greedy algorithm.
In this work, we define sharpness for submodular functions as a candidate explanation for this phenomenon. The sharpness criterion is inspired by the concept of strong convexity in convex optimization. We show that the greedy algorithm provably performs better as the sharpness of the submodular function increases. This improvement ties closely to the faster convergence rates of the first order methods for strongly convex functions. Finally, we perform a computational study to empirically support our theoretical results and show that sharpness explains the greedy performance better than other justifications in the literature.
 Haar Graph Pooling 
 Deep Graph Neural Networks (GNNs) are useful models for graph classification and graph-based regression tasks. In these tasks, graph pooling is a critical ingredient by which GNNs adapt to input graphs of varying size and structure. We propose a new graph pooling operation based on compressive Haar transforms --- \emph{HaarPooling}. HaarPooling implements a cascade of pooling operations; it is computed by following a sequence of clusterings of the input graph. A HaarPooling layer transforms a given input graph to an output graph with a smaller node number and the same feature dimension; the compressive Haar transform filters out fine detail information in the Haar wavelet domain.
In this way, all the HaarPooling layers together synthesize the features of any given input graph into a feature vector of uniform size. Such transforms provide a sparse characterization of the data and preserve the structure information of the input graph. GNNs implemented with standard graph convolution layers and HaarPooling layers achieve state of the art performance on diverse graph classification and regression problems.
 Safe screening rules for L0-regression 
 We give safe screening rules to eliminate variables from regression with L0 regularization or cardinality constraint. These rules are based on guarantees that a feature may or may not be selected in an optimal solution. 
The screening rules can be computed from a convex relaxation solution in linear time, without solving the L0 optimization problem. Thus, they can be used in a preprocessing step to safely remove variables from consideration apriori. 
Numerical experiments on real and synthetic data indicate that, 
on average, 76\% of the variables can be fixed to their optimal values, hence, reducing the computational burden for optimization substantially. Therefore, the proposed fast and effective screening rules extend the scope of
algorithms for L0 regression to larger data sets.
 Decentralised Learning with Random Features and Distributed Gradient Descent 
 We investigate the generalisation performance of Distributed Gradient Descent with implicit regularisation and random features in the homogenous setting where a network of agents are given data sampled independently from the same unknown distribution. Along with reducing the memory footprint, random features are particularly convenient in this setting as they provide a common parameterisation across agents that allows to overcome previous difficulties in implementing decentralised kernel regression. Under standard source and capacity assumptions, we establish high probability bounds on the predictive performance for each agent as a function of the step size, number of iterations, inverse spectral gap of the communication matrix and number of random features. By tuning these parameters, we obtain statistical rates that are minimax optimal with respect to the total number of samples in the network. The algorithm provides a linear improvement over single-machine gradient descent in memory cost and, when agents hold enough data with respect to the network size and inverse spectral gap, a linear speed up in computational run-time for any network topology. We present simulations that show how the number of random features, iterations and samples impact predictive performance.
 Two Routes to Scalable Credit Assignment without Weight Symmetry 
 The neural plausibility of backpropagation has long been disputed, primarily for its use of non-local weight transport --- the biologically dubious requirement that one neuron instantaneously measure the synaptic weights of another. Until recently, attempts to create local learning rules that avoid weight transport have typically failed in the large-scale learning scenarios where backpropagation shines, e.g. ImageNet categorization with deep convolutional networks. Here, we investigate a recently proposed local learning rule that yields competitive performance with backpropagation and find that it is highly sensitive to metaparameter choices, requiring laborious tuning that does not transfer across network architecture. Our analysis indicates the underlying mathematical reason for this instability, allowing us to identify a more robust local learning rule that better transfers without metaparameter tuning. Nonetheless, we find a performance and stability gap between this local rule and backpropagation that widens with increasing model depth. We then investigate several non-local learning rules that relax the need for instantaneous weight transport into a more biologically-plausible "weight estimation" process, showing that these rules match state-of-the-art performance on deep networks and operate effectively in the presence of noisy updates. Taken together, our results suggest two routes towards the discovery of neural implementations for credit assignment without weight symmetry: further improvement of local rules so that they perform consistently across architectures and the identification of biological implementations for non-local learning mechanisms.
 Healing Gaussian Process Experts 
 Gaussian processes (GPs) are nonparametric Bayesian models that have been applied to regression and classification problems. One of the approaches to alleviate their cubic training cost is the use of local GP experts trained on subsets of the data. In particular, product-of-expert models combine the predictive distributions of local experts through a tractable product operation.
While these expert models allow for massively distributed computation, their predictions typically suffer from erratic behaviour of the mean or uncalibrated uncertainty quantification. By calibrating predictions via a tempered softmax weighting, we provide a solution to these problems for multiple product-of-expert models, including the generalised product of experts and the robust Bayesian committee machine. Furthermore, we leverage the optimal transport literature and propose a new product-of-expert model that combines predictions of local experts by computing their Wasserstein barycenter, which can be applied to both regression and classification.
 Equivariant Flows: exact likelihood generative learning for symmetric densities. 
 Normalizing flows  are  exact-likelihood  generative  neural  networks  which  approximately transform  samples from a simple prior distribution to samples of the probability distribution of interest.  
Recent work showed that such generative models can be utilized in statistical mechanics to sample equilibrium states of many-body systems in physics and chemistry. 
To scale and generalize these results, it is essential that the natural symmetries in the probability density – in physics defined by the invariances of the target potential – are built into the flow.  
We provide a theoretical sufficient criterium showing that the distribution generated by \textit{equivariant} normalizing flows is invariant with respect to these symmetries by design. 
Furthermore, we propose building blocks for flows preserving symmetries which are usually found in physical/chemical many-body particle systems.
Using benchmark systems motivated from molecular physics, we demonstrate that those symmetry preserving flows can provide better generalization capabilities and sampling efficiency.
 Customizing ML Predictions for Online Algorithms 
 Traditionally, online algorithms optimize the worst-case competitive ratio between the algorithm and the optimal solution. To overcome the inherent pessimism of worst-case analysis, a popular line of recent research incorporates ML advice in the design of online algorithms to improve their performance in typical instances. These papers treat the ML algorithm as a black-box, and redesign online algorithms to take advantage of ML predictions. In this paper, we ask the complementary question: can we redesign ML algorithms to provide better predictions for online algorithms? We explore this question in the context of the classic rent-or-buy problem, and show that incorporating optimization benchmarks directly in ML loss functions leads to significantly better performance. We support this finding both through theoretical bounds and numerical simulations, and posit that ``learning for optimization'' is a fertile area for future research.
 Goal-Aware Prediction: Learning to Model What Matters 
 Learned dynamics models combined with both planning and policy learning algorithms have shown promise in enabling artificial agents to learn to perform many diverse tasks with limited supervision. However, one of the fundamental challenges in using a learned forward dynamics model is that there exists a mismatch between the objective of the learned model (future state reconstruction), and the downstream planner or policy (completing a specified task). This issue is exacerbated by vision-based control tasks in diverse real-world environments, where the complexity of the real world dwarfs model capacity. In this paper, we propose to direct prediction towards task relevant information, enabling the model to be aware of the current task and encouraging it to only model relevant quantities of the state space, resulting in a learning objective that more closely matches the downstream task. Further, we do so in an entirely self-supervised manner, without the need for a reward function or image labels. We find that our method more effectively models the relevant parts of a scene conditioned on the goal, and as a result significantly outperforms standard task-agnostic dynamics models
and model-free reinforcement learning. 
 The Non-IID Data Quagmire of Decentralized Machine Learning 
 Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across locations/devices. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization layers; and (iii) the degree of skewness is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the skew-induced accuracy loss of batch normalization.

 Learning Efficient Multi-agent Communication: An Information Bottleneck Approach 
 We consider the problem of the limited-bandwidth communication for multi-agent reinforcement learning, where agents cooperate with the assistance of a communication protocol and a scheduler. The protocol and scheduler jointly determine which agent is communicating what message and to whom. Under the limited bandwidth constraint, a communication protocol is required to generate informative messages. Meanwhile, an unnecessary communication connection should not be established because it occupies limited resources in vain. In this paper, we develop an Informative Multi-Agent Communication (IMAC) method to learn efficient communication protocols as well as scheduling. First, from the perspective of communication theory, we prove that the limited bandwidth constraint requires low-entropy messages throughout the transmission. Then inspired by the information bottleneck principle, we learn a valuable and compact communication protocol and a weight-based scheduler. To demonstrate the efficiency of our method, we conduct extensive experiments in various cooperative and competitive multi-agent tasks with different numbers of agents and different bandwidths. We show that IMAC converges faster and leads to efficient communication among agents under the limited bandwidth as compared to many baseline methods.
 Learning Near Optimal Policies with Low Inherent Bellman Error 
 We study the exploration problem with approximate linear action-value functions in episodic reinforcement learning under the notion of low inherent Bellman error, a condition normally employed to show convergence of approximate value iteration. We relate this condition to other common frameworks and show that it is strictly more general than the low rank (or linear) MDP assumption of prior work. 
We provide an algorithm with a rate optimal regret bound for this setting. While computational tractability questions remain open, this enriches the class of MDPs with a linear representation for the action-value function where statistically efficient reinforcement learning is possible. 
 Continuous-time Lower Bounds for Gradient-based Algorithms 
 This article derives lower bounds on the convergence rate of continuous-time gradient-based optimization algorithms. The algorithms are subjected to a time-normalization constraint that avoids a reparametrization of time in order to make the discussion of continuous-time convergence rates meaningful. We reduce the multi-dimensional problem to a single dimension, recover well-known lower bounds from the discrete-time setting, and provide insights into why these lower bounds occur. We further explicitly provide algorithms that achieve the proposed lower bounds, even when the function class under consideration includes certain non-convex functions.
 Data Valuation using Reinforcement Learning 
 Quantifying the value of data is a fundamental problem in machine learning and has multiple important use cases: (1) building insights about the dataset and task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. We propose Data Valuation using Reinforcement Learning (DVRL), to adaptively learn data values jointly with the predictor model. DVRL uses a data value estimator (DVE) to learn how likely each datum is used in training of the predictor model. DVE is trained using a reinforcement signal that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across numerous datasets and application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6% and 10.8%, respectively. 
 Small-GAN: Speeding up GAN Training using Core-Sets  
 Recent work suggests that Generative Adversarial Networks (GANs) benefit disproportionately from large mini-batch sizes. This finding is interesting but also discouraging -- large batch sizes are slow and expensive to emulate on conventional hardware. Thus, it would be nice if there were some trick by which we could generate batches that were effectively big though small in practice. In this work, we propose such a trick, inspired by the use of Coreset-selection in active learning. When training a GAN, we draw a large batch of samples from the prior and then compress that batch using Coreset-selection. To create effectively large batches of real images, we create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset-selection on those projected embeddings at training time. We conduct experiments showing that this technique substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it helps us use GANs to reach a new state of the art in anomaly detection.
 PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization 
 Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary.
We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous  state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve  human performance on multiple datasets.
 Scalable Exact Inference in Multi-Output Gaussian Processes 
 Multi-output Gaussian processes (MOGPs) leverage the flexibility and interpretability of GPs while capturing structure across outputs, which is desirable, for example, in spatio-temporal modelling. The key problem with MOGPs is their computational scaling O(n^3 p^3), which is cubic in the number of both inputs n (e.g., time points or locations) and outputs p. For this reason, a popular class of MOGPs assumes that the data live around a low-dimensional linear subspace, reducing the complexity to O(n^3 m^3). However, this cost is still cubic in the dimensionality of the subspace m, which is still prohibitively expensive for many applications. We propose the use of a sufficient statistic of the data to accelerate inference and learning in MOGPs with orthogonal bases. The method achieves linear scaling in m, hence allowing these models to scale to virtually any m, without sacrificing significant expressivity or requiring approximation. This advance opens up a wide range of real-world tasks and can be combined with existing GP approximations in a plug-and-play way. We demonstrate the efficacy of the method on various synthetic and real-world data sets.
 Optimization and Analysis of the pAp@k Metric for Recommender Systems 
 Modern recommendation and notification systems must be robust to data imbalance, limitations on the number of recommendations/notifications, and heterogeneous engagement profiles across users. The pAp@k metric, which combines the partial-AUC and the precision@k metrics, was recently proposed to evaluate such recommendation systems and has been used in real-world deployments. Conceptually, pAp@k measures the probability of correctly ranking a top-ranked positive instance over top-ranked negative instances. Due to the combinatorial aspect surfaced by top-ranked points, little is known about the characteristics and optimization methods of pAp@k. In this paper, we analyze the learning-theoretic properties of pAp@k and propose novel surrogates that are consistent under certain data regularity conditions. We then provide gradient descent based algorithms to optimize the surrogates directly. Our analysis and experimental evaluation suggest that pAp@k indeed exhibits a certain dual behavior with respect to partial-AUC and precision@k. Moreover, the proposed methods outperform all the baselines in various applications. Taken together, our results motivate the use of pAp@k for large-scale recommender systems.
 Choice Set Optimization Under Discrete Choice Models of Group Decisions 
 The way that people make choices or exhibit preferences can be strongly affected by the set of available alternatives, often called the choice set. Furthermore, there are usually heterogeneous preferences, either at an individual level within small groups or within sub-populations of large groups. Given the availability of choice data, there are now many models that capture this behavior in order to make effective predictions. However, there is little work in understanding how directly changing the choice set can be used to influence a group's preferences or decisions. Here, we use discrete choice modeling to develop an optimization framework of such interventions for several problems of group influence, including maximizing agreement or disagreement and promoting a particular choice. We show that these problems are NP-hard in general but imposing restrictions reveals a fundamental boundary: promoting an item is easier than maximizing agreement or disagreement. After, we design approximation algorithms for the hard problems and show that they work extremely well for real-world choice data.
 Stochastic Latent Residual Video Prediction 
 Designing video prediction models that account for the inherent uncertainty of the future is challenging. Most works in the literature are based on stochastic image-autoregressive recurrent networks, which raises several performance and applicability issues. An alternative is to use fully latent temporal models which untie frame synthesis and temporal dynamics. However, no such model for stochastic video prediction has been proposed in the literature yet, due to design and training difficulties. In this paper, we overcome these difficulties by introducing a novel stochastic temporal model whose dynamics are governed in a latent space by a residual update rule. This first-order scheme is motivated by discretization schemes of differential equations. It naturally models video dynamics as it allows our simpler, more interpretable, latent model to outperform prior state-of-the-art methods on challenging datasets.
 OPtions as REsponses: Grounding behavioural hierarchies in multi-agent reinforcement learning 
 This paper investigates generalisation in multi-agent games, where the generality of the agent can be evaluated by playing against opponents it hasn't seen during training. We propose two new games with concealed information and complex, non-transitive reward structure (think rock-paper-scissors). It turns out that most current deep reinforcement learning methods fail to efficiently explore the strategy space, thus learning policies that generalise poorly to unseen opponents. We then propose a novel hierarchical agent architecture, where the hierarchy is grounded in the game-theoretic structure of the game -- the top level chooses strategic responses to opponents, while the low level implements them into policy over primitive actions. This grounding facilitates credit assignment across the levels of hierarchy. Our experiments show that the proposed hierarchical agent is capable of generalisation to unseen opponents, while conventional baselines fail to generalise whatsoever.
 Parallel Algorithm for Non-Monotone DR-Submodular Maximization 
 In this work, we give a new parallel algorithm for the problem of maximizing a non-monotone diminishing returns submodular function subject to a cardinality constraint. For any desired accuracy $\epsilon$, our algorithm achieves a $1/e - \epsilon$ approximation using $O(\log{n} \log(1/\epsilon) / \epsilon^3)$ parallel rounds of function evaluations. The approximation guarantee nearly matches the best approximation guarantee known for the problem in the sequential setting and the number of parallel rounds is nearly-optimal for any constant $\epsilon$. Previous algorithms achieve worse approximation guarantees using $\Omega(\log^2{n})$ parallel rounds. Our experimental evaluation suggests that our algorithm obtains solutions whose objective value nearly matches the value obtained by the state of the art sequential algorithms, and it outperforms previous parallel algorithms in number of parallel rounds, iterations, and solution quality.
 Associative Memory in Iterated Overparameterized Sigmoid Autoencoders 
 Recent work suggests that overparameterized autoencoders can be trained to implement associative memory via iterative maps. This phenomenon happens when converged input-output Jacobian of the network has all eigenvalue norms strictly below one. In this work, we theoretically analyze this behavior for sigmoid networks by leveraging recent developments in deep learning theories, especially the Neural Tangent Kernel (NTK) theory. We find that overparameterized sigmoid autoencoders can have attractors in the NTK limit for both training with a single example and multiple examples under certain conditions. In particular, for multiple training examples, we find that the norm of the largest Jacobian eigenvalue drops below one with increasing input norm, leading to associative memory. 
 Low-loss connection of weight vectors: distribution-based approaches 
 Recent research shows that sublevel sets of the loss surfaces of overparameterized networks are connected, exactly or approximately. We describe and compare experimentally a panel of methods used to connect two low-loss points by a low-loss curve on this surface. Our methods vary in accuracy and complexity. Most of our methods are based on ''macroscopic'' distributional assumptions and are insensitive to the detailed properties of the points to be connected. Some methods require a prior training of a ''global connection model'' which can then be applied to any pair of points. The accuracy of the method generally correlates with its complexity and sensitivity to the endpoint detail.
 Transformation of ReLU-based recurrent neural networks from discrete-time to continuous-time 
 Recurrent neural networks (RNN) as used in machine learning are commonly formulated in discrete time, i.e. as recursive maps. This brings a lot of advantages for training models on data, e.g. for the purpose of time series prediction or dynamical systems identification, as powerful and efficient inference algorithms exist for discrete time systems and numerical integration of differential equations is not necessary. On the other hand, mathematical analysis of dynamical systems inferred from data is often more convenient and enables additional insights if these are formulated in continuous time, i.e. as systems of ordinary (or partial) differential equations (ODE). Here we show how to perform such a translation from discrete to continuous time for a particular class of ReLU-based RNN. We prove three theorems on the mathematical equivalence between the discrete and continuous time formulations under a variety of conditions, and illustrate how to use our mathematical results on different machine learning and nonlinear dynamical systems examples.
 Understanding and Stabilizing GANs' Training Dynamics Using Control Theory 
 Generative adversarial networks (GANs) are effective in generating realistic images but the training is often unstable.
There are existing efforts that model the training dynamics of GANs in the parameter space but the analysis cannot directly motivate practically effective stabilizing methods.
To this end, we present a conceptually novel perspective from control theory to directly model the dynamics of GANs in the frequency domain and provide simple yet effective methods to stabilize GAN's training.
We first analyze the training dynamic of a prototypical Dirac GAN and adopt the widely-used closed-loop control (CLC) to improve its stability. We then extend CLC to stabilize the training dynamic of normal GANs, which can be implemented as an L2 regularizer on the output of the discriminator. Empirical results show that our method can effectively stabilize the training and obtain state-of-the-art performance on data generation tasks.
 On hyperparameter tuning in general clustering problemsm 
 Tuning hyperparameters for unsupervised learning problems is difficult in general due to the lack of ground truth for validation. However, the success of most clustering methods depends heavily on the correct choice of the involved hyperparameters. Take for example the Lagrange multipliers of penalty terms in semidefinite programming (SDP) relaxations of community detection in networks, or the bandwidth parameter needed in the Gaussian kernel used to construct similarity matrices for spectral clustering. Despite the popularity of these clustering algorithms, there are not many provable methods for tuning these hyperparameters. In this paper, we provide a overarching framework with provable guarantees for tuning hyperparameters in the above class of problems under two different models. Our framework can be augmented with a cross validation procedure  to do model selection  as well. In a variety of simulation and real data experiments, we show that our framework outperforms other widely used tuning procedures in a broad range of parameter settings.
 Learning and sampling of atomic interventions from observations 
 We study the problem of efficiently estimating the effect of an intervention on a single variable using observational samples. Our goal is to give algorithms with polynomial time and sample complexity in a non-parametric setting.

Tian and Pearl (AAAI '02) have exactly characterized the class of causal graphs for which causal effects of atomic interventions can be identified from observational data. We make their result quantitative.  Suppose 𝒫 is a causal model on a set V of n observable variables with respect to a given causal graph G,  and let do(x) be an identifiable intervention on a variable X.  We show that assuming that G has bounded in-degree and bounded c-components and that the observational distribution satisfies a strong positivity condition:

(i) [Evaluation] There is an algorithm that outputs with probability 2/3 an evaluator for a distribution P^ that satisfies TV(P(V | do(x)), P^(V)) < eps using m=O~(n/eps^2) samples from P and O(mn) time. The evaluator can return in O(n) time the probability P^(v) for any assignment v to V.
 
(ii) [Sampling] There is an algorithm that outputs with probability 2/3 a sampler for a distribution P^ that satisfies TV(P(V | do(x)), P^(V)) < eps using m=O~(n/eps^2) samples from P and O(mn) time. The sampler returns an iid sample from P^ with probability 1-delta in O(n log(1/delta)/eps) time. 

We extend our techniques to estimate P(Y | do(x)) for a subset Y of variables of interest. We also show lower bounds for the sample complexity, demonstrating that our sample complexity has optimal dependence on the parameters n and eps as well as the strong positivity parameter.
 DINO: Distributed Newton-Type Optimization Method 
 We present a novel communication-efficient Newton-type algorithm for finite-sum optimization over a distributed computing environment. Our method, named DINO, overcomes both theoretical and practical shortcomings of similar existing methods. Under minimal assumptions, we guarantee global sub-linear convergence of DINO to a first-order stationary point for general non-convex functions and arbitrary data distribution over the network. Furthermore, for functions satisfying Polyak-Lojasiewicz (PL) inequality, we show that DINO enjoys a linear convergence rate. Our proposed algorithm is practically parameter free, in that it will converge regardless of the selected hyper-parameters, which are easy to tune. Additionally, its sub-problems are simple linear least-squares, for which efficient solvers exist, and numerical simulations demonstrate the efficiency of DINO as compared with similar alternatives.
 Faster Graph Embeddings via Coarsening 
 Graph embeddings are a ubiquitous tool for machine learning tasks, such as node classification and link prediction, on graph-structured data. However, computing the embeddings for large-scale graphs is prohibitively inefficient even if we are interested only in a small subset of relevant vertices. To address this, we present an efficient graph coarsening approach, based on Schur complements, for computing the embedding of the relevant vertices. We prove that these embeddings are preserved exactly by the Schur complement graph that is obtained via Gaussian elimination on the non-relevant vertices. As computing Schur complements is expensive, we give a nearly-linear time algorithm that generates a coarsened graph on the relevant vertices that provably matches the Schur complement in expectation. Our experiments involving prediction tasks on graphs demonstrate that computing embeddings on the coarsened graph, rather than the entire graph, leads to significant time savings without sacrificing accuracy.

 A Flexible Latent Space Model for Multilayer Networks 
 Entities often interact with each other through multiple types of relations, which are often represented as multilayer networks.  Multilayer networks among the same set of nodes usually share common structures, while each layer can possess its distinct node connecting behaviors. This paper proposes a flexible latent space model for multilayer networks for the purpose of capturing such characteristics. Specifically, the proposed model embeds each node with a latent vector shared among layers and a layer-specific effect for each layer; both elements together with a layer-specific connectivity matrix determine edge formations. To fit the model, we develop a projected gradient descent algorithm for efficient parameter estimation.  We also establish theoretical properties of the maximum likelihood estimators and show that the upper bound of the common latent structure's estimation error is inversely proportional to the number of layers under mild conditions. The superior performance of the proposed model is demonstrated through simulation studies and applications to two real-world data examples.  
 Learning Similarity Metrics for Numerical Simulations 
 We propose a neural network-based approach that computes a stable and generalizing metric (LSiM) to compare data from a variety of numerical simulation sources. We focus on scalar time-dependent 2D data that commonly arises from motion and transport-based partial differential equations (PDEs). Our method employs a Siamese network architecture that is motivated by the mathematical properties of a metric. We leverage a controllable data generation setup with PDE solvers to create increasingly different outputs from a reference simulation in a controlled environment. A central component of our learned metric is a specialized loss function that introduces knowledge about the correlation between single data samples into the training process. To demonstrate that the proposed approach outperforms existing metrics for vector spaces and other learned, image-based metrics, we evaluate the different methods on a large range of test data. Additionally, we analyze generalization benefits of an adjustable training data difficulty and demonstrate the robustness of LSiM via an evaluation on three real-world data sets.
 Neural Topic Modeling with Continual Lifelong Learning 
 Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.
 Tightening Exploration in Upper Confidence Reinforcement Learning 
 The upper confidence reinforcement learning (UCRL2) strategy introduced in \citep{jaksch2010near} is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this strategy and its variants have remained until now mostly theoretical as numerical experiments on simple environments exhibit long burn-in phases before the learning takes place. Motivated by practical efficiency, we present UCRL3, following  the lines of UCRL2, but with two key modifications:
	First, it uses state-of-the-art time-uniform concentration inequalities to compute confidence sets on the reward and transition distributions for each state-action pair. To further tighten exploration, we introduce an adaptive computation of the support of each transition distributions. This enables to revisit the extended value iteration procedure to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism.
	We demonstrate, through numerical experiments on standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable to derive a regret bound for UCRL3 improving on UCRL2,
 that for the first time makes appear a notion of local diameter and effective support, thanks to variance-aware concentration bounds.
 CoMic: Co-Training and Mimicry for Reusable Skills 
 Learning to control complex bodies and reuse learned behaviors is a longstanding challenge in continuous control. We study the problem of learning reusable humanoid skills by imitating motion capture data and co-training with complementary tasks. We show that it is possible to learn reusable skills through reinforcement learning on 50 times more motion capture data than prior work. We systematically compare a variety of different network architectures across different data regimes both in terms of imitation performance as well as transfer to challenging locomotion tasks. Finally we show that it is possible to interleave the motion capture tracking with training on complementary tasks, enriching the resulting skill space, and enabling the reuse of skills not well covered by the motion capture data such as getting up from the ground or catching a ball.
 Self-supervised Label Augmentation via Input Transformations 
 Self-supervised learning, which learns by constructing artificial labels given only the input signals, has recently gained considerable attention for learning representations with unlabeled datasets, i.e., learning without any human-annotated supervision. In this paper, we show that such a technique can be used to significantly improve the model accuracy even under fully-labeled datasets. Our scheme trains the model to learn both original and self-supervised tasks, but is different from conventional multi-task learning frameworks that optimize the summation of their corresponding losses. Our main idea is to learn a single unified task with respect to the joint distribution of the original and self-supervised labels, i.e., we augment original labels via self-supervision. This simple, yet effective approach allows to train models easier by relaxing a certain invariant constraint during learning the original and self-supervised tasks simultaneously. It also enables an aggregated inference which combines the predictions from different augmentations to improve the prediction accuracy. Furthermore, we propose a novel knowledge transfer technique, which we refer to as self-distillation, that has the effect of the aggregated inference in a single (faster) inference. We demonstrate the large accuracy improvement and wide applicability of our framework on various fully-supervised settings, e.g., the few-shot and imbalanced classification scenarios.
 Improving Robustness of Deep-Learning-Based Image Reconstruction 
 Deep-learning-based methods for different applications have been shown vulnerable to adversarial examples. These examples make deployment of such models in safety-critical tasks questionable. Use of deep neural networks as inverse problem solvers has generated much excitement for medical imaging including CT and MRI, but recently a similar vulnerability has also been demonstrated for these tasks. We show that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of the signal-space as in previous work. In this paper, we propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. We introduce an auxiliary network to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we show for a linear reconstruction scheme the min-max formulation results in a singular-value(s) filter regularized solution, which suppresses the effect of adversarial examples occurring because of ill-conditioning in the measurement matrix. We find that a linear network using the proposed min-max learning scheme indeed converges to the same solution. In addition, for non-linear Compressed Sensing (CS) reconstruction using deep networks, we show significant improvement in robustness using the proposed approach over other methods. We complement the theory by experiments for CS on two different datasets and evaluate the effect of increasing perturbations on trained networks. We find the behavior for ill-conditioned and well-conditioned measurement matrices to be qualitatively different.
 Reserve Pricing in Repeated Second-Price Auctions with Strategic Bidders 
 We study revenue optimization learning algorithms for repeated second-price auctions with reserve where a seller interacts with multiple strategic bidders each of which holds a fixed private valuation for a good and seeks to maximize his expected future cumulative discounted surplus.	
We propose a novel algorithm that  has strategic regret upper bound of $O(\log\log T)$ for worst-case valuations.
This pricing is based on our novel transformation that upgrades an algorithm designed for the setup with a single buyer to the multi-buyer case. 
We provide theoretical guarantees on the ability of a transformed algorithm to  learn the valuation of a strategic buyer, which has uncertainty about 
the future due to the presence of rivals.
 FormulaZero: Distributionally Robust Online Adaptation via Offline Population Synthesis 
 Balancing performance and safety is crucial to deploying autonomous vehicles in multi-agent environments. In particular, autonomous racing is a domain that penalizes safe but conservative policies, highlighting the need for robust, adaptive strategies. Current approaches either make simplifying assumptions about other agents or lack robust mechanisms for online adaptation. This work makes algorithmic contributions to both challenges. First, to generate a realistic, diverse set of opponents, we develop a novel method for self-play based on replica-exchange Markov chain Monte Carlo. Second, we propose a distributionally robust bandit optimization procedure that adaptively adjusts risk aversion relative to uncertainty in beliefs about opponents’ behaviors. We rigorously quantify the tradeoffs in performance and robustness when approximating these computations in real-time motion-planning, and we demonstrate our methods experimentally on autonomous vehicles that achieve scaled speeds comparable to Formula One racecars.
 Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction 
 Solving large complex partial differential equations (PDEs), such as those that arise in computational fluid dynamics (CFD), is a computationally expensive process. This has motivated the use of deep learning approaches to approximate the PDE solutions, yet the simulation results predicted from these approaches typically do not generalize well to truly novel scenarios. In this work, we develop a hybrid (graph) neural network that combines a traditional graph convolutional network with an embedded differentiable fluid dynamics simulator inside the network itself. By combining an actual CFD simulator (run on a much coarser resolution representation of the problem) with the graph network, we show that we can both generalize well to new situations and benefit from the substantial speedup of neural network CFD predictions, while also substantially outperforming the coarse CFD simulation alone.
 From Local SGD to Local Fixed Point Methods for Federated Learning 
 Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one  based on a fixed number of  local steps, and the other based on randomized  computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach.
 Frequentist Uncertainty in Recurrent Neural Networks via Blockwise Influence Functions 
 Recurrent neural networks (RNNs) are instrumental in modelling sequential and time-series data. Yet, when using RNNs to inform decision-making, predictions by themselves are not sufficient — we also need estimates of predictive uncertainty. Existing approaches for uncertainty quantification in RNNs are based predominantly on Bayesian methods; these are computationally prohibitive, and require major alterations to the RNN architecture and training. Capitalizing on ideas from classical jackknife resampling, we develop a frequentist alternative that: (a) is computationally efficient, (b) does not interfere with model training or compromise its accuracy, (c) applies to any RNN architecture, and (d) provides theoretical coverage guarantees on the estimated uncertainty intervals. Our method derives predictive uncertainty from the variability of the (jackknife) sampling distribution of the RNN outputs, which is estimated by repeatedly deleting "blocks" of (temporally-correlated) training data, and collecting the predictions of the RNN re-trained on the remaining data. To avoid computationally expensive re-training, we utilize influence functions to estimate the effect of removing training data blocks on the learned RNN parameters. Using data from a critical care medical setting, we demonstrate the utility of uncertainty quantification in sequential decision-making.
 Do RNN and LSTM have Long Memory? 
 The LSTM network was proposed to overcome the difficulty in learning long-term dependence, and has made significant advancements in applications. With its success and drawbacks in mind, we raise the question - do RNN and LSTM have long memory? We answer it partially by proving that RNN and LSTM do not have long memory from a time series perspective. Since the term "long memory" is still not well-defined for a network, we propose a new definition for long memory network. To verify our theory, we make minimal modifications to RNN and LSTM and convert them to long memory networks, and illustrate their superiority in modeling long-term dependence of various datasets.
 Online metric algorithms with untrusted predictions 
 Machine-learned predictors, although achieving very good results for inputs resembling training data, cannot possibly provide perfect predictions in all situations. Still, decision-making systems that are based on such predictors need not only to benefit from good predictions but also to achieve a decent performance when the predictions are inadequate. In this paper, we propose a prediction setup for Metrical Task Systems (MTS), a broad class of online decision-making problems including, e.g., caching, k-server and convex body chasing. We utilize results from the theory of online algorithms to show how to make the setup robust. We extend our setup in two ways, (1) adapting it beyond MTS to the online matching on the line problem, and (2) specifically for caching, slightly enriching the predictor’s output to achieve an improved dependence on the prediction error. Finally, we present an empirical evaluation of our methods on real world datasets, which suggests practicality.
 Radioactive data: tracing through training 
 We want to detect whether a particular image dataset has been used to train a model. We propose a new technique, radioactive data, that makes imperceptible changes to this dataset such that any model trained on it will bear an identifiable mark. The mark is robust to strong variations such as different architectures or optimization methods. Given a trained model, our technique detects the use of radioactive data and provides a level of confidence (p-value). Our experiments on large-scale benchmarks (Imagenet), using standard architectures (Resnet-18, VGG-16, Densenet-121) and training procedures, show that we can detect usage of radioactive data with high confidence (p<0.0001) even when only 1% of the data used to trained our model is radioactive. 
Our method is robust to data augmentation and the stochasticity of deep network optimization. 
As a result, it offers a much higher signal-to-noise ratio than data poisoning and backdoor methods.
 Distributionally Robust Policy Evaluation and Learning in Offline Contextual Bandits 
 Policy learning using historical observational data is an important problem that has found widespread applications. However, existing literature rests on the crucial assumption that the future environment where the learned policy will be deployed is the same as the past environment that has generated the data–an assumption that is often false or too coarse an approximation. In this paper, we lift this assumption and aim to learn a distributionally robust policy with bandit observational data. We propose a novel learning algorithm that is able to learn a robust policy to adversarial perturbations and unknown covariate shifts. We first present a policy evaluation procedure in the ambiguous environment and also give a heuristic algorithm to solve the distributionally robust policy learning problems efficiently. Additionally, we provide extensive simulations to demonstrate the robustness of our policy.

 Data-Dependent Differentially Private Parameter Learning for Directed Graphical Models 
 Directed graphical models (DGMs) are a class of probabilistic models that are widely used for predictive analysis in sensitive domains such as medical diagnostics. In this paper, we present an algorithm for differentially-private learning of the parameters of a DGM. Our solution optimizes for the utility of inference queries over the DGM and \textit{adds noise that is customized to the properties of the private input dataset and the graph structure of the DGM}. To the best of our knowledge, this is the first explicit data-dependent privacy budget allocation algorithm in the context of DGMs. We compare our algorithm with a standard data-independent approach over a diverse suite of  benchmarks and demonstrate that our solution requires a privacy budget that is roughly $3\times$ smaller to obtain the same or higher utility.
 Learning to Branch for Multi-Task Learning 
 Training multiple tasks jointly in one deep network yields reduced latency during inference and better performance over the single-task counterpart by sharing certain layers of a network. However, over-sharing a network could erroneously enforce over-generalization, causing negative knowledge transfer across tasks. Prior works rely on human intuition or pre-computed task relatedness scores for ad hoc branching structures. They provide sub-optimal end results and often require huge efforts for the trial-and-error process.

In this work, we present an automated multi-task learning algorithm that learns where to share or branch within a network, designing an effective network topology that is directly optimized for multiple objectives across tasks. Specifically, we propose a novel tree-structured design space that casts a tree branching operation as a gumbel-softmax sampling procedure. This enables differentiable network splitting that is end-to-end trainable. We validate the proposed method on controlled synthetic data, CelebA, and Taskonomy. 
 Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization 
 Stochastic gradient methods with momentum are widely used in applications and at the core of optimization subroutines in many popular machine learning libraries. However, their sample complexities have never been obtained for problems that are non-convex and non-smooth. This paper establishes the convergence rate of a stochastic subgradient method with a momentum term of Polyak type for a broad class of non-smooth, non-convex, and constrained optimization problems. Our key innovation is the construction of a special Lyapunov function for which the proven complexity can be achieved without any tunning of the momentum parameter. For smooth problems, we extend the known complexity bound to the constrained case and demonstrate how the unconstrained case can be analyzed under weaker assumptions than the state-of-the art. Numerical results confirm our theoretical developments.
 Teaching with Limited Information on the Learner's Behaviour 
 Machine Teaching studies how efficiently a Teacher  can guide a Learner to a target hypothesis. We focus on the model of Machine Teaching with a black box learner introduced in [Dasgupta et al., ICML 2019], where the teaching is done  interactively   without having any knowledge of the Learner's   algorithm and  class of hypotheses, apart from the fact that it contains the target hypothesis $h^*$. 

We first refine some existing results for this   model and,  then, we study new variants of it. Motivated  by the realistic possibility that  $h^*$ is not available to the learner, we consider the case where the teacher can only aim at having the learner converge to a best available approximation of $h^*$. We also consider weaker black box learners, where, in each round, the choice of the consistent hypothesis returned to the Teacher is not adversarial, and in particular, we show that better provable bounds can be obtained for a type of Learner  that moves to the next  hypothesis smoothly, preferring hypotheses that are  close  to the current one; and for another type of Learner that can provide to the Teacher hypotheses chosen at random among those consistent with the examples received so far.  
Finally, we present an empirical evaluation of  our basic interactive teacher on real datasets.

 Spectral Graph Matching and Regularized Quadratic Relaxations: Algorithm and Theory 
 Graph matching, also known as network alignment, aims at recovering the latent vertex correspondence between two unlabeled, edge-correlated weighted graphs. To tackle this task, we propose a spectral method, GRAph Matching by Pairwise eigen-Alignments (GRAMPA), which first constructs a similarity matrix as a weighted sum of outer products between all pairs of eigenvectors of the two graphs, and then outputs a matching by a simple rounding procedure. For a universality class of correlated Wigner models, GRAMPA achieves exact recovery of the latent matching between two graphs with edge correlation $1 - 1/\mathrm{polylog}(n)$ and average degree at least $\mathrm{polylog}(n)$. This matches the state-of-the-art guarantees for polynomial-time algorithms established for correlated Erd\H{o}s-R\'{e}nyi graphs, and significantly improves over existing spectral methods. The superiority of GRAMPA is also demonstrated on a variety of synthetic and real datasets, in terms of both statistical accuracy and computational efficiency.
 Recurrent Hierarchical Topic-Guided RNN for Language Generation 
 To simultaneously capture syntax and global semantics from a text corpus, we propose a new larger-context recurrent neural network (RNN)-based language model, which extracts recurrent hierarchical semantic structure via a dynamic
deep topic model to guide natural language generation. Moving beyond a conventional RNN-based language model that ignores long-range word dependencies and sentence order, the proposed model captures not only intra-sentence word dependencies, but also temporal transitions between sentences and inter-sentence topic dependences. For inference, we develop a hybrid of stochastic gradient Markov chain Monte Carlo and recurrent autoencoding variational Bayes. Experimental results on a variety of real-world text corpora demonstrate that the proposed model not only outperforms larger-context RNN-based language models, but also learns interpretable recurrent multilayer topics and generates diverse sentences and paragraphs that are syntactically correct and semantically coherent.
 Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits 
 Probabilistic circuits (PCs) are a promising avenue for probabilistic modeling, as they permit a wide range of exact and efficient inference routines.
Recent ``deep-learning-style'' implementations of PCs strive for a better scalability, but are still difficult to train on real-world data, due to their sparsely connected computational graphs.
In this paper, we propose Einsum Networks (EiNets), a novel implementation design for PCs, improving prior art in several regards.
At their core, EiNets combine a large number of arithmetic operations in a single monolithic einsum-operation, leading to speedups and memory savings of up to two orders of magnitude, in comparison to previous implementations.
As an algorithmic contribution, we show that the implementation of Expectation-Maximization (EM) can be simplified for PCs, by leveraging automatic differentiation.
Furthermore, we demonstrate that EiNets scale well to datasets which were previously out of reach, such as SVHN and CelebA, and that they can be used as faithful generative image models.
 Learning Quadratic Games on Networks 
 Individuals, or organizations, cooperate with or compete against one another in a wide range of practical situations. In the economics literature, such strategic interactions are often modeled as games played on networks, where an individual's payoff depends not only on her action but also that of her neighbors. The current literature has largely focused on analyzing the characteristics of network games in the scenario where the structure of the network, which is represented by a graph, is known beforehand. It is often the case, however, that the actions of the players are readily observable while the underlying interaction network remains hidden. In this paper, we propose two novel frameworks for learning, from the observations on individual actions, network games with linear-quadratic payoffs, and in particular the structure of the interaction network. Our frameworks are based on the Nash equilibrium of such games and involve solving a joint optimization problem for the graph structure and the individual marginal benefits. We test the proposed frameworks in synthetic settings and further study several factors that affect their learning performance. Moreover, with experiments on three real world examples, we show that our methods can effectively and more accurately learn the games than the baselines. The proposed approach is among the first of its kind for {learning quadratic games, and have both theoretical and practical implications for understanding strategic interactions in a network environment.
 What is Local Optimality in Nonconvex-Nonconcave Minimax Optimization? 
 Minimax optimization has found extensive applications in modern machine learning, in settings such as generative adversarial networks (GANs), adversarial training and multi-agent reinforcement learning. As most of these applications involve continuous nonconvex-nonconcave formulations, a very basic question arises---``what is a proper definition of local optima?''

Most previous work answers this question using classical notions of equilibria from simultaneous games, where the min-player and the max-player act simultaneously. In contrast, most applications in machine learning, including GANs and adversarial training, correspond to sequential games, where the order of which player acts first is crucial (since minimax is in general not equal to maximin due to the nonconvex-nonconcave nature of the problems). The main contribution of this paper is to propose a proper mathematical definition of local optimality for this sequential setting---local minimax, as well as to present its properties and existence results. Finally, we establish a strong connection to a basic local search algorithm---gradient descent ascent (GDA): under mild conditions, all stable limit points of GDA are exactly local minimax points up to some degenerate points.
 Generalization Guarantees for Sparse Kernel Approximation with Entropic Optimal Features 
 Despite their success, kernel methods suffer from a massive computational cost in practice. In this paper, in lieu of commonly used kernel expansion with respect to $N$ inputs, we develop a novel optimal design maximizing the entropy among kernel features. This procedure results in a kernel expansion with respect to entropic optimal features (EOF), improving the data representation dramatically due to features dissimilarity. Under mild technical assumptions, our generalization bound shows that with only $O(N^{\frac{1}{4}})$ features (disregarding logarithmic factors), we can achieve the optimal statistical accuracy (i.e., $O(1/\sqrt{N})$). The salient feature of our design is its sparsity that significantly reduces the time and space cost. Our numerical experiments on benchmark datasets verify the superiority of EOF over the state-of-the-art in kernel approximation. 
 GraphOpt: Learning Optimization Models of Graph Formation 
 Formation mechanisms are fundamental to the study of complex networks, but learning them from observations is challenging. In real-world domains, one often has access only to the final constructed graph, instead of the full construction process, and observed graphs exhibit complex, non-local structural properties. In this work, we propose GraphOpt, an end-to-end framework that jointly learns an implicit model of graph structure formation and discovers an underlying optimization mechanism in the form of a latent objective function. The learned objective can serve as an explanation for the observed graph properties, thereby lending itself to transfer across different graphs within a given domain. GraphOpt poses link formation in graphs as a sequential 
decision-making process and solves it using an efficient maximum entropy based inverse reinforcement learning algorithm. Further, it employs a novel continuous latent action space induced from node representations  to promote scalability. We demonstrate empirically that GraphOpt discovers a latent objective and a robust stochastic policy that enable construction of graphs with properties similar to those in observed graph, transfer across graphs with different characteristics, and exhibit competitive performance on conventional downstream tasks such as link prediction, without being explicitly trained on these new graphs or task.
 A Chance-Constrained Generative Framework for Sequence Optimization 
 Deep generative modeling has achieved many successes for continuous data generation, such as producing realistic images and controlling their properties (e.g., styles). However, the development of generative modeling techniques for optimizing discrete data, such as sequences or strings, still lags behind largely due to the challenges in modeling complex and long-range constraints, including both syntax and semantics, in discrete structures. For example, to generate a string representing a molecule structure or a mathematical expression with a  desired quantitative property, we need to both ensure the validity of the generated string subject to a grammar and model the string representation so that it is predictive of the property.  In this paper, we formulate the sequence optimization task as a chance-constrained sampling problem. The key idea is to enforce a high probability of generating valid sequences and also optimizes the property of interest. We propose a novel minmax algorithm based a tightening of the chance constraint, by jointly tightening a bound of the valid chance and optimizing the expected property. Extensive experimental results in three domains, including arithmetic expressions, Python programs, and SMILES strings for molecules, demonstrate the superiority of our approach over the existing sequence optimization methods. In particular, it is able to achieve the state-of-the-art performance in the molecule optimization task where the current best methods are graph-based.
 Bridging the Gap Between f-GANs and Wasserstein GANs 
 Generative adversarial networks (GANs) variants approximately minimize divergences between the model and the data distribution using a discriminator. Wasserstein GANs (WGANs) enjoy superior empirical performance, however, unlike in f-GANs, the discriminator does not provide an estimate for the ratio between model and data densities, which is useful in applications such as inverse reinforcement learning. To overcome this limitation, we propose an new training objective where we additionally optimize over a set of importance weights over the generated samples. By suitably constraining the feasible set of importance weights, we obtain a family of objectives which includes and generalizes the original f-GAN and WGAN objectives. We show that a natural extension outperforms WGANs while providing density ratios as in f-GAN, and demonstrate empirical success on distribution modeling, density ratio estimation and image generation, where we achieve state-of-the-art FID scores on CIFAR10 generation.
 The Implicit Regularization of Stochastic Gradient Flow for Least Squares 
 We study the implicit regularization of mini-batch stochastic gradient descent, when applied to the fundamental problem of least squares regression.  We leverage a continuous-time stochastic differential equation having the same moments as stochastic gradient descent, which we call stochastic gradient flow.  We give a bound on the excess risk of stochastic gradient flow at time $t$, over ridge regression with tuning parameter $\lambda = 1/t$.  The bound may be computed from explicit constants (e.g., the mini-batch size, step size, number of iterations), revealing precisely how these quantities drive the excess risk.  Numerical examples show the bound can be small, indicating a tight relationship between the two estimators.  We give a similar result relating the coefficients of stochastic gradient flow and ridge.  These results hold under no conditions on the data matrix $X$, and across the entire optimization path (not just at convergence).
 Deep Streaming Label Learning 
 In multi-label learning, each instance can be associated with multiple and non-exclusive labels. Previous studies assume that all the labels in the learning process are fixed and static; however, they ignore the fact that the labels will emerge continuously in changing environments. In order to fill in these research gaps, we propose a novel deep neural network (DNN) based framework, Deep Streaming Label Learning (DSLL), to classify instances with newly emerged labels effectively. DSLL can explore and incorporate the knowledge from past labels and historical models to understand and develop emerging new labels. DSLL consists of three components: 1) a streaming label mapping to extract deep relationships between new labels and past labels with a novel label-correlation aware loss; 2) a streaming feature distillation propagating feature-level knowledge from the historical model to a new model; 3) a senior student network to model new labels with the help of knowledge learned from the past. Theoretically, we prove that DSLL admits tight generalization error bounds for new labels in the DNN framework. Experimentally, extensive empirical results show that the proposed method performs significantly better than the existing state-of-the-art multi-label learning methods to handle the continually emerging new labels.
 Learning to Encode Position for Transformer with Continuous Dynamical Model 
 We introduce a new way of learning to encode position information for non-recurrent models, such as Transformer models. Unlike RNN and LSTM, which contain inductive bias by loading the input tokens sequentially, non-recurrent models are less sensitive to position. The main reason is that position information among input units is not encoded inherently, i.e., they are permutation equivalent, this problem justifies why all of the existing models are accompanied by position encoding/embedding layer at the input. However, this solution has clear limitations: the sinusoidal position encoding is not flexible enough as it is manually designed and does not contain any learnable parameters, whereas the position embedding restricts the maximum length of input sequences. It is thus desirable to design a new position layer that contains learnable parameters to adjust to different datasets and different architectures. At the same time, we would also like it to extrapolate in accordance with the variable length of inputs. In our proposed solution, we borrow from the recent Neural ODE approach, which may be viewed as a versatile continuous version of a ResNet. This model is capable of modeling many kinds of dynamical systems. We model the evolution of encoded results along position index by such a dynamical system, thereby overcoming the above limitations of existing methods. We evaluate our new position layers on a variety of neural machine translation and language understanding tasks, the experimental results show consistent improvements over the baselines.
 Adversarial Filters of Dataset Biases 
 Large neural models have demonstrated human-level performance on language and vision benchmarks such as ImageNet and Stanford Natural Language Inference (SNLI). Yet, their performance degrades considerably when tested on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting on spurious dataset biases. 

We investigate one recently proposed approach, AFLite, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLite, by situating it in the generalized framework for optimum bias reduction. Our experiments show that as a result of the substantial reduction of these biases, models trained on the filtered datasets yield better generalization to out-of-distribution tasks, especially when the benchmarks used for training are over-populated with biased samples. We show that AFLite is broadly applicable to a variety of both real and synthetic datasets for reduction of measurable dataset biases and provide extensive supporting analyses. Finally, filtering results in a large drop in model performance (e.g., from 92% to 62% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks.
 The Usual Suspects? Reassessing Blame for VAE Posterior Collapse 
 In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.
 Reverse-engineering deep ReLU networks 
 It has been widely assumed that a neural network cannot be recovered from its outputs, as the  network depends on its parameters in a highly nonlinear way. Here, we prove that in fact it is often possible to identify the architecture, weights, and biases of an unknown deep ReLU network by observing only its output. Every ReLU network defines a piecewise linear function, where the boundaries between linear regions correspond to inputs for which some neuron in the network switches between inactive and active ReLU states. By dissecting the set of region boundaries into components associated with particular neurons, we show both theoretically and empirically that it is possible to recover the weights of neurons and their arrangement within the network, up to isomorphism.
 Learning to Simulate and Design for Structural Engineering 
 In the architecture and construction industries, structural design for large buildings has always been laborious, time-consuming, and difficult to optimize. It is an iterative process that involves two steps: analyzing the current structural design by a slow and computationally expensive simulation, and then manually revising the design based on professional experience and rules. In this work, we propose an end-to-end learning pipeline to solve the size design optimization problem, which is to design the optimal cross-sections for columns and beams, given the design objectives and building code as constraints. We pre-train a graph neural network as a surrogate model to not only replace the structural simulation for speed but also use its differentiable nature to provide gradient signals to the other graph neural network for size optimization. Our results show that the pre-trained surrogate model can predict simulation results accurately, and the trained optimization model demonstrates the capability of designing convincing cross-section designs for buildings under various scenarios.
 On Lp-norm Robustness of Ensemble Decision Stumps and Trees 
 Recent papers have demonstrated that ensemble stumps and trees could be vulnerable to small input perturbations, so robustness verification and defense for those models have become an important research problem. However, due to the structure of decision trees, where each node makes decision purely based on one feature value, all the previous works only consider the $\ell_\infty$ norm perturbation. To study robustness with respect to a general $\ell_p$ norm perturbation, one has to consider correlation between perturbations on different features, which has not been handled by previous algorithms. In this paper, we study the robustness verification and defense with respect to general $\ell_p$ norm perturbation for ensemble trees and stumps. For robustness verification, we prove that exact verification is NP-complete for $p\in(0, \infty)$ while polynomial time algorithms exist for $p=0$ or $\infty$. Approximation algorithms based on dynamic programming is then developed for verifying ensemble trees and stumps. For robustness training, we propose the first certified defense method for training ensemble stumps and trees with respect to $\ell_p$ norm perturbations. The effectiveness of proposed algorithms is verified empirically on real datasets. 
 Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent 
 Recently there are a considerable amount of work devoted to the study of the algorithm stability and  generalization for stochastic gradient descent (SGD). However, the existing stability analysis requires to impose restrictive assumptions on the boundedness of gradients, strong smoothness and convexity of loss functions. In this paper, we provide a fine-grained analysis of stability and generalization for SGD by substantially relaxing these assumptions. Firstly, we establish stability and generalization for SGD by removing the existing bounded gradient assumptions. The key idea is the introduction of a new stability measure called  on-average model stability, for which we develop novel bounds controlled by the risks of SGD iterates. This yields  generalization bounds depending on the behavior of the best model, and leads to the first-ever-known fast bounds in the low-noise setting using stability approach. Secondly, the smoothness assumption is relaxed by considering loss functions with Holder continuous gradients for which we show that optimal bounds are still achieved by balancing computation and stability. Finally, we study learning problems with (strongly) convex objectives but non-convex loss functions, and provide applications where the existing stability bounds fail.
 Spectral Frank-Wolfe Algorithm: Strict Complementarity and Linear Convergence 
 We develop a novel variant of the classical Frank-Wolfe algorithm, which we call spectral Frank-Wolfe, for convex optimization over a spectrahedron. The spectral Frank-Wolfe algorithm has a novel ingredient: it computes a few eigenvectors of the gradient and solves a small-scale subproblem in each iteration. Such a procedure overcomes the slow convergence of the classical Frank-Wolfe algorithm due to ignoring eigenvalue coalescence. We demonstrate that strict complementarity of the optimization problem is key to proving linear convergence of various algorithms, such as the spectral Frank-Wolfe algorithm as well as the projected gradient method and its accelerated version. We showcase that the strict complementarity is equivalent to the eigengap assumption on the gradient at the optimal solution considered in the literature. As a byproduct of this observation, we also develop a generalized block Frank-Wolfe algorithm and prove its linear convergence.
 Stabilizing Transformers for Reinforcement Learning 
 Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP). Harnessing the transformer’s ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. 
 Fast OSCAR and OWL with Safe Screening Rules 
 Ordered Weight $L_{1}$-Norms (OWL) is a new family of regularizers for high-dimensional sparse regression. However, due to the non-separable penalty, existing algorithms are either invalid or inefficient when either the size of the feature or sample is large. To address this challenge, we propose the first safe screening rule for the OWL regularized regression, which effectively avoids the updates of the parameters whose coefficients must be zeros. Moreover, we prove the proposed screening rule can be safely applied to the standard proximal gradient methods. More importantly, our screening rule can also be safely applied to stochastic proximal gradient methods in large-scale learning, which is the first safe screening rule in the stochastic setting. Experimental results on a variety of datasets show that the screening rule leads to a significant computation gain without any loss of accuracy, compared to exiting competitive algorithms.  
 Calibration, Entropy Rates, and Memory in Language Models 
 Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that state-of-the-art language models, including LSTMs and Transformers, are miscalibrated: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.
 Superpolynomial Lower Bounds for Learning One-Layer Neural Networks using Gradient Descent 
 We give the first superpolynomial lower bounds for learning one-layer neural networks with respect to the Gaussian distribution for a broad class of algorithms.  In the regression setting, we prove that gradient descent run on any classifier with respect to square loss will fail to achieve small test error in polynomial time.  Prior work held only for gradient descent run with small batch sizes and sufficiently smooth classifiers. For classification, we give a stronger result, namely that any statistical query (SQ) algorithm will fail to achieve small test error in polynomial time.  Our lower bounds hold for commonly used activations such as ReLU and sigmoid.  The core of our result relies on a novel construction of a simple family of neural networks that are exactly orthogonal with respect to all spherically symmetric distributions.
 An end-to-end Differentially Private Latent Dirichlet Allocation Using a Spectral Algorithm 
 We provide an end-to-end differentially private spectral algorithm for learning LDA, based on matrix/tensor decompositions, and establish theoretical guarantees on utility/consistency of the estimated model parameters. We represent the spectral algorithm as a computational graph. Noise can be injected along the edges of this graph to obtain differential privacy. We identify subsets of edges, named ``configurations'', such that adding noise to all edges in such a subset guarantees differential privacy of the end-to-end spectral algorithm. We characterize the sensitivity of the edges with respect to the input and thus estimate the amount of noise to be added to each edge for any required privacy level. We then characterize the utility loss  for each configuration as a function of injected noise.  Overall, by combining the sensitivity and utility characterization, we obtain an end-to-end differentially private spectral algorithm for LDA and identify which configurations outperform others under specific regimes. We are the first to achieve utility guarantees under a required level of differential privacy for learning in LDA. We additionally show that our method systematically outperforms differentially private variational inference.
 Set Functions for Time Series  
 Despite the eminent successes of deep neural networks, many architectures are often hard to transfer to irregularly-sampled and asynchronous time series that commonly occur in real-world datasets, especially in healthcare applications. This paper proposes a novel approach for classifying irregularly-sampled time series with unaligned measurements, focusing on high scalability and data efficiency. Our method SeFT (Set Functions for Time Series) is based on recent advances in differentiable set function learning, extremely parallelizable with a beneficial memory footprint, thus scaling well to large datasets of long time series and online monitoring scenarios. Furthermore, our approach permits quantifying per-observation contributions to the classification outcome. We extensively compare our method with existing algorithms on multiple healthcare time series datasets and demonstrate that it performs competitively whilst significantly reducing runtime.
 Is Local SGD Better than Minibatch SGD? 
 We study local SGD (also known as parallel SGD and federated SGD), a natural and frequently used distributed optimization method. Its theoretical foundations are currently lacking and we highlight how all existing error guarantees in the convex setting are dominated by a simple baseline, minibatch SGD. (1) For quadratic objectives we prove that local SGD strictly dominates minibatch SGD and that accelerated local SGD is minmax optimal for quadratics; (2) For general convex objectives we provide the first guarantee that at least \emph{sometimes} improves over minibatch SGD, but our guarantee does not always improve over, nor even match, minibatch SGD; (3) We show that indeed local SGD does \emph{not} dominate minibatch SGD by presenting a lower bound on the performance of local SGD that is worse than the minibatch SGD guarantee.
 Handling the Positive-Definite Constraint in the Bayesian Learning Rule 
 Bayesian learning rule is a recently proposed variational inference method, which not only contains many existing learning algorithms as special cases but also enables the design of new algorithms.
Unfortunately, when posterior parameters lie in an open constraint set, the rule may not satisfy the constraints and require line-searches which could slow down the algorithm.
In this paper, we fix this issue for the positive-definite constraint by proposing an improved rule that naturally handles the constraint.
Our modification is obtained using Riemannian gradient methods, and is valid when the approximation attains a block-coordinate natural parameterization (e.g., Gaussian distributions and their mixtures).
Our method outperforms existing methods without any significant increase in computation. 
Our work makes it easier to apply the learning rule in presence of positive-definite constraints in parameter spaces.
 Information Particle Filter Tree: An Online Algorithm for POMDPs with Belief-Based Rewards on Continuous Domains 
 Partially Observable Markov Decision Processes (POMDPs) inherently gather the information necessary to act optimally under uncertainties. The framework can be extended to model pure information gathering tasks by considering belief-based rewards. This allows us to use reward shaping to guide POMDP planning to informative beliefs by using a weighted combination of the original reward and the expected information gain as the objective. In this work we propose a novel online algorithm, Information Particle Filter Tree (IPFT), to solve problems with belief-dependent rewards on continuous domains. It simulates particle-based belief trajectories in a Monte Carlo Tree Search (MCTS) approach to construct a search tree in the belief space. The evaluation shows that the consideration of information gain greatly improves the performance in problems where information gathering is an essential part of the optimal policy.
 Intrinsic Reward Driven Imitation Learning via Generative Model 
 Imitation learning in a high-dimensional environment is challenging. Most inverse reinforcement learning (IRL) methods fail to outperform the demonstrator in such a high-dimensional environment, e.g., Atari domain. To address this challenge, we propose a novel reward learning module to generate intrinsic reward signals via a generative model. Our generative method can perform better forward state transition and backward action encoding, which improves the module's dynamics modeling ability in the environment. Thus, our module provides the imitation agent both the intrinsic intention of the demonstrator and a better exploration ability, which is critical for the agent to outperform the demonstrator. Empirical results show that our method outperforms state-of-the-art IRL methods on multiple Atari games, even with one-life demonstration. Remarkably, our method achieves performance that is up to 5 times the performance of the demonstration.
 Kinematic State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning 
 We present an algorithm, HOMER, for exploration and reinforcement learning in rich observation environments that are summarizable by an unknown latent state space. The algorithm interleaves representation learning to identify a new notion of kinematic state abstraction with strategic exploration to reach new states using the learned abstraction. The algorithm provably explores the environment with sample complexity scaling polynomially in the number of latent states and the time horizon, and, crucially, with no dependence on the size of the observation space, which could be infinitely large. This exploration guarantee further enables sample-efficient global policy optimization for any reward function. On the computational side, we  show that the algorithm can be implemented efficiently whenever certain supervised learning problems are tractable.  Empirically, we evaluate HOMER on a challenging exploration problem, where we show that the algorithm is more sample efficient than standard reinforcement learning baselines.
 Batch Stationary Distribution Estimation 
 We consider the problem of approximating the stationary distribution of
an ergodic Markov chain given a set of sampled transitions. Classical simulation-based approaches assume access to the underlying process so that trajectories of sufficient length can be gathered to approximate stationary sampling. Instead, we consider an alternative setting where a \emph{fixed} set of transitions has been collected beforehand, by a separate, possibly unknown procedure. The goal is still to estimate properties of the stationary distribution, but without additional access to the underlying system. We propose a consistent estimator that is based on recovering a correction ratio function over the given data. In particular, we develop a variational power method (VPM) that provides provably consistent estimates under general conditions. In addition to unifying a number of existing approaches from different subfields, we also find that VPM yields significantly better estimates across a range of problems, including queueing, stochastic differential equations, post-processing MCMC, and off-policy evaluation.
 Let's Agree to Agree: Neural Networks Share Classification Order on Real Datasets 
 We report a series of robust empirical observations, whereby deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries - models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn and benchmark datasets. Thus, when fixing the architecture, we show synthetic datasets where this pattern ceases to exist. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results echo how neural networks discover structure in natural datasets.
 StochasticRank: Global Optimization of Scale-Free Discrete Functions 
 In this paper, we introduce a powerful and efficient framework for the direct optimization of ranking metrics. The problem is ill-posed due to the discrete structure of the loss, and to deal with that, we introduce two important techniques: a stochastic smoothing and a novel gradient estimate based on partial integration. We also address the problem of smoothing bias and present a universal solution for a proper debiasing. To guarantee the global convergence of our method, we adopt a recently proposed Stochastic Gradient Langevin Boosting algorithm. Our algorithm is implemented as a part of the CatBoost gradient boosting library and outperforms the existing approaches on several learning to rank datasets. In addition to ranking metrics, our framework applies to any scale-free discreet loss function.
 Knowing The What But Not The Where in Bayesian Optimization 
 Bayesian optimization has demonstrated impressive success in finding the optimum input x∗ and output f ∗ = f(x∗) = max f(x) of a black-box function f . In some applications, however, the optimum output is known in advance and the goal is to find the corresponding optimum input. Existing work in Bayesian optimization (BO) has not effectively exploited the knowledge of f ∗ for optimization. In this paper, we consider a new setting in BO in which the knowledge of the optimum output is available. Our goal is to exploit the knowledge about f ∗ to search for the input x∗ efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information about the optimum output. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization, which exploit the knowledge about the optimum output to identify the optimum input more efficient. We show that our approaches work intuitively and quantitatively better performance against standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.
 When Explanations Lie: Why Many Modified BP Attributions Fail 
 Attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically.
 Robust Bayesian Classification Using An Optimistic Score Ratio 
 We consider the optimistic score ratio for robust  Bayesian classification when the class-conditional distribution of the features is not perfectly known. The optimistic score searches for the distribution that is most plausible to explain the observed test sample among all distributions belonging to the class-dependent ambiguity set which is prescribed using a moment-based divergence. We show that the classification approach using optimistic score ratio is conceptually attractive, delivers rigorous statistical guarantees and is computationally tractable. We showcase the power of the proposed optimistic score ratio classifier on both synthetic and empirical data.
 Bandits with Adversarial Scaling 
 We study "adversarial scaling", a multi-armed bandit model where rewards have a stochastic and an adversarial component. Our model captures display advertising where the "click-through-rate" can be decomposed to a (fixed across time) arm-quality component and a  non-stochastic user-relevance component (fixed across arms). Despite the relative stochasticity of our model, we demonstrate two settings where most bandit algorithms suffer. On the positive side, we show that two algorithms, one from the action elimination and one from the mirror descent family are adaptive enough to be robust to adversarial scaling. Our results shed light on the robustness of adaptive parameter selection in stochastic bandits, which may be of independent interest.
 Generalized Neural Policies for Relational MDPs 
 A Relational Markov Decision Process (RMDP) is a first-order representation to express all instances of a single probabilistic planning domain with possibly unbounded number of objects. 
Early work in RMDPs outputs generalized (instance-independent) first-order policies or value functions as a means to solve all instances of a domain at once. Unfortunately, this line of work met with limited success due to inherent limitations of the representation space used in such policies or value functions. Can neural models provide the missing link by easily representing more complex generalized policies, thus making them effective on all instances of a given domain?

We present the first neural approach for solving RMDPs, expressed in the probabilistic planning language of RDDL. Our solution first converts an RDDL instance into a ground DBN. We then extract a graph structure from the DBN. We train a relational neural model that computes an embedding for each node in the graph and also scores each ground action as a function over the first-order action variable and object embeddings on which the action is applied. In essence, this represents a neural generalized policy for the whole domain. Given a new test problem of the same domain, we can compute all node embeddings using trained parameters and score each ground action to choose the best action using a single forward pass without any retraining. Our experiments on nine RDDL domains from IPPC demonstrate that neural generalized policies are significantly better than random and sometimes even more effective than training a state-of-the-art deep reactive policy from scratch.
 Fairwashing explanations with off-manifold detergent 
 Explanation methods promise to make black-box classifiers more transparent.
As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users.
In this paper, we show both theoretically and experimentally that these hopes are presently unfounded.
Specifically, we show that, for any classifier $g$, one can always construct another classifier $\tilde{g}$ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps.
We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets.
Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust.
 Correlation Clustering with Asymmetric Classification Errors 
 In the Correlation Clustering problem, we are given a weighted graph $G$ with its edges labelled as "similar" or "dissimilar" by a binary classifier. The goal is to produce a clustering that minimizes the weight of "disagreements": the sum of the weights of "similar" edges across clusters and "dissimilar" edges within clusters. We study the correlation clustering problem under the following assumption: Every "similar" edge $e$ has weight $w_e \in [ \alpha w, w ]$ and every "dissimilar" edge $e$ has weight $w_e \geq \alpha w$ (where $\alpha \leq 1$ and $w > 0$ is a scaling parameter). We give a $(3 + 2 \log_e (1/\alpha))$ approximation algorithm for this problem. This assumption captures well the scenario when classification errors are asymmetric. Additionally, we show an asymptotically matching Linear Programming integrality gap of $\Omega(\log 1/\alpha)$.
 Sample Complexity Bounds for 1-bit Compressive Sensing and Binary Stable Embeddings with Generative Priors 
 The goal of standard 1-bit compressive sensing is to accurately recover an unknown sparse vector from binary-valued measurements, each indicating the sign of a linear function of the vector.  Motivated by recent advances in compressive sensing with generative models, where a generative modeling assumption replaces the usual sparsity assumption, we study the problem of 1-bit compressive sensing with generative models. We first consider noiseless 1-bit measurements, and provide sample complexity bounds for approximate recovery under i.i.d.~Gaussian measurements and a Lipschitz continuous generative prior, as well as a near-matching algorithm-independent lower bound. Moreover, we demonstrate that the Binary $\epsilon$-Stable Embedding property, which characterizes the robustness of the reconstruction to measurement errors and noise, also holds for 1-bit compressive sensing with Lipschitz continuous generative models with sufficiently many Gaussian measurements.  In addition, we apply our results to neural network generative models, and provide a proof-of-concept numerical experiment demonstrating significant improvements over sparsity-based approaches.
 Problems with Shapley-value-based explanations as feature importance measures 
 Game-theoretic formulations of feature importance have become popular as a way to "explain" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.
 Stochastic Differential Equations with Variational Wishart Diffusions 
 We present a Bayesian non-parametric way of inferring stochastic differential equations for both regression tasks and continuous-time dynamical modelling. The work has high emphasis on the stochastic part of the differential equation, also known as the diffusion, and modelling it by means of Wishart processes. Further, we present a semiparametric approach that allows the framework to scale to high dimensions. This successfully leads us onto how to model both latent and autoregressive temporal systems with conditional heteroskedastic noise. We provide experimental evidence that modelling diffusion often improves performance and that this randomness in the differential equation can be essential to avoid overfitting.
 Convex Representation Learning for Generalized Invariance in Semi-Inner-Product Space 
 Invariance (defined in a general sense) has been one of the most effective priors for representation learning.  Direct factorization of parametric models is feasible only for a small range of invariances, while regularization approaches, despite improved generality, lead to nonconvex optimization.  In this work, we develop a \emph{convex} representation learning algorithm for a variety of generalized invariances that can be modeled as semi-norms.  Novel Euclidean embeddings are introduced for kernel representers in a semi-inner-product space, and approximation bounds are established.  This allows invariant representations to be learned efficiently and effectively as confirmed in our experiments, along with accurate predictions.
 Adversarial Neural Pruning with Latent Vulnerability Suppression 
 Despite the remarkable performance of deep neural networks on various computer vision tasks, they are known to be highly susceptible to adversarial perturbations, which makes it challenging to deploy them in real-world safety-critical applications. In this paper, we conjecture that the leading cause of this adversarial vulnerability is the distortion in the latent feature space, and provide methods to suppress them effectively. Explicitly, we define \textbf{vulnerability} for each latent feature and then propose a new loss for adversarial learning, \textbf{Vulnerability Suppression (VS)} loss, that aims to minimize the feature-level vulnerability during training. We further propose a Bayesian framework to prune features with high vulnerability to reduce both vulnerability and loss on adversarial samples. We validate our \textbf{Adversarial Neural Pruning (ANP)} method on multiple benchmark datasets, on which it not only obtains state-of-the-art adversarial robustness but also improves the performance on clean examples, using only a fraction of the parameters used by the full network. Further qualitative analysis suggests that the improvements come from the suppression of feature-level vulnerability.
 Adversarial Attacks on Copyright Detection Systems 
 It is well-known that many machine learning models are susceptible to adversarial attacks, in which an attacker evades a classifier by making small perturbations to inputs. This paper discusses how industrial copyright detection tools, which serve a central role on the web, are susceptible to adversarial attacks. We discuss a range of copyright detection systems, and why they are particularly vulnerable to attacks. These vulnerabilities are especially apparent for neural network based systems. As proof of concept, we describe a well-known music identification method and implement this system in the form of a neural net.  We then attack this system using simple gradient methods, and show that it is easily broken with white box attacks. By scaling these perturnations up, we are able to create transfer attacks on industrial systems, such as the AudioTag copyright detector and YouTube's Content ID system, using perturbations that are audible but significantly smaller than a random baseline. Our goal is to raise awareness of the threats posed by adversarial examples in this space and to highlight the importance of hardening copyright detection systems to attacks.

 Understanding the Impact of Model Incoherence on Convergence of Incremental SGD with Random Reshuffle 
 Although SGD with random reshuffle has been widely-used in machine learning applications, there is a limited understanding of how model characteristics affect the convergence of the algorithm. In this work, we introduce model incoherence to characterize the diversity of model characteristics and study its impact on convergence of SGD with random reshuffle \shaocong{under weak strong convexity}. Specifically, {\em minimizer incoherence} measures the discrepancy between the global minimizers of a sample loss and those of the total loss and affects the convergence error of SGD with random reshuffle.  In particular, we show that the variable sequence generated by SGD with random reshuffle converges to a certain global minimizer of the total loss under full minimizer coherence. The other {\em curvature incoherence} measures the quality of condition numbers of the sample losses and determines the convergence rate of SGD.  With model incoherence, our results show that SGD has a faster convergence rate and smaller convergence error under random reshuffle than those under random sampling, and hence provide justifications to the superior practical performance of SGD with random reshuffle.
 Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation 
 Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.
 A Distributional Framework For Data Valuation 
 Shapley value is a classic notion from game theory, historically used to quantify the contributions of individuals within groups, and more recently applied to assign values to data points when training machine learning models. Despite its foundational role, a key limitation of the data Shapley framework is that it only provides valuations for points within a fixed data set. It does not account for statistical aspects of the data and does not give a way to reason about points outside the data set. To address these limitations, we propose a novel framework -- distributional Shapley -- where the value of a point is defined in the context of an underlying data distribution. We prove that distributional Shapley has several desirable statistical properties; for example, the values are stable under perturbations to the data points themselves and to the underlying data distribution. We leverage these properties to develop a new algorithm for estimating values from data, which comes with formal guarantees and runs two orders of magnitude faster than state-of-the-art algorithms for computing the (non-distributional) data Shapley values. We apply distributional Shapley to diverse data sets and demonstrate its utility in a data market setting.
 Implicit Class-Conditioned Domain Alignment for Unsupervised Domain Adaptation 
 We present an approach for unsupervised domain adaptation---with a strong focus on practical considerations of within-domain class imbalance and between-domain class distribution shift---from a class-conditioned domain alignment perspective. Current methods for class-conditioned domain alignment aim to explicitly minimize a loss function based on pseudo-label estimations of the target domain. However, these methods suffer from pseudo-label bias in the form of error accumulation. We propose a method that removes the need for explicit optimization of model parameters from pseudo-labels directly. Instead, we present a sampling-based implicit alignment approach where the sample selection procedure is implicitly guided by the pseudo-labels. Theoretical analysis shows that implicit alignment facilitates adversarial domain-invariant representation learning. Empirical results and ablation studies confirm the effectiveness of the proposed approach. In particular, our method exhibits superior robustness in the presence of extreme within-domain class imbalance and between-domain class distribution shift.
 Inter-domain Deep Gaussian Processes with RKHS Fourier Features 
 Inter-domain Gaussian processes (GPs) allow for high flexibility and low computational cost when performing approximate inference in GP models. They are particularly suitable for modeling data exhibiting global function behavior but are limited to stationary covariance functions and thus fail to model non-stationary data effectively. We propose Inter-domain Deep Gaussian Processes with RKHS Fourier Features, an extension of shallow inter-domain GPs that combines the advantages of inter-domain and deep Gaussian processes (DGPs) and demonstrate how to leverage existing approximate inference approaches to perform simple and scalable approximate inference on Inter-domain Deep Gaussian Processes. We assess the performance of our method on a wide range of prediction problems and demonstrate that it outperforms inter-domain GPs and DGPs on challenging large-scale and high-dimensional real-world datasets exhibiting both global behavior as well as a high-degree of non-stationarity.
 Partial Trace Regression and Low-Rank Kraus Decomposition 
 The trace regression model, a direct extension to the well-studied linear regression model, allows one to map matrices to real-valued outputs. We here introduce a yet more general model, namely the partial trace regression model, a family of linear mappings from matrix-valued inputs to matrix-valued outputs; this model subsumes the trace regression model and thus the linear regression model. Borrowing tools from quantum information theory, where partial trace operators have been extensively studied, we propose a framework for learning partial trace regression models from data by taking advantage of the so-called low-rank Kraus representation of completely positive maps. We show the relevance of our framework with synthetic and real-world experiments conducted for both i) matrix-to-matrix regression and ii) positive semidefinite matrix completion, two tasks which can be formulated as partial trace regression problems.
 Invariant Risk Minimization Games 
 The standard risk minimization paradigm of machine learning is brittle when operating in environments whose test distributions are different from the training distribution due to spurious correlations. Training on data from many environments and finding invariant predictors reduces the effect of spurious features by concentrating models on features that have a causal relationship with the outcome. In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a simple training algorithm that uses best response dynamics and, in our experiments, yields similar or better empirical accuracy with much lower variance than the challenging bi-level optimization problem of Arjovsky et.al 2019. One key theoretical contribution is showing that the set of Nash equilibria for the proposed game are equivalent to the set of invariant predictors for any finite number of environments, even with nonlinear classifiers and transformations. As a result, our method also retains the generalization guarantees to a large set of environments shown in Arjovsky et.al 2019. The proposed algorithm adds to the collection of successful game-theoretic machine learning algorithms such as generative adversarial networks.

 Low-Rank Bottleneck in Multi-head Attention Models 
 Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation, which we further validate with our experiments. As a solution we propose to set the head size of an attention unit to input sequence length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and with better performance scaling.
 Two Simple Ways to Learn Individual Fairness Metric from Data 
 Individual fairness was proposed to address some of the shortcomings of group fairness. Despite its benefits, it requires a task specific fairness metric that encodes our intuition of what is fair and what is unfair for the ML task at hand. Ambiguity in this metric is the main barrier to wider adoption of individual fairness. In this paper, we present two simple algorithms that learn effective fair metrics from a variety of datasets. We verify empirically that fair training with these metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches. 
 Multiresolution Tensor Learning for Efficient and Interpretable Spatial Analysis 
 Efficient and interpretable spatial analysis is crucial in many fields such as geology, sports, and climate science. Large-scale spatial data often contains complex higher-order correlations across features and locations. While tensor latent factor models can describe higher-order correlations, they are inherently computationally expensive to train. Furthermore, for spatial analysis, these models should not only be predictive but also be spatially coherent. However, latent factor models are sensitive to initialization and can yield inexplicable results. We develop a novel Multiresolution Tensor Learning (MRTL) algorithm for efficiently learning interpretable spatial patterns. MRTL initializes the latent factors from an approximate full-rank tensor model for improved interpretability and progressively learns from a coarse resolution to the fine resolution for an enormous computation speedup. We also prove the theoretical convergence and computational complexity of MRTL. When applied to two real-world datasets, MRTL demonstrates 4 ~ 5 times speedup compared to a fixed resolution while yielding accurate and interpretable models.
 Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding 
 Deep Bayesian latent variable models have enabled new approaches to both model and data compression. Here, we propose a new algorithm for compressing latent representations in deep probabilistic models, such as variational autoencoders, in post-processing. The approach thus separates model design and training from the compression task. Our algorithm generalizes arithmetic coding to the continuous domain, using adaptive discretization accuracy that exploits estimates of posterior uncertainty. A consequence of the "plug and play" nature of our approach is that various rate-distortion trade-offs can be achieved with a single trained model, eliminating the need to train and store multiple models for different bit rates. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single machine learning model. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method.
 Meta-learning with Stochastic Linear Bandits 
 We investigate meta-learning procedures in the setting of stochastic linear bandits tasks. The goal is to select a learning algorithm which works well on average over a class of bandits tasks, that are sampled from a task-distribution. Inspired by recent work on learning-to-learn linear regression, we consider a class of bandit algorithms that implement a regularized version of the well-known OFUL algorithm, where the regularization is a square euclidean distance to a bias vector. We first study the benefit of the biased OFUL algorithm in terms of regret minimization. We then propose two strategies to estimate the bias within the learning-to-learn setting. We show both theoretically and experimentally, that when the number of tasks grows and the variance of the task-distribution is small, our strategies have a significant advantage over learning the tasks in isolation.
 Learning for Dose Allocation in Adaptive Clinical Trials with Safety Constraints 
 Phase I dose-finding trials are increasingly challenging as the relationship between efficacy and toxicity of new compounds (or combination of them) becomes more complex. Despite this, most commonly used methods in practice focus on identifying a Maximum Tolerated Dose (MTD) by learning only from toxicity events. We present a novel adaptive clinical trial methodology, called Safe Efficacy Exploration Dose Allocation (SEEDA), that aims at maximizing the cumulative efficacies while satisfying the toxicity safety constraint with high probability. We evaluate performance objectives that have operational meanings in practical clinical trials, including cumulative efficacy, recommendation/allocation success probabilities, toxicity violation probability, and sample efficiency. An extended SEEDA-Plateau algorithm that is tailored for the increase-then-plateau efficacy behavior of molecularly targeted agents (MTA) is also presented. Through numerical experiments using both synthetic and real-world datasets, we show that SEEDA outperforms state-of-the-art clinical trial designs by finding the optimal dose with higher success rate and fewer patients.
 Amortised Learning by Wake-Sleep 
 Models that employ latent variables to capture structure in observed data lie at the heart of many current unsupervised learning algorithms, but exact maximum-likelihood learning for powerful and flexible latent-variable models is almost always intractable. Thus, state-of-the-art approaches either abandon the maximum-likelihood framework entirely, or else rely on a variety of variational approximations to the posterior distribution over the latents. Here, we propose an alternative approach that we call amortised learning. Rather than computing an approximation to the posterior over latents, we use a wake-sleep Monte-Carlo strategy to learn a function that directly estimates the maximum-likelihood parameter updates. Amortised learning is possible whenever samples of latents and observations can be simulated from the generative model, treating the model as a ``black box''. We demonstrate its effectiveness on a wide range of complex models, including those with latents that are discrete or supported on non-Euclidean spaces. 
 From ImageNet to Image Classification: Contextualizing Progress on Benchmarks 
 Creating machine learning datasets often necessitates the use of automated data retrieval and crowdsourced annotation, giving rise to an inevitably noisy pipeline. We perform large-scale human studies to investigate the impact of such a pipeline on ImageNet---one of the key datasets driving progress in computer vision. We find that seemingly innocuous design choices (e.g., exact task setup, filtering procedure, annotators employed) can have an unexpected impact on the resulting dataset---including the introduction of spurious correlations that state-of-the-art models exploit. Overall, our results highlight a misalignment between the way we train our models and the task we actually expect them to solve, emphasizing the need for fine-grained evaluation techniques that go beyond average-case accuracy.

 Likelihood-free MCMC with Amortized Approximate Ratio Estimators 
 Posterior inference with an intractable likelihood is becoming an increasingly common task in scientific domains which rely on sophisticated computer simulations. Typically, these forward models do not admit tractable densities forcing practitioners to rely on approximations. This work introduces a novel approach to address the intractability of the likelihood and the marginal model. We achieve this by learning a flexible amortized estimator which approximates the likelihood-to-evidence ratio. We demonstrate that the learned ratio estimator can be embedded in \textsc{mcmc} samplers to approximate likelihood-ratios between consecutive states in the Markov chain, allowing us to draw samples from the intractable posterior. Techniques are presented to improve the numerical stability and to measure the quality of an approximation. The accuracy of our approach is demonstrated on a variety of benchmarks against well-established techniques. Scientific applications in physics show its applicability.
 A Sample Complexity Separation between Non-Convex and Convex Meta-Learning 
 One popular trend in meta-learning is to learn from many training tasks a common initialization for a gradient-based method that can be used to solve a new task with few samples. The theory of meta-learning is still in its early stages, with several recent learning-theoretic analyses of methods such as Reptile \cite{nichol:18} being for {\em convex models}. This work shows that convex-case analysis might be insufficient to understand the success of meta-learning, and that even for non-convex models it is important to look inside the optimization black-box, specifically at properties of the optimization trajectory. We construct a simple meta-learning instance that captures the problem of one-dimensional subspace learning. For the convex formulation of linear regression on this instance, we show that the new task sample complexity of any {\em initialization-based meta-learning} algorithm is $\Omega(d)$, where $d$ is the input dimension. In contrast, for the non-convex formulation of a two layer linear network on the same instance, we show that both Reptile and multi-task representation learning can have new task sample complexity of $\gO(1)$, demonstrating a separation from convex meta-learning. Crucially, analyses of the training dynamics of these methods reveal that they can meta-learn the correct subspace onto which the data should be projected.
 My Fair Bandit: Distributed Learning of Max-Min Fairness with Multi-player Bandits 
 Consider N cooperative but non-communicating players where each plays one out of M arms for T turns. Players have different utilities for each arm, representable as an NxM matrix. However, these utilities are unknown to the players. In each turn players receive noisy observations of their utility for their selected arm. However, if any other players selected the same arm that turn, they will all receive zero utility due to the conflict. No other communication or coordination between the players is possible. Our goal is to design a distributed algorithm that learns the matching between players and arms that achieves max-min fairness while minimizing the regret. We present an algorithm and prove that it is regret optimal up to a log(log T) factor. This is the first max-min fairness multi-player bandit algorithm with (near) order optimal regret. 
 Almost Tune-Free Variance Reduction 
 The variance reduction class of algorithms including the representative ones, SVRG and SARAH, have well documented merits for empirical risk minimization problems. However, they require grid search to tune parameters (step size and the number of iterations per inner loop) for optimal performance. This work introduces `almost tune-free' SVRG and SARAH schemes equipped with i) Barzilai-Borwein (BB) step sizes; ii) averaging; and, iii) the inner loop length adjusted to the BB step sizes. In particular, SVRG, SARAH, and their BB variants are first reexamined through an `estimate sequence' lens to enable new averaging methods that tighten their convergence rates theoretically, and improve their performance empirically when the step size or the inner loop length is chosen large. Then a simple yet effective means to adjust the number of iterations per inner loop is developed to enhance the merits of the proposed averaging schemes and BB step sizes. Numerical tests corroborate the proposed methods.
 Optimal Non-parametric Learning in Repeated Contextual Auctions with  Strategic Buyer 
 We study learning algorithms that optimize revenue in repeated contextual posted-price auctions where a seller interacts with a single strategic buyer that seeks to maximize his cumulative discounted surplus.
The buyer's valuation of a good is a fixed private function of a $d$-dimensional context (feature) vector that describes the good being sold.
In contrast to existing studies on repeated contextual auctions with strategic buyer, in our work, the seller is not assumed to know the parametric model that underlies this valuation function.
We introduce a novel non-parametric learning algorithm that is horizon-independent and has tight strategic regret upper bound of $\Theta(T^{d/(d+1)})$.
We also non-trivially generalize several value-localization techniques of non-contextual repeated auctions  to make them effective in  the considered contextual non-parametric learning of the buyer valuation function. 
 Self-Attentive Hawkes Process 
 Capturing the occurrence dynamics is crucial to predicting which type of events will happen next and when. A common method to do this is Hawkes processes. To enhance their capacity, recurrent neural networks (RNNs) have been incorporated due to RNNs’ successes in processing sequential data such as languages. Recent evidence suggests self-attention is more competent than RNNs in dealing with languages. However, we are unaware of the effectiveness of selfattention in the context of Hawkes processes. This study attempts to fill the gap by designing a self-attentive Hawkes process (SAHP). The SAHP employed self-attention to summarize influence from history events and compute the probability of the next event. One deficit of the conventional self-attention is that position embeddings only
considered order numbers in a sequence, which ignored time intervals between temporal events. To overcome this deficit, we modified the conventional method by translating time intervals into phase shifts of sinusoidal functions. Experiments
on goodness-of-fit and prediction tasks showed the improved capability of SAHP. Furthermore, the SAHP is more interpretable than RNN-based counterparts because the learnt attention weights revealed contributions of one event type to the happening of another type. To the best of our knowledge, this is the first work that studies the effectiveness of self-attention in Hawkes processes
 Efficient Policy Learning from Surrogate-Loss Classification Reductions 
 Recent work on policy learning from observational data has highlighted the importance of efficient policy evaluation and has proposed reductions to weighted (cost-sensitive) classification. But, efficient policy evaluation need not yield efficient estimation of policy parameters. We consider the estimation problem given by a weighted surrogate-loss classification with any score function, either direct, inverse-propensity-weighted, or doubly robust. We show that, under a correct specification assumption, the weighted classification formulation need not be efficient for policy parameters. We draw a contrast to actual (possibly weighted) binary classification, where correct specification implies a parametric model, while for policy learning it only implies a semi-parametric model. In light of this, we instead propose an estimation approach based on generalized method of moments, which is efficient for the policy parameters. We propose a particular method based on recent developments on solving moment problems using neural networks and demonstrate the efficiency and regret benefits of this method empirically.
 Reducing Sampling Error in Batch Temporal Difference Learning 
 Temporal difference (TD) learning is one of the main foundations of modern reinforcement learning. This paper studies the use of TD(0) to estimate the value function of a given \textit{evaluation} policy from a batch of data. In this batch setting, we show that TD(0) may converge to an inaccurate value function because the update following an action is weighted according to the number of times that action occurred in the batch -- not the true probability of the action under the evaluation policy. To address this limitation, we introduce \textit{policy sampling error corrected}-TD(0) (PSEC-TD(0)). PSEC-TD(0) first estimates the empirical distribution of actions in each state in the batch and then uses importance sampling to correct for the mismatch between the empirical weighting and the correct weighting for updates following each action. We refine the concept of a certainty-equivalence estimate and argue that PSEC-TD(0) converges to a more desirable fixed-point than TD(0) for a fixed batch of data. Finally, we conduct an empirical evaluation of PSEC-TD(0) on two batch value function learning tasks and show that PSEC-TD(0) produces value function estimates with lower mean squared error than the standard TD(0) algorithm in both discrete and continuous control tasks.
 Provably Convergent Two-Timescale Off-Policy Actor-Critic with Function Approximation 
 We present the first provably convergent two-timescale off-policy actor-critic algorithm (COF-PAC) with function approximation.
Key to COF-PAC is the introduction of a new critic, the emphasis critic, 
which is trained via Gradient Emphasis Learning (GEM), 
a novel combination of the key ideas of Gradient Temporal Difference Learning and Emphatic Temporal Difference Learning.
With the help of the emphasis critic and the canonical value function critic, 
we show convergence for COF-PAC,
where the critics are linear and the actor can be nonlinear. 
 Variational Imitation Learning with Diverse-quality Demonstrations 
 Learning from demonstrations can be challenging when the quality of demonstrations is diverse, and even more so when the quality is unknown and there is no additional information to estimate the quality. We propose a new method for imitation learning in such scenarios. We show that simple quality-estimation approaches might fail due to compounding error, and fix this issue by jointly estimating both the quality and reward using a variational approach. Our method is easy to implement within reinforcement-learning frameworks and also achieves state-of-the-art performance on continuous-control benchmarks.Our work enables scalable and data-efficient imitation learning under more realistic settings than before.
 Negative Sampling in Semi-Supervised learning 
 We introduce Negative Sampling in Semi-Supervised Learning (NS^3L), a simple, fast, easy to tune algorithm for semi-supervised learning (SSL). NS^3L is motivated by the success of negative sampling/contrastive estimation. We demonstrate that adding the NS^3L loss to state-of-the-art SSL algorithms, such as the Virtual Adversarial Training (VAT), significantly improves upon vanilla VAT and its variant, VAT with Entropy Minimization. By adding the NS^3L loss to MixMatch, the current state-of-the-art approach on semi-supervised tasks, we observe significant improvements over vanilla MixMatch. We conduct extensive experiments on the CIFAR10, CIFAR100, SVHN and STL10 benchmark datasets. Finally, we perform an ablation study for NS3L regarding its hyperparameter tuning.
 Improved Optimistic Algorithms for Logistic Bandits 
 The generalized linear bandit framework has attracted a lot of attention in recent years by extending the well-understood linear setting and allowing to model richer reward structures. It notably covers the logistic model, widely used when rewards are binary. For logistic bandits, the frequentist regret guarantees of existing algorithms are $\tilde{\mathcal{O}}(\kappa \sqrt{T})$, where $\kappa$ is a problem-dependent constant. Unfortunately, $\kappa$ can be arbitrarily large as it scales exponentially with the size of the decision set. This may lead to significantly loose regret bounds and poor empirical performance. In this work, we study the logistic bandit with a focus on the prohibitive dependencies introduced by $\kappa$. We propose a new optimistic algorithm based on a finer examination of the non-linearities of the reward function. We show that it enjoys a $\tilde{\mathcal{O}}(\sqrt{T})$ regret with no dependency in $\kappa$, but for a second order term. Our analysis is based on a new tail-inequality for self-normalized martingales, of independent interest.
 Online Learned Continual Compression with Adaptive Quantization Modules 
 We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoder in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning.  This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression.  Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks with images, LiDAR, and reinforcement learning agents.
 Learning De-biased Representations with Biased Representations 
 Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on \textit{in-distribution} learning scenarios has led interesting advances, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles). Such biased models fail to generalise when the bias shifts to a different class. The \textit{cross-bias generalisation} problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be \textit{different} from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. Our experiments and analyses show that our method discourages models from taking bias shortcuts, resulting in improved generalisation.
 Explainable and Discourse Topic-aware Neural Language Understanding 
 Marrying topic models and language models expose language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document. This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion. Moreover, we retain sentence-topic associations along with document-topic association by modeling topical discourse for every sentence in the document. We present a novel neural composite language model that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models. Experiments over a range of tasks such as language modeling, word sense disambiguation, document classification, retrieval and text generation demonstrate ability of the proposed model in improving language understanding.
 Unlabelled Data Improves Bayesian Uncertainty Calibration under Covariate Shift   
 Modern neural networks have proven to be powerful function approximators, providing state of the art performance in a wide variety of applications. They however fall short in their ability to quantify their confidence in their predictions, which can be crucial in high-stakes applications involving critical decision-making. Bayesian neural networks (BNNs) aim to solve this problem by placing a prior distribution over the network parameters, thus inducing a posterior predictive distribution that encapsulates any uncertainty about the prediction. While existing variants of BNNs are able to produce reliable, albeit approximate, uncertainty estimates over in-distribution data, it has been shown that they tend to be over-confident in predictions made on target data whose distribution over features differs from the training data, i.e., the covariate shift setup. In this paper, we develop an approximate Bayesian inference scheme based on posterior regularisation, where we use information from unlabelled target data to produce more appropriate uncertainty estimates for ''covariate-shifted'' predictions. Our regulariser can be easily applied to many of the current network architectures and inference schemes --- here, we demonstrate its usefulness in Monte Carlo Dropout, showing that it much more appropriately quantifies its uncertainty with very little extra work. Empirical evaluations demonstrate that our method performs competitively compared to Bayesian and frequentist approaches to uncertainty estimation in neural networks.
 Exploration Through Bias: Revisiting Biased Maximum Likelihood Estimation in Stochastic Multi-Armed Bandits 
 We propose a new family of bandit algorithms, that are formulated in a general way based on the Biased Maximum Likelihood Estimation (BMLE) method originally appearing in the adaptive control literature. We design the reward-bias term to tackle the exploration and exploitation tradeoff for stochastic bandit problems. We provide a general recipe for the BMLE algorithm and derive a simple explicit closed-form expression for the index of an arm for exponential family reward distributions. We prove that the derived BMLE indices achieve a logarithmic finite-time regret bound and hence attain order-optimality, for both exponential families and the cases beyond parametric distributions. Through extensive simulations, we demonstrate that the proposed algorithms achieve regret performance comparable to the best of several state-of-the-art baseline methods, while being computationally efficient in comparison to other best-performing methods. The generality of the proposed approach makes it possible to address more complex models, including general adaptive control of Markovian systems.
 FR-Train: A mutual information-based approach to fair and robust training 
 Trustworthy AI is a critical issue in machine learning where, in addition to training a model that is accurate, one must consider both fair and robust training in the presence of data bias and poisoning. However, the existing model fairness techniques mistakenly view poisoned data as an additional bias, resulting in severe performance degradation. To fix this problem, we propose FR-Train, which holistically performs fair and robust model training. We provide a mutual information-based interpretation of an existing adversarial training-based fairness-only method, and apply this idea to architect an additional discriminator that can identify poisoned data using a clean validation set and reduce its influence. In our experiments, FR-Train shows almost no decrease in fairness and accuracy in the presence of data poisoning by both mitigating the bias and defending against poisoning. We also demonstrate how to construct clean validation sets using crowdsourcing, and release new benchmark datasets.
 PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination 
 We develop a novel method, called PoWER-BERT,
for improving the inference time of the popular
BERT model, while maintaining the accuracy. It
works by: a) exploiting redundancy pertaining to
word-vectors (intermediate encoder outputs) and
eliminating the redundant vectors. b) determining which word-vectors to eliminate by developing a strategy for measuring their significance,
based on the self-attention mechanism; c) learning how many word-vectors to eliminate by augmenting the BERT model and the loss function.
Experiments on the standard GLUE benchmark
shows that PoWER-BERT achieves up to 4.5x reduction in inference time over BERT with < 1%
loss in accuracy. We show that PoWER-BERT offers significantly better trade-off between accuracy and inference time compared to prior methods. We demonstrate that our method attains up
to 6.8x reduction in inference time with < 1%
loss in accuracy when applied over ALBERT, a
highly compressed version of BERT.
 Estimating the Number and Effect Sizes of Non-null Hypotheses 
 We study the problem of estimating the distribution of effect sizes (the mean of the test statistic under the alternate hypothesis) in a multiple testing setting. Knowing this distribution allows us to calculate the power (type II error) of any experimental design. We show that it is possible to estimate this distribution using an inexpensive pilot experiment, which takes significantly fewer samples than would be required by an experiment that identified the discoveries. Our estimator can be used to guarantee the number of discoveries that will be made using a given experimental design in a future experiment. We prove that this simple and computationally efficient estimator enjoys a number of favorable theoretical properties, and demonstrate its effectiveness on data from a gene knockout experiment on influenza inhibition in Drosophila.
 Variance Reduced Coordinate Descent with Acceleration: New Method With a Surprising Application to Finite-Sum Problems 
 We propose an accelerated version of stochastic variance reduced coordinate descent -- ASVRCD. As other variance reduced coordinate descent methods such as SEGA or SVRCD, our method can deal with problems that include a non-separable and non-smooth regularizer, while accessing a random block of partial derivatives in each iteration only. However, ASVRCD incorporates Nesterov's momentum, which offers favorable iteration complexity guarantees over both  SEGA and SVRCD. As a by-product of our theory, we show that a variant of Katyusha (Allen-Zhu, 2017) is a specific case of ASVRCD, recovering the optimal oracle complexity for the finite sum objective.
 On the Generalization Benefit of Noise in Stochastic Gradient Descent 
 It has long been argued that minibatch stochastic gradient descent can generalize better than large batch gradient descent in deep neural networks. However recent papers have questioned this claim, arguing that this effect is simply a consequence of suboptimal hyperparameter tuning or insufficient compute budgets when the batch size is large. In this paper, we perform carefully designed experiments and rigorous hyperparameter sweeps on a range of popular models, which verify that small or moderately large batch sizes can substantially outperform very large batches on the test set. This occurs even when both models are trained for the same number of iterations and large batches achieve smaller training losses. Our results confirm that the noise in stochastic gradients can enhance generalization. We study how the optimal learning rate schedule changes as the epoch budget grows, and we provide a theoretical account of our observations based on the stochastic differential equation perspective of SGD dynamics.
 Kernel interpolation with continuous volume sampling 
 A fundamental task in kernel methods is to pick nodes and weights, so as to approximate a given function from an RKHS by the weighted sum of kernel translates located at the nodes. This is the crux of kernel density estimation, kernel quadrature, or interpolation from discrete samples. Furthermore, RKHSs offer a convenient mathematical and computational framework. We introduce and analyse continuous volume sampling (VS), the continuous counterpart -for choosing node locations- of a discrete distribution introduced in (Deshpande & Vempala, 2006).
Our contribution is theoretical: we prove almost optimal bounds for interpolation and quadrature under VS. While similar bounds already exist for some specific RKHSs using ad-hoc node constructions, VS offers bounds that apply to any Mercer kernel and depend on the spectrum of the associated integration operator. We emphasize that, unlike previous randomized approaches that rely on regularized leverage scores or determinantal point processes, evaluating the pdf of VS only requires pointwise evaluations of the kernel. VS is thus naturally amenable to MCMC samplers.
 Temporal Logic Point Processes 
 We propose a modeling framework for event data, which excels in small data regime with the ability to incorporate domain knowledge. Our framework will model the intensities of the event starts and ends via a set of first-order temporal logic rules. Using softened representation of temporal relations, and a weighted combination of logic rules, our framework can also deal with uncertainty in event data. Furthermore, many existing point process models can be interpreted as special cases of our framework given simple temporal logic rules. We derive a maximum likelihood estimation procedure for our model, and show that it can lead to accurate predictions when data are sparse and domain knowledge is critical. 
 DropNet: Reducing Neural Network Complexity via Iterative Pruning 
 Modern deep neural networks require a significant amount of computing time and power to train and deploy, which limits their usage on edge devices. Inspired by the iterative weight pruning in the Lottery Ticket Hypothesis, we propose DropNet, an iterative pruning method which prunes nodes/filters to reduce network complexity. DropNet iteratively removes nodes/filters with the lowest average post-activation value across all training samples. Empirically, we show that DropNet is robust across a wide range of scenarios, including MLPs and CNNs using the MNIST and CIFAR datasets. We show that up to 90% of the nodes/filters can be removed without any significant loss of accuracy. The final pruned network performs well even with reinitialisation of the weights and biases. DropNet also achieves similar accuracy to an oracle which greedily removes nodes/filters one at a time to minimise training loss, highlighting its effectiveness.
 Efficient nonparametric statistical inference on population feature importance using Shapley values 
 The true population-level importance of a variable in a prediction task provides useful knowledge about the underlying data-generating mechanism and can help in deciding which measurements to collect in subsequent experiments. Valid statistical inference on this importance is a key component in understanding the population of interest. We present a computationally efficient procedure for estimating and obtaining valid statistical inference on the \textbf{S}hapley \textbf{P}opulation \textbf{V}ariable \textbf{I}mportance \textbf{M}easure (SPVIM). Although the computational complexity of the true SPVIM scales exponentially with the number of variables, we propose an estimator based on randomly sampling only $\Theta(n)$ feature subsets given $n$ observations. We prove that our estimator converges at an asymptotically optimal rate. Moreover, by deriving the asymptotic distribution of our estimator, we construct valid confidence intervals and hypothesis tests. Our procedure has good finite-sample performance in simulations, and for an in-hospital mortality prediction task produces similar variable importance estimates when different machine learning algorithms are applied.
 Active Learning on Attributed Graphs via Graph   Cognizant Logistic Regression and Preemptive Query Generation 
 Node classification in attributed graphs is an important task in multiple practical settings, but it can often be difficult or expensive to obtain labels. Active learning can improve the achieved classification performance for a given budget on the number of queried labels. The best existing methods are based on graph neural networks, but they often perform poorly unless a sizeable validation set of labelled nodes is available in order to choose good hyperparameters. We propose a novel graph-based active learning algorithm for the task of node classification in attributed graphs. Our algorithm uses graph cognizant logistic regression,
equivalent to a linearized graph-convolutional neural network
(GCNN), for the prediction phase and maximizes the expected error
reduction in the query phase. To reduce the delayexperienced by a labeller interacting with the system, we derive a preemptive querying system that calculates a new query during the labelling process. To address the setting where learning starts with almost no labelled data, we also develop a hybrid algorithm that performs adaptive model averaging of label propagation and linearized GCNN inference. We conduct experiments on four public benchmark datasets, demonstrating a significant improvement over state-of-the-art approaches. We illustrate the practical value of the method by applying it to a private commercial dataset that is used for the task of identifying faulty links in a microwave link network.
 Interpretations are Useful: Penalizing Explanations to Align Neural Networks with Prior Knowledge 
 For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective. Too often, the litany of proposed explainable deep learning methods stop at the first step, providing practitioners with insight into a model, but no way to act on it. In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods to increase the predictive accuracy of a deep learning model. In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by inserting domain knowledge into the model  via explanations. We demonstrate the ability of CDEP to increase performance on an array of toy and real datasets.
 Self-concordant analysis of Frank-Wolfe algorithm 
 Projection-free optimization via different variants of the Frank-Wolfe (FW) method has become one of the cornerstones in optimization for machine learning since in many cases the linear minimization oracle is much cheaper to implement than projections and some sparsity needs to be preserved. In a number of applications, e.g. Poisson inverse problems or quantum state tomography, the loss is given by a
self-concordant (SC) function having unbounded curvature, implying absence of theoretical guaranteesfor the existing FW methods. We use the
theory of SC functions to provide a new adaptive step size for FW methods and prove global convergence rate O(1/k), k being the iteration counter. If the problem can be represented by a local linear minimization oracle, we are the first
to propose a FW method with linear convergence rate without assuming neither strong convexity nor a Lipschitz continuous gradient.
 Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures 
 This paper shows that deep learning (DL) representations of data produced by generative adversarial nets (GANs) are random vectors which fall within the class of so-called \textit{concentrated} random vectors. Further exploiting the fact that Gram matrices, of the type $G = X^\intercal X$ with $X=[x_1,\ldots,x_n]\in \mathbb{R}^{p\times n}$ and $x_i$ independent concentrated random vectors from a mixture model, behave asymptotically (as $n,p\to \infty$) as if the $x_i$ were drawn from a Gaussian mixture, suggests that DL representations of GAN-data can be fully described by their first two statistical moments for a wide range of standard classifiers. Our theoretical findings are validated by generating images with the BigGAN model and across different popular deep representation networks.
 Causal Effect Identifiability under Partial-Observability 
 Causal effect identifiability is concerned with establishing the effect of intervening on a set of variables on another set of variables from observational or interventional distributions under causal assumptions that are usually encoded in the form of a causal graph. Most of the results of this literature implicitly assume that every variable modeled in the graph is measured in the available distributions. In practice, however, the data collections of the different studies considered do not measure the same variables, consistently. In this paper, we study the causal effect identifiability problem when the available distributions may be associated with different sets of variables, which we refer to as identification under partial-observability. We study a number of properties of the factors that comprise a causal effect under various levels of abstraction, and then characterize the relationship between them with respect to their status relative to the identification of a targeted intervention. We establish a sufficient graphical criterion for determining whether the effects are identifiable from partially-observed distributions. Finally, building on these graphical properties, we develop an algorithm that returns a formula for a causal effect in terms of the available distributions.
 Convergence Rates of Variational Inference in Sparse Deep Learning 
 Variational inference is becoming more and more popular for approximating intractable posterior distributions in Bayesian statistics and machine learning. Meanwhile, a few recent works have provided theoretical justification and new insights on deep neural networks for estimating smooth functions in usual settings such as nonparametric regression. In this paper, we show that variational inference for sparse deep learning retains precisely the same generalization properties than exact Bayesian inference. In particular, we show that a wise choice of the neural network architecture leads to near-minimax rates of convergence for H\"older smooth functions. Additionally, we show that the model selection framework over the architecture of the network via ELBO maximization does not overfit and adaptively achieves the optimal rate of convergence.
 Leveraging Frequency Analysis for Deep Fake Image Recognition 
 Deep neural networks can generate images that are astonishingly realistic, so much so that it is often hard for untrained humans to distinguish them from actual photos. These achievements have been largely made possible by Generative Adversarial Networks (GANs). While these deep fake images have been thoroughly investigated in the image domain - a classical approach from the area of image forensics - an analysis in the frequency domain has been missing. This paper addresses this shortcoming and our results reveal, that in frequency space, GAN-generated images exhibit severe artifacts that can be easily identified. We perform a comprehensive analysis, showing that these artifacts are consistent across different neural network architectures, data sets, and resolutions. In a further investigation, we demonstrate that these artifacts are caused by upsampling operations found in all current GAN architectures, indicating a structural and fundamental problem in the way images are generated via GANs. Based on this analysis, we demonstrate how the frequency representation can be used to automatically identify deep fake images, surpassing state-of-the-art methods.
 Low-Variance and Zero-Variance Baselines for Extensive-Form Games 
 Extensive-form games (EFGs) are a common model of multi-agent interactions with imperfect information. State-of-the-art algorithms for solving these games typically perform full walks of the game tree that can prove prohibitively slow in large games. Alternatively, sampling-based methods such as Monte Carlo Counterfactual Regret Minimization walk one or more trajectories through the tree, touching only a fraction of the nodes on each iteration, at the expense of requiring more iterations to converge due to the variance of sampled values. In this paper, we extend recent work that uses baseline estimates to reduce this variance. We introduce a framework of baseline-corrected values in EFGs that generalizes the previous work. Within our framework, we propose new baseline functions that result in significantly reduced variance compared to existing techniques. We show that one particular choice of such a function --- predictive baseline --- is provably optimal under certain sampling schemes. This allows for efficient computation of zero-variance value estimates even along sampled trajectories.
 Subspace Fitting Meets Regression: The Effects of Supervision and  Orthonormality Constraints on Double Descent of Generalization Errors 
 We study the linear subspace fitting problem in the overparameterized setting, where the estimated subspace can perfectly interpolate the training examples. Our scope includes the least-squares solutions to subspace fitting tasks with varying levels of supervision in the training data (i.e., the proportion of input-output examples of the desired low-dimensional mapping) and orthonormality of the vectors defining the learned operator. This flexible family of problems connects standard, unsupervised subspace fitting that enforces strict orthonormality with a corresponding regression task that is fully supervised and does not constrain the linear operator structure. This class of problems is defined over a supervision-orthonormality plane, where each coordinate induces a problem instance with a unique pair of supervision level and softness of orthonormality constraints. We explore this plane and show that the generalization errors of the corresponding subspace fitting problems follow double descent trends as the settings become more supervised and less orthonormally constrained. 

 Online Continual Learning from Imbalanced Data 
 A well-documented weakness of neural networks is the fact that they suffer from catastrophic forgetting when trained on data provided by a non-stationary distribution. Recent work in the field of continual learning attempts to understand and overcome this issue. Unfortunately, the majority of relevant work embraces the implicit assumption that the distribution of observed data is perfectly balanced. In contrast, humans and animals learn from observations that are temporally correlated and severely imbalanced. Motivated by this remark, we aim to evaluate memory population methods that are used in online continual learning, when dealing with highly imbalanced and temporally correlated streams of data. More importantly, we introduce a new memory population approach, which we call class-balancing reservoir sampling (CBRS). We demonstrate that CBRS outperforms the state-of-the-art memory population algorithms in a considerably challenging learning setting, over a range of different datasets, and for multiple architectures. Finally, we probe the computational efficiency of CBRS compared to the state of the art, both in terms of time and memory overhead. 
 When deep denoising meets iterative phase retrieval 
 Recovering a signal from its Fourier intensity underlies many important applications, including lensless imaging and imaging through scattering media. Conventional algorithms for retrieving the phase suffer when noise is present but display global convergence when given clean data. Neural networks have been used to improve algorithm robustness, but efforts to date are sensitive to initial conditions and give inconsistent performance. Here, we combine iterative methods from phase retrieval with image statistics from deep denoisers, via regularization-by-denoising. The resulting methods inherit the advantages of each approach and outperform other noise-robust phase retrieval algorithms. Our work paves the way for hybrid imaging methods that integrate machine-learned constraints in conventional algorithms.
 Anderson Acceleration of Proximal Gradient Methods 
 Anderson acceleration is a well-established and simple technique for speeding up fixed-point computations with countless applications. This work introduces novel methods for adapting Anderson acceleration to (non-smooth and constrained) proximal gradient algorithms. Under some technical conditions, we extend the existing local convergence results of Anderson acceleration for smooth fixed-point mappings to the proposed scheme. We also prove analytically that it is not, in general, possible to guarantee global convergence of native Anderson acceleration. We therefore propose a simple scheme for stabilization that combines the global worst-case guarantees of proximal gradient methods with the local adaptation and practical speed-up of Anderson acceleration. We also provide the first applications of Anderson acceleration to non-Euclidean geometry.
 Asynchronous Coagent Networks 
 Coagent policy gradient algorithms (CPGAs) are reinforcement learning algorithms for training a class of stochastic neural networks called coagent networks. In this work, we prove that CPGAs converge to locally optimal policies. Additionally, we extend prior theory to encompass asynchronous and recurrent coagent networks. These extensions facilitate the straightforward design and analysis of hierarchical reinforcement learning algorithms like the option-critic, and eliminate the need for complex derivations of customized learning rules for these algorithms.
 Multi-objective Bayesian Optimization using Pareto-frontier Entropy 
 This paper studies an entropy-based multi-objective Bayesian optimization (MBO). The entropy search is successful approach to Bayesian optimization. However, for MBO, existing entropy-based methods ignore trade-off among objectives or introduce unreliable approximations. We propose a novel entropy-based MBO called Pareto-frontier entropy search (PFES) by considering the entropy of Pareto-frontier, which is an essential notion of the optimality of the multi-objective problem. Our entropy can incorporate the trade-off relation of the optimal values, and further, we derive an analytical formula without introducing additional approximations or simplifications to the standard entropy search setting. We also show that our entropy computation is practically feasible by using a recursive decomposition technique which has been known in studies of the Pareto hyper-volume computation. Besides the usual MBO setting, in which all the objectives are simultaneously observed, we also consider the ``decoupled'' setting, in which the objective functions can be observed separately. PFES can easily adapt to the decoupled setting by considering the entropy of the marginal density for each output dimension. This approach incorporates dependency among objectives conditioned on Pareto-frontier, which is ignored by the existing method. Our numerical experiments show effectiveness of PFES through several benchmark datasets.
 An Imitation Learning Approach for Cache Replacement 
 Program execution speed critically depends on reducing cache misses, as cache misses are orders of magnitude slower than hits.  To reduce cache misses, we focus on the problem of cache replacement: choosing which cache line to evict upon inserting a new line.  This is challenging because it requires planning far ahead and currently there is no known practical solution.  As a result, current replacement policies typically resort to heuristics designed for specific common access patterns, which fail on more diverse and complex access patterns.  In contrast, we propose an imitation learning approach to automatically learn cache access patterns by leveraging Belady’s, an oracle policy that computes the optimal eviction decision given the future cache accesses.  While directly applying Belady's is infeasible since the future is unknown, we train a policy conditioned only on past accesses that accurately approximates Belady's even on diverse and complex access patterns.  When evaluated on four of the most memory-intensive SPEC applications, our learned policy reduces cache miss rates by 15% over the current state of the art.  In addition, on a large-scale web search benchmark, our learned policy reduces cache miss rates by 66% over a conventional LRU policy.  We release a Gym environment to facilitate research in this area, as data is plentiful, and further advancements can have significant real-world impact.
 Variable Skipping for Autoregressive Range Density Estimation 
 Deep autoregressive models compute point likelihood estimates of individual data points. However, many applications (i.e., database cardinality estimation), require estimating range densities, a capability that is under-explored by current neural density estimation literature. In these applications, fast and accurate range density estimates over high-dimensional data directly impact user-perceived performance. In this paper, we explore a technique for accelerating range density estimation over deep autoregressive models. This technique, called variable skipping, exploits the sparse structure of range density queries to avoid sampling unnecessary variables during approximate inference. We show that variable skipping provides 10-100x efficiency improvements, enables complex applications such as text pattern matching, and can be realized via a simple data augmentation procedure without changing the usual maximum likelihood objective.
 Understanding Self-Training for Gradual Domain Adaptation 
 Machine learning systems must adapt to data distributions that evolve over time, in applications ranging from sensor networks and self-driving car perception modules to brain-machine interfaces. We consider gradual domain adaptation, where the goal is to adapt an initial classifier trained on a source domain given only unlabeled data that shifts gradually in distribution towards a target domain. We prove the first non-vacuous upper bound on the error of self-training with gradual shifts, under settings where directly adapting to the target domain can result in unbounded error. The theoretical analysis leads to algorithmic insights, highlighting that regularization and label sharpening are essential even when we have infinite data, and suggesting that self-training works particularly well for shifts with small Wasserstein-infinity distance. Leveraging the gradual shift structure leads to higher accuracies on a rotating MNIST dataset and a realistic Portraits dataset.
 Semi-Supervised StyleGAN for Disentanglement Learning 
 Disentanglement learning is crucial for obtaining disentangled representations and controllable generation. Current disentanglement methods face several inherent limitations: difficulty with high-resolution images, primarily on learning disentangled representations, and non-identifiability due to the unsupervised setting. To alleviate these limitations, we design new architectures and loss functions based on StyleGAN (Karras et al., 2019), for semi-supervised high-resolution disentanglement learning. We create two complex high-resolution synthetic datasets for systematic testing. We investigate the impact of limited supervision and find that using only 0.25%~2.5% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We propose new metrics to quantify generator controllability, and observe there may exist a crucial trade-off between disentangled representation learning and controllable generation. We also consider semantic fine-grained image editing to achieve better generalization to unseen images. 
 When Demands Evolve Larger and Noisier: Learning and Earning in a Growing Environment 
 We consider a single-product dynamic pricing problem under a specific non-stationary setting, where the demand grows over time in expectation and possibly gets noisier. The decision maker dynamically sets price and learns the unknown price elasticity, with the goal of maximizing expected cumulative revenue. We prove matching upper and lower bounds on regret and provide near-optimal pricing policies. We show how the change in demand uncertainty over time affects the optimal policy design and demonstrate how the order of optimal regret depends on the magnitude of demand uncertainty evolvement. Moreover, we distinguish between the \textit{any-time} situation and the \textit{fixed-time} situation by whether the seller knows the total number of time periods $T$ in advance or not, and show that they surprisingly render different optimal regret orders. We then extend the demand model to a more general case allowing for an additional intercept term and present a novel and near-optimal algorithm for the extended model. Finally, we consider an analogous non-stationary setting in the canonical multi-armed bandit problem, and points out that the \textit{any-time} situation and the \textit{fixed-time} situation render the same optimal regret order in a simple form, in contrast to the dynamic pricing problem.
 Super-efficiency of automatic differentiation for functions defined as a minimum 
 In min-min optimization or max-min optimization, one has to compute the gradient of a function defined as a minimum. In most cases, the minimum has no closed-form, and an approximation is obtained via an iterative algorithm. There are two usual ways of estimating the gradient of the function: using either an analytic formula obtained by assuming exactness of the approximation, or automatic differentiation through the algorithm. In this paper, we study the asymptotic error made by these estimators as a function of the optimization error. We find that the error of the automatic estimator is close to the square of the error of the analytic estimator, reflecting a super-efficiency phenomenon. The convergence of the automatic estimator greatly depends on the convergence of the Jacobian of the algorithm. We analyze it for gradient descent and stochastic gradient descent and derive convergence rates for the estimators in these cases. Our analysis is backed by numerical experiments on toy problems and on Wasserstein barycenter computation. Finally, we discuss the computational complexity of these estimators and give practical guidelines to chose between them.
 PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions 
 Recent research has shown that incorporating equivariance into neural network architectures is very helpful, and there have been some works investigating the equivariance of networks under group actions. However, as digital images and feature maps are on the discrete meshgrid, corresponding equivariance-preserving transformation groups are very limited. 
In this work, we deal with this issue from the connection between convolutions and partial differential operators (PDOs). In theory, assuming inputs to be smooth, we transform PDOs and propose a system which is equivariant to a much more general continuous group, the n-dimension Euclidean group. In implementation, we discretize the system using the numerical schemes of PDOs, deriving approximately equivariant convolutions (PDO-eConvs). Theoretically, the approximation error of PDO-eConvs is of the quadratic order. It is the first time that the error analysis is provided when the equivariance is approximate. Extensive experiments on rotated MNIST and natural image classification show that PDO-eConvs perform competitively yet use parameters much more efficiently. Particularly, compared with Wide ResNets, our methods result in comparable results using only 12.6% parameters.
 Empirical Study of the Benefits of Overparameterization in Learning Latent Variable Models 
 One of the most surprising and exciting discoveries in supervised learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it was observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). We perform an empirical study of different aspects of overparameterization in unsupervised learning of latent variable models via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that across a variety of models (noisy-OR networks, sparse coding, probabilistic context-free grammars) and training algorithms (variational inference, alternating minimization, expectation-maximization), overparameterization can significantly increase the number of ground truth latent variables recovered.

 Adversarial Mutual Information for Text Generation 
 Recent advances in maximizing mutual information (MI) between the source and target have demonstrated its effectiveness in text generation. However, previous works paid little attention to modeling the backward network of MI (i.e. dependency from the target to the source), which is crucial to the tightness of the variational information maximization lower bound. In this paper, we propose Adversarial Mutual Information (AMI): a text generation framework which is formed as a novel saddle point (min-max) optimization aiming to identify joint interactions between the source and target. Within this framework, the forward and backward networks are able to iteratively promote or demote each other's generated instances by comparing the real and synthetic data distributions. We also develop a latent noise sampling strategy that leverages random variations at the high-level semantic space to enhance the long term dependency in the generation process. Extensive experiments based on different text generation tasks demonstrate that the proposed AMI framework can significantly outperform several strong baselines, and we also show that AMI has potential to lead to a tighter lower bound of maximum mutual information for the variational information maximization problem.
 Stochastic Frank-Wolfe for Constrained Finite-Sum Minimization 
 We propose a novel Stochastic Frank-Wolfe ($\equiv$ Conditional Gradient) algorithm with fixed batch size tailored to the constrained optimization of a finite sum of smooth objectives. We make use of a primal-dual interpretation of the Frank-Wolfe algorithm. 

Recent work to design stochastic variants of the Frank-Wolfe algorithm fall into two categories: algorithms with increasing batch size, and algorithms with constant batch size. The former have faster convergence rates but are impractical; the latter are practical but slower. Our method combines the advantages of both: it converges for any constant batch size, and has faster theoretical worst case rates than previous fixed batch size algorithms. Our experiments also show faster empirical convergence than previous fixed batch methods for several tasks. 

Finally, we construct a stochastic estimator of the Frank-Wolfe gap.
It allows us to bound the true Frank-Wolfe gap, which is a primal-dual gap in the convex case and a measure of stationarity in general. 
Our gap estimator can therefore be used as a practical stopping criterion in all cases.
 Analytic Marching: An Analytic Meshing Solution from Deep Implicit Surface Networks 
 This paper studies a problem of learning surface mesh via implicit functions in an emerging field of deep learning surface reconstruction, where implicit functions are popularly implemented as multi-layer perceptrons (MLPs) with rectified linear units (ReLU). To achieve meshing from the learned implicit functions, existing methods adopt the de-facto standard algorithm of marching cubes; while promising, they suffer from loss of precision learned in the MLPs, due to the discretization nature of marching cubes. Motivated by the knowledge that a ReLU based MLP partitions its input space into a number of linear regions, we identify from these regions analytic cells and faces that are associated with zero-level isosurface of the implicit function, and characterize the conditions under which the identified faces are guaranteed to connect and form a closed, piecewise planar surface. We propose a naturally parallelizable algorithm of analytic marching to exactly recover the mesh captured by a learned MLP. Experiments on deep learning mesh reconstruction verify the advantages of our algorithm over existing ones.
 Randomized Block-Diagonal Preconditioning for Parallel Learning 
 We study preconditioned gradient-based optimization methods where the preconditioning matrix has block-diagonal form. Such a structural constraint comes with the advantage that the update computation can be parallelized across multiple independent tasks. Our main contribution is to demonstrate that the convergence of these methods can significantly be improved by a randomization technique which corresponds to repartitioning coordinates across tasks during the optimization procedure. We provide a theoretical analysis that accurately characterizes the expected convergence gains of repartitioning and validate our findings empirically on various traditional machine learning tasks. From an implementation perspective, block-separable models are well suited for parallelization and, when shared memory is available, randomization can be implemented on top of existing methods very efficiently to improve convergence.
 Invertible generative models for inverse problems: mitigating representation error and dataset bias 
 Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.  Given a trained generative model, we study the empirical risk formulation of the desired inverse problem under a regularization that promotes high likelihood images, either directly by penalization or algorithmically by initialization. For compressive sensing, invertible priors can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than GAN priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  We additionally compare performance for compressive sensing to unlearned methods, such as the deep decoder, and we establish theoretical bounds on expected recovery error in the case of a linear invertible model.
 Variational Label Enhancement 
 Label distribution  covers a certain number of labels, representing the degree to which each label describes the instance. The learning process on the	instances labeled by label distributions is called label distribution learning (LDL). Unfortunately, many training sets only contain  simple logical labels rather than label distributions due to the difficulty of obtaining the label distributions directly.  To solve this problem,  we consider the label distributions as the latent vectors and  infer the label distributions from the logical labels in the training datasets by using variational inference. After that, we induce a predictive model to train the label distribution data by employing the multi-output regression technique. The recovery experiment  on thirteen real-world LDL  datasets  and the predictive experiment on ten multi-label learning datasets validate the advantage of our approach  over the state-of-the-art  approaches. 
 Efficient Domain Generalization via Common-Specific Low-Rank Decomposition 
 Domain generalization refers to the task of training a model which generalizes to new domains that  are  not  seen  during  training.   We  present CSD (Common Specific Decomposition), for this setting, which jointly learns a common component (which generalizes to new domains) and a domain specific component (which overfits on training domains).  The domain specific components are discarded after training and only the common component is retained. The algorithm is extremely simple and involves only modifying the final linear classification layer of any given neural network architecture.  We show that CSD either matches or beats state of the art approaches for domain generalization based on domain erasure and domain perturbed data augmentation. Further diagnostics on rotated MNIST, where domains are interpretable, confirm the hypothesis that CSD successfully disentangles common and domain specific components and hence leads to better domain generalization.
 SimGANs: Simulator-Based Generative Adversarial Networks for ECG Synthesis to Improve Deep ECG Classification 
 Generating training examples for supervised tasks is a long sought after goal in AI. We study the problem of heart signal electrocardiogram (ECG) synthesis for improved heartbeat classification. ECG synthesis is challenging: the generation of training examples for such biological-physiological systems is not straightforward, due to their dynamic nature in which the various parts of the system interact in complex ways.
However, an understanding of these dynamics has been developed for years in the form of mathematical process simulators. We study how to incorporate this knowledge into the generative process by leveraging a biological simulator for the task of ECG classification.
Specifically, we use a system of ordinary differential equations representing heart dynamics, and incorporate this ODE system into the optimization process of a generative adversarial network to create biologically plausible ECG training examples. 
We perform empirical evaluation and show that heart simulation knowledge during the generation process improves ECG classification.
 NGBoost: Natural Gradient Boosting for Probabilistic Prediction 
 We present Natural Gradient Boosting (NGBoost), an algorithm for generic probabilistic prediction via gradient boosting. Typical regression models return a point estimate, conditional on covariates, but probabilistic regression models output a full probability distribution over the outcome space, conditional on the covariates. This allows for predictive uncertainty estimation - crucial in applications like healthcare and weather forecasting. NGBoost generalizes gradient boosting to probabilistic regression by treating the parameters of the conditional distribution as targets for a multiparameter boosting algorithm. Furthermore, we show how the Natural Gradient is required to correct the training dynamics of our multiparameter boosting approach. NGBoost can be used with any base learner, any family of distributions with continuous parameters, and any scoring rule. NGBoost matches or exceeds the performance of existing methods for probabilistic prediction while offering additional benefits in flexibility, scalability, and usability. An open-source implementation is available at github.com/stanfordmlgroup/ngboost.
 All in the (Exponential) Family: Information Geometry and Thermodynamic Variational Inference 
 While the Evidence Lower Bound (ELBO) has become a ubiquitous objective for variational inference, the recently proposed Thermodynamic Variational Objective (TVO) leverages thermodynamic integration to provide a tighter and more general family of bounds.  In previous work, the tightness of these bounds was not known, grid search was used to choose a `schedule' of intermediate distributions, and model learning suffered with ostensibly tighter bounds. We interpret the geometric mixture curve common to TVO and related path sampling methods using the geometry of exponential families, which allows us to characterize the gap in TVO bounds as a sum of KL divergences along a given path.  Further, we propose a principled technique for choosing intermediate distributions using equal spacing in the moment parameters of our exponential family.  We demonstrate that this scheduling approach adapts to the shape of the integrand defining the TVO objective and improves overall performance. Additionally, we derive a reparameterized gradient estimator which empirically allows the TVO to benefit from additional, well chosen partitions. Finally, we provide a unified framework for understanding thermodynamic integration and the TVO in terms of Taylor series remainders.
 Sparse Convex Optimization via Adaptively Regularized Hard Thresholding 
 The goal of Sparse Convex Optimization is to optimize a convex function $f$ under a sparsity constraint $s\leq s^*\gamma$, where $s^*$ is the target number of non-zero entries in a feasible solution (sparsity) and $\gamma\geq 1$ is an approximation factor. There has been a lot of work to analyze the sparsity guarantees of various algorithms (LASSO, Orthogonal Matching Pursuit (OMP), Iterative Hard Thresholding (IHT)) in terms of the Restricted Condition Number $\kappa$. The best known algorithms guarantee to find an approximate solution of value $f(x^*)+\epsilon$ with the sparsity bound of $\gamma = O\left(\kappa\min\left\{\log \frac{f(x^0)-f(x^*)}{\epsilon}, \kappa\right\}\right)$, where $x^*$ is the target solution. We present a new Adaptively Regularized Hard Thresholding (ARHT) algorithm that makes significant progress on this problem by bringing the bound down to $\gamma=O(\kappa)$, which has been shown to be tight for a general class of algorithms including LASSO, OMP, and IHT. This is achieved without significant sacrifice in the runtime efficiency compared to the fastest known algorithms. We also provide a new analysis of OMP with Replacement (OMPR) for general $f$, under the condition $s > s^* \frac{\kappa^2}{4}$, which yields Compressed Sensing bounds under the Restricted Isometry Property (RIP). When compared to other Compressed Sensing approaches, it has the advantage of providing a strong tradeoff between the RIP condition and the solution sparsity, while working for any general function $f$ that meets the RIP condition.
 Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling 
 We present a new method for evaluating and training unnormalized density models. Our approach only requires access to the gradient of the unnormalized model’s log-density. We  estimate the Stein discrepancy between the data density p(x) and the model density q(x) based on a vector function of the data.  We parameterize this function with a neural network and fit its parameters to maximize this discrepancy.  This yields a novel goodness-of-fit test which outperforms existing methods on high dimensional data. Furthermore, optimizing q(x) to minimize this discrepancy produces a novel method for training unnormalized models. This training method can fit large unnormalized models faster than existing approaches. The ability to both learn and compare models is a unique feature of the proposed method.
 On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings 
 We study Nesterov's accelerated gradient method in the stochastic approximation setting (unbiased gradients with bounded variance) and the finite sum setting (where randomness is due to sampling mini-batches). To build better insight into the behavior of Nesterov's method in stochastic settings, we focus throughout on objectives that are smooth, strongly-convex, and twice continuously differentiable. In the stochastic approximation setting, Nesterov's method converges to a neighborhood of the optimal point at the same accelerated rate as in the deterministic setting. Perhaps surprisingly, in the finite-sum setting we prove that Nesterov's method may diverge with the usual choice of step-size and momentum, unless additional conditions on the problem related to conditioning and data coherence are satisfied. Our results shed light as to why Nesterov's method may fail to converge or achieve acceleration in the finite-sum setting.
 Enhancing Simple Models by Exploiting What They Already Know 
 There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.
 Quantum Expectation-Maximization for Gaussian Mixture Models 
 We define a quantum version of Expectation-Maximization (QEM), a fundamental tool in unsupervised machine learning, often used to solve Maximum Likelihood (ML) and Maximum A Posteriori (MAP) estimation problems. We use QEM to fit a Gaussian Mixture Model, and show how to generalize it to fit mixture models with base distributions in the exponential family. Given quantum access to a dataset, our algorithm has convergence and precision guarantees similar to the classical algorithm, while the runtime is polylogarithmic in the number of elements in the training set and polynomial in other parameters, such as the dimension of the feature space and the number of components in the mixture. We discuss the performance of the algorithm on datasets that are expected to be classified successfully by classical EM and provide guarantees for its runtime.
 Causal Effect Estimation and Optimal Dose Suggestions in Mobile Health 
 In this article, we propose novel structural nested models to estimate causal effects of continuous treatments based on mobile health data. To find the treatment regime which optimizes the short-term outcomes for the patients, we define the weighted lag K advantage. The optimal treatment regime is then defined to be the one which maximizes this advantage. This method imposes minimal assumptions on the data generating process. Statistical inference can also be provided for the estimated parameters. Simulation studies and an application to the Ohio type 1 diabetes dataset show that our method could provide meaningful insights for dose suggestions with mobile health data. 
 Bisection-Based Pricing for Repeated Contextual Auctions against Strategic Buyer 
 We are interested in learning algorithms that optimize revenue in repeated contextual posted-price auctions where a single seller faces a single strategic buyer. In our setting, the buyer  maximizes his expected cumulative discounted surplus, and his valuation of a good is assumed to be a fixed function of a $d$-dimensional context (feature) vector. We introduce a novel deterministic learning algorithm that is based on ideas of the Bisection method and has strategic regret upper bound of $O(\log^2 T)$. Unlike previous works, our algorithm does not require any assumption on the distribution of context information, and the regret guarantee holds for any realization of feature vectors (adversarial upper bound). To construct our algorithm we non-trivially adopted techniques of integral geometry to act against buyer strategicness and improved the penalization trick to work in contextual auctions.
 Optimizing Dynamic Structures with Bayesian Generative Search 
 Kernel selection for kernel-based methods is prohibitively expensive due to the NP-hard nature of discrete optimization. Since gradient-based optimizers are not applicable due to the lack of a differentiable objective function, many state-of-the-art solutions resort to heuristic search or gradient-free optimization. These approaches, however, require imposing restrictive assumptions on the explorable space of structures such as limiting the active candidate pool, thus depending heavily on the intuition of domain experts. This paper instead proposes \textbf{DTERGENS}, a novel generative search framework that constructs and optimizes a high-performance composite kernel expressions generator. \textbf{DTERGENS} does not restrict the space of candidate kernels and is capable of obtaining flexible length expressions by jointly optimizing a generative termination criterion. We demonstrate that our framework explores more diverse kernels and obtains better performance than state-of-the-art approaches on many real-world predictive tasks.
 ConQUR: Mitigating Delusional Bias in Deep Q-Learning  
 Delusional bias is a fundamental source of error in approximate Q-learning. To date, the only techniques that explicitly address delusion require comprehensive search using tabular value estimates. In this paper, we develop efficient methods to mitigate delusional bias by training Q-approximators with labels that are "consistent" with the underlying greedy policy class. We introduce a simple penalization scheme that encourages Q-labels used across training batches to remain (jointly) consistent with the expressible policy class. We also propose a search framework that allows multiple Q-approximators to be generated and tracked, thus mitigating the effect of premature (implicit) policy commitments. Experimental results demonstrate that these methods can improve the performance of Q-learning in a variety of Atari games, sometimes dramatically.
 Double-Loop Unadjusted Langevin Algorithm 
 A well-known first-order method for sampling from  log-concave probability distributions is the Unadjusted Langevin Algorithm (ULA). This work 
proposes a new annealing step-size schedule for ULA, which allows to prove new convergence guarantees for sampling from a smooth log-concave distribution, which are not covered by existing state-of-the-art convergence guarantees. To establish this result, we derive a new theoretical bound that relates the Wasserstein distance to total variation distance between any two log-concave distributions that complements the reach of Talagrand $T_2$ inequality. Moreover, applying this new step size schedule to an existing constrained sampling algorithm, we show state-of-the-art convergence rates for sampling from a constrained log-concave distribution, as well as improved dimension dependence.  
 Learning Selection Strategies in Buchberger’s Algorithm 
 Studying the set of exact solutions of a system of polynomial equations largely depends on a single iterative algorithm, known as Buchberger’s algorithm. Optimized versions of this algorithm are crucial for many computer algebra systems (e.g., Mathematica, Maple, Sage). We introduce a new approach to Buchberger’s algorithm that uses reinforcement learning agents to perform S-pair selection, a key step in the algorithm. We then study how the difficulty of the problem depends on the choices of domain and distribution of polynomials, about which little is known. Finally, we train a policy model using proximal policy optimization (PPO) to learn S-pair selection strategies for random systems of binomial equations. In certain domains, the trained model outperforms state-of-the-art selection heuristics both in number of iterations of the algorithm and total number of polynomial additions performed. These results provide a proof-of-concept that recent developments in machine learning have the potential to improve performance of algorithms in symbolic computation.
 Video Prediction via Example Guidance 
 In video prediction tasks, one major challenge is to capture the multi-modal nature of future contents and dynamics.
In this work, we propose a simple yet effective framework that can predict diverse and plausible future states.
The key insight is that the potential distribution of a sequence could be approximated with analogous ones in a repertoire of training pool, namely, expert examples.
By further incorporating a novel optimization scheme into the training procedure, plausible and diverse predictions can be sampled efficiently from distribution constructed from the retrieved examples. 
Meanwhile, our method could be seamlessly integrated with existing stochastic predictive models; significant enhancement is observed with comprehensive experiments in both quantitative and qualitative aspects.
We also demonstrate the generalization ability to predict the motion of unseen class, i.e., without access to corresponding data during training phase.
 Generative Adversarial Imitation Learning with Neural Network Parameterization: Global Optimality and Convergence Rate 
 Generative adversarial imitation learning (GAIL) demonstrates tremendous success in practice, especially when combined with neural networks. Different from reinforcement learning, GAIL learns both policy and reward function from expert (human) demonstration. Despite its empirical success, it remains unclear whether GAIL with neural networks converges to the globally optimal solution. The major difﬁculty comes from the nonconvex-nonconcave minimax optimization structure. To bridge the gap between practice and theory, we analyze a gradient-based algorithm with alternating updates and establish its sublinear convergence to the globally optimal solution. To the best of our knowledge, our analysis establishes the global optimality and convergence rate of GAIL with neural networks for the ﬁrst time.
 Coresets for Clustering in Graphs of Bounded Treewidth 
 We initiate the study of coresets for clustering in graph metrics, i.e., the shortest-path metric of edge-weighted graphs. Such clustering problems are essential to data analysis and used for example in road networks and data visualization. A coreset is a compact summary of the data that approximately preserves the clustering objective for every possible center set, and it offers significant efficiency improvements in terms of running time, storage, and communication, including in streaming and distributed settings. Our main result is a near-linear time construction of a coreset for k-Median in a general graph $G$, with size $O_{\epsilon, k}(\tw(G))$ where $\tw(G)$ is the treewidth of $G$, and we complement the construction with a nearly-tight size lower bound. The construction is based on the framework of Feldman and Langberg [STOC 2011], and our main technical contribution, as required by this framework, is a uniform bound of $O(\tw(G))$ on the shattering dimension under any point weights. We validate our coreset on real-world road networks, and our scalable algorithm constructs tiny coresets with high accuracy, which translates to a massive speedup of existing approximation algorithms such as local search for graph k-Median.
 Unsupervised Speech Decomposition via Triple Information Bottleneck 
 Speech information can be roughly decomposed into four components: language content, timbre, pitch, and rhythm. Obtaining disentangled representations of these components is useful in many speech analysis and generation applications. Recently, state-of-the-art voice conversion systems have led to speech representations that can disentangle speaker-dependent and independent information. However, these systems can only disentangle timbre, while information about pitch, rhythm and content is still mixed together. Further disentangling the remaining speech components is an under-determined problem in the absence of explicit annotations for each component, which are difficult and expensive to obtain. In this paper, we propose SpeechFlow, which can blindly decompose speech into its four components by introducing three carefully designed information bottlenecks. SpeechFlow is among the first algorithms that can separately perform style transfer on timbre, pitch and rhythm without text labels.
 Deep k-NN for Noisy Labels 
 Modern machine learning models are often trained on examples with noisy labels that hurt performance and are hard to identify. In this paper, we provide an empirical study showing that a simple k-nearest neighbor-based filtering approach on the logit layer of a preliminary model can  remove mislabeled training data and produce more accurate models than some recently proposed methods. We also provide new statistical guarantees into its efficacy.

 On the Number of Linear Regions of Convolutional Neural Networks 
 One fundamental problem in deep learning is understanding the outstanding performance of deep Neural Networks (NNs) in practice. One explanation for the superiority of NNs is that they can realize a large class of complicated functions, i.e., they have powerful expressivity. The expressivity of a ReLU NN can be quantified by the maximal number of linear regions it can separate its input space into. In this paper, we provide several mathematical results needed for studying the linear regions of CNNs, and use them to derive the maximal and average numbers of linear regions for one-layer ReLU CNNs. Furthermore, we obtain upper and lower bounds for the number of linear regions of multi-layer ReLU CNNs. Our results suggest that deeper CNNs have more powerful expressivity than their shallow counterparts, while CNNs have more expressivity than fully-connected NNs per parameter. 
 Stabilizing Differentiable Architecture Search via Perturbation-based Regularization 
 Differentiable architecture search (DARTS) is a prevailing NAS solution to identify architectures. Based on the continuous relaxation of the architecture space, DARTS learns a differentiable architecture weight and largely reduces the search cost. However, its stability and generalizability have been challenged for yielding deteriorating architectures as the search proceeds. We find that the precipitous validation loss landscape, which leads to a dramatic performance drop when distilling the final architecture, is an essential factor that causes instability. Based on this observation, we propose a perturbation-based regularization, named SmoothDARTS (SDARTS), to smooth the loss landscape and improve the generalizability of DARTS. In particular, our new formulations stabilize DARTS by either random smoothing or adversarial attack. The search trajectory on NAS-Bench-1Shot1 demonstrates the effectiveness of our approach and due to the improved stability, we achieve performance gain across various search spaces on 4 datasets. Furthermore, we mathematically show that SDARTS implicitly regularizes the Hessian norm of the validation loss, which accounts for a smoother loss landscape and improved performance.
 Learning to Navigate in Synthetically Accessible Chemical Space Using Reinforcement Learning 
 Over the last decade, there has been significant progress in the field of machine learning-based de novo drug discovery, particularly in generative modeling of chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure the synthetic accessibility nor provide the synthetic routes of the proposed small molecules which limits their applicability. In this work, we propose a novel reinforcement learning (RL) setup for drug discovery that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo compound design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. Our end-to-end approach achieves state-of-the-art performance when compared against other generative approaches for drug discovery. Moreover, we leverage our approach in a proof-of-concept that mimics the drug discovery process by generating novel HIV drug candidates. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.
 Description Based Text Classification with Reinforcement Learning 
 The task of text classification is usually divided into two stages: text feature extraction and classification. In this standard formalization, categories are merely represented as indexes in the label vocabulary, and the model lacks for explicit instructions on what to classify. Inspired by the current trend of formalizing NLP problems as question answering tasks, we propose a new framework for text classification, in which each category label is associated with a category description. Descriptions are generated by hand-crafted templates or using abstractive/extractive models from reinforcement learning. The concatenation of the description and the text is fed to the classifier to decide whether or not the current label should be assigned to the text. The proposed strategy forces the model to attend to the most salient texts with respect to the label, which can be regarded as a hard version of attention, leading to better performances. We observe significant performance boosts over strong baselines on a wide range of text classification tasks including single-label classification, multi-label classification and multi-aspect sentiment analysis.
 Familywise Error Rate Control by Interactive Unmasking 
 We propose a method for multiple hypothesis testing with familywise error rate (FWER) control, called the i-FWER test. Most testing methods are predefined algorithms that do not allow modifications after observing the data. However, in practice, analysts tend to choose a promising algorithm after observing the data; unfortunately, this violates the validity of the conclusion. The i-FWER test allows much flexibility: a human (or a computer program acting on the human's behalf) may adaptively guide the algorithm in a data-dependent manner. We prove that our test controls FWER if the analysts adhere to a particular protocol of “masking” and “unmasking”. We demonstrate via numerical experiments the power of our test under structured non-nulls, and then explore new forms of masking.
 Multi-Agent Determinantal Q-Learning 
 Centralized training with decentralized execution has become an important paradigm in multi-agent learning. Though practical, current methods rely on restrictive assumptions to decompose the centralized  value function across agents for execution. In this paper, we eliminate this restriction by proposing multi-agent determinantal Q-learning. Our method is established on Q-DPP, a novel extension of determinantal point process (DPP) to multi-agent setting. Q-DPP promotes agents to acquire diverse behavioral models; this allows  a natural factorization of the joint Q-functions with no need for \emph{a priori} structural constraints on the value function or special network architectures. We demonstrate that Q-DPP generalizes major solutions including VDN, QMIX, and QTRAN on decentralizable cooperative tasks. To efficiently draw samples  from Q-DPP, we develop a linear-time sampler with theoretical approximation guarantee. Our sampler also benefits exploration by  coordinating agents to cover orthogonal directions in the state space during training. We evaluate our algorithm on multiple cooperative benchmarks; its effectiveness has been demonstrated when compared with the state-of-the-art. 
 DROCC: Deep Robust One-Class Classification 
 Classical approaches for one-class problems such as one-class SVM (Schölkopf et al., 1999) and isolation forest (Liu et al., 2008) require careful feature engineering when applied to structured domains like images. To alleviate this concern, state-of-the-art methods like DeepSVDD (Ruff et al., 2018) consider the natural alternative of minimizing a classical one -class loss applied to the learned final layer representations. However, such an approach suffers from the fundamental drawback that a representation that simply collapses all the inputs minimizes the one class loss; heuristics to mitigate collapsed representations provide limited benefits. In this work, we propose Deep Robust One Class Classification (DROCC) method that is robust to such a collapse by training the network to distinguish the training points from their perturbations, generated adversarially. DROCC is motivated by the assumption that the interesting class lies on a locally linear low dimensional manifold. Empirical evaluation
demonstrates DROCC’s effectiveness on two different one-class problem settings and on a range of real-world datasets across different domains—images (CIFAR and ImageNet), audio and timeseries, offering up to 20% increase in accuracy
over the state-of-the-art in anomaly detection.
 Communication-Efficient Federated Learning with Sketching 
 Existing approaches to federated learning suffer from a communication bottleneck as well as convergence issues due to sparse client participation. In this paper we introduce a novel algorithm, called FedSketchedSGD, to overcome these challenges. FedSketchedSGD compresses model updates using a Count Sketch, and then takes advantage of the mergeability of sketches to combine model updates from many workers. A key insight in the design of FedSketchedSGD is that, because the Count Sketch is linear, momentum and error accumulation can both be carried out within the sketch. This allows the algorithm to move momentum and error accumulation from clients to the central aggregator, overcoming the challenges of sparse client participation while still achieving high compression rates. We prove that FedSketchedSGD has favorable convergence guarantees, and we demonstrate its empirical effectiveness by training two residual networks and a transformer model.
 Structured Prediction with Partial Labelling through the Infimum Loss 
   Annotating datasets is one of the main costs in nowadays supervised learning.
  The goal of weak supervision is to enable models to learn using only forms of
  labelling which are cheaper to collect, as partial labelling. This is a type of
  incomplete annotation where, for each datapoint, supervision is cast as a set
  of labels containing the real one.  The problem of supervised learning with
  partial labelling has been studied for specific instances such as
  classification, multi-label, ranking or segmentation, but a general framework
  is still missing. This paper provides a unified framework based on structured
  prediction and on the concept of {\em infimum loss} to deal with partial
  labelling over a wide family of learning problems and loss functions. The
  framework leads naturally to explicit algorithms that can be easily
  implemented and for which proved statistical consistency and learning rates.
  Experiments confirm the superiority of the proposed approach over commonly
  used baselines. 
 The Complexity of Finding Stationary Points with Stochastic Gradient Descent 
 We study the iteration complexity of stochastic gradient descent (SGD) for minimizing the gradient norm of smooth, possibly nonconvex functions. We provide several results, implying that the classical $\mathcal{O}(\epsilon^{-4})$ upper bound (for making the average gradient norm less than $\epsilon$) cannot be improved upon, unless a combination of additional assumptions is made. Notably, this holds even if we limit ourselves to convex quadratic functions. We also show that for nonconvex functions, the feasibility of minimizing gradients with SGD is surprisingly sensitive to the choice of optimality criteria.
 Robust One-Bit Recovery via ReLU Generative Networks: Near-Optimal Statistical Rate and Global Landscape Analysis 
 We study the robust one-bit compressed sensing problem whose goal is to design an algorithm that faithfully recovers any sparse target vector $\theta_0\in\mathbb{R}^d$  \textit{uniformly} via $m$ quantized noisy measurements. Specifically, we consider a new framework for this problem where the sparsity is implicitly enforced via mapping a low dimensional representation $x_0 \in \RR^k$  through a known $n$-layer ReLU generative network $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$ such that $\theta_0 = G(x_0)$.  Such a framework poses low-dimensional priors on $\theta_0$ without a known sparsity basis. We propose to recover the target $G(x_0)$ solving an unconstrained empirical risk minimization (ERM). Under a weak \textit{sub-exponential measurement assumption}, we establish a joint statistical and computational analysis. In particular, we prove that the ERM estimator in this new framework achieves a statistical rate of $m=\tilde{\mathcal{O}}(kn \log d /\varepsilon^2)$ recovering any $G(x_0)$ uniformly up to an error $\varepsilon$. When the network is shallow (i.e., $n$ is small), we show this rate matches the information-theoretic lower bound up to logarithm factors on $\varepsilon^{-1}$. From the lens of computation, we prove that under proper conditions on the ReLU weights, our proposed empirical risk, despite non-convexity, has no stationary point outside of small neighborhoods around the true representation $x_0$ and its negative multiple. Furthermore, we show that the global minimizer of the empirical risk stays within the neighborhood around $x_0$ rather than its negative multiple under further assumptions on ReLU weights.
 Recovery of sparse signals from a mixture of linear samples 
 Mixture of linear regressions is a popular learning theoretic model that is used widely to represent heterogeneous data. In the simplest form, this model assumes that the labels are generated from either of two different linear models and mixed together. Recent works of Yin et al. and Krishnamurthy et al., 2019, focus on an experimental design setting of model recovery for this problem. It is assumed that the features can be designed and queried with to obtain their label. When queried, an oracle randomly selects one of the two different sparse linear models and generates a label accordingly. How many such oracle queries are needed to recover both of the models simultaneously? This can also be though of as a generalization of the well-known compressed sensing problem (Cand\`es and Tao, 2005, Donoho, 2006).
In this work we address this query complexity problem and provide efficient algorithms that improves on the previously best known results. 
 DeltaGrad: Rapid retraining of machine learning models 
 Machine learning models are not static and may need to be retrained on slightly different datasets, for instance, with the addition or deletion of a set of datapoints. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantification. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapidly retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.
 Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach 
 Most recommender systems (RS) research assumes that a user's utility can be maximized independently of the utility of the other agents (e.g., other users, content providers). In realistic settings, this is often not true -- the dynamics of an RS ecosystem couple the long-term utility of all agents. In this work, we explore
settings in which content providers cannot remain viable unless they receive a
certain level of user engagement. We formulate this problem as one of equilibrium selection in
the induced dynamical system, and show that it can be solved as an
optimal constrained matching problem. Our model
ensures the system reaches an equilibrium with maximal social welfare supported
by a sufficiently diverse set of viable providers.
We demonstrate that even in a simple, stylized dynamical RS model, the standard 
myopic approach to recommendation - always matching a user to the best provider - performs poorly. We develop several scalable techniques to solve the
matching problem, and also draw connections to various notions of user regret and fairness, arguing that these outcomes are fairer in a utilitarian sense.
 A Flexible Framework for Nonparametric Graphical Modeling that Accommodates Machine Learning 
 Graphical modeling has been broadly useful for exploring the dependence structure among features in a dataset. However, the strength of graphical modeling hinges on our ability to encode and estimate conditional dependencies. In particular, commonly used measures such as partial correlation are only meaningful under strongly parametric (in this case, multivariate Gaussian) assumptions. These assumptions are unverifiable, and there is often little reason to believe they hold in practice. In this paper, we instead consider 3 non-parametric measures of conditional dependence. These measures are meaningful without structural assumptions on the multivariate distribution of the data. In addition, we show that for 2 of these measures there are simple, strong plug-in estimators that require only the estimation of a conditional mean. These plug-in estimators (1) are asymptotically linear and non-parametrically efficient, (2) allow incorporation of flexible machine learning techniques for conditional mean estimation, and (3) enable the construction of valid Wald-type confidence intervals. In addition, by leveraging the influence function of these estimators, one can obtain intervals with simultaneous coverage guarantees for all pairs of features.
 Optimizing Data Usage via Differentiable Rewards 
 To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that ``adapts'' to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently  updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.
 Principled learning method for Wasserstein distributionally robust optimization with local perturbations 
 Wasserstein distributionally robust optimization (WDRO) attempts to learn a model that minimizes the local worst-case risk in the vicinity of the empirical data distribution defined by Wasserstein ball. While WDRO has received attention as a promising tool for inference since its introduction, its theoretical understanding has not been fully matured. Gao et al. (2017) proposed a minimizer based on a tractable approximation of the local worst-case risk, but without showing risk consistency. In this paper, we propose a minimizer based on a novel approximation theorem and provide the corresponding risk consistency results. Furthermore, we develop WDRO inference for locally perturbed data that include the Mixup (Zhang et al., 2017) as a special case. We show that our approximation and risk consistency results naturally extend to the cases when data are locally perturbed. Numerical experiments demonstrate robustness of the proposed method using image classification datasets. Our results show that the proposed method achieves significantly higher accuracy than baseline models on noisy datasets.
 Learning Mixtures of Graphs from Epidemic Cascades  
 We consider the problem of learning the weighted edges of a balanced mixture of two undirected graphs from epidemic cascades. While mixture models are popular modeling tools, algorithmic development with rigorous guarantees has lagged. Graph mixtures are apparently no exception: until now, very little is known about whether this problem is solvable. 
     
     To the best of our knowledge, we establish the first necessary and sufficient conditions for this problem to be solvable in polynomial time on edge-separated graphs. When the conditions are met, i.e., when the graphs are connected with at least three edges, we give an efficient algorithm for learning the weights of both graphs with optimal sample complexity (up to log factors). 
     
     We give complementary results and provide sample-optimal (up to log factors) algorithms for mixtures of directed graphs of out-degree at least three, and for mixture of undirected graphs of unbalanced and/or unknown priors.
 Cost-effectively Identifying Causal Effect When Only Response Variable Observable 
 In many real tasks, we care about how to make decisions other than mere predictions on an event, e.g. how to increase the revenue next month instead of knowing it will drop. The key is to identify the causal effects on the desired event. Pearl proposed do-calculus to make it given the knowledge of causal structure (Pearl, 2009). But sometimes, we have to discover it at first. In this paper, we propose a novel solution for this challenging task where only the response variable is observable under intervention. By an active strategy introducing limited interventions and exploiting the exact distribution of the response variable, the proposed approach can cost-effectively identify the causal effect of each intervention, and thus guide the decision-making. Theoretical analysis along with empirical studies is presented to show that our approach can achieve causal effect identification with fewer interventions.
 Combinatorial Pure Exploration for Dueling Bandit 
 In this paper, we study combinatorial pure exploration for dueling bandits (CPE-DB): we have multiple candidates for multiple positions as modeled by a bipartite graph, and in each round we sample a duel of two candidates on one position and observe who wins in the duel, with the goal of finding the best candidate-position matching with high probability after multiple rounds of samples. CPE-DB is an adaptation of the original combinatorial pure exploration for multi-armed bandit (CPE-MAB) problem to the dueling bandit setting. We consider both the Borda winner and the Condorcet winner cases. For Borda winner, we establish a reduction of the problem to the original CPE-MAB setting and design PAC and exact algorithms that achieve both the sample complexity similar to that in the CPE-MAB setting (which is nearly optimal for a subclass of problems)  and polynomial running time per round. For Condorcet winner, we first design a fully polynomial time approximation scheme (FPTAS) for the offline problem of finding the Condorcet winner with known winning probabilities, and then use the FPTAS as an oracle to design a novel pure exploration algorithm CAR-Cond with sample complexity analysis. CAR-Cond is the first algorithm with polynomial running time per round for identifying the Condorcet winner in CPE-DB.
 Pretrained Generalized Autoregressive Model with Adaptive Probabilistic Label Cluster for Extreme Multi-label Text Classification 
 Extreme multi-label text classification (XMTC) is a task for tagging a given text with the most relevant labels from an extremely large label set. We propose a novel deep learning method called APLC-XLNet.   Our approach fine-tunes  the  recently  released  generalized  autoregressive  pretraining model (XLNet) to learn the dense representation for the input text.   We propose the Adaptive Probabilistic Label Cluster (APLC) to approximate the cross entropy loss by exploiting the  unbalanced  label  distribution  to  form  clusters that explicitly reduce the computational time. Our experiments, carried out on five benchmark datasets, show that our approach significantly outperforms existing state-of-the-art methods. The code of our method will be released publicly at GitHub.
 Finite-Time Convergence in Continuous-Time Optimization 
 In this paper, we investigate a Lyapunov-like differential inequality that allows us to establish finite-time stability of a continuous-time state-space dynamical system represented via a multivariate ordinary differential equation or differential inclusion. Equipped with this condition, we successfully synthesize first and second-order dynamical systems that achieve finite-time convergence to the minima of a given sufficiently regular cost function. As a byproduct, we show that the p-rescaled gradient flow (p-RGF) proposed by Wibisono et al. (2016) is indeed finite-time convergent, provided the cost function is gradient dominated of order q in (1,p). Thus, we effectively bridge a gap between the p-RGF and the normalized gradient flow (NGF) (p=\infty) proposed by Cortes (2006) in his seminal paper in the context of multi-agent systems. We discuss strategies to discretize our proposed flows and conclude by conducting some numerical experiments to illustrate our results.
 Optimizing Multiagent Cooperation via Policy Evolution and Shared Experiences 
 Many cooperative multiagent reinforcement learning environments provide agents with a sparse team-based reward, as well as a dense agent-specific reward that incentivizes learning basic skills. Training policies solely on the team-based reward is often difficult due to its sparsity. Also, relying solely on the agent-specific reward is sub-optimal because it usually does not capture the team coordination objective. A common approach is to use reward shaping to construct a proxy reward by combining the individual rewards. However, this requires manual tuning for each environment. We introduce Multiagent Evolutionary Reinforcement Learning (MERL), a split-level training platform that handles the two objectives separately through two optimization processes. An evolutionary algorithm maximizes the sparse team-based objective through neuroevolution on a population of teams. Concurrently, a gradient-based optimizer trains policies to only maximize the dense agent-specific rewards. The gradient-based policies are periodically added to the evolutionary population as a way of information transfer between the two optimization processes. This enables the evolutionary algorithm to use skills learned via the agent-specific rewards toward optimizing the global objective. Results demonstrate that MERL significantly outperforms state-of-the-art methods, such as MADDPG, on a number of difficult coordination benchmarks.
 How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization 
 Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.
 Message Passing Least Squares: A Unified Framework for Fast and Robust Group Synchronization 
 We propose an efficient algorithm for solving robust group synchronization given adversarially corrupted group ratios. We first present a theoretically guaranteed message passing algorithm that estimates the corruption levels of the group ratios. We then propose a novel weighted least squares method to estimate the group elements, where the weights are initialized using the estimated corruption levels and are iteratively updated  by incorporating cycle consistency information. We demonstrate the superior performance of our algorithm over state-of-the-art methods for $SO(3)$ synchronization using both synthetic and real data.
 Universal Asymptotic Optimality of Polyak Momentum 
 We consider the average-case runtime analysis of algorithms for minimizing quadratic objectives.
In this setting, and contrary to the more classical worst-case analysis,
non-asymptotic convergence rates and optimal algorithms depend on the full spectrum of the Hessian through its expected spectral distribution. 
Under mild assumptions, we show that these optimal methods converge asymptotically towards Polyak momentum \emph{independently} of the expected spectral density. This makes Polyak momentum universally (i.e., independent of the spectral distribution) asymptotically average-case optimal.
 Approximation Capabilities of Neural ODEs and Invertible Residual Networks 
 Recent interest in invertible models and normalizing flows has resulted in new architectures that ensure invertibility of the network model. Neural ODEs and i-ResNets are two recent techniques for constructing models that are invertible, but it is unclear if they can be used to approximate any continuous invertible mapping. Here, we show that out of the box, both of these architectures are limited in their approximation capabilities. We then show how to overcome this limitation: we prove that any homeomorphism on a $p$-dimensional Euclidean space can be approximated by a Neural ODE or an i-ResNet operating on a $2p$-dimensional Euclidean space. We conclude by showing that capping a Neural ODE or an i-ResNet with a single linear layer is sufficient to turn the model into a universal approximator for non-invertible continuous functions.
 Moniqua: Modulo Quantized Communication in Decentralized SGD 
 Running Stochastic Gradient Descent (SGD) in a decentralized fashion has shown promising results.
In this paper we propose Moniqua, a technique that allows decentralized SGD to use quantized communication.
We prove in theory that Moniqua communicates a provably bounded number of bits per iteration, while converging at the same asymptotic rate as the original algorithm does with full-precision communication.
Moniqua improves upon prior works in that it (1) requires zero additional memory, (2) works with 1-bit quantization, and (3) is applicable to a variety of decentralized algorithms.
We demonstrate empirically that Moniqua converges faster with respect to wall clock time than other quantized decentralized algorithms. 
We also show that Moniqua is robust to very low bit-budgets, allowing  $1$-bit-per-parameter communication without compromising validation accuracy when training ResNet20 and ResNet110 on CIFAR10.
 A general recurrent state space framework for modeling neural dynamics during decision-making 
 An open question in systems and computational neuroscience is how neural circuits accumulate evidence towards a decision. Fitting models of decision-making theory to neural activity helps answer this question, but current approaches limit the number of these models that we can fit to neural data. Here we propose a general framework for modeling neural activity during decision-making. The framework includes the canonical drift-diffusion model and enables extensions such as multi-dimensional accumulators, variable and collapsing boundaries, and discrete jumps. Our framework is based on constraining the parameters of recurrent state space models, for which we introduce a scalable variational Laplace-EM inference algorithm. We applied the modeling approach to spiking responses recorded from monkey parietal cortex during two decision-making tasks. We found that a two-dimensional accumulator better captured the trial-averaged responses of a set of parietal neurons than a single accumulator model. Next, we identified a variable lower boundary in the responses of a parietal neuron during a random dot motion task.
 Semiparametric Nonlinear Bipartite Graph Representation Learning with Provable Guarantees 
 Graph representation learning is a ubiquitous task in machine learning where the goal is to embed each vertex into a low-dimensional vector space. We consider the bipartite graph and formalize its representation learning problem as a statistical estimation problem of parameters in a semiparametric exponential family distribution. The bipartite graph is assumed to be generated by a semiparametric exponential family distribution, whose parametric component is given by the proximity of outputs of two one-layer neural networks, while nonparametric (nuisance) component is the base measure. Neural networks take high-dimensional features as inputs and output embedding vectors. In this setting, the representation learning problem is equivalent to recovering the weight matrices. The main challenges of estimation arise from the nonlinearity of activation functions and the nonparametric nuisance component of the distribution. To overcome these challenges, we propose a pseudo-likelihood objective based on the rank-order decomposition technique and focus on its local geometry. We show that the proposed objective is strongly convex in a neighborhood around the ground truth, so that a gradient descent-based method achieves linear convergence rate. Moreover, we prove that the sample complexity of the problem is linear in dimensions (up to logarithmic factors), which is consistent with parametric Gaussian models. However, our estimator is robust to any model misspecification within the exponential family, which is validated in extensive experiments.
 Invariant Causal Prediction for Block MDPs 
 Generalization across environments is critical to the successful application of reinforcement learning (RL) algorithms to real-world challenges. In this work we propose a method for learning state abstractions which generalize to novel observation distributions in the multi-environment RL setting. We prove that for certain classes of environments, this approach outputs, with high probability, a state abstraction corresponding to the causal feature set with respect to the return. We give empirical evidence that analogous methods for the nonlinear setting can also attain improved generalization over single- and multi-task baselines. Lastly, we provide bounds on model generalization error in the multi-environment setting, in the process showing a connection between causal variable identification and the state abstraction framework for MDPs.
 Characterizing Distribution Equivalence and Structure Learning for Cyclic and Acyclic Directed Graphs 
 The main approach to defining equivalence among acyclic directed causal graphical models is based on the conditional independence relationships in the distributions that the causal models can generate, in terms of the Markov equivalence. However, it is known that when cycles are allowed in the causal structure, conditional independence may not be a suitable notion for equivalence of two structures, as it does not reflect all the information in the distribution that is useful for identification of the underlying structure. In this paper, we present a general, unified notion of equivalence for linear Gaussian causal directed graphical models, whether they are cyclic or acyclic. In our proposed definition of equivalence, two structures are equivalent if they can generate the same set of data distributions. We also propose a weaker notion of equivalence called quasi-equivalence, which we show is the extent of identifiability from observational data. We propose analytic as well as graphical methods for characterizing the equivalence of two structures. Additionally, we propose a score-based method for learning the structure from observational data, which successfully deals with both acyclic and cyclic structures.
 Variational Inference for Sequential Data with Future Likelihood Estimates 
 The recent development of flexible and scalable variational inference algorithms has popularized the use of deep probabilistic models in a wide range of applications. However, learning and reasoning about high-dimensional models with non-differentiable densities are still a challenge. For such a model, inference algorithms struggle to estimate the gradients of variational objectives accurately, due to high variance in their estimates. To tackle this challenge, we present a novel variational inference algorithm for sequential data, which performs well even when the density from the model is not differentiable, for instance, due to the use of discrete random variables. The key feature of our algorithm is that it estimates future likelihoods at all time steps. The estimated future likelihoods form the core of our new low-variance gradient estimator. We formally analyze our gradient estimator from the perspective of variational objective, and show the effectiveness of our algorithm with synthetic and real datasets.
 What can I do here? A Theory of Affordances in Reinforcement Learning 
 Reinforcement learning algorithms usually assume that all actions are always available to an agent. However, both people and animals understand the general link between the features of their environment and the actions that are feasible. Gibson (1977) coined the term ``affordances'' to describe the fact that certain states enable an agent to do certain actions, in the context of embodied agents. In this paper, we develop a theory of affordances for agents who learn and plan in Markov Decision Processes. Affordances play a dual role in this case. On one hand, they allow faster planning, by reducing the number of actions available in any given situation. On the other hand, they facilitate more efficient and precise learning of transition models from data, especially when such models require function approximation. We establish these properties through theoretical results as well as illustrative examples. We also propose an approach to learn affordances and use it to estimate transition models that are simpler and generalize better.
 Generalisation error in learning with random features and the hidden manifold model 
 We study generalized linear regression and classification for a synthetically generated dataset encompassing different problems of interest, such as learning
with random features, neural networks in the lazy training regime, and the hidden manifold model.  We consider the high-dimensional regime and using the replica
method from statistical physics, we provide a closed-form expression for the asymptotic generalisation performance in these problems, valid in both the under- and over-parametrised regimes and for a broad choice of generalized linear model loss functions. In particular, we show how to obtain analytically the so-called double descent behaviour for logistic regression with a peak at the interpolation threshold, we illustrate the superiority of orthogonal against random Gaussian projections in learning with random features, and discuss the role played by correlations in the data generated by the hidden manifold model. Beyond the interest in these particular problems, the theoretical formalism introduced in this manuscript provides a path to further extensions to more complex tasks.
 Communication-Efficient Distributed PCA by Riemannian Optimization 
 In this paper, we study the leading eigenvector problem in a statistically distributed setting and propose a communication-efficient algorithm based on Riemannian optimization, which trades local computation for global communication. Theoretical analysis shows that the proposed algorithm linearly converges to the centralized empirical risk minimization solution regarding the number of communication rounds. When the number of data points in local machines is sufficiently large, the proposed algorithm achieves a significant reduction of communication cost over existing distributed PCA algorithms. Superior performance in terms of communication cost of the proposed algorithm is verified on real-world and synthetic datasets.
 Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure 
 In this paper we propose a scalable, unsupervised approach for detecting anomalies in the Internet of Things (IoT). Complex devices are connected daily and eagerly generate vast streams of multidimensional telemetry. These devices often operate in distinct modes based on external conditions (day/night, occupied/vacant, etc.), and to prevent complete or partial system outage, we would like to recognize as early as possible when these devices begin to operate outside the normal modes. We propose an unsupervised anomaly detection method that creates a negative sample from the positive, observed sample, and trains a classifier to distinguish between positive and negative samples. Using the Concentration Phenomenon, we explain why such a classifier ought to establish suitable decision boundaries between normal and anomalous regions, and show how Integrated Gradients can attribute the anomaly to specific dimensions within the anomalous state vector. We have demonstrated that negative sampling with random forest or neural network classifiers yield significantly higher AUC scores compared to state-of-the-art approaches against benchmark anomaly detection datasets, and a multidimensional, multimodal dataset from real climate control devices. Finally, we describe how negative sampling with neural network classifiers have been successfully deployed at large scale to predict failures in real time in over 15,000 climate-control and power meter devices in 145 office buildings within the California Bay Area.
 Fiduciary Bandits 
 Recommendation systems often face exploration-exploitation tradeoffs: the system can only learn about the desirability of new options by recommending them to some user. Such systems can thus be modeled as multi-armed bandit settings; however, users are self-interested and cannot be made to follow recommendations. We ask whether exploration can nevertheless be performed in a way that scrupulously respects agents' interests---i.e., by a system that acts as a \emph{fiduciary}. 
More formally, we introduce a  model  in  which  a recommendation system faces an exploration-exploitation tradeoff under the constraint that it can never recommend any action that it knows  yields lower reward in expectation than an agent would achieve if it acted alone. Our main contribution is a positive result: an asymptotically optimal, incentive compatible, and \emph{ex-ante} individually rational recommendation algorithm.
 MetaFun: Meta-Learning with Iterative Functional Updates 
 We develop a functional encoder-decoder approach to supervised meta-learning, where labeled data is encoded into an infinite-dimensional functional representation rather than a finite-dimensional one. Furthermore, rather than directly producing the representation, we learn a neural update rule resembling functional gradient descent which iteratively improves the representation. The final representation is used to condition the decoder to make predictions on unlabeled data. Our approach is the first to demonstrates the success of encoder-decoder style meta-learning methods like conditional neural processes on large-scale few-shot classification benchmarks such as miniImageNet and tieredImageNet, where it achieves state-of-the-art performance.
 No-Regret Exploration in Goal-Oriented Reinforcement Learning 
 Many popular reinforcement learning problems (e.g., navigation in a maze, some Atari games, mountain car) are instances of the episodic setting under its stochastic shortest path (SSP) formulation, where an agent has to achieve a goal state while minimizing the cumulative cost. Despite the popularity of this setting, the exploration-exploitation dilemma has been sparsely studied in general SSP problems, with most of the theoretical literature focusing on different problems (i.e., fixed-horizon and infinite-horizon) or making the restrictive loop-free SSP assumption (i.e., no state can be visited twice during an episode). In this paper, we study the general SSP problem with no assumption on its dynamics (some policies may actually never reach the goal). We introduce UC-SSP, the first no-regret algorithm in this setting, and prove a regret bound scaling as $\widetilde{\mathcal{O}}( D S \sqrt{ A D K})$ after $K$ episodes for any unknown SSP with $S$ states, $A$ actions, positive costs and SSP-diameter $D$, defined as the smallest expected hitting time from any starting state to the goal. We achieve this result by crafting a novel stopping rule, such that UC-SSP may interrupt the current policy if it is taking too long to achieve the goal and switch to alternative policies that are designed to rapidly terminate the episode.
 On Variational Learning of Controllable Representations for Text without Supervision 
 The variational autoencoder (VAE) can learn the manifold of natural images on certain datasets, as evidenced by meaningful interpolating or extrapolating in the continuous latent space. However, on discrete data such as text, it is unclear if unsupervised learning can discover similar latent space that allows controllable manipulation. In this work, we find that sequence VAEs trained on text fail to properly decode when the latent codes are manipulated, because the modified codes often land in holes or vacant regions in the aggregated posterior latent space, where the decoding network fails to generalize. Both as a validation of the explanation and as a fix to the problem, we propose to constrain the posterior mean to a learned probability simplex, and performs manipulation within this simplex. Our proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, our method outperforms unsupervised baselines and strong supervised approaches on text style transfer. On automatic evaluation metrics used in text style transfer, even with the decoding network trained from scratch, our method achieves comparable results with state-of-the-art supervised approaches leveraging large-scale pre-trained models for generation. Furthermore, it is capable of performing more flexible fine-grained control over text generation than existing methods.
 Transfer Learning without Knowing: Reprogramming Black-box Machine Learning Models with Scarce Data and Limited Resources 
 Current transfer learning methods are mainly based on finetuning a pretrained model with target-domain data. Motivated by the techniques from adversarial machine learning (ML) that are capable of manipulating the model prediction via data perturbations, in this paper we propose a novel approach, black-box adversarial reprogramming (BAR), that repurposes a well-trained black-box ML model (e.g., a prediction API or a proprietary software) for solving different ML tasks, especially in the scenario with scarce data and constrained resources. The rationale lies in exploiting high-performance but unknown ML models to gain learning capability for transfer learning. Using zeroth order optimization and multi-label mapping techniques, BAR can reprogram a black-box ML model solely based on its input-output responses without knowing the model architecture or changing any parameter. More importantly, in the limited medical data setting, on autism spectrum disorder classification, diabetic retinopathy detection, and melanoma detection tasks, BAR outperforms state-of-the-art methods and yields comparable performance to the vanilla adversarial reprogramming method requiring complete knowledge of the target ML model. BAR also outperforms baseline transfer learning approaches by a significant margin, demonstrating cost-effective means and new insights for transfer learning.
 Learning Autoencoders with Relational Regularization 
 We propose a new algorithmic framework for learning autoencoders of data distributions. 
In this framework, we minimize the discrepancy between the model distribution and the target one, with relational regularization on learnable latent prior. 
This regularization penalizes the fused Gromov-Wasserstein (FGW) distance between the latent prior and its corresponding posterior, which allows us to learn a structured prior distribution associated with the generative model in a flexible way. 
Moreover, it helps us co-train multiple autoencoders even if they are with heterogeneous architectures and incomparable latent spaces. 
We implement the framework with two scalable algorithms, making it applicable for both probabilistic and deterministic autoencoders. 
Our relational regularized autoencoder (RAE) outperforms existing methods, e.g., variational autoencoder, Wasserstein autoencoder, and their variants, on generating images. 
Additionally, our relational co-training strategy of autoencoders achieves encouraging results in both synthesis and real-world multi-view learning tasks.
 Fast and Private Submodular and $k$-Submodular Functions Maximization with Matroid Constraints 
 The problem of maximizing nonnegative monotone submodular functions under a certain constraint has been intensively studied in the last decade, and a wide range of efficient approximation algorithms have been developed for this problem. Many machine learning problems, including data summarization and influence maximization, can be naturally modeled as the problem of maximizing monotone submodular functions. However, when such applications involve sensitive data about individuals, their privacy concerns should be addressed. In this paper, we study the problem of maximizing monotone submodular functions subject to matroid constraints in the framework of differential privacy. We provide $(1-\frac{1}{\mathrm{e}})$-approximation algorithm which improves upon the previous results in terms of approximation guarantee. This is done with an almost cubic number of function evaluations in our algorithm.

Moreover, we study $k$-submodularity, a natural generalization of submodularity. We give the first $\frac{1}{2}$-approximation algorithm that preserves differential privacy for maximizing monotone $k$-submodular functions subject to matroid constraints. The approximation ratio is asymptotically tight and is obtained with an almost linear number of function evaluations.
 One Size Fits All: Can We Train One Denoiser for All Noise Levels? 
 When using deep neural networks for estimating signals such as denoising images, it is generally preferred to train one network and apply it to all noise levels. The de facto training protocol to achieve this goal is to train the network with noisy samples whose noise levels are uniformly distributed across the range of interest. However, why should we allocate the samples uniformly? Can we have more training samples that are less noisy, and fewer samples that are more noisy? What is the optimal distribution? How do we obtain such optimal distribution? The goal of this paper is to address these questions. In particular, we show that the sampling problem can be formulated as a minimax risk optimization. We show that, although the neural networks are non-convex, the minimax problem itself is convex. We derive a dual ascent algorithm to determine the optimal distribution of which the convergence is guaranteed. We show that the framework is general not only to denoising but any trainable estimators where there is a range of uncertainty conditions. We demonstrate applications in image denoising, low-light reconstruction, and super-resolution.
 Efficient Robustness Certificates for Discrete Data: Sparsity-Aware Randomized Smoothing for Graphs, Images and More 
 Existing techniques for certifying robustness of models for discrete data either work only for a small class of models or are general at the expense of efficiency or tightness. Moreover, they do not account for sparsity in the input which, as our findings show, is often essential for obtaining non-trivial guarantees. We propose a model-agnostic certificate based on the randomized smoothing framework which subsumes earlier work and is tight, efficient, and sparsity-aware. Its computational complexity does not depend on the number of discrete categories or the dimension of the input (e.g. the graph size), making it highly scalable. We show the effectiveness of our approach on a wide variety of models, datasets, and tasks -- specifically highlighting its use for Graph Neural Networks. GNNs have become widely used, yet are highly sensitive to adversarial attacks. So far, obtaining provable guarantees has been difficult due to the discrete and non-i.i.d. nature of graph data. Our method can certify any GNN and handles perturbations to both the graph structure and the node attributes.
 The Cost-free Nature of Optimally Tuning Tikhonov Regularizers and Other Ordered Smoothers 
 We consider the problem of selecting the best estimator among a family of Tikhonov regularized estimators, or, alternatively, to select a linear combination of these regularizers that is as good as the best regularizer in the family. Our theory reveals that if the Tikhonov regularizers share the same penalty matrix with different tuning parameters, a convex procedure based on $Q$-aggregation achieves the mean square error of the best estimator, up to a small error term no larger than $C\sigma^2$, where $\sigma^2$ is the noise level and $C>0$ is an absolute constant. Remarkably, the error term does not depend on the penalty matrix or the number of estimators as long as they share the same penalty matrix, i.e., it applies to any grid of tuning parameters, no matter how large the cardinality of the grid is. This reveals the surprising "cost-free" nature of optimally tuning Tikhonov regularizers, in striking contrast with the existing literature on aggregation of estimators where one typically has to pay a cost of $\sigma^2\log(M)$ where $M$ is the number of estimators in the family. The result holds, more generally, for any family of ordered linear smoothers. This encompasses Ridge regression as well as Principal Component Regression. The result is extended to the problem of tuning Tikhonov regularizers with different penalty matrices.
 Mutual Transfer Learning for Massive Data 
 In the transfer learning problem, the target and the source data domains are typically known. In this article, we study a new paradigm called mutual transfer learning where among many heterogeneous data domains, every data domain could potentially be the target of interest, and it could also be a useful source to help the learning in other data domains. However, it is important to note that given a target not every data domain can be a successful source; only data sets that are similar enough to be thought as from the same population can be useful sources for each other. Under this mutual learnability assumption, a confidence distribution fusion approach is proposed to recover the mutual learnability relation in the transfer learning regime. Our proposed method achieves the same oracle statistical inferential accuracy as if the true learnability structure were known. It can be implemented in an efficient parallel fashion to deal with large-scale data. Simulated and real examples are analyzed to illustrate the usefulness of the proposed method.
 Weakly-Supervised Disentanglement Without Compromises 
 Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation.
First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed.
Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.
 Working Memory Graphs 
 Transformers have increasingly outperformed gated RNNs in obtaining new state-of-the-art results on supervised tasks involving text sequences. 
Inspired by this trend, we study the question of how Transformer-based models can improve the performance of sequential decision-making agents.
We present the Working Memory Graph (WMG), an agent that employs multi-head self-attention to reason over a dynamic set of vectors representing observed and recurrent state.
We evaluate WMG in three environments featuring factored observation spaces: a Pathfinding environment that requires complex reasoning over past observations, BabyAI gridworld levels that involve text instructions, and Sokoban which emphasizes future planning.
We find that the combination of WMG's Transformer-based architecture with factored observation spaces leads to significant gains in learning efficiency compared to other architectures across all tasks.
Our results imply that for environments where it is possible to factorize environment observations, WMG's Transformer-based architecture can dramatically boost sample efficiency.
 Sparsified Linear Programming for Zero-Sum Equilibrium Finding 
 Computational equilibrium finding in large zero-sum extensive-form imperfect-information games has led to significant recent AI breakthroughs. The fastest algorithms for the problem are new forms of counterfactual regret minimization (Brown & Sandholm, 2019). In this paper we present a totally different approach to the problem, which is competitive and often orders of magnitude better than the prior state of the art. The equilibrium-finding problem can be formulated as a linear program (LP) (Koller et al., 1994), but solving it as an LP has not been scalable due to the memory requirements of LP solvers, which can often be quadratically worse than CFR-based algorithms. We give an efficient practical algorithm that factors a large payoff matrix into a product of two matrices that are typically dramatically sparser. This allows us to express the equilibrium-finding problem as a linear program with size only a logarithmic factor worse than CFR, and thus allows linear program solvers to run on such games. With experiments on poker endgames, we demonstrate in practice, for the first time, that modern linear program solvers are competitive against even game-specific modern variants of CFR in solving large extensive-form games, and can be used to compute exact solutions unlike iterative algorithms like CFR.
 Predicting Choice with Set-Dependent Aggregation 
 Providing users with alternatives to choose from is an essential component of many online platforms, making the accurate prediction of choice vital to their success. A renewed interest in learning choice models has led to improved modeling power, but most current methods are either limited in the type of choice behavior they capture, cannot be applied to large-scale data, or both.

Here we propose a learning framework for predicting choice that is accurate, versatile, and theoretically grounded. Our key modeling point is that to account for how humans choose, predictive models must be expressive enough to accommodate complex choice patterns but structured enough to retain statistical efficiency. Building on recent results in economics, we derive a class of models that achieves this balance, and propose a neural implementation that allows for scalable end-to-end training. Experiments on three large choice datasets demonstrate the utility of our approach.
 The Sample Complexity of Best-$k$ Items Selection from Pairwise Comparisons 
 This paper studies the sample complexity (aka number of comparisons) bounds for the active best-$k$ items selection from pairwise comparisons. From a given set of items, the learner can make pairwise comparisons on every pair of items, and each comparison returns an independent noisy result about the preferred item. At any time, the learner can adaptively choose a pair of items to compare according to past observations (i.e., active learning). The learner's goal is to find the (approximately) best-$k$ items with a given confidence while trying to use as few comparisons as possible. In this paper, we study two problems: (i) finding the probably approximately correct (PAC) best-$k$ items and (ii) finding the exact best-$k$ items, both under strong stochastic transitivity and stochastic triangle inequality. For PAC best-$k$ items selection, we first show a lower bound and then propose an algorithm whose sample complexity upper bound matches the lower bound up to a constant factor. For the exact best-$k$ items selection, we first prove a worst-instance lower bound. We then propose two algorithms based on our PAC best items selection algorithms, of which one works for $k=1$ and is sample complexity optimal up to a loglog factor, and the other works for all values of $k$ and is sample complexity optimal up to a log factor. 
 Robustness to Spurious Correlations via Human Annotations 
 The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption---useful correlations between features and labels at training time can become useless or even harmful at test time. For example, high obesity is generally predictive for heart disease, but this relation may not hold for smokers who generally have lower rates of obesity and higher rates of heart disease. We present a framework for making models robust to spurious correlations by leveraging humans' common sense knowledge of causality. Specifically, we use human annotation to augment each training example with a potential unmeasured variable (i.e. an underweight patient with heart disease may be a smoker), which reduces the problem to a covariate shift problem. We then introduce a new distributionally robust optimization objective over unmeasured variables (UV-DRO) to control the worst-case loss over possible test-time shifts. Empirically, we show 5--10% improvements on a digit recognition task confounded by rotation, and 1.5--5% gains on the task of predicting arrests from NYPD Police Stops confounded by location.
 VFlow: More Expressive Generative Flows with Variational Data Augmentation 
 Generative flows are promising tractable models for density modeling that define probabilistic distributions with invertible transformations. However, tractability imposes architectural constraints on generative flows. In this work, we study a previously overlooked constraint that all the intermediate representations must have the same dimensionality with the data due to invertibility, limiting the width of the network. We propose VFlow to tackle this constraint on dimensionality. VFlow augments the data with extra dimensions and defines a maximum evidence lower bound (ELBO) objective for estimating the distribution of augmented data jointly with the variational data augmentation distribution. Under mild assumptions, we show that the maximum ELBO solution of VFlow is always better than the original maximum likelihood solution. For image density modeling on the CIFAR-10 dataset, VFlow achieves a new state-of-the-art 2.98 bits per dimension.
 Boosting Deep Neural Network Efficiency with Dual-Module Inference 
 Using Deep Neural Networks (DNNs) in machine learning tasks is promising in delivering high-quality results but challenging to meet stringent latency requirements and energy constraints because of the memory-bound and the compute-bound execution pattern of DNNs. We propose a big-little dual-module inference to dynamically skip unnecessary memory access and computation to speedup DNN inference. Leveraging the error-resilient feature of nonlinear activation functions used in DNNs, we propose to use a lightweight little module that approximates the original DNN layer, which is referred to as the big module, to compute activations of the insensitive region that are more error-resilient. The expensive memory access and computation of the big module can be reduced as the results are only used in the sensitive region. For memory-bound models, our method can reduce the overall memory access by 40% on average and achieve 1.54x to 1.75x speedup on a commodity CPU-based server platform with a negligible impact on model quality. In addition, our method can reduce the operations of the compute-bound ResNet model by 3.02x, with only a 0.5% accuracy drop.
 Option Discovery in the Absence of Rewards with Manifold Analysis 
 Options have been shown to be an effective tool in reinforcement learning, facilitating improved exploration and learning. In this paper, we present an approach based on spectral graph theory and derive an algorithm that systematically discovers options without access to a specific reward or task assignment. As opposed to the common practice used in previous methods, our algorithm makes full use of the spectrum of the graph Laplacian.
Incorporating modes associated with higher graph frequencies unravels domain subtleties, which are shown to be useful for option discovery. Using geometric and manifold-based analysis, we present a theoretical justification for the algorithm. In addition, we showcase its performance on several domains, demonstrating clear improvements compared to competing methods.
 Can autonomous vehicles identify, recover from, and adapt to distribution shifts? 
 Out-of-distribution (OOD) driving scenarios are a common failure of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaption to OOD scenes can mitigate their adverse effects. However, no benchmark evaluating OOD detection and adaption currently exists to compare methods. In this paper, we introduce an autonomous car novel-scene benchmark, \texttt{CARNOVEL}, to evaluate the robustness of driving agents to a suite of tasks involving distribution shift. We also highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called \emph{robust imitative planning} (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident but catastrophic extrapolations in out-of-training-distribution scenes. When the model's uncertainty quantification is insufficient to suggest a safe course of action by itself, it is used to query the driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term \emph{adaptive robust imitative planning} (AdaRIP).
 The Effect of Natural Distribution Shift on Question Answering Models 
 We build four new test sets for the Stanford Question Answering Dataset (SQuAD) and evaluate the ability of question-answering systems to generalize to new data. In the original Wikipedia domain, we find no evidence of adaptive overfitting despite several years of test-set reuse. On datasets derived from New York Times articles, Reddit posts, and Amazon product reviews, we observe average performance drops of 3.0, 12.6, and 14.0 F1, respectively, across a broad range of models. In contrast, a strong human baseline matches or exceeds the performance of SQuAD models on the original domain and exhibits little to no drop in new domains. Taken together, our results confirm the surprising resilience of the holdout method and emphasize the need to move towards evaluation metrics that incorporate robustness to natural distribution shifts.
 Learning Optimal Tree Models under Beam Search 
 Retrieving relevant targets from an extremely large target set under computation and time limits is a common challenge for information retrieval and recommendation systems. Tree models, which formulate targets as leaves in a tree hierarchy and associate tree nodes with trainable node-wise scorers, have attracted a lot of interests in tackling this challenge due to its logarithmic computational complexity in both training and testing. Tree-based deep models (TDMs) and probabilistic label trees (PLTs) are two kinds of representative tree models. Though achieving many practical successes, existing tree models still suffer from training-testing discrepancy: in testing they usually leverage beam search to retrieve targets from the tree, which is not considered in the training loss function. As a result, even the optimal node-wise scorers with respect to the training loss can lead to suboptimal retrieval results when they are used in testing to retrieve targets via beam search. 
In this paper, we take a first step towards understanding the discrepancy by developing the definition of Bayes optimality and calibration under beam search as general analyzing tools, and prove that neither TDMs nor PLTs are Bayes optimal under beam search. To eliminating the discrepancy, we propose a novel training loss function with a beam search based subsampling method for training Bayes optimal tree models under beam search. Experiments on both synthetic and real data verify our analysis and demonstrate the superiority of our methods.

 Enhanced POET: Open-ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions 
 Creating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges, and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential.  Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved through other means.
 Kernelized Stein Discrepancy Tests of Goodness-of-fit  for Time-to-Event Data 
 Survival Analysis and Reliability Theory are concerned with the analysis of time-to-event data, in which observations correspond to waiting times until an event of interest, such as death from a particular disease or failure of a component in a mechanical system. This type of data is unique due to the presence of censoring, a type of missing data that occurs when we do not observe the actual time of the event of interest but instead we have access to an approximation for it given by random interval in which the observation is known to belong.

Most traditional methods are not designed to deal with censoring, and thus we need to adapt them to censored time-to-event data. In this paper, we focus on non-parametric Goodness-of-Fit testing procedures based on combining the Stein's method and kernelized discrepancies. While for uncensored data, there is a natural way of implementing a kernelized Stein discrepancy test, for censored data there are several options, each of them with different advantages and disadvantages. In this paper we propose a collection of kernelized Stein discrepancy tests for time-to-event data, and we study each of them theoretically and empirically. Our experimental results show that our proposed methods perform better than existing tests, including previous tests based on a kernelized maximum mean discrepancy.
 Bounding the fairness and accuracy of classifiers from population statistics 
 We consider the study of a classification model whose properties are impossible to estimate using a validation set, either due to the absence of such a set or because access to the classifier, even as a black-box, is impossible. Instead, only aggregate statistics on the rate of positive predictions in each of several sub-populations are available, as well as the true rates of positive labels in each of these sub-populations.  We show that these aggregate statistics can be used to lower-bound the discrepancy of a classifier, which is a measure that balances inaccuracy and unfairness. To this end, we define a new measure of unfairness, equal to the fraction of the population on which the classifier behaves differently, compared to its global, ideally fair behavior, as defined by the measure of equalized odds.  We propose an efficient and practical procedure for finding the best possible lower bound on the discrepancy of the classifier, given the aggregate statistics, and demonstrate in experiments the empirical tightness of this lower bound, as well as its possible uses on various types of problems, ranging from estimating the quality of voting polls to measuring the effectiveness of patient identification from internet search queries. The code and data are available at https://github.com/sivansabato/bfa.
 Linear Mode Connectivity and the Lottery Ticket Hypothesis 
 We introduce "instability analysis," which assesses whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise. We find that standard vision models become stable in this way early in training. From then on, the outcome of optimization is determined to within a linearly connected region.

We use instability to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained to full accuracy from initialization. We find that these subnetworks only reach full accuracy when they are stable, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (Resnet-50 and Inception-v3 on ImageNet).
 Gradient Temporal-Difference Learning with Regularized Corrections 
 Value function learning remains a critical component of many reinforcement learning systems. Many algorithms are based on temporal difference (TD) updates, which have well-documented divergence issues, even though potentially sound alternatives exist like Gradient TD. Unsound approaches like Q-learning and TD remain popular because divergence seems rare in practice and these algorithms typically perform well. However, recent work with large neural network learning systems reveals that instability is more common than previously thought. Practitioners face a difficult dilemma: choose an easy to use and performant TD method, or a more complex algorithm that is more sound but harder to tune, less sample efficient, and underexplored with control. In this paper, we introduce a new method called TD with Regularized Corrections (TDRC), that attempts to balance ease of use, soundness, and performance. It behaves as well as TD, when TD performs well, but is sound even in cases where TD diverges. We characterize the expected update for TDRC, and show that it inherits soundness guarantees from Gradient TD, and converges to the same solution as TD. Empirically, TDRC exhibits good performance and low parameter sensitivity across several problems.
 Double Reinforcement Learning for Efficient and Robust Off-Policy Evaluation 
 Off-policy evaluation (OPE) in reinforcement learning allows one to evaluate novel decision policies without needing to conduct exploration, which is often costly or otherwise infeasible. We consider for the first time the semiparametric efficiency limits of OPE in Markov decision processes (MDPs), where actions, rewards, and states are memoryless. We show existing OPE estimators may fail to be efficient in this setting. We develop a new estimator based on cross-fold estimation of $q$-functions and marginalized density ratios, which we term double reinforcement learning (DRL). We show that DRL is efficient when both components are estimated at fourth-root rates and is also doubly robust when only one component is consistent.
We investigate these properties empirically and demonstrate the performance benefits due to harnessing memorylessness.
 Learning Representations that Support Extrapolation 
 Extrapolation -- the ability to make inferences that go beyond the scope of one's experiences -- is a hallmark of human intelligence. By contrast, the generalization exhibited by contemporary neural network algorithms is largely limited to interpolation between data points in their training corpora. In this paper, we consider the challenge of learning representations that support extrapolation. We introduce a novel visual analogy benchmark that allows the graded evaluation of extrapolation as a function of distance from the convex domain defined by the training data. We also introduce a simple technique, context normalization, that encourages representations that emphasize the relations between objects. We find that this technique enables a significant improvement in the ability to extrapolate, considerably outperforming a number of competitive techniques.

 On Leveraging Pretrained GANs for Generation with Limited Data 
 Recent work has shown generative adversarial networks (GANs) can generate highly realistic images, that are often indistinguishable (by humans) from real images. Most images so generated are not contained in the training dataset, suggesting potential for augmenting training sets with GAN-generated data. While this scenario is of particular relevance when there are limited data available, there is still the issue of training the GAN itself based on that limited data. To facilitate this, we leverage existing GAN models pretrained on large-scale datasets (like ImageNet) to introduce additional knowledge (which may not exist within the limited data), following the concept of transfer learning. Demonstrated by natural-image generation, we reveal that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be transferred to facilitate generation in a perceptually-distinct target domain with limited training data. To further adapt the transferred filters to the target domain, we propose adaptive filter modulation (AdaFM). An extensive set of experiments is presented to demonstrate the effectiveness of the proposed techniques on generation with limited data.
 Alleviating Privacy Attacks via Causal Learning 
 Machine learning models, especially deep neural networks have been shown to reveal membership information of inputs in the training data.
Such membership inference attacks are a serious
privacy concern, for example, patients providing
medical records to build a model that detects HIV
would not want their identity to be leaked. Further, we show that the attack accuracy amplifies when the model is used to predict samples that
come from a different distribution than the training set, which is often the case in real world applications. Therefore, we propose the use of causal
learning approaches where a model learns the
causal relationship between the input features and
the outcome. An ideal causal model is known
to be invariant to the training distribution and
hence generalizes well to shifts between samples
from the same distribution and across different
distributions. First, we prove that models learned
using causal structure provide stronger differential privacy guarantees than associational models under reasonable assumptions. Next, we show
that causal models trained on sufficiently large
samples are robust to membership inference attacks across different distributions of datasets and those trained on smaller sample sizes always have
lower attack accuracy than corresponding associational models. Finally, we confirm our theoretical claims with experimental evaluation on 4 datasets
with moderately complex Bayesian networks and
an image dataset of colored MNIST. We observe that neural
network-based associational models exhibit upto
80% attack accuracy under different test distributions and sample sizes whereas causal models exhibit attack accuracy close to a random guess.
Our results confirm the value of the generalizability of causal models in reducing susceptibility to privacy attacks.
 From Importance Sampling to Doubly Robust Policy Gradient 
 We show that on-policy policy gradient (PG) and its variance reduction variants can be derived by taking finite-difference of function evaluations supplied by estimators from the importance sampling (IS) family for off-policy evaluation (OPE). Starting from the doubly robust (DR) estimator (Jiang & Li, 2016), we provide a simple derivation of a very general and flexible form of PG, which subsumes the state-of-the-art variance reduction technique (Cheng et al., 2019) as its special case and immediately hints at further variance reduction opportunities overlooked by existing literature. We analyze the variance of the new DR-PG estimator, compare it to existing methods as well as the Cramer-Rao lower bound of policy gradient, and empirically show its effectiveness.
 Optimal Statistical Guaratees for Adversarially Robust Gaussian Classification 
 Adversarial robustness has become a fundamental requirement in modern machine learning applications. Yet, there has been surprisingly little statistical understanding so far. In this paper, we provide the first result of the \emph{optimal} minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model proposed by \cite{schmidt2018adversarially}. The results are stated in terms of the \emph{Adversarial Signal-to-Noise Ratio (AdvSNR)}, which
generalizes a similar notion for standard linear classification to the adversarial setting. For the Gaussian mixtures with AdvSNR value of $r$, we prove an excess risk lower bound of order $\Theta(e^{-(\frac{1}{2}+o(1)) r^2} \frac{d}{n})$ and design a computationally efficient estimator that achieves this optimal rate. Our results built upon minimal assumptions while cover a wide spectrum of adversarial perturbations including $\ell_p$ balls for any $p \ge 1$.
 Continuous Graph Neural Networks 
 This  paper  builds  on  the  connection  between graph  neural  networks  and  traditional  dynamical systems. We propose continuous graph neural networks (CGNN), which generalise existing graph neural networks with discrete dynamics in that they can be viewed as a specific discretisation scheme. The key idea is how to characterise the continuous dynamics of node representations, i.e. the derivatives of node representations, w.r.t. time.Inspired by existing diffusion-based methods on graphs (e.g. PageRank and epidemic models on social networks), we define the derivatives as a combination of the current node representations,the representations of neighbors, and the initial values of the nodes. We propose and analyse two possible dynamics on graphs—including each dimension of node representations (a.k.a. the feature channel) change independently or interact with each other—both with theoretical justification. The proposed continuous graph neural net-works are robust to over-smoothing and hence allow us to build deeper networks, which in turn are able to capture the long-range dependencies between nodes. Experimental results on the task of node classification demonstrate the effectiveness of our proposed approach over competitive baselines.
 Hypernetwork approach to generating point clouds 
 In this work, we propose a novel method for generating 3D point clouds that leverage properties of hyper networks. Contrary to the existing methods that learn only the representation of a 3D object, our approach simultaneously finds a representation of the object and its 3D surfaces. The main idea of our HyperCloud method is to build a hyper network that returns weights of a particular neural network (target network) trained to map points from a uniform unit ball distribution into a 3D shape. As a consequence, a particular 3D shape can be generated using point-by-point sampling from the assumed prior distribution and transforming sampled points with the target network. Since the hyper network is based on an auto-encoder architecture trained to reconstruct realistic 3D shapes, the target network weights can be considered a parametrisation of the surface of a 3D shape, and not a standard representation of point cloud usually returned by competitive approaches. The proposed architecture allows to find mesh-based representation of 3D objects in a generative manner, while providing point clouds en pair in quality with the state-of-the-art methods. 


 “Other-Play” for Zero-Shot Coordination 
 We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g.humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents as well as with human players than SP agents.
 Designing Optimal Dynamic Treatment Regimes: A Causal Reinforcement Learning Approach 
 A dynamic treatment regime (DTR) consists of a sequence of decision rules, one per stage of intervention, that dictates how to determine the treatment assignment to patients based on evolving treatments and covariates' history. These regimes are particularly effective for managing chronic disorders and is arguably one of the critical ingredients underlying more personalized decision-making systems. All reinforcement learning algorithms for finding the optimal DTR in online settings will suffer O(\sqrt{|D_{X, S}|T}) regret on some environments, where T is the number of experiments, and D_{X, S} is the domains of treatments X and covariates S. This implies T = O (|D_{X, S}|) trials to generate an optimal DTR. In many applications, domains of X and S could be so enormous that the time required to ensure appropriate learning may be unattainable. We show that, if the causal diagram of the underlying environment is provided, one could achieve regret that is exponentially smaller than D_{X, S}. In particular, we develop two online algorithms that satisfy such regret bounds by exploiting the causal structure underlying the DTR; one is based on the principle of optimism in the face of uncertainty (OFU-DTR), and the other uses the posterior sampling learning (PS-DTR). Finally, we introduce efficient methods to accelerate these online learning procedures by leveraging the abundant, yet biased observational (non-experimental) data.
 Neural Clustering Processes 
 Probabilistic clustering models (or equivalently, mixture models) are basic building blocks in countless statistical models and involve latent random variables over discrete spaces. For these models, posterior inference methods can be inaccurate and/or very slow. In this work we introduce deep network architectures trained with labeled samples from any generative model of  clustered datasets. At test time, the networks generate approximate posterior samples of cluster labels for any new dataset of arbitrary size. We develop two complementary approaches to this task, requiring  either O(N) or O(K) network forward passes per dataset, where N is the dataset size and  K the number of clusters. Unlike previous approaches, our methods sample the labels of all the data points from a well-defined posterior, and can learn nonparametric Bayesian posteriors since they do not limit the number of mixture components. As a scientific application, we present a novel approach to neural spike sorting for high-density multielectrode arrays. 
 Neural Kernels Without Tangents 
 We investigate the connections between neural networks and simple building blocks in kernel space. In particular, using well established feature space tools such as direct sum, averaging, and moment lifting, we present an algebra for creating “compositional” kernels from bags of features. We show that these operations correspond to many of the building blocks of “neural tangent kernels (NTK)”. Experimentally, we show that there is a correlation in test error between neural network architectures and the associated kernels. We construct a simple neural network architecture using only 3x3 convolutions, 2x2 average pooling, ReLU, and optimized with SGD and MSE loss that achieves 96% accuracy on CIFAR10, and whose corresponding compositional kernel achieves 90% accuracy. We also use our constructions to investigate the relative performance of neural networks, NTKs, and compositional kernels in the small dataset regime. In particular, we find that compositional kernels outperform NTKs and neural networks outperform both kernel methods.
 Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data 
 The translation equivariance of convolutional layers enables CNNs to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data.
We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.
 Improving Molecular Design by Stochastic Iterative Target Augmentation 
 Generative models in molecular design tend to be richly parameterized, data-hungry neural models, as they must create complex structured objects as outputs. Estimating such models from data may be challenging due to the lack of sufficient training data. In this paper, we propose a surprisingly effective self-training approach for iteratively creating additional molecular targets. We first pre-train the generative model together with a simple property predictor. The property predictor is then used as a likelihood model for filtering candidate structures from the generative model. Additional targets are iteratively produced and used in the course of stochastic EM iterations to maximize the log-likelihood that the candidate structures are accepted. A simple rejection (re-weighting) sampler suffices to draw posterior samples since the generative model is already reasonable after pre-training. We demonstrate significant gains over strong baselines for both unconditional and conditional molecular design. In particular, our approach outperforms the previous state-of-the-art in conditional molecular design by over 10% in absolute gain.
 Schatten Norms in Matrix Streams: Hello Sparsity, Goodbye Dimension 
 Spectral functions of large matrices contains important structural information about the underlying data, and is thus becoming increasingly important. Many times, large matrices representing real-world data are sparse or doubly sparse (i.e., sparse in both rows and columns), and are accessed as a stream of updates, typically organized in row-order. In this setting, where space (memory) is the limiting resource, all known algorithms require space that is polynomial in the dimension of the matrix, even for sparse matrices. We address this challenge by providing the first algorithms whose space requirement is independent of the matrix dimension, assuming the matrix is doubly-sparse and presented in row-order. Our algorithms approximate the Schatten p-norms, which we use in turn to approximate other spectral functions, such as logarithm of the determinant, trace of matrix inverse, and Estrada index. We validate these theoretical performance bounds by numerical experiments on real-world matrices representing social networks. We further prove that multiple passes are unavoidable in this setting, and show extensions of our primary technique, including a trade-off between space requirements and number of passes.
 Breaking the Curse of Space Explosion: Towards Efficient NAS with Curriculum Search 
 Neural architecture search (NAS) has become an important approach to automatically find effective architectures. To cover all possible good architectures, we need to search in an extremely large search space with billions of candidate architectures. More critically, given a large search space, we may face a very challenging issue of space explosion. However, due to the limitation of computational resources, we can only sample a very small proportion of the architectures, which provides insufficient information for the training. As a result, existing methods may often produce suboptimal architectures. To alleviate this issue, we propose a curriculum search method that starts from a small search space and gradually incorporates the learned knowledge to guide the search in a large space. With the proposed search strategy, our Curriculum Neural Architecture Search (CNAS) method significantly improves the search efficiency and finds better architectures than existing NAS methods. Extensive experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of the proposed method.
 SIGUA: Forgetting May Make Learning with Noisy Labels More Robust 
 Given data with noisy labels, over-parameterized deep networks can gradually memorize the data, and fit everything in the end. Although equipped with corrections for noisy labels, many learning methods in this area still suffer overfitting due to undesired memorization. In this paper, to relieve this issue, we propose stochastic integrated gradient underweighted ascent (SIGUA): in a mini-batch, we adopt gradient descent on good data as usual, and learning-rate-reduced gradient ascent} on bad data; the proposal is a versatile approach where data goodness or badness is w.r.t. desired or undesired memorization given a base learning method. Technically, SIGUA pulls optimization back for generalization when their goals conflict with each other; philosophically, SIGUA shows forgetting undesired memorization can reinforce desired memorization. Experiments demonstrate that SIGUA successfully robustifies two typical base learning methods, so that their performance is often significantly improved.
 Word-Level Speech Recognition With a Letter to Word Encoder 
 We propose a direct-to-word sequence model which uses a word network to learn word embeddings from letters. The word network can be integrated seamlessly with arbitrary sequence models including Connectionist Temporal Classification and encoder-decoder models with attention. We show our direct-to-word model can achieve word error rate gains over sub-word level models for speech recognition. We also show that our direct-to-word approach retains the ability to predict words not seen at training time without any retraining. Finally, we demonstrate that a word-level model can use a larger stride than a sub-word level model while maintaining accuracy. This makes the model more efficient both for training and inference.
 Online Bayesian Moment Matching based SAT Solver Heuristics 
 In this paper, we present a Bayesian Moment Matching (BMM) based method aimed at solving the initialization problem in Boolean SAT solvers.  The initialization problem can be stated as follows: given a SAT formula φ, compute an initial order over the variables of φ and values/polarity for these variables such that the runtime of SAT solvers on input φ is minimized. At the start of a solver run, our BMM-based methods compute a posterior probability distribution for an assign- ment to the variables of the input formula after analyzing its clauses. This probability distribution is then used by the solver to initialize its search.  We perform extensive experiments to evaluate the efficacy of our BMM-based heuristic against 4 other initialization methods (random, survey propagation, Jeroslow-Wang, and default) in state-of-the-art solvers, MapleCOMSPS and MapleLCMDistChronotBT over the SAT competition 2018 application benchmark, as well as the best-known solvers in the cryptographic category, namely, CryptoMiniSAT, Glucose and MapleSAT.  On the cryptographic benchmark, BMM-based solvers out-perform all other initialization methods. Further, the BMM-based MapleCOMSPS significantly out-perform the same solver using all other initialization methods by 12 additional instances solved and better average runtime, over the SAT 2018 competition benchmark.

We performed extensive experiments to evaluate the efficacy of our BMM-based heuristics on SAT competition 2018 application and hard cryptographic benchmarks. We implemented our heuristics in state-of-the-art solvers (MapleCOMSPS and MapleLCMDistChronotBT for SAT competition 2018 application benchmarks) and CryptoMiniSAT, Glucose and MapleSAT (in the context of cryptographic benchmarks), against 4 other initialization methods. Our solvers out-perform the baselines by solving 12 more instances from the SAT competition 2018 application benchmark and are %40 faster on average in solving hard cryptographic instances.
 Unsupervised Discovery of Interpretable Directions in the GAN Latent Space 
 The latent spaces of GAN models often have semantically meaningful directions. Moving in these directions corresponds to human-interpretable image transformations, such as zooming or recoloring, enabling a more controllable generation process. However, the discovery of such directions is currently performed in a supervised manner, requiring human labels, pretrained models, or some form of self-supervision. These requirements severely restrict a range of directions existing approaches can discover.
In this paper, we introduce an unsupervised method to identify interpretable directions in the latent space of a pretrained GAN model. By a simple model-agnostic procedure, we find directions corresponding to sensible semantic manipulations without any form of (self-)supervision. Furthermore, we reveal several non-trivial findings, which would be difficult to obtain by existing methods, e.g., a direction corresponding to background removal. As an immediate practical benefit of our work, we show how to exploit this finding to achieve competitive performance for weakly-supervised saliency detection. The implementation of our method is available online.
 Safe Reinforcement Learning in Constrained Markov Decision Processes 
 Safe reinforcement learning has been a promising approach for optimizing the policy of an agent that operates in safety-critical applications. In this paper, we propose an algorithm, SNO-MDP, that explores and optimizes Markov decision processes under unknown safety constraints. Specifically, we take a step-wise approach for optimizing safety and cumulative reward.  In our method, the agent first learns safety constraints by expanding the safe region, and then optimizes the cumulative reward in the certified safe region. We provide theoretical guarantees on both the satisfaction of the safety constraint and the near-optimality of the cumulative reward under proper regularity assumptions. In our experiments, we demonstrate the effectiveness of SNO-MDP through two experiments: one uses a synthetic data in a new, openly-available environment named GP-Safety-Gym, and the other simulates Mars surface exploration by using real observation data.
 Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning 
 What goals should a multi-goal reinforcement learning agent pursue during training in long-horizon tasks? When the desired (test time) goal distribution is too distant to offer a useful learning signal, we argue that the agent should not pursue unobtainable goals. Instead, it should set its own intrinsic goals that maximize the entropy of the historical achieved goal distribution. We propose to optimize this objective by having the agent pursue past achieved goals in sparsely explored areas of the goal space, which focuses exploration on the frontier of the achievable goal set.
We show that our strategy achieves an order of magnitude better sample efficiency than the prior state of the art on long-horizon multi-goal tasks including maze navigation and block stacking. 
 Domain Adaptive Imitation Learning 
 We study the question of how to imitate tasks across domains with discrepancies such as embodiment, viewpoint, and dynamics mismatch. Many prior works require paired, aligned demonstrations and an additional RL step that requires environment interactions. However, paired, aligned demonstrations are seldom obtainable and RL procedures are expensive. 
In this work, we formalize the Domain Adaptive Imitation Learning (DAIL) problem - a unified framework for imitation learning in the presence of viewpoint, embodiment, and/or dynamics mismatch. Informally, DAIL is the process of learning how to perform a task optimally, given demonstrations of the task in a distinct domain. We propose a two step approach to DAIL: alignment followed by adaptation. In the alignment step we execute a novel unsupervised MDP alignment algorithm, Generative Adversarial MDP Alignment (GAMA), to learn state and action correspondences from \emph{unpaired, unaligned} demonstrations. In the adaptation step we leverage the correspondences to zero-shot imitate tasks across domains. To describe when DAIL is feasible via alignment and adaptation, we introduce a theory of MDP alignability.
We experimentally evaluate GAMA against baselines in embodiment, viewpoint, and dynamics mismatch scenarios where aligned demonstrations don't exist and show the effectiveness of our approach
 Individual Calibration with Randomized Forecasting 
 Machine learning applications often require calibrated predictions, e.g. a 90\% credible interval should contain the true outcome 90\% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. 

We show that calibration for individual samples is possible in the regression setup if and only if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.  
 Duality in RKHSs with Infinite Dimensional Outputs: Application to Robust Losses 
 Operator-Valued Kernels (OVKs) and associated vector-valued Reproducing Kernel Hilbert Spaces provide an elegant way to extend scalar kernel methods when the output space is a Hilbert space. Although primarily used in finite dimension for problems like multi-task regression, the ability of this framework to deal with infinite dimensional output spaces unlocks many more applications, such as functional regression, structured output prediction, and structured data representation. However, these sophisticated schemes crucially rely on the kernel trick in the output space, so that most of previous works have focused on the square norm loss function, completely neglecting robustness issues that may arise in such surrogate problems. To overcome this limitation, this paper develops a duality approach that allows to solve OVK machines for a wide range of loss functions. The infinite dimensional Lagrange multipliers are handled through a Double Representer Theorem, and algorithms for \epsilon-insensitive losses and the Huber loss are thoroughly detailed. Robustness benefits are emphasized by a theoretical stability analysis, as well as empirical improvements on structured data applications.
 The Boomerang Sampler 
 This paper introduces the boomerang sampler as a novel class of continuous-time non-reversible Markov chain Monte Carlo algorithms. The methodology begins by representing the target density as a density, $e^{-U}$, with respect to a prescribed (usually) Gaussian measure and constructs a continuous trajectory consisting of a piecewise circular path. The method moves from one circular orbit to another according to a rate function which can be written in terms of $U$. We demonstrate that the method is easy to implement and demonstrate empirically that it can out-perform existing benchmark piecewise deterministic Markov processes such as the bouncy particle sampler and the Zig-Zag. In the Bayesian statistics context, these competitor algorithms are of substantial interest in the large data context due to the fact that they can adopt data subsampling techniques which are exact (ie induce no error in the stationary distribution). We demonstrate theoretically and empirically that we can also construct a control-variate subsampling boomerang sampler which is also exact, and which possesses remarkable scaling properties in the large data limit. We furthermore illustrate a factorised version on the simulation of diffusion bridges.
 Online Multi-Kernel Learning with Graph-Structured Feedback 
 Multi-kernel learning (MKL) exhibits reliable performance in nonlinear function approximation tasks. Instead of using one kernel, it learns the optimal kernel from a pre-selected dictionary of kernels.  The selection of the dictionary has crucial impact on both the performance and complexity of MKL. Specifically, inclusion of a large number of irrelevant kernels may impair the accuracy, and increase the complexity of MKL algorithms. To enhance the accuracy, and alleviate the computational burden, the present paper develops a novel scheme which actively chooses relevant kernels. The proposed framework models the pruned kernel combination as feedback collected from a graph, that is refined 'on the fly.' Leveraging the random feature approximation, we propose an online scalable multi-kernel learning approach with graph feedback, and prove that the proposed algorithm enjoys sublinear regret. Numerical tests on real datasets demonstrate the effectiveness of the novel approach.
 Aggregation of Multiple Knockoffs 
 We develop an extension of the knockoff inference procedure, introduced by Barber & Candes (2015). This new method, called Aggregation of Multiple Knockoffs (AKO), addresses the instability inherent to the random nature of knockoff-based inference. Specifically, AKO improves both the stability and power compared with the original knockoff algorithm while still maintaining guarantees for false discovery rate control. We provide a new inference procedure, prove its core properties, and demonstrate its benefits in a set of experiments on synthetic and real datasets.
 Orthogonalized SGD and Nested Architectures for Anytime Neural Networks 
 We propose a novel variant of SGD customized for training network architectures that support anytime behavior: such networks produce a series of increasingly accurate outputs over time.  Efficient architectural designs for these networks focus on re-using internal state; subnetworks must produce representations relevant for both immediate prediction as well as refinement by subsequent network stages.  We consider traditional branched networks as well as a new class of recursively nested networks. Our new optimizer, Orthogonalized SGD, dynamically re-balances task-specific gradients when training a multitask network.  In the context of anytime architectures, this optimizer projects gradients from later outputs onto a parameter subspace that does not interfere with those from earlier outputs.  Experiments demonstrate that training with Orthogonalized SGD significantly improves generalization accuracy of anytime networks.

 Simple and Deep Graph Convolutional Networks 
 Graph convolutional networks (GCNs) are a powerful deep learning approach for graph-structured data.  Recently, GCNs and subsequent variants have shown superior performance in various application areas on real-world datasets. Despite their success, most of the current GCN models are shallow, due to the {\em over-smoothing} problem. In this paper, we study the problem of designing and analyzing deep graph convolutional networks.  We propose the GCNII, an extension of the vanilla GCN model with two simple yet effective techniques: {\em Initial residual} and {\em Identity mapping}. We provide theoretical and empirical evidence that the two techniques effectively relieves the problem of over-smoothing. Our experiments show that the deep GCNII model outperforms the state-of-the-art methods on various semi- and full-supervised tasks.  
 Learning and Simulation in Generative Structured World Models 
 Despite several recent advances in object-oriented generative temporal models, there are a few key challenges. First, while many of these achievements are indispensable for a general world model, it is unclear how we can combine the benefits of each method into a unified model. Second, despite using generative model objectives, abilities for object detection and tracking are mainly investigated, leaving the crucial ability of generation largely under question. Third, a few key abilities for more faithful generation such as multi-modal uncertainty and situated behavior are missing. In this paper, we introduce Generative Structured World Models (G-SWM). The G-SWM not only unifies the key properties of previous models in a principled framework but also achieves two crucial new abilities, multi-modal uncertainty and situated behavior. By investigating the generation ability in comparison to the previous models, we demonstrate that G-SWM achieves the best or comparable performance for all experiment settings including a few complex settings that have not been tested before.
 The Tree Ensemble Layer: Differentiability meets Conditional Computation 
 Neural networks and tree ensembles are state-of-the-art learners, each with its unique statistical and computational advantages. We aim to combine these advantages by introducing a new layer for neural networks, composed of an ensemble of differentiable decision trees (a.k.a. soft trees). While differentiable trees demonstrate promising results in the literature, in practice they are typically slow in training and inference as they do not support conditional computation. We mitigate this issue by introducing a new sparse activation function for sample routing, and implement true conditional computation by developing specialized forward and backward propagation algorithms that exploit sparsity. Our efficient algorithms pave the way for jointly training over deep and wide tree ensembles using first-order methods (e.g., SGD). Experiments on 23 classification datasets indicate over $10$x speed-ups compared to the differentiable trees used in the literature and over $20$x reduction in the number of parameters compared to gradient boosted trees, while maintaining competitive performance. Moreover, experiments on CIFAR, MNIST, and Fashion MNIST indicate that replacing dense layers in CNNs with our tree layer reduces the  test loss by $7$-$53\%$ and the number of parameters by $8$x. We provide an open-source TensorFlow implementation with a Keras API.
 Provably Efficient Exploration in Policy Optimization 
 While policy-based reinforcement learning (RL) achieves tremendous successes in practice, it is significantly less understood in theory, especially compared with value-based RL. In particular, it remains elusive how to design a provably efficient policy optimization algorithm that incorporates exploration. To bridge such a gap, this paper proposes an Optimistic variant of the Proximal Policy Optimization algorithm (OPPO), which follows an "optimistic version" of the policy gradient direction. This paper proves that, in the problem of episodic Markov decision process with linear function approximation, unknown transition, and adversarial reward with full-information feedback, OPPO achieves $\tilde{O}(\sqrt{d^3 H^3 T})$ regret. Here $d$ is the feature dimension, $H$ is the episode horizon, and $T$ is the total number of steps. To the best of our knowledge, OPPO is the first provably efficient policy optimization algorithm that explores.
 Training Linear Neural Networks: Non-Local Convergence and Complexity Results 
 Linear networks provide valuable insight into the workings of neural networks in general.

In this paper, we improve the state of the art in (Bah et al., 2019) by identifying conditions under which gradient flow successfully trains a linear network, in spite of the non-strict saddle points present in the optimization landscape.

We also improve the state of the art for computational complexity of training linear networks in (Arora et al., 2018a) by establishing non-local linear convergence rates for gradient flow.

Crucially, these new results are not in the lazy training regime, cautioned against in (Chizat et al., 2019; Yehudai & Shamir, 2019).

Our results require the network to have a layer with one neuron, which corresponds to the popular spiked covariance model in statistics, and subsumes the important case of networks with a scalar output. Extending these results to all linear networks remains an open problem.
 Finite-Time Last-Iterate Convergence for Multi-Agent Learning in Games 
 In this paper, we consider multi-agent learning via online gradient descent in a class of games called $\lambda$-cocoercive games, a fairly broad class of games that admits many Nash equilibria and that properly includes unconstrained strongly monotone games. We characterize the finite-time last-iterate convergence rate for joint OGD learning on $\lambda$-cocoercive games; further, building on this result, we develop a fully adaptive OGD learning algorithm that does not require any knowledge of problem parameter (e.g. cocoercive constant $\lambda$) and show, via a novel double-stopping time technique, that this adaptive algorithm achieves same finite-time last-iterate convergence rate as non-adaptive counterpart. Subsequently, we extend OGD learning to the noisy gradient feedback case and establish last-iterate convergence results--first qualitative almost sure convergence, then quantitative finite-time convergence rates-- all under non-decreasing step-sizes. To our knowledge, we provide the first set of results that fill in several gaps of the existing multi-agent online learning literature, where three aspects--finite-time convergence rates, non-decreasing step-sizes, and fully adaptive algorithms have been unexplored before.
 Lorentz Group Equivariant Neural Network for Particle Physics 
 We present a neural network architecture that is fully equivariant with respect to transformations under the Lorentz group, a fundamental symmetry of space and time in physics. The architecture is based on the theory of the finite-dimensional representations of the Lorentz group and the equivariant nonlinearity involves the tensor product. For classification tasks in particle physics, we show that such an equivariant architecture leads to drastically simpler models that have relatively few learnable parameters and are much more physically interpretable than leading approaches that use CNNs and point cloud approaches. The performance of the network is tested on a public classification dataset [https://zenodo.org/record/2603256] for tagging top quark decays given energy-momenta of jet constituents produced in proton-proton collisions.
 Adversarial Attacks on Probabilistic Autoregressive Forecasting Models 
 We develop an effective generation of adversarial attacks on neural models that output a sequence of probability distributions rather than a sequence of single values. This setting includes the recently proposed deep probabilistic autoregressive forecasting models that estimate the probability distribution of a time series given its past and achieve state-of-the-art results in a diverse set of application domains. The key technical challenge we address is how to effectively differentiate through the Monte-Carlo estimation of statistics of the output sequence joint distribution. Additionally, we extend prior work on probabilistic forecasting to the Bayesian setting which allows conditioning on future observations, instead of only on past observations. We demonstrate that our approach can successfully generate attacks with small input perturbations in two challenging tasks where robust decision making is crucial -- stock market trading and prediction of electricity consumption.

 Countering Language Drift with Seeded Iterated Learning 
 Supervised learning methods excel at capturing statistical properties of language when trained over large text corpora. Yet, these models often produce inconsistent outputs in goal-oriented language setting as they are not trained to complete the underlying task. Moreover, as soon as the agents are fine-tuned to maximize task completion, they suffer from the so-called language drift phenomenon: they slowly lose syntactic and semantic properties of language as they only focus on solving the task. In this paper, we propose a generic approach to counter language drift by using iterated learning. We iterate between finetuning agents with interactive training steps, and periodically replacing them with new agents that are seeded from last iteration and trained to imitate the latest finetuned models. Iterated learning does not require external syntactic constraint nor semantic knowledge, making it a valuable task-agnostic finetuning protocol. We first explore iterated learning in the Lewis Game. We then scale-up the approach in the translation game. In both settings our results show that iterated learning drastically counters language drift as well as improves the task completion metric.
 Near-linear time Gaussian process optimization with adaptive batching and resparsification 
 Gaussian processes (GP) are one of the most successful frameworks to model uncertainty. However, GP optimization (e.g., GP-UCB) suffers from major scalability issues. Experimental time grows linearly with the number of evaluations, unless candidates are selected in batches (e.g., using GP-BUCB) and evaluated in parallel. Furthermore, computational cost is often prohibitive since algorithms such as GP-BUCB require a time at least quadratic in the number of dimensions and iterations to select each batch.

In this paper, we introduce BBKB (Batch Budgeted Kernel Bandits), the first no-regret GP optimization algorithm that provably runs in near-linear time and selects candidates in batches. This is obtained with a new guarantee for the tracking of the posterior variances that allows BBKB to choose increasingly larger batches, improving over GP-BUCB. Moreover, we show that the same bound can be used to adaptively delay costly updates to the sparse GP approximation used by BBKB, achieving a near-constant per-step amortized cost. These findings are then confirmed in several experiments, where BBKB is much faster than state-of-the-art methods.
 Topological Autoencoders 
 We propose a novel approach for preserving topological structures of the input space in latent representations of autoencoders. Using persistent homology, a technique from topological data analysis, we calculate topological signatures of both the input and latent space to derive a topological loss term.  Under weak theoretical assumptions, we construct this loss in a differentiable manner, such that the encoding learns to retain multi-scale connectivity information.  We show that our approach is theoretically well-founded and that it exhibits favourable latent representations on a synthetic manifold as well as on real-world image data sets, while preserving low reconstruction errors.
 Monte-Carlo Tree Search as Regularized Policy Optimization 
 The combination of Monte-Carlo tree search (MCTS) with deep reinforcement learning has led to groundbreaking results in  artificial intelligence. However, AlphaZero, the current state-of-the-art MCTS algorithm still relies on handcrafted  heuristics that are only partially understood. In this paper, we show that AlphaZero's search heuristic, along with other common ones, can be interpreted as an approximation to the solution of a specific regularized policy optimization problem. With this insight, we propose a variant of AlphaZero which uses the exact solution to this policy optimization problem, and show experimentally that it reliably outperforms the original algorithm in multiple domains.
 Obtaining Adjustable Regularization for Free via Iterate Averaging 
 Regularization for optimization is a crucial technique to avoid overfitting in machine learning. In order to obtain the best performance, we usually train a model by tuning the regularization parameters. It becomes costly, however, when a single round of training takes significant amount of time. Very recently, Neu and Rosasco show that if we run stochastic gradient descent (SGD) on linear regression problems, then by averaging the SGD iterates properly, we obtain a regularized solution. It left open whether the same phenomenon can be achieved for other optimization problems and algorithms. In this paper, we establish a complete theory by showing an averaging scheme that provably converts the iterates of SGD on an arbitrary strongly convex and smooth objective function to its regularized counterpart with an adjustable regularization parameter. Our approaches can be used for accelerated and preconditioned optimization methods as well. We further show that the same methods work empirically on more general optimization objectives including neural networks. In sum, we obtain adjustable regularization for free for a large class of optimization problems and resolve an open question raised by Neu and Rosasco.
 The Shapley Taylor Interaction Index 
 The attribution problem, that is the problem of attributing a model's prediction to its base features, is well-studied. We extend the notion of attribution to also apply to feature interactions.

The Shapley value is a commonly used method to attribute a model's prediction to its base features. We propose a generalization of the Shapley value called Shapley-Taylor index that attributes the model's prediction to interactions of subsets of features up to some size $k$. The method is analogous to how the truncated Taylor Series decomposes the function value at a certain point using its  derivatives at a different point. In fact, we show that the Shapley Taylor index is equal to the Taylor Series of the multilinear extension of the set-theoretic behavior of the model.   

We axiomatize this method using the standard Shapley axioms---linearity, dummy, symmetry and efficiency---and an additional axiom that we call the interaction distribution axiom. This new axiom explicitly characterizes how interactions are distributed for a class of functions that model pure interaction. 

We contrast the Shapley-Taylor index against the previously proposed Shapley Interaction index from the cooperative game theory literature. We also apply the Shapley Taylor index to three models and identify interesting qualitative insights.
 Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support 
 Universal probabilistic programming systems (PPSs) provide a powerful framework for specifying rich and complex probabilistic models. They further attempt to automate the process of drawing inferences from these models, but doing this successfully is severely hampered by the wide range of non-standard models they can express. As a result, although one can specify complex models in a universal PPS, the provided inference engines often fall far short of what is required. In particular, we show they produce surprisingly unsatisfactory performance for models where the support may vary between executions, often doing no better than importance sampling from the prior. To address this, we introduce a new inference framework: Divide, Conquer, and Combine, which remains efficient for such models, and show how it can be implemented as an automated and general-purpose PPS inference engine. We empirically demonstrate substantial performance improvements over existing approaches on two examples.
 Abstraction Mechanisms Predict Generalization in Deep Neural Networks 
 A longstanding problem for Deep Neural Networks (DNNs) is understanding their puzzling ability to generalize well. We approach this problem through the unconventional angle of \textit{cognitive abstraction mechanisms}, drawing inspiration from recent neuroscience work, allowing us to define the Cognitive Neural Activation metric (CNA) for DNNs, which is the correlation between information complexity (entropy) of given input and the concentration of higher activation values in deeper layers of the network. The CNA is highly predictive of generalization ability, outperforming norm-and-margin-based generalization metrics on an extensive evaluation of over 100 dataset-and-network-architecture combinations, especially in cases where additive noise is present and/or training labels are corrupted. These strong empirical results show the usefulness of CNA as a generalization metric, and encourage further research on the connection between information complexity and representations in the deeper layers of networks in order to better understand the generalization capabilities of DNNs.
 Towards Understanding the Regularization of Adversarial Robustness on Neural Networks 
 The problem of adversarial examples has shown that modern Neural Network (NN) models could be rather fragile. Among the most promising techniques to solve the problem, one is to require the model to be epsilon-adversarially robust (AR); that is, to require the model not to change predicted labels when any given input examples are perturbed within a certain range. However, it is observed that such methods would lead to standard performance degradation, i.e., the degradation on natural examples. In this work, we study the degradation through the regularization perspective. We identify quantities from generalization analysis of NNs; with the identified quantities we empirically find that AR is achieved by regularizing/biasing NNs towards less confident solutions by making the changes in the feature space (induced by changes in the instance space) of most layers smoother uniformly in all directions; so to a certain extent, it prevents sudden change in prediction w.r.t. perturbations. However, the end result of such smoothing concentrates samples around decision boundaries, resulting in less confident solutions, and leads to worse standard performance. Our studies suggest that one might consider ways that build AR into NNs in a gentler way to avoid the problematic regularization.
 Nearly Linear Row Sampling Algorithm for Quantile Regression 
 We give a row sampling algorithm for the quantile loss function with sample complexity nearly linear in the dimensionality of the data, improving upon the previous best algorithm whose sampling complexity has at least cubic dependence on the dimensionality. Based upon our row sampling algorithm, we give the fastest known algorithm for quantile regression and a graph sparsification algorithm for balanced directed graphs. Our main technical contribution is to show that Lewis weights sampling, which has been used in row sampling algorithms for $\ell_p$ norms, can also be applied in row sampling algorithms for a variety of loss functions. We complement our theoretical results by experiments to demonstrate the practicality of our approach. 
 Learning the piece-wise constant graph structure of a varying Ising model 
 This work focuses on the estimation of multiple change-points in a time-varying Ising model that evolves piece-wise constantly. The aim is to identify both the moments at which significant changes occur in the Ising model, as well as the underlying graph structures.
For this purpose, we propose to estimate the neighborhood of each node by maximizing a penalized version of its conditional log-likelihood. The objective of the penalization is twofold: it imposes sparsity in the learned graphs and, thanks to a fused-type penalty, it also enforces them to evolve piece-wise constantly. Using few assumptions, we provide two change-points consistency theorems. Those are the first in the context of unknown number of change-points detection in time-varying Ising model. Finally, experimental results on several synthetic datasets and a real-world dataset demonstrate the performance of our method.
 Implicit competitive regularization in GANs 
 To improve the stability of GAN training we need to understand why they can produce realistic samples.
Presently, this is attributed to properties of the divergence obtained under an optimal discriminator.
This argument has a fundamental flaw:\\
If we do not impose regularity of the discriminator, it can exploit visually imperceptible errors of the generator to always achieve the maximal generator loss.
In practice, gradient penalties are used to regularize the discriminator.
However, this needs a metric on the space of images that captures visual similarity. 
Such a metric is not known, which explains the limited success of gradient penalties in stabilizing GANs.\\
We argue that the performance of GANs is instead due to the implicit competitive regularization (ICR) arising from the simultaneous optimization of generator and discriminator.
ICR promotes solutions that \emph{look real} to the discriminator and thus leverages its inductive biases to generate realistic images.
We show that opponent-aware modelling of generator and discriminator, as present in competitive gradient descent (CGD), can significantly strengthen ICR and thus stabilize GAN training without explicit regularization.
In our experiments, we use an existing implementation of WGAN-GP and show that by training it with CGD we can improve the inception score (IS) on CIFAR10 for a wide range of scenarios, without any hyperparameter tuning.
The highest IS is obtained by combining CGD with the WGAN-loss, without any explicit regularization.
 Striving for simplicity and performance in off-policy DRL: Output Normalization and Non-Uniform Sampling 
 We aim to develop off-policy DRL algorithms that not only exceed state-of-the-art performance but are also simple and minimalistic. For standard continuous control benchmarks, Soft Actor Critic (SAC), which employs entropy maximization, currently provides state-of-the-art performance. We first demonstrate that the entropy term in SAC addresses action saturation due to the bounded nature of the action spaces. With this insight, we propose a streamlined algorithm with a simple normalization scheme or with inverted gradients. We show that both approaches can match SAC's sample efficiency performance without the need of entropy maximization. We then propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training. Extensive experimental results demonstrate that our proposed sampling scheme leads to state of the art sample efficiency on challenging continuous control tasks. We combine all of our findings into one simple algorithm, which we call Streamlined Off Policy with Emphasizing Recent Experience, for which we provide robust public-domain code. 
 A Free-Energy Principle for Representation Learning 
 This paper employs a formal connection of machine learning with thermodynamics to characterize the quality of learnt representations for transfer learning. We discuss how information-theoretic functionals such as rate, distortion and classification loss of a model lie on a convex, so-called equilibrium surface. We prescribe dynamical processes to traverse this surface under constraints, e.g., an iso-classification process that trades off rate and distortion to keep the classification loss unchanged. We demonstrate how this process can be used for transferring representations from a source dataset to a target dataset while keeping the classification loss constant. Experimental validation of the theoretical results is provided on standard image-classification datasets.
 Minimax Pareto Fairness: A Multi Objective Perspective 
 In this work we formulate and formally characterize group fairness as a multi-objective optimization problem, where each sensitive group risk is a separate objective. We propose a fairness criterion where a classifier achieves minimax risk and is Pareto-efficient w.r.t. all groups, avoiding unnecessary harm, and can lead to the best zero-gap model if policy dictates so. We provide a simple optimization algorithm compatible with deep neural networks to satisfy these constraints. Since our method does not require test-time access to sensitive attributes, it can be applied to reduce worst-case classification errors between outcomes in unbalanced classification problems. We test the proposed methodology on real case-studies of predicting income, ICU patient mortality, skin lesions classification, and assessing credit risk, demonstrating how our framework compares favorably to other approaches.
 Taylor Expansion Policy Optimization 
 In this work, we investigate the application of Taylor expansions in reinforcement learning. In particular, we propose Taylor Expansion Policy Optimization, a policy optimization formalism that generalizes prior work as a first-order special case. We also show that Taylor expansions intimately relate to off-policy evaluation. Finally, we show that this new formulation entails modifications which  improve the performance of several state-of-the-art distributed algorithms.
 Generalized and Scalable Optimal Sparse Decision Trees 
 Decision tree optimization is notoriously difficult from a computational perspective but essential for the field of interpretable machine learning. Despite efforts over the past 40 years, only recently have optimization breakthroughs been made that have allowed practical algorithms to find optimal decision trees. These new techniques have the potential to trigger a paradigm shift, where, it is possible to construct sparse decision trees to efficiently optimize a variety of objective functions, without relying on greedy splitting and pruning heuristics that often lead to suboptimal solutions. The contribution in this work is to provide a general framework for decision tree optimization that addresses the two significant open problems in the area: treatment of imbalanced data and fully optimizing over continuous variables. We present techniques that produce optimal decision trees over variety of objectives including F-score, AUC, and partial area under the ROC convex hull. We also introduce a scalable algorithm that produces provably optimal results in the presence of continuous variables and speeds up decision tree construction by several order of magnitude relative to  the state-of-the art.
 ACFlow: Flow Models for Arbitrary Conditional Likelihoods 
 Understanding the dependencies among features of a dataset is at the core of most unsupervised learning tasks. However, a majority of generative modeling approaches are focused solely on the joint distribution $p(x)$ and utilize models where it is intractable to obtain the conditional distribution of some arbitrary subset of features $x_u$ given the rest of the observed covariates $x_o$: $p(x_u \mid x_o)$. Traditional conditional approaches provide a model for a \emph{fixed} set of covariates conditioned on another \emph{fixed} set of observed covariates. Instead, in this work we develop a model that is capable of yielding \emph{all} conditional distributions $p(x_u \mid x_o)$ (for arbitrary $x_u$) via tractable conditional likelihoods. We propose a novel extension of (change of variables based) flow generative models, arbitrary conditioning flow models (ACFlow). ACFlow can be conditioned on arbitrary subsets of observed covariates, which was previously infeasible. We further extend ACFlow to model the joint distributions $p(x)$ and arbitrary marginal distributions $p(x_u)$. We also apply ACFlow to the imputation of features, and develop a unified platform for both multiple and single imputation by introducing an auxiliary objective that provides a principled single ``best guess'' for flow models. Extensive empirical evaluations show that our model achieves state-of-the-art performance in modeling arbitrary conditional likelihoods in addition to both single and multiple imputation in synthetic and real-world datasets.
 A Simple Framework for Contrastive Learning of Visual Representations 
 This paper presents a simple framework for contrastive representation learning. The framework, SimCLR, simplifies recently proposed approaches and requires neither specific architectural modifications nor a memory bank. In order to understand what enables the contrastive prediction task to learn useful representations, we systematically study the major components in the framework. We empirically show that 1) composition of data augmentations plays a critical role in defining the predictive tasks that enable effective representation learning, 2) introducing a learned nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the representation, and 3) contrastive learning benefits from a larger batch size and more training steps compared to the supervised counterpart. By combining our findings, we improve considerably over previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on the representation of our best model achieves 76.5% top-1 accuracy, a 7% relative improvement over previous state-of-the-art. When fine-tuned on 1% of labels, our model achieves 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.
 Inertial Block Proximal Methods for Non-Convex Non-Smooth Optimization 
 We propose inertial versions of block coordinate descent methods for solving non-convex non-smooth composite optimization problems. Our methods possess three main advantages compared to current state-of-the-art accelerated first-order methods: (1) they allow using two different extrapolation points to evaluate the gradients and to add the inertial force (we will empirically show that it is more efficient than using a single extrapolation point), (2) they allow to randomly select the block of variables to update, and (3) they do not require a restarting step. We prove the subsequential convergence of the generated sequence under mild assumptions, prove the global convergence under some additional assumptions, and provide convergence rates. We deploy the proposed methods to solve non-negative matrix factorization (NMF) and show that they compete favorably with the state-of-the-art NMF algorithms. Additional experiments on non-negative approximate canonical polyadic decomposition, also known as nonnegative tensor factorization, are also provided.  
 Why Are Learned Indexes So Effective? 
 A recent trend in algorithm design consists of augmenting classic data structures with machine learning models, which are better suited to reveal and exploit patterns and trends in the input data so to achieve outstanding practical improvements in space occupancy and time efficiency.  
This is especially known in the context of indexing data structures where, despite few attempts in evaluating their asymptotic efficiency, theoretical results are yet missing in showing that learned indexes are provably better than classic indexes, such as B+ trees and their variants.
In this paper, we present the first mathematically-grounded answer to this open problem. We obtain this result by discovering and exploiting a link between the original problem and a mean exit time problem over a proper stochastic process which, we show, is related to the space and time occupancy of those learned indexes. Our general result is then specialised to five well-known distributions: Uniform, Lognormal, Pareto, Exponential, and Gamma; and it is corroborated in precision and robustness by a large set of experiments.
 Strategic Classification is Causal Modeling in Disguise 
 Consequential decision-making incentivizes individuals to strategically adapt
their behavior to the specifics of the decision rule. While a long line of work has viewed strategic adaptation as gaming and sought to mitigate its effects, recent work instead seeks to design classifiers that incentivize individuals to improve a desired quality. Key to both accounts is a cost function that dictates which adaptations are rational to undertake. In this work, we develop a causal framework for strategic adaptation. Our causal perspective clearly distinguishes between gaming and improvement and reveals an important obstacle to incentive design. We prove any procedure for designing classifiers that incentivize improvement must inevitably solve a non-trivial causal inference problem. Moreover, we show a similar result holds for designing cost functions that satisfy the requirements of previous work. With the benefit of hindsight, our results show much of the prior work on strategic classification is causal modeling in disguise.

 Global Concavity and Optimization in a Class of Dynamic Discrete Choice Models 
 Discrete choice models with unobserved heterogeneity are commonly used Econometric models for dynamic Economic behavior which have been adopted in practice to predict behavior of individuals and firms from schooling and job choices to strategic decisions in market competition. These models feature optimizing agents who choose among a finite set of options in a sequence of periods and receive choice-specific payoffs that depend on both variables that are observed by the agent and recorded in the data and variables that are only observed by the agent but not recorded in the data. Existing work in Econometrics assumes that optimizing agents are fully rational and requires finding a functional fixed point to find the optimal policy. We show that in an important class of discrete choice models the value function is globally concave in the policy. That means that simple algorithms that do not require fixed point computation, such as the policy gradient algorithm, globally converge to the optimal policy. This finding can both be used to relax behavioral assumption regarding the optimizing agents and to facilitate Econometric analysis of dynamic behavior. In particular, we demonstrate significant computational advantages in using a simple implementation policy gradient algorithm over existing "nested fixed point" algorithms used in Econometrics.
 How to Solve Fair k-Center in Massive Data Models 
 Fueled by massive data, important decision making is being automated with the help of algorithms, therefore, fairness in algorithms has become an especially important research topic. In this work, we design new streaming and distributed algorithms for the fair k-center problem that models fair data summarization. The streaming and distributed models of computation have an attractive feature of being able to handle massive data sets that do not fit into main memory. Our main contributions are: (a) the first distributed algorithm; which has provably constant approximation ratio and is extremely parallelizable, and (b) a two-pass streaming algorithm with a provable approximation guarantee matching the best known algorithm (which is not a streaming algorithm). Our algorithms have the advantages of being easy to implement in practice, being fast with linear running times, having very small working memory and communication, and outperforming existing algorithms on several real and synthetic data sets. To complement our distributed algorithm, we also give a hardness result for natural distributed algorithms, which holds for even the special case of k-center.
 Guided Learning of Nonconvex Models through Successive Functional Gradient Optimization 
 This paper presents a framework of successive functional gradient optimization for training nonconvex models such as neural networks, where training is driven by mirror descent in a function space.  We provide a theoretical analysis and empirical study of the training method derived from this framework.  It is shown that the method leads to better performance than that of standard training techniques.
 Structured Policy Iteration for Linear Quadratic Regulator 
 Linear quadratic regulator (LQR) is one of the most popular frameworks to tackle continuous Markov decision process tasks. With its fundamental theory and tractable optimal policy, LQR has been revisited and analyzed in recent years, in terms of reinforcement learning scenarios such as the model-free or model-based setting. In this paper, we introduce the Structured Policy Iteration (S-PI) for LQR, a method capable of deriving a structured linear policy. Such a structured policy with (block) sparsity or low-rank can have significant advantages over the standard LQR policy: more interpretable, memory-efficient, and well-suited for the distributed setting. In order to derive such a policy, we first cast a regularized LQR problem when the model is known. Then, our Structured Policy Iteration (S-PI) algorithm, which takes a policy evaluation step and a policy improvement step in an iterative manner, can solve this regularized LQR efficiently. We further extend the S-PI algorithm to the model-free setting where a smoothing procedure is adopted to estimate the gradient. In both the known-model and model-free setting, we prove convergence analysis under the proper choice of parameters. Finally, the experiments demonstrate the advantages of S-PI in terms of balancing the LQR performance and level of structure by varying the weight parameter.
 Amortized Population Gibbs Samplers with Neural Sufficient Statistics 
 Amortized variational methods have proven difficult to scale to structured problems, such as inferring positions of multiple objects from video images. We develop amortized population Gibbs (APG) samplers, a class of scalable methods that frame structured variational inference as adaptive importance sampling. APG samplers construct high-dimensional proposals by iterating over updates to lower-dimensional blocks of variables. We train each conditional proposal by minimizing the inclusive KL divergence with respect to the conditional posterior. To appropriately account for the size of the input data, we develop a new parameterization in terms of neural sufficient statistics. Experiments show that APG samplers can be used to train highly-structured deep generative models in an unsupervised manner, and achieve substantial improvements in inference accuracy relative to standard autoencoding variational methods.
 Augmenting Continuous Time Bayesian Networks with Clocks 
 Structured stochastic processes evolving in continuous time present a widely adopted framework to model phenomena occurring in nature and engineering. However, such models are often chosen to satisfy the Markov property to maintain tractability. One of the more popular of such memoryless models is Continuous Time Bayesian Networks (CTBNs). In this work, we lift its restriction to exponential survival times to arbitrary distributions. Current extensions achieve this via auxiliary states, which hinder tractability. To avoid that, we introduce a set of node-wise clocks to construct a collection of graph-coupled semi-Markov chains. We provide algorithms for parameter and structure inference, which make use of local dependencies and conduct experiments on synthetic data and data-sets generated through a benchmark tool for gene regulatory networks. In doing so, we point out advantages compared to current CTBN extensions.
 Incidence Networks for Geometric Deep Learning 
 Sparse incidence tensors can represent a variety of structured data. For example, we may represent attributed graphs using their node-node, node-edge, or edge-edge incidence matrices. In higher dimensions, incidence tensors can represent simplicial complexes and polytopes. In this paper, we formalize incidence tensors, analyze their structure, and present the family of equivariant networks that operate on them. We show that any incidence tensor decomposes into invariant subsets. This decomposition, in turn, leads to a decomposition of the corresponding equivariant linear maps, for which we prove an efficient pooling-and-broadcasting implementation. We demonstrate the effectiveness of this family of networks by reporting state-of-the-art on graph learning tasks for many targets in the QM9 dataset.
 Efficiently sampling functions from Gaussian process posteriors 
 Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a method's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, where quantities of interest are ultimately defined by integrating over posterior distributions. However, these algorithms' inner workings rarely allow for closed-form integration, giving rise to a need for Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a factorization of Gaussian processes that naturally lends itself to efficient sampling, by allowing accurate representation of entire function draws. Building off of this factorization, we propose decoupled sampling, an easy-to-use and general-purpose approach for fast posterior sampling. As a drop-in approach to sampling, decoupled sampling seamlessly pairs with sparse approximations to Gaussian processes to afford scalability both during training and at test time. In a series of experiments designed to test sampling schemes' statistical behavior and practical ramifications, we empirically show that functions drawn using decoupled sampling faithfully represent Gaussian process posteriors at a fraction of the cost.
 Towards a General Theory of Infinite-Width Limits of Neural Classifiers 
 Generally, obtaining theoretical guarantees for neural networks training appears to be a hard problem.
Recent research has been focused on studying this problem in the limit of infinite width and two different theories have been developed: mean-field (MF) limit theory and kernel limit theory.
We propose a general framework that provides a link between these seemingly distinct limit theories.
Our framework out of the box gives rise to a discrete-time MF limit — a setup that to the best of our knowledge was not previously explored in literature. 
We prove a convergence theorem for it and show that it provides a more reasonable approximation for finite-width nets compared to NTK limit if learning rates are not very small.
Also, our framework suggests a different type of infinite-width limits, not covered by both MF and kernel limit theories.
We show that for networks with more than two hidden layers RMSProp training has a non-trivial MF limit, but GD training does not have one.
Overall, our framework demonstrates that both MF and NTK limits have considerable limitations in approximating finite-sized neural nets, indicating the need for designing more accurate infinite-width approximations for them.
 Fast Differentiable Sorting and Ranking 
 Sorting is an elementary building block of modern software.  In machine learning and statistics, it is commonly used in robust statistics, order statistics and ranking metrics. However, sorting is a piecewise linear function and as a result includes many kinks at which it is non-differentiable.  More problematic, the ranking operator is a piecewise constant function, meaning that its derivatives are null or undefined. While numerous works have proposed differentiable proxies to sorting and ranking, they do not achieve the $O(n \log n)$ time complexity one could expect from a sorting or ranking operation. In this paper, we propose the first differentiable sorting and ranking operators with $O(n \log n)$ time and $O(n)$ space complexity.  Our proposal in addition enjoys exact computation and differentiation.  We achieve this feat by casting differentiable sorting and ranking as projections onto a permutahedron, the convex hull of permutations, and using a reduction to isotonic optimization.  Empirically, we confirm that our approach is an order of magnitude faster than existing approaches. We also showcase two novel applications: differentiable Spearman's rank coefficient and differentiable least trimmed squares.
 Graph-based Nearest Neighbor Search: From Practice to Theory 
 Graph-based approaches are empirically shown to be very successful for the nearest neighbor search (NNS). However, there has been very little research on their theoretical guarantees. In this work, we fill this gap and rigorously analyze the performance of graph-based NNS algorithms, specifically focusing on the low-dimensional (d << log n) regime. In addition to the basic greedy algorithm on the nearest neighbor graph, we also analyze the most successful heuristics commonly used in practice: speeding up via adding shortcut edges and improving accuracy via maintaining a dynamic list of candidates. We believe that our theoretical results supported by experimental analysis are an important step towards understanding the limits and benefits of graph-based NNS algorithms.
 Energy-Based Processes for Exchangeable Data 
 Recently there has been growing interest in modeling sets with exchangeability such as point clouds. A shortcoming of current approaches is that they restrict the cardinality of the sets considered or can only express limited forms of distribution over unobserved data. To overcome these limitations, we introduce Energy-Based Processes (EBPs), which extend energy based models to exchangeable data while allowing neural network parameterizations of the energy function. A key advantage of these models is the ability to express more flexible distributions over sets without restricting their cardinality. We develop an efficient training procedure for EBPs that demonstrates state-of-the-art performance on a variety of tasks such as point cloud generation, classification, denoising, and image completion
 Go Wide, Then Narrow: Efficient Training of Deep Thin Networks 
 We propose an efficient algorithm to train  a very deep and thin network with theoretic guarantee. Our method is motivated by model compression, and consists of three stages. In the first stage, we widen the deep thin network and train it until convergence. In the second stage, we use this well trained deep wide network to warm up or initialize the original deep thin network. In the last stage, we train this well initialized deep thin network until convergence. The key ingredient of our method is  its second stage, in which the thin network is gradually warmed up by imitating the intermediate outputs of the wide network from bottom to top. We establish theoretical guarantee using mean field analysis. We show that our method is provably more efficient than directly training a deep thin network from scratch. We also conduct empirical evaluations on image classification and language modeling. By training with our approach, ResNet50 can outperform  ResNet101 which is normally trained as in the literature, and BERT_BASE can be comparable with BERT_LARGE. 
 Small Data, Big Decisions: Model Selection in the Small-Data Regime 
 Highly overparametrized neural networks can display curiously strong generalization performance -- a phenomenon that has recently garnered a wealth of theoretical and empirical research in order to better understand it.
In contrast to most previous work, which typically considers the performance as a function of the model size, in this paper we empirically study the generalization performance as the size of the training set varies over multiple orders of magnitude.
These systematic experiments lead to some interesting and potentially very useful 
observations; perhaps most notably that training on smaller subsets of the
data can lead to more reliable model selection decisions whilst simultaneously enjoying smaller computational overheads.
Our experiments furthermore allow us to estimate Minimum Description Lengths for common datasets given modern neural network architectures, thereby paving the way for principled model selection taking into account Occams-razor.

 Graph Random Neural Features for Distance-Preserving Graph Representations 
 We present Graph Random Neural Features (GRNF), a novel embedding method from graph-structured data to real vectors based on a family of graph neural networks.  The embedding naturally deals with graph isomorphism and preserves the metric structure of the graph domain, in probability.  In addition to being an explicit embedding method, it also allows us to efficiently and effectively approximate graph metric distances (as well as complete kernel functions); a criterion to select the embedding dimension trading off the approximation accuracy with the computational cost is also provided.  GRNF can be used within traditional processing methods or as a training-free input layer of a graph neural network.  The theoretical guarantees that accompany GRNF ensure that the considered graph distance is metric, hence allowing to distinguish any pair of non-isomorphic graphs. 
 The Differentiable Cross-Entropy Method 
 We study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that enables us to differentiate the output of CEM with respect to the objective function's parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline where this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show how to embed optimal action sequences into a lower-dimensional space. This enables us to use policy optimization to fine-tune modeling components by differentiating through the CEM-based controller.
 Responsive Safety in Reinforcement Learning 
 CMDPs formalize the problem of safe reinforcement learning by exposing a cost signal alongside the reward and limiting its accumulation.  Lagrangian method are the most commonly used algorithms for the resulting constrained optimization problem.  Yet they are known to oscillate and overshoot cost limits, causing constraint-violating behavior during training.  In this paper, we aim to correct this shortcoming.  We begin by proposing a novel modification to the classic Lagrangian method: we add a ``proportional'' term to the Lagrange multiplier update and show that it induces favorable learning dynamics through damping.  This intuition leads to our introduction of PID control for the Lagrange multiplier in constrained RL, which we cast as a dynamical system.  We conduct extensive experiments in a deep RL setting, in which our methods set a new state of the art by dramatically reducing constraint violations while maintaining high returns.  Moreover, we show significant improvements in robustness to hyperparameters.  Unlike other recent algorithms, ours remains nearly as simple to derive and implement as the baseline Lagrangian method.
 Privately Learning Markov Random Fields 
   We consider the problem of learning Markov Random Fields (including
  the prototypical example, the Ising model) under the constraint of
  differential privacy.  Our learning goals include both
  \emph{structure learning}, where we try to estimate the underlying
  graph structure of the model, as well as the harder goal of
  \emph{parameter learning}, in which we additionally estimate the
  parameter on each edge.  We provide algorithms and lower bounds for
  both problems under a variety of privacy constraints --
  namely pure, concentrated, and approximate differential privacy.
  While non-privately, both learning goals enjoy roughly the same
  complexity, we show that this is not the case under differential
  privacy.  In particular, only structure learning under approximate
  differential privacy maintains the non-private logarithmic
  dependence on the dimensionality of the data, while a change in
  either the learning goal or the privacy notion would necessitate a
  polynomial dependence. As a result, we show that the privacy
    constraint imposes a strong separation between these two learning
    problems in the high-dimensional data regime.
 Sparse Gaussian Processes with Spherical Harmonic Features 
 We introduce a new class of interdomain variational Gaussian processes (GP) where data is mapped onto the unit hypersphere in order to use spherical harmonic representations. Our inference scheme is comparable to variational Fourier features, but it does not suffer from the curse of dimensionality, and leads to diagonal covariance matrices between inducing variables. This enables a speed-up in inference, because it bypasses the need to invert large covariance matrices. Our experiments show that our model is able to fit a regression model for a dataset with 6 million entries two orders of magnitude faster compared to standard sparse GPs, while retaining state of the art accuracy. We also demonstrate competitive performance on classification with non-conjugate likelihoods.
 ControlVAE: Controllable Variational Autoencoder 
 Variational Autoencoders (VAE) and their variants have been widely used in a variety of applications, such as dialog generation, image generation and disentangled representation learning. However, the existing VAE models have some limitations in different applications. For example, a VAE easily suffers from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller, inspired by automatic control theory, with the basic VAE to improve the performance of resulting generative models. Specifically, we design a new non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically tune the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. The framework is evaluated using three applications; namely, language modeling, disentangled representation learning, and image generation. The results show that ControlVAE can achieve better disentangling and reconstruction quality than the existing methods. For language modelling, it not only averts the KL-vanishing, but also improves the diversity of generated text. Finally, we also demonstrate that ControlVAE improves the reconstruction quality of generated images compared to the original VAE.
 Goodness-of-Fit Tests for Inhomogeneous Random Graphs 
 Hypothesis testing of random networks is an emerging area of modern research, especially in the high-dimensional regime, where the  number of samples is smaller or comparable to the size of the graph.  In this paper we consider the goodness-of-fit testing problem for large inhomogeneous random (IER) graphs, where given a (known) reference symmetric matrix $Q \in [0, 1]^{n \times n}$ and $m$ independent samples from an IER graph given by an unknown  symmetric matrix $P \in [0, 1]^{n \times n}$, the goal is to test the hypothesis $P=Q$ versus $||P-Q|| \geq \varepsilon$, where $||\cdot||$ is some specified norm on symmetric matrices. Building on recent related work on two-sample testing for IER graphs, we derive the optimal minimax sample complexities for the goodness-of-fit problem in various natural norms, such as the Frobenius norm and the operator norm. We also propose practical implementations of natural test statistics, using their asymptotic distributions and through the parametric bootstrap. We compare the performances of the different tests in simulations, and show that the proposed  tests outperform the baseline tests across various natural random graphs models.
 On Second-Order Group Influence Functions for Black-Box Predictions 
 With the rapid adoption of machine learning systems in sensitive applications, there is an increasing need to make black-box models explainable. Often we want to identify an influential group of training samples in a particular test prediction for a given machine learning model. Existing influence functions tackle this problem by using first-order approximations of the effect of removing a sample from the training set on model parameters. To compute the influence of a group of training samples (rather than an individual point) in model predictions, the change in optimal model parameters after removing that group from the training set can be large. Thus, in such cases, the first-order approximation can be loose. In this paper, we address this issue and propose second-order influence functions for identifying influential groups in test-time predictions. For linear models, across different sizes and types of groups, we show that using the proposed second-order influence function improves the correlation between the computed influence values and the ground truth ones. We also show that second-order influence functions could be used with optimization techniques to improve the selection of the most influential group for a test-sample.
 Learning the Valuations of a $k$-demand Agent 
 We study problems where a learner aims to learn the valuations of an agent by observing which goods he buys under varying price vectors.  More specifically, we consider the case of a $k$-demand agent, whose valuation over the goods is additive when receiving up to $k$ goods, but who has no interest in receiving more than $k$ goods.  We settle the query complexity for the active-learning (preference elicitation) version, where the learner chooses the prices to post, by giving a {\em biased binary search} algorithm, generalizing the classical binary search procedure.
We complement our query complexity upper bounds by lower bounds that match up to lower-order terms.  We also study the passive-learning version in which the learner does not control the prices, and instead they are sampled from some distribution.  We show that in the PAC model for passive learning, any {\em empirical risk minimizer} has a sample complexity that is optimal up to a factor of $\widetilde{O}(k)$.
 Bidirectional Model-based Policy Optimization 
 Model-based reinforcement learning approaches leverage a forward dynamics model to support planning and decision making, which, however, may fail catastrophically if the model is inaccurate. Although there are several existing methods dedicated to combating the model error, the potential of the single forward model is still limited. In this paper, we propose to additionally construct a backward dynamics model to reduce the reliance on accuracy in forward model predictions. We develop a novel method, called Bidirectional Model-based Policy Optimization (BMPO) to utilize both the forward model and backward model to generate short branched rollouts for policy optimization. Furthermore, we theoretically derive a tighter bound of return discrepancy, which shows the superiority of BMPO against the one using merely the forward model. Extensive experiments demonstrate that BMPO outperforms state-of-the-art model-based methods in terms of sample efficiency and asymptotic performance.
 Probing Emergent Semantics in Predictive Agents via Question Answering 
 Recent work has shown how predictive modeling can endow agents with rich knowledge of their surroundings, improving their ability to act in complex environments. We propose question-answering as a general paradigm to decode and understand the representations that such agents develop, applying our method to two recent approaches to predictive modelling - action-conditional CPC (Guo et al., 2018) and SimCore (Gregor et al., 2019). After training agents with these predictive objectives in a visually-rich, 3D environment with an assortment of objects, colors, shapes, and spatial configurations, we probe their internal state representations with a host of synthetic (English) questions, without backpropagating gradients from the question-answering decoder into the agent. The performance of different agents when probed in this way reveals that they learn to encode factual, and seemingly compositional, information about objects, properties and spatial relations from their physical environment. Our approach is intuitive, i.e. humans can easily interpret the responses of the model as opposed to inspecting continuous vectors, and model-agnostic, i.e. applicable to any modeling approach. By revealing the implicit knowledge of objects, quantities, properties and relations acquired by agents as they learn, question-conditional agent probing can stimulate the design and development of stronger predictive learning objectives.
 Consistent Structured Prediction with Max-Min Margin Markov Networks 
 Max-margin methods for binary classification such as the support vector machine (SVM) have been extended to the structured prediction setting under the name of max-margin Markov networks ($M^3N$), or more generally structural SVMs. These methods are able to model interactions between output parts and incorporate a cost between labels. Unfortunately, these methods are inconsistent when the relationship between inputs and labels is far from deterministic.
To overcome such limitations, in this paper we go beyond max-margin, defining the learning problem in terms of a ``max-min'' margin formulation. The resulting method, which we name max-min margin Markov networks ($M^4N$), provides a correction of the $M^3N$ loss that is key to achieve consistency in the general case. In this paper, we prove consistency and finite sample generalization bounds for $M^4N$ and provide an explicit algorithm to compute the estimator. The algorithm has strong statistical and computational guarantees: in a worst case scenario it achieves a generalization error of $O(1/\sqrt{n})$ for a total cost of $O(n\sqrt{n})$ marginalization-oracle calls, which have essentially the same cost as the max-oracle from $M^3N$. Experiments on multi-class classification and handwritten character recognition demonstrate the effectiveness of the proposed method over $M^3N$ networks.
 Defense Through Diverse Directions 
 In this work we develop a novel Bayesian neural network methodology to achieve strong adversarial robustness without the need for online adversarial training. Unlike previous efforts in this direction, we do not rely solely on the stochasticity of network weights by minimizing the divergence between the learned parameter distribution and a prior. Instead, we additionally require that the model maintain some expected uncertainty with respect to all input covariates. We demonstrate that by encouraging the network to distribute evenly across inputs, the network becomes less susceptible to localized, brittle features which imparts a natural robustness to targeted perturbations. We show empirical robustness on several benchmark datasets.
 Learning disconnected manifolds: a no GAN's land 
 Typical architectures of Generative Adversarial Networks make use of a unimodal latent/input distribution transformed by a continuous generator. Consequently, the modeled distribution always has connected support which is cumbersome when learning a disconnected set of manifolds. We formalize this problem by establishing a "no free lunch" theorem for the disconnected manifold learning stating an upper-bound on the precision of the targeted distribution. This is done by building on the necessary existence of a low-quality region where the generator continuously samples data between two disconnected modes.  Finally, we derive a rejection sampling method based on the norm of generator’s Jacobian and show its efficiency on several generators including BigGAN.
 Optimistic Policy Optimization with Bandit Feedback 
 Policy optimization methods are one of the most widely used classes of Reinforcement Learning (RL) algorithms. Yet, so far, such methods have been mostly analyzed from an optimization perspective, without addressing the problem of exploration, or by making strong assumptions on the interaction with the environment. 
In this paper we consider model-based RL in the tabular finite-horizon MDP setting with unknown transitions and bandit feedback. For this setting, we propose an optimistic trust region policy optimization (TRPO) algorithm for which we establish $\tilde O(\sqrt{S^2 A H^4 K})$ regret for stochastic rewards. Furthermore, we prove $\tilde O( \sqrt{ S^2 A H^4 }  K^{2/3} ) $ regret for adversarial rewards. Interestingly, this result matches previous bounds derived for the bandit feedback case, yet with known transitions. To the best of our knowledge, the two results are the first sub-linear regret bounds obtained for policy optimization algorithms with unknown transitions and bandit feedback.
 Near-optimal Regret Bounds for Stochastic Shortest Path 
 Stochastic shortest path (SSP) is a well-known problem in planning and control, in which an agent has to reach a goal state in minimum total expected cost.
In the learning formulation of the problem, the agent is unaware of the environment dynamics (i.e., the transition function) and has to repeatedly play for a given number of episodes, while learning  the problem's optimal solution. 
Unlike other well-studied models in reinforcement learning (RL), the length of an episode is not predetermined (or bounded) and is influenced by the agent's actions. 
Recently, \cite{tarbouriech2019noregret} studied this problem in the context of regret minimization, and provided an algorithm whose regret bound is inversely proportional to the square root of the minimum instantaneous cost.
In this work we  remove this dependence on the minimum cost---we give an algorithm that guarantees a regret bound of $\widetilde{O}(B^{3/2} S \sqrt{A K})$, where $B$ is an upper bound on the expected cost of the optimal policy, $S$ is the number of states, $A$ is the number of actions and $K$  is the total number of episodes. We additionally show that any learning algorithm must have at least $\Omega(B \sqrt{S A K})$ regret in the worst case.
 Optimal Bounds between f-Divergences and Integral Probability Metrics 
 The families of f-divergences (e.g. the Kullback-Leibler divergence) and Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies) are commonly used in optimization and estimation. In this work, we systematically study the relationship between these two families from the perspective of convex duality. Starting from a tight variational representation of the f-divergence, we derive a generalization of the moment generating function, which we show exactly characterizes the best lower bound of the f-divergence as a function of a given IPM. Using this characterization, we obtain new bounds on IPMs defined by classes of unbounded functions, while also recovering in a unified manner well-known results for bounded and subgaussian functions (e.g. Pinsker's inequality and Hoeffding's lemma).
 Coresets for Data-efficient Training of Machine Learning Models 
 Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.
 An end-to-end approach for the verification problem: learning the right distance 
 In this contribution, we augment the metric learning setting by introducing a parametric pseudo-distance, trained jointly with the encoder. Several interpretations are thus drawn for the learned distance-like model's output. We first show it approximates a likelihood ratio which can be used for hypothesis tests, and that it further induces a large divergence across the joint distributions of pairs of examples from the same and from different classes. Evaluation is performed under the verification setting consisting of determining whether sets of examples belong to the same class, even if such classes are novel and were never presented to the model during training. Empirical evaluation shows such method defines an end-to-end approach for the verification problem, able to attain better performance than simple scorers such as those based on cosine similarity and further outperforming widely used downstream classifiers. We further observe training is much simplified under the proposed approach compared to metric learning with actual distances, requiring no complex scheme to harvest pairs of examples.
 Stochastic Gauss-Newton Algorithms for Nonconvex Compositional Optimization 
 We develop two new stochastic Gauss-Newton algorithms for solving a class of stochastic non-convex compositional optimization problems frequently arising in practice. We consider both the expectation and finite-sum settings under standard assumptions. We use both standard stochastic and SARAH estimators for approximating function values and Jacobians. In the expectation case, we establish $O(\varepsilon^{-2})$ iteration complexity to achieve a stationary point in expectation and estimate the total number of stochastic oracle calls for both function values and its Jacobian, where $\varepsilon$ is a desired accuracy. In the finite sum case, we also estimate the same iteration complexity and the total oracle calls with high probability. To our best knowledge, this is the first time such global stochastic oracle complexity is established for stochastic Gauss-Newton methods. We illustrate our theoretical results via numerical examples on both synthetic and real datasets.
 Beyond UCB: Optimal and Efficient Contextual Bandits with Regression Oracles 
 A fundamental challenge in contextual bandits is to develop flexible, general-purpose algorithms with computational requirements no worse than classical supervised learning tasks such as classification and regression. Algorithms based on regression have shown promising empirical success, but theoretical guarantees have remained elusive except in special cases. We provide the first universal and optimal reduction from contextual bandits to online regression. We show how to transform any oracle for online regression with a given value function class into an algorithm for contextual bandits with the induced policy class, with no overhead in runtime or memory requirements. We characterize the minimax rates for contextual bandits with general, potentially nonparametric function classes, and show that our algorithm is minimax optimal whenever the oracle obtains the optimal rate for regression. Compared to previous results, our algorithm requires no distributional assumptions beyond realizability, and works even when contexts are chosen adversarially.
 Structure Adaptive Algorithms for Stochastic Bandits 
 We study reward maximisation in a wide class of structured stochastic multi-armed bandit problems, where the mean rewards of arms satisfy some given structural constraints, e.g. linear, unimodal, sparse, etc. Our aim is to develop methods that are \emph{flexible} (in that they easily adapt to different structures), \emph{powerful} (in that they perform well empirically and/or provably match instance-dependent lower bounds) and \emph{efficient} in that the per-round computational burden is small.
We develop asymptotically optimal algorithms from instance-dependent lower-bounds using iterative saddle-point solvers. Our approach generalises recent iterative methods for pure exploration to reward maximisation, where a major challenge arises from the estimation of the sub-optimality gaps and their reciprocals. Still we manage to achieve all the above desiderata. Notably, our technique avoids the computational cost of the full-blown saddle point oracle employed by previous work, while at the same time enabling finite-time regret bounds. 
Our experiments reveal that our method successfully leverages the structural assumptions, while its regret is at worst comparable to that of vanilla UCB.

 Multinomial Logit Bandit with Low Switching Cost 
 We study multinomial logit bandit with limited adaptivity, where the algorithms change their exploration actions as infrequently as possible when achieving almost optimal minimax regret. We propose two measures of adaptivity: the assortment switching cost and the more fine-grained item switching cost. We present an anytime algorithm (AT-DUCB) with $O(N \log T)$ assortment switches, almost matching the lower bound $\Omega(\frac{N \log T}{ \log \log T})$. In the fixed-horizon setting, our algorithm FH-DUCB incurs $O(N \log \log T)$ assortment switches, matching the asymptotic lower bound. We also present the ESUCB algorithm with item switching cost $O(N \log^2 T)$.
 Optimal Randomized First-Order Methods for Least-Squares Problems 
 We provide an exact asymptotic analysis of the performance of some fast randomized algorithms for solving overdetermined least-squares problems. We consider first-order methods, where the gradients are pre-conditioned by an approximation of the Hessian, based itself on a subspace embedding of the data matrix. This class of algorithms encompasses several randomized methods among the fastest solvers for least-squares problems. We focus on two classical embeddings, namely, Gaussian projections and subsampled randomized Hadamard transforms (SRHT). Our key technical innovation is the derivation of the limiting spectral density of SRHT embeddings. Leveraging this novel result, we derive the family of normalized orthogonal polynomials of the SRHT density and we find the optimal pre-conditioned first-order method and its rate of convergence. Our analysis of Gaussian embeddings proceeds similarly, and leverages classical random matrix theory results. In particular, we show that for a given sketch size, SRHT embeddings exhibits a faster rate of convergence than Gaussian embeddings. Then, we propose a new algorithm by optimizing the computational complexity over the choice of the sketching dimension. To our knowledge, our resulting algorithm yields the best known complexity for solving least-squares problems.


 Learning to Combine Top-Down and Bottom-Up Signals in Recurrent Neural Networks with Attention over Modules 
 Robust perception relies on both bottom-up and top-down signals.  Bottom-up signals consist of what's directly observed through sensation.  Top-down signals consist of beliefs and expectations based on past experience and the current reportable short-term memory, such as how the phrase `peanut butter and ...' will be completed.  The optimal combination of bottom-up and top-down information remains an open question, but the manner of combination must be dynamic and both context and task dependent. To effectively utilize the wealth of potential top-down information available, and to prevent the cacophony of intermixed signals in a bidirectional architecture, mechanisms are needed to restrict information flow.  We explore deep recurrent neural net architectures in which bottom-up and top-down signals are dynamically combined using attention.  Modularity of the architecture further restricts the sharing and communication of information. Together, attention and modularity direct information flow, which leads to reliable performance improvements in perceptual and language tasks, and in particular improves robustness to distractions and noisy data.  We demonstrate on a variety of benchmarks in language modeling, sequential image classification, video prediction and reinforcement learning that the \emph{bidirectional} information flow can improve results over strong baselines.  
 Hallucinative Topological Memory for Zero-Shot Visual Planning 
 In visual planning (VP), an agent learns to plan goal-directed behavior from observations of a dynamical system obtained offline, e.g., images obtained from self-supervised robot interaction. Bearing similarity with batch reinforcement learning (RL), VP algorithms essentially combine data-driven perception and planning. Most previous works on VP approached the problem by planning in a learned latent space, resulting in low-quality visual plans, and difficult training algorithms. Here, instead, we propose a simple VP method that plans directly in image space and displays competitive performance. We build on the semi-parametric topological memory (SPTM) method: image samples are treated as nodes in a graph, the graph connectivity is learned from image sequence data, and planning can be performed using conventional graph search methods. We make two modifications to SPTM, to make it suitable for VP. First, we propose an energy-based graph connectivity function that admits stable training using contrastive predictive coding. Second, to allow zero-shot planning in new domains, we learn a conditional VAE model that generates images given a context of the domain, and use these hallucinated samples for building the connectivity graph and planning. We show that this simple approach is competitive with SOTA VP methods, in terms of both image fidelity and success rate when using the plan to guide a trajectory-following controller.

 Multiclass Neural Network Minimization via Tropical Newton Polytope Approximation 
 The field of tropical algebra is closely linked with the domain of neural networks with piecewise linear activations, since their output can be described via tropical polynomials in the max-plus semiring.  In this work, we attempt to make use of methods stemming from a form of approximate division of such polynomials, which relies on the approximation of their Newton Polytopes, in order to minimize networks trained for multiclass classification problems. We make theoretical contributions in this domain, by proposing and analyzing methods which seek to reduce the size of such networks. In addition, we make experimental evaluations on the MNIST and Fashion-MNIST datasets, with our results demonstrating a significant reduction in network size, while retaining adequate performance.
 Spread Divergence 
 For distributions $p$ and $q$ with different supports, the divergence $\div{p}{q}$ may not exist. We define a spread divergence $\sdiv{p}{q}$ on modified $p$ and $q$ and describe sufficient conditions for the existence of such a divergence. We demonstrate how to maximize the discriminatory power of a given divergence by parameterizing and learning the spread. We also give examples of using a spread divergence to train and improve implicit generative models, including linear models (Independent Components Analysis) and non-linear models (Deep Generative Networks). 
 Online Learning with Dependent Stochastic Feedback Graphs 
 A general framework for online learning with partial information is one where feedback graphs specify which losses can be observed by the learner. We study a
challenging scenario where feedback graphs vary stochastically with time and, more importantly, where graphs and losses are dependent. This scenario
appears in several real-world applications that we describe where the outcome of actions are correlated. We devise a new algorithm for this setting that exploits
the stochastic properties of the graphs and that benefits from favorable regret guarantees. We present a detailed theoretical analysis of this algorithm, and also report the results of a series of experiments on real-world datasets, which show that our algorithm outperforms standard baselines for online learning with feedback graphs.
 Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE 
 The empirical performance of neural ordinary differential equations (NODEs) is significantly inferior to discrete-layer models on benchmark tasks (e.g. image classification). We demonstrate an explanation is the inaccuracy of existing gradient estimation methods: the adjoint method has numerical errors in reverse-mode integration; the naive method suffers from a redundantly deep computation graph. We propose the Adaptive Checkpoint Adjoint (ACA) method: ACA applies a trajectory checkpoint strategy which records the forward- mode trajectory as the reverse-mode trajectory to guarantee accuracy; ACA deletes redundant components for shallow computation graphs; and ACA supports adaptive solvers. On image classification tasks, compared with the adjoint and naive method, ACA achieves half the error rate in half the training time; NODE trained with ACA outperforms ResNet in both accuracy and test-retest reliability. On time-series modeling, ACA outperforms competing methods. Furthermore, NODE with ACA can incorporate physical knowledge to achieve better accuracy.
 Semi-Supervised Learning with Normalizing Flows 
 Normalizing flows transform a latent distribution through an invertible neural network for a flexible and pleasingly simple approach to generative modelling, while preserving an exact likelihood. We propose FlowGMM, an end-to-end approach to generative semi supervised learning with normalizing flows, using a latent Gaussian mixture model. FlowGMM is distinct in its simplicity, unified treatment of labelled and unlabelled data with an exact likelihood, interpretability, and broad applicability beyond image data. We show promising results on a wide range of applications, including AG-News and Yahoo Answers text data, tabular data, and semi-supervised image classification. We also show that FlowGMM can discover interpretable structure, provide real-time optimization-free feature visualizations, and specify well calibrated predictive distributions.
 Self-PU: Self Boosted and Calibrated Positive-Unlabeled Training 
 Many real-world applications have to tackle the Positive-Unlabeled (PU) learning problem, i.e., learning binary classifiers from a large amount of unlabeled data and a few labeled positive examples. While current state-of-the-art methods employ importance reweighting to design various biased or unbiased risk estimators, they completely ignored the learning capability of the model itself, which could provide reliable supervision. This motivates us to propose a novel Self-PU learning framework, which seamlessly integrates PU learning and self-training. Self-PU highlights three ``self''-oriented building blocks: a self-paced training algorithm that adaptively discovers and augments confident positive/negative examples as the training proceeds; a self-reweighted, instance-aware loss; and a self-distillation scheme that introduces teacher-students learning as an effective regularization for PU learning. We demonstrate the state-of-the-art performance of Self-PU on common PU learning benchmarks (MNIST and CIFAR10), which compare favorably against the latest competitors. Moreover, we study a real-world application of PU learning, i.e., classifying brain images of Alzheimer's Disease. Self-PU obtains significantly improved results on the renowned Alzheimer's Disease Neuroimaging Initiative (ADNI) database over existing methods.
 BoXHED: Boosted eXact Hazard Estimator with Dynamic covariates 
 The proliferation of medical monitoring devices makes it possible to track health vitals at high frequency, enabling the development of dynamic health risk scores that change with the underlying readings. Survival analysis, in particular hazard estimation, is well-suited to analyzing this stream of data to predict disease onset as a function of the time-varying vitals. This paper introduces the software package BoXHED (pronounced `box-head') for nonparametrically estimating hazard functions via gradient boosting. BoXHED 1.0 is a novel tree-based implementation of the generic estimator proposed in Lee et al. (2017), which was designed for handling time-dependent covariates in a fully nonparametric manner. BoXHED is also the first publicly available software implementation for Lee et al. (2017). Applying BoXHED to cardiovascular disease onset data from the Framingham Heart Study reveals novel interaction effects among known risk factors, potentially resolving an open question in clinical literature.
 Sequential Transfer in Reinforcement Learning with a Generative Model 
 We are interested in how to design reinforcement learning agents that provably reduce the sample complexity for learning new tasks by transferring knowledge from previously-solved ones. The availability of solutions to related problems poses a fundamental trade-off: whether to seek policies that are expected to immediately achieve high (yet sub-optimal) performance in the new task or whether to seek information to quickly identify an optimal solution, potentially at the cost of poor initial behaviour. In this work, we focus on the second objective when the agent has access to a generative model of state-action pairs. First, given a set of solved tasks containing an approximation of the target one, we design an algorithm that quickly identifies an accurate solution by seeking the state-action pairs that are most informative for this purpose. We derive PAC bounds on its sample complexity which clearly demonstrate the benefits of using this kind of prior knowledge. Then, we show how to learn these approximate tasks sequentially by reducing our transfer setting to a hidden Markov model and employing spectral methods to recover its parameters. Finally, we empirically verify our theoretical findings in simple simulated domains.
 One-shot Distributed Ridge Regression in High Dimensions 
 To scale up data analysis, distributed and parallel computing approaches are increasingly needed. Here we study a fundamental problem in this area: How to do ridge regression in a distributed computing environment? We study one-shot methods constructing weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high dimensional model where each predictor has a small effect, we discover several new phenomena including that the efficiency depends strongly on the signal strength, but does not degrade with many workers, the risk decouples over machines, and the unexpected consequence that the optimal weights do not sum to unity. We also propose a new optimally weighted one-shot ridge regression algorithm. Our results are supported by simulations and real data analysis.
 Can Stochastic Zeroth-Order Frank-Wolfe Method Converge Faster for Non-Convex Problems? 
 Frank-Wolfe algorithm is an efficient method for optimizing  non-convex constrained problems. However, most of existing methods focus on the first-order case. In real-world applications, the gradient is not always available. To address the problem  of lacking gradient in many applications, we propose two new stochastic zeroth-order Frank-Wolfe algorithms and theoretically proved that they have a faster convergence rate than existing methods for non-convex problems. Specifically, the function queries oracle of the proposed faster zeroth-order Frank-Wolfe (FZFW) method is $O(\frac{n^{1/2}d}{\epsilon^2})$  which can match the iteration complexity of the first-order counterpart approximately. As for the proposed faster zeroth-order conditional gradient sliding (FZCGS) method, its function queries oracle is  improved to $O(\frac{n^{1/2}d}{\epsilon})$, indicating that its iteration complexity is even better than that of its first-order counterpart NCGS-VR. In other words, the iteration complelxity of the  accelerated first-order Frank-Wolfe method NCGS-VR is suboptimal. 
Then, we  proposed a new algorithm to improve its IFO (incremental first-order oracle) to $O(\frac{n^{1/2}}{\epsilon})$. At last, the empirical studies on benchmark datasets validate our theoretical results.
 Emergence of Separable Manifolds in Deep Language Representations 
 Artificial neural networks (ANNs) have shown much empirical success in solving perceptual tasks across various cognitive modalities. While they are only loosely inspired by the biological brain, recent studies report considerable similarities between  representations extracted from task-optimized ANNs and neural populations in the brain. ANNs have subsequently become a popular model class to infer computational principles underlying complex cognitive functions, and in turn, they have also emerged as a natural testbed for applying methods originally developed to probe information in neural populations. In this work, we utilize mean-field theoretic manifold analysis, a recent  technique from computational neuroscience, to analyze the high dimensional geometry of language representations from large-scale contextual embedding models. We explore representations from different model families (BERT, RoBERTa, GPT-2, etc.) and find evidence for emergence of linguistic manifolds across layer depth (e.g., manifolds for part-of-speech and combinatory categorial grammar tags). We further observe that different encoding schemes used to obtain the representations lead to differences in whether these linguistic manifolds emerge in earlier or later layers of the network. In addition, we find that the emergence of linear separability in these manifolds is driven by a combined reduction of manifolds’ radius, dimensionality and inter-manifold correlations.
 Leveraging Procedural Generation to Benchmark Reinforcement Learning 
 We introduce Procgen Benchmark, a suite of 16 procedurally generated game-like environments designed to benchmark both sample efficiency and generalization in reinforcement learning. We believe that the community will benefit from increased access to high quality training environments, and we provide detailed experimental protocols for using this benchmark. We empirically demonstrate that diverse environment distributions are essential to adequately train and evaluate RL agents, thereby motivating the extensive use of procedural content generation. We then use this benchmark to investigate the effects of scaling model size, finding that larger models significantly improve both sample efficiency and generalization.
 A distributional view on multi objective policy optimization 
 Many real-world problems require trading off multiple competing objectives. However, these objectives are often in different units with different scales, which can make it challenging for practitioners to express numerical preferences over objectives in their native units. In this paper we propose a novel algorithm for multi-objective reinforcement learning that enables setting desired preferences for objectives in a scale-invariant way. We propose to learn a target local policy for each objective, and we use supervised learning to fit a parametric policy to a combination of these distributions. We demonstrate the effectiveness of our approach on challenging high-dimensional real and simulated robotics tasks, and show that setting different preferences in our framework allows us to trace out the space of nondominated solutions.
 Missing Data Imputation using Optimal Transport 
 Missing data is a crucial issue when applying machine learning algorithms to real-world datasets. Starting from the simple assumption that two batches extracted randomly from the same dataset should share the same distribution, we leverage optimal transport distances to quantify that criterion and turn it into a loss function to impute missing data values. We propose practical methods to minimize these losses using end-to-end learning, that can exploit or not parametric assumptions on the underlying distributions of values. We evaluate our methods on datasets from the UCI repository, in MCAR, MAR and MNAR settings. These experiments show that OT-based methods match or out-perform state-of-the-art imputation methods, even for high percentages of missing values. 
 Near-optimal sample complexity bounds for learning Latent $k-$polytopes and applications to Ad-Mixtures 
 Recently near-optimal bounds on sample complexity of Mixture of Gaussians was shown in the seminal paper \cite{HSNCAY18}.
No such results are known for Ad-mixtures.
In this paper we show that $O^*(dk/m)$ samples are sufficient to learn each of
$k-$ topic vectors of LDA, a popular Ad-mixture model, with vocabulary size $d$
and $m\in \Omega(1)$ words per document, to any constant error in $L_1$ norm.

This is a corollary of the major contribution of the current paper: the first sample complexity upper bound for the problem (introduced in \cite{BK20})
of learning the vertices of a Latent $k-$ Polytope in ${\bf R}^d$, given perturbed points from it.
The bound,  $O^*(dk/\beta)$, is optimal and applies to many stochastic models including LDA, Mixed Membership block Models(MMBM),Dirichlet Simplex Nest,  and large class of Ad-mixtures.
The parameter, $\beta$ depends on the probability laws governing individual models and in many cases can be expressed very succintly, e.g. it is equal to the average degree of
each node for MMBM, and equal to $m$ in LDA.
The tightness is proved by a nearly matching lower of $\Omega^*(dk/\beta)$ by a combinatorial construction based on a code-design.
Our upper bound proof combines two novel methods. The first is {\it vertex set certification} which, for any $k-$polytope $K$ gives convex geometry based sufficient conditions for a set of $k$ points from a larger candidate set
to be close in Hausdorff distance to the set of $k$ vertices of the polytope.
The second is {\it subset averaging} which uses $\beta$ to prove that the set of averages of all large subsets of data is a good candidate set.

 Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization 
 Training machine learning models to be robust against adversarial inputs poses seemingly insurmountable challenges. To better understand model robustness, we consider the underlying problem of learning robust representations. We develop a general definition of representation vulnerability that captures the maximum change of mutual information between the input and output distributions, under the worst-case input distribution perturbation. We prove a theorem that establishes a lower bound on the minimum adversarial risk that can be achieved for any downstream classifier based on this definition. We then propose an unsupervised learning method for obtaining intrinsically robust representations by maximizing the worst-case mutual information between input and output distributions. Experiments on downstream classification tasks and analyses of saliency maps support the robustness of the representations found using unsupervised learning with our training principle. 
  Predicting deliberative outcomes 
 We extend structured prediction to deliberative outcomes. Specifically, we learn parameterized games that can map any inputs to equilibria as the outcomes. Standard structured prediction models rely heavily on global scoring functions and are therefore unable to model individual player preferences or how they respond to others asymmetrically. Our games take as input, e.g., UN resolution to be voted on, and map such contexts to initial strategies, player utilities, and interactions. Players are then thought to repeatedly update their strategies in response to weighted aggregates of other players' choices towards maximizing their individual utilities. The output from the game is a sample from the resulting (near) equilibrium mixed strategy profile. We characterize conditions under which players converge to an equilibrium in such games and when the game parameters can be provably recovered from observations. Empirically, we demonstrate on two real voting datasets that our games can recover interpretable strategic interactions, and predict strategies for players in new settings. 
 Efficiently Solving MDPs with Stochastic Mirror Descent 
 In this paper we present a unified framework based on primal-dual stochastic mirror descent for approximately solving infinite-horizon Markov decision processes (MDPs) given a generative model. When applied to an average-reward MDP with \A total actions and mixing time bound \tmix our method computes an \eps-optimal policy with an expected \Otil(\tmix^2 \A \eps^{-2}) samples from the state-transition matrix, removing the ergodicity dependence of prior art.  When applied to a \gamma-discounted MDP with A total actions our method computes an  eps-optimal policy with an expected \Otil((1-\gamma)^{-4} \A \eps^{-2}) samples, improving over the best-known primal-dual methods while matching the state-of-the-art up to a (1-\gamma)^{-1} factor. Both methods are model-free, update state values and policy simultaneously, and run in time linear in the number of samples taken.
 Topologically Densified Distributions 
 We study regularization in the context of small sample-size learning with over-parametrized neural networks. Specifically, we shift focus from architectural properties, such as norms on the network weights, to properties of the internal representations before a linear classifier. Specifically, we impose a topological constraint on samples drawn from the probability measure induced in that space. This provably leads to mass concentration effects around the representations of training instances, i.e., a property beneficial for generalization. By leveraging previous work to impose topological constrains in a neural network setting, we provide empirical evidence (across various vision benchmarks) to support our claim for better generalization.
 TaskNorm: Rethinking Batch Normalization for Meta-Learning 
 Modern meta-learning approaches for image classification rely on increasingly deep networks to achieve state-of-the-art performance, making batch normalization an essential component of meta-learning pipelines. However, the hierarchical nature of the meta-learning setting presents several challenges that can render conventional batch normalization ineffective, giving rise to the need to rethink normalization in this setting.  We evaluate a range of approaches to batch normalization for meta-learning scenarios, and develop a novel approach that we call TaskNorm. Experiments on fourteen datasets demonstrate that the choice of batch normalization has a dramatic effect on both classification accuracy and training time for both gradient based- and gradient-free meta-learning approaches. Importantly, TaskNorm is found to consistently improve performance. Finally, we provide a set of best practices for normalization that will allow fair comparison of meta-learning algorithms.
 Puzzle Mix: Exploiting Saliency and Local Statistics for Optimal Mixup 
 While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets.
 Tensor denoising and completion based on ordinal observations 
 Higher-order tensors arise frequently in applications such as neuroimaging, recommendation system, and social network analysis. We consider the problem of low-rank tensor estimation from possibly incomplete, ordinal-valued observations. Two related problems are studied, one on tensor denoising and another on tensor completion. We propose a multi-linear cumulative link model, develop a rank-constrained M-estimator, and obtain theoretical accuracy guarantees. Our mean squared error bound enjoys a faster convergence rate than previous results, and we show that the proposed estimator is minimax optimal under the class of low-rank models. Furthermore, the procedure developed serves as an efficient completion method which guarantees consistent recovery of an order-K (d,...,d)-dimensional low-rank tensor using only O(Kd) noisy, quantized observations. We demonstrate the outperformance of our approach over previous methods on the tasks of clustering and collaborative filtering. 
 Provably Efficient Model-based Policy Adaptation 
 The high sample complexity of reinforcement learning challenges its use in practice. A promising approach is to quickly adapt pre-trained policies to new environments. Existing methods for this policy adaptation problem typically rely on domain randomization and meta-learning, by sampling from some distribution of target environments during pre-training, and thus face difficulty on out-of-distribution target environments. We propose new model-based mechanisms that are able to make online adaptation in unseen target environments, by combining ideas from no-regret online learning and adaptive control. We prove that the approach learns policies in the target environment that can quickly recover trajectories from the source environment, and establish the rate of convergence in general settings. We demonstrate the benefits of our approach for policy adaptation in a diverse set of continuous control tasks, achieving the performance of state-of-the-art methods with much lower sample complexity. 
 Momentum Improves Normalized SGD 
 We provide an improved analysis of normalized SGD showing that adding momentum provably removes the need for large batch sizes on non-convex objectives. Then, we consider the case of objectives with bounded second derivative and show that in this case a small tweak to the momentum formula allows normalized SGD with momentum to find an $\epsilon$-critical point in $O(1/\epsilon^{3.5})$ iterations, matching the best-known rates without accruing any logarithmic factors or dependence on dimension. We provide an adaptive learning rate schedule that automatically improves convergence rates when the variance in the gradients is small. Finally, we show that our method is effective when employed on popular large scale tasks such as ResNet-50 and BERT pretraining, matching the performance of the disparate methods used to get state-of-the-art results on both tasks.
 Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-Layer Networks 
 We develop exact representations of two layer neural networks with rectified linear units in terms of a single convex program with number of variables polynomial in the number of training samples and number of hidden neurons. Our theory utilizes semi-infinite duality and minimum norm regularization. Moreover, we show that certain standard multi-layer convolutional neural networks are equivalent to L1 regularized linear models in a polynomial sized discrete Fourier feature space. We also introduce exact semi-definite programming representations of convolutional and fully connected linear multi-layer networks which are polynomial size in both the sample size and dimension. 
 Implicit differentiation of Lasso-type models for hyperparameter optimization 
 Setting regularization parameters for Lasso-type estimators is notoriously difficult, though crucial for obtaining the best accuracy. The most popular hyperparameter optimization approach is grid-search on a held-out dataset. However, grid-search requires to choose a predefined grid of parameters and scales exponentially in the number of parameters. Another class of approaches casts hyperparameter optimization as a bi-level optimization problem, typically solved by gradient descent. The key challenge for these approaches is the estimation of the gradient w.r.t. the hyperparameters. Computing that gradient via forward or backward automatic differentiation usually suffers from high memory comsumption, while implicit differentiation typically involves solving a linear system which can be prohibitive and numerically unstable. In addition, implicit differentiation usually assumes smooth loss functions, which is not the case of Lasso-type problems. This work introduces an efficient implicit differentiation algorithm, without matrix inversion, tailored for Lasso-type problems. Our proposal scales to high-dimensional data by leveraging the sparsity of the solutions. Empirically, we demonstrate that the proposed method outperforms a large number of standard methods for hyperparameter optimization.
 T-Basis: a Compact Representation for Neural Networks 
 We introduce T-Basis, a novel concept for a compact representation of a set of tensors, each of an arbitrary shape, which is often seen in Neural Networks. Each of the tensors in the set is modelled using Tensor Rings, though the concept is applicable to other Tensor Networks as well. Owing its name to the T-shape of nodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally shaped three-dimensional tensors, used to represent Tensor Ring nodes. Such representation allows us to parameterize the tensor set with a small number of parameters (coefficients of the T-Basis tensors), scaling logarithmically with the size of each tensor in the set, and linearly with the dimensionality of T-Basis. We evaluate the proposed approach on the task of neural network compression, and demonstrate that it reaches high compression rates at acceptable performance drops. Finally, we analyze memory and operation requirements of the compressed networks, and conclude that T-Basis networks are equally well suited for training and inference in resource-constrained environments, as well as usage on the edge devices.
 The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization 
 Modern deep learning models employ considerably more parameters than required to fit the training data. Whereas conventional statistical wisdom suggests such models should drastically overfit, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a double descent curve, in which increasing a model's capacity causes its test error to first decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has nonmonotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratic with the dataset size.

 Stochastic Optimization for Non-convex Inf-Projection Problems 
 In this paper, we study a family of non-convex and possibly non-smooth inf-projection minimization problems, where the target objective function is equal to minimization of a joint function over another variable. This problem include difference of convex (DC) functions and a family of bi-convex functions as special cases. We develop stochastic algorithms and establish their first-order convergence for finding a (nearly) stationary solution of the target non-convex function under different conditions of the component functions. To the best of our knowledge, this is the first work that comprehensively studies stochastic optimization of non-convex inf-projection minimization problems with provable convergence guarantee. Our algorithms enable efficient stochastic optimization of a family of non-decomposable DC functions and a family of bi-convex functions. To demonstrate the power of the proposed algorithms we consider an important application in variance-based regularization, and experiments verify the effectiveness of our inf-projection based formulation and the proposed stochastic algorithm in comparison with previous stochastic algorithms based on the min-max formulation for achieving the same effect.
 Learning General-Purpose Controllers via Locally Communicating Sensorimotor Modules 
 Reinforcement learning is typically concerned with learning control policies tailored to a particular agent. We investigate whether there exists a single policy that generalizes to controlling a wide variety of agent morphologies - ones in which even dimensionality of state and action spaces changes. Such a policy would distill general and modular sensorimotor patterns that can be applied to control arbitrary agents. We propose a policy expressed as a collection of identical modular neural network components for each of the agent’s actuators. Every module is only responsible for controlling its own actuator and receives information from its local sensors. In addition, messages are passed between modules, propagating information between distant modules. A single modular policy can successfully generate locomotion behaviors for over 20 planar morphologies such as monopod hoppers, quadrupeds, bipeds and generalize to variants not seen during training - a process that would normally require training and manual hyper-parameter tuning for each morphology. We observe a wide variety of drastically diverse locomotion styles across morphologies as well as centralized coordination emerging via message passing between decentralized modules purely from the reinforcement learning objective.
 On Layer Normalization in the Transformer Architecture 
 The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.
 Fractal Gaussian Networks: A sparse random graph model based on Gaussian Multiplicative Chaos 
 We propose a novel stochastic network model, called Fractal Gaussian Network (FGN), that embodies well-defined and analytically tractable fractal structures. Such fractal structures have been empirically observed in diverse applications. FGNs interpolate continuously between the popular purely random geometric graphs (a.k.a. the Poisson Boolean network), and random graphs with increasingly fractal behavior. In fact, they form a parametric family of sparse random geometric graphs that are parametrised by a fractality parameter $\nu$ which governs the strength of the fractal structure. FGNs are driven by the latent spatial geometry of Gaussian Multiplicative Chaos (GMC), a canonical model of fractality in its own right. We explore the natural question of detecting the presence of fractality and the problem of parameter estimation based on observed network data. Finally, we explore fractality in community structures by unveiling a natural stochastic block model in the setting of FGNs.

 Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations 
 Adversarial examples are malicious inputs crafted to induce misclassification. Commonly studied \emph{sensitivity-based} adversarial examples introduce semantically-small changes to an input that result in a different model prediction. 
This paper studies a complementary failure mode, \emph{invariance-based} adversarial examples, that introduce minimal semantic changes that modify an input's true label yet preserve the model's prediction.
We demonstrate fundamental tradeoffs between these two types of adversarial examples.

We show that defenses against sensitivity-based attacks 
actively harm a model's accuracy on invariance-based attacks, and that new approaches are needed to resist both attack types.
In particular, we break state-of-the-art adversarially-trained and \emph{certifiably-robust} models by generating small perturbations that the models are (provably) robust to, yet that change an input's class according to human labelers.
Finally, we formally show that the existence of excessively invariant classifiers arises from the presence of \emph{overly-robust} predictive features in standard datasets. 
 Non-autoregressive Translation with Disentangled Context Transformer 
 State-of-the-art neural machine translation models generate a translation from left to right and every step is conditioned on the previously generated tokens. The sequential nature of this generation process causes fundamental latency in inference since we cannot generate multiple tokens in each sentence in parallel. We propose an attention-masking based model, called Disentangled Context (DisCo) transformer, that simultaneously generates all tokens given different contexts. The DisCo transformer is trained to predict every output token given an arbitrary subset of the other reference tokens. We also develop the parallel easy-first inference algorithm, which iteratively refines every token in parallel and reduces the number of required iterations. Our extensive experiments on 7 translation directions with varying data sizes demonstrate that our model achieves competitive, if not better, performance compared to the state of the art in non-autoregressive machine translation while significantly reducing decoding time on average. 
 Randomly Projected Additive Gaussian Processes for Regression 
 Gaussian processes (GPs) provide flexible distributions over functions, with inductive biases controlled by a kernel. However, in many applications 
Gaussian processes can struggle with even moderate input dimensionality. Learning a low dimensional projection can help alleviate this curse of dimensionality, but introduces many trainable hyperparameters, which can be cumbersome, especially in the small data regime. We use additive sums of kernels for GP regression, where each kernel operates on a different random projection of its inputs. Surprisingly, we find that as the number of random projections increases, the predictive performance of this approach quickly converges to the performance of a kernel operating on the original full dimensional inputs, over a wide range of data sets, even if we are projecting into a single dimension. As a consequence, many problems can remarkably be reduced to one dimensional input spaces, without learning a transformation. We prove this convergence and its rate, and additionally propose a deterministic approach that converges more quickly than purely random projections. Moreover, we demonstrate our approach can achieve faster inference and improved predictive accuracy for high-dimensional inputs compared to kernels in the original input space. 
 Online Learning for Active Cache Synchronization 
 Existing multi-armed bandit (MAB) models make two implicit assumptions: an arm generates a payoff only when it is played, and the agent observes every payoff that is generated. This paper introduces synchronization bandits, a MAB variant where all arms generate costs at all times, but the agent observes an arm's instantaneous cost only when the arm is played. Synchronization MABs are inspired by online caching scenarios such as Web crawling, where an arm corresponds to a cached item and playing the arm means downloading its fresh copy from a server. While not refreshed, each cached item grows progressively stale with time, continuously generating stochastic costs due to degraded cache performance, but the cache doesn't know how much until it refreshes the item and computes the difference between the item’s fresh version and the old one. We present MirrorSync, an online learning algorithm for synchronization bandits, establish an adversarial regret of $O(T^{2/3})$ for it, and show how to make it efficient in practice.
 Doubly Stochastic Variational Inference for Neural Processes with Hierarchical Latent Variables 
 Neural processes (NPs) constitute a family of variational approximate models for stochastic processes with promising properties in computational efficiency and uncertainty quantification. These processes use neural networks with latent variable inputs to induce a predictive distribution.
However, the expressiveness of vanilla NPs is limited as they only use a global latent variable, while target-specific local variation may be crucial sometimes. To address this challenge, we investigate NPs systematically and present a new variant of NP model that we call Doubly Stochastic Variational Neural Process (DSVNP). This model combines the global latent variable and local latent variables for prediction. We evaluate this model in several experiments, and our results demonstrate competitive prediction performance in multi-output regression and uncertainty estimation in classification.
 The Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits 
 We propose a neural information processing system which is obtained by re-purposing the function of a biological neural circuit model to govern simulated and real-world control tasks. Inspired by the structure of the nervous system of the soil-worm, C. elegans, we introduce ordinary neural circuits (ONCs), defined as the model of biological neural circuits reparameterized for the control of alternative tasks. We first demonstrate that ONCs realize networks with higher maximum flow compared to arbitrary wired networks. We then learn instances of ONCs to control a series of robotic tasks, including the autonomous parking of a real-world rover robot. For reconfiguration of the purpose of the neural circuit, we adopt a search-based optimization algorithm. Ordinary neural circuits perform on par and, in some cases, significantly surpass the performance of contemporary deep learning models. ONC networks are compact, 77% sparser than their counterpart neural controllers, and their neural dynamics are fully interpretable at the cell-level.
 Disentangling Trainability and Generalization in Deep Neural Networks 
 A longstanding goal in the theory of deep learn-ing is to characterize the conditions under whicha given neural network architecture will be train-able, and if so, how well it might generalize tounseen data. In this work, we provide such a char-acterization in the limit of very wide and verydeep networks, for which the analysis simplifiesconsiderably.  For wide networks, the trajectoryunder gradient descent is governed by the NeuralTangent Kernel (NTK), and for deep networks,the NTK itself maintains only weak data depen-dence.  By analyzing the spectrum of the NTK,we formulate necessary conditions for trainabilityand generalization across a range of architectures,including Fully Connected Networks (FCNs) andConvolutional  Neural  Networks  (CNNs).   Weidentify large regions of hyperparameter spacefor which networks can memorize the training setbut completely fail to generalize.  We find thatCNNs without global average pooling behave al-most identically to FCNs,  but that CNNs withpooling have markedly different and often bettergeneralization performance. A thorough empiri-cal investigation of these theoretical results showsexcellent agreement on real datasets.
 Margin-aware Adversarial Domain Adaptation with Optimal Transport 
 In this paper, we propose a new theoretical analysis of unsupervised domain adaptation that relates notions of large margin separation, adversarial learning and optimal transport. This analysis generalizes previous work on the subject by providing a bound on the target margin violation rate, thus reflecting a better control of the quality of separation between classes in the target domain than bounding the misclassification rate. The bound also highlights the benefit of a large margin separation on the source domain for adaptation and introduces an optimal transport (OT) based distance between domains that has the virtue of being task-dependent, contrary to other approaches. From the obtained theoretical results, we derive a novel algorithmic solution for domain adaptation that introduces a novel shallow OT-based adversarial approach and outperforms other OT-based DA baselines on several simulated and real-world classification tasks.
 Optimally Solving Two-Agent Decentralized POMDPs Under One-Sided Information Sharing  
 Optimally solving decentralized partially observable Markov decision processes under either full or no information sharing received significant attention in recent years. However, little is known about how partial information sharing affects existing theory and algorithms. This paper addresses this question for a team of two agents, with one-sided information sharing---\ie both agents have imperfect information about the state of the world, but only one has access to what the other sees and does. From the perspective of a central planner, we show that the original problem can be reformulated into an equivalent information-state Markov decision process and solved as such. Besides, we prove that the optimal value function exhibits a specific form of uniform continuity. We also present a heuristic search algorithm utilizing this property and providing the first results for this family of problems.
 Progressive Identification of True Labels for Partial-Label Learning 
 Partial-label learning is one of the important weakly supervised learning problems, where each training example is equipped with a set of candidate labels that contains the true label. Most existing methods elaborately designed learning objectives as constrained optimizations that must be solved in specific manners, making their computational complexity a bottleneck for scaling up to big data. The goal of this paper is to propose a novel framework of partial-label learning without implicit assumptions on the model or optimization algorithm. More specifically, we propose a general estimator of the classification risk, theoretically analyze the classifier-consistency, and establish an estimation error bound. We then explore a progressive identification method for approximately minimizing the proposed risk estimator, where the update of the model and identification of true labels can be conducted in a seamless manner. The resulting algorithm is model-independent and loss-independent, and compatible with stochastic optimization. Thorough experiments demonstrate it sets the new state of the art.
 Undirected Graphical Models as Approximate Posteriors 
 The representation of the approximate posterior is a critical aspect of effective variational autoencoders (VAEs). Poor choices for the approximate posterior have a detrimental impact on the generative performance of VAEs due to the mismatch with the true posterior. We extend the class of posterior models that may be learned by using undirected graphical models. We develop an efficient method to train undirected approximate posteriors by showing that the gradient of the training objective with respect to the parameters of the undirected posterior can be computed by backpropagation through Markov chain Monte Carlo updates. We apply these gradient estimators for training discrete VAEs with Boltzmann machines as approximate posteriors and demonstrate that undirected models outperform previous results obtained using directed graphical models.
 Clinician-in-the-Loop Decision Making: Reinforcement Learning with Near-Optimal Set-Valued Policies 
 Standard reinforcement learning aims to find an optimal policy that identifies the best action for each state. However, in healthcare settings, many actions may be near-equivalent with respect to the reward (e.g., survival). We consider an alternative objective -- learning set-valued policies to capture near-equivalent actions that lead to similar cumulative rewards. We propose a model-free, off-policy algorithm based on temporal difference learning and a near-greedy action selection heuristic. We analyze the theoretical properties of the proposed algorithm, providing optimality guarantees and demonstrate our approach on simulated environments and a real clinical task. Empirically, the proposed algorithm exhibits reasonably good convergence properties and discovers meaningful near-equivalent actions. Our work provides theoretical, as well as practical, foundations for clinician-in-the-loop decision making, in which clinicians/patients can incorporate additional knowledge (e.g., side effects and patient preference) to distinguish among near-equivalent actions. 
 Breaking the Curse of Many Agents: Provable Mean Embedding $Q$-Iteration for Mean-Field Reinforcement Learning 
 Multi-agent reinforcement learning (MARL) achieves significant empirical successes. However, MARL suffers from the curse of many agents. In this paper, we exploit the symmetry of agents in MARL. In the most generic form, we study a mean-field MARL problem. Such a mean-field MARL is defined on mean-field states, which are distributions that are supported on continuous space. Based on the mean embedding of the distributions, we propose MF-FQI algorithm, which solves the mean-field MARL and establishes a non-asymptotic analysis for MF-FQI algorithm. We highlight that MF-FQI algorithm enjoys a ``blessing of many agents'' property in the sense that a larger number of observed agents improves the performance of MF-FQI algorithm.
 It's Not What Machines Can Learn, It's What We Cannot Teach 
 Can deep neural networks learn to solve any task, and in particular problems of high complexity?
This question attracts a lot of interest, with recent works tackling computationally hard tasks such as the traveling salesman problem and satisfiability.
In this work we offer a different perspective on this question.
Given the common assumption that NP != coNP we prove that any polynomial-time sample generator for an NP-hard problem samples, in fact, from an easier sub-problem.
We empirically explore a case study, Conjunctive Query Containment, and show how common data generation techniques generate biased data-sets that lead practitioners to over-estimate model accuracy.
Our results suggest that machine learning approaches that require training on a dense uniform sampling from the target distribution cannot be used to solve computationally hard problems, the reason being the difficulty of generating sufficiently large and unbiased training sets.

 A Generative Model for Molecular Distance Geometry 
 Computing equilibrium states for many-body systems, such as molecules, is a long-standing challenge. In the absence of methods for generating statistically independent samples, great computational effort is invested in simulating these systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates such samples for molecules from their graph representations.
Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties.
 Normalized Loss Functions for Deep Learning with Noisy Labels 
 Robust loss functions are essential for training accurate deep neural networks (DNNs) in the presence of noisy (incorrect) labels. It has been shown that the commonly used Cross Entropy (CE) loss is not robust to noisy labels. Whilst new loss functions have been designed, they are only partially robust. In this paper, we theoretically show by applying a simple normalization that: \emph{any loss can be made robust to noisy labels}. However, in practice, simply being robust is not sufficient for a loss function to train accurate DNNs. By investigating several robust loss functions, we find that they suffer from a problem of {\em underfitting}. To address this, we propose a framework to build robust loss functions called \emph{Active Passive Loss} (APL). APL combines two robust loss functions that mutually boost each other. Experiments on benchmark datasets demonstrate that the family of new loss functions created by our APL framework can consistently outperform state-of-the-art methods by large margins, especially under large noise rates such as 60\% or 80\% incorrect labels.
 Multidimensional Shape Constraints 
 We propose new multi-input shape constraints across four intuitive categories:  complements, diminishers, dominance, and unimodality constraints. We show these shape constraints can be checked and even enforced when training machine-learned models for linear models, generalized additive models, and the nonlinear function class of multi-layer lattice models. Toy examples and real-world experiments illustrate how the different shape constraints can be used to increase interpretability and better regularize machine-learned models. 
 Few-shot Domain Adaptation by Causal Mechanism Transfer 
 We study few-shot supervised domain adaptation (DA) for regression problems, where only a few labeled target domain data and many labeled source domain data are available. Many of the current DA methods base their transfer assumptions on either parametrized distribution shift or apparent distribution similarities, e.g., identical conditionals or small distributional discrepancies. However, these assumptions may preclude the possibility of adaptation from intricately shifted and apparently very different distributions. To overcome this problem, we propose mechanism transfer, a meta-distributional scenario in which a data generating mechanism is invariant among domains. This transfer assumption can accommodate nonparametric shifts resulting in apparently different distributions while providing a solid statistical basis for DA. We take the structural equations in causal modeling as an example and propose a novel DA method, which is shown to be useful both theoretically and experimentally. Our method can be seen as the first attempt to fully leverage the invariance of structural causal models for DA.
 Latent Space Factorisation and Manipulation via Matrix Subspace Projection 
 We tackle the problem disentangling the latent space of an autoencoder in order to separate labelled attribute information from other characteristic information. This then allows us to change selected attributes while preserving other information. Our method, matrix subspace projection, is much simpler than previous approaches to latent space factorisation, for  example not requiring multiple discriminators or a careful weighting among loss functions.
Furthermore our new model can be applied to autoencoders as a plugin, and works across diverse domains such as images or text. We demonstrate the utility of our method for attribute manipulation in autoencoders trained across varied domains, using both human evaluation and automated methods. The quality of generation of our new model (e.g. reconstruction, conditional generation) is highly competitive to a number of strong baselines. 
 Stronger and Faster Wasserstein Adversarial Attacks 
 Deep models, while being extremely flexible and accurate, are surprisingly vulnerable to ``small, imperceptible'' perturbations known as adversarial attacks. While the majority of existing attacks focuses on measuring perturbations under the $\ell_p$ metric, Wasserstein distance, which takes geometry in pixel space into account, has long known to be a better metric for measuring image quality and has recently risen as a compelling alternative to the $\ell_p$ metric in adversarial attacks. However, constructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms. We address this gap in two ways: (a) we develop an exact yet efficient projection operator to enable a stronger projected gradient attack; (b) we show for the first time that conditional gradient method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints. Our algorithms not only converge faster but also generate much stronger attacks. For instance, we decrease the accuracy of a residual network on CIFAR-10 to less than $30\%$ within a Wasserstein perturbation ball of radius $0.005$, in contrast to $65.2\%$ using the  previous state-of-the-art attack based on approximate projection.
 Representing Unordered Data Using Multiset Automata and Complex Numbers 
 Unordered, variable-sized inputs arise in many settings across multiple fields.  The ability for set- and multiset- oriented neural networks to handle this type of input has been the focus of much work in recent years.  We propose to represent multisets using complex-weighted multiset automata and show how the multiset representations of certain existing neural architectures can be viewed as special cases of ours.  Namely, (1) we provide a new theoretical and intuitive justification for the Transformer model's representation of positions using sinusoidal functions, and (2) we extend the DeepSets model to use complex numbers, enabling it to outperform the existing model on an extension of one of their tasks.  
 Unbiased Risk Estimators Can Mislead: A Case Study of Learning with Complementary Labels 
 In weakly supervised learning, unbiased risk estimators (URE) are powerful tools for estimating the risk of classifiers when the training distribution differs from the test distribution. However, they lead to overfitting in many problem settings if deep networks are chosen as the classifiers. In this paper, we investigate reasons for such overfitting by studying learning with complementary labels. We argue that the quality of gradient estimation matters more than risk estimation in risk minimization. Theoretically, we find UREs give unbiased gradient estimators (UGE). Empirically, we find UGEs have a huge variance, though the bias is zero; their direction is far away from the true gradient in expectation, though the expected direction is the same as the true gradient. Hence we advocate to use biased risk estimators by taking into account the bias-variance tradeoff and the directional similarity of gradient estimation, and experiments show that they successfully mitigate the overfitting due to UREs/UGEs.
 A simpler approach to accelerated optimization: iterative averaging meets optimism 
 Recently there have been several attempts to improve the rates of convergence achievable for smooth objectives. In particular, several recent papers have attempted to extend Nesterov's accelerated algorithm to stochastic and variance-reduced optimization. In this paper, we show that there is a simpler approach to obtaining accelerated rates: applying generic, well-known optimistic online learning algorithms and using the online average of their predictions to query the (deterministic or stochastic) first-order optimization oracle at each time step. In particular, we tighten the recent results of Cutkosky (2019) to demonstrate theoretically that online averaging results in a reduced optimization gap, independently of the algorithm involved. Then, we show that a simple tuning of existing generic optimistic online learning algorithms (e.g., Joulani et al [2017]), when combined with the reduced error quantified above, naturally results in optimal accelerated rates. \todoc{what would you consider unnatural? get rid of this word?}
Importantly, the smooth objective may or may not be strongly-convex, and the rates are nevertheless optimal for both stochastic and deterministic first-order oracles. 
We further show that the same ideas transfer to variance-reduced optimization. In each case, the proofs are much simpler than the previous work, such as the new derivations of accelerated algorithms based on a primal-dual view (Wang and Abernethy, 2018) or the ideas based on linear coupling (Allen-Zhu and Orecchia, 2017). Importantly, we also provide algorithms that maintain the ``universality'' property, meaning that the same algorithm achieves the optimal rate for smooth and non-smooth objectives without further prior knowledge, generalizing the results of Kavis et al (2019) and solving a number of their open problems.

 Representation Learning via Adversarially-Contrastive Optimal Transport 
 In this paper, we study the problem of learning compact (low-dimensional) representations for sequential data that captures its implicit spatio-temporal cues. To maximize extraction of such informative cues from the data, we set the problem within the context of contrastive representation learning and to that end propose a novel objective via optimal transport. Specifically, our formulation seeks a low-dimensional subspace representation of the data that jointly (i) maximizes the distance of the data (embedded in this subspace) from an adversarial data distribution under the optimal transport, a.k.a. the Wasserstein distance, (ii) captures the temporal order, and (iii) minimizes the data distortion. To generate the adversarial distribution, we propose to use a Generative Adversarial Network (GAN) with novel regularizers. Our full objective can be cast as a subspace learning problem on the Grassmann manifold, and can be solved efficiently via Riemannian optimization. To empirically study our formulation, we provide elaborate experiments on the task of human action recognition in video sequences. Our results demonstrate state-of-the-art performance against challenging baselines.
 Simple and Scalable Epistemic Uncertainty Estimation Using a Single Deep Deterministic Neural Network 
 We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon Deep Ensembles on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN, while maintaining competitive accuracy.
 Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data 
 This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.
 When Does Self-Supervision Help Graph Convolutional Networks? 
 Self-supervision as an emerging learning technique has been employed to train convolutional neural networks (CNNs) for more transferrable, generalizable, and robust representation learning of image data.  Its introduction to graph convolutional networks (GCNs) operating on graph data is however rarely explored.  In this study, we report the first systematic exploration and assessment of incorporating self-supervision into  GCNs.  We first elaborate three mechanisms to incorporate self-supervision into GCNs, analyze the limitations of pretraining & finetuning and self-training, and proceed to focus on multi-task learning. Moreover, we design three novel self-supervised learning tasks for GCNs with both theoretical rationales and numerical comparisons.  Lastly, we further integrate multi-task self-supervision into graph adversarial training. Our results show that, with properly designed task forms and incorporation mechanisms, self-supervision benefits GCNs in gaining more generalizability and robustness. 
 Angular Visual Hardness 
 Recent convolutional neural networks (CNNs) have led to impressive performance but often suffer from poor calibration. They tend to be overconfident, with the model confidence not always reflecting the underlying true ambiguity and hardness. In this paper, we propose angular visual hardness (AVH), a score given by the normalized angular distance between the sample feature embedding and the target classifier to measure sample hardness. We validate this score with in-depth and extensive scientific study and observe that CNN models with the highest accuracy also have the best AVH scores. This agrees with an earlier finding that state-of-art models improve on the classification of harder examples. We observe that the training dynamics of AVH is vastly different compared to the training loss. Specifically, AVH quickly reaches a plateau for all samples even though the training loss keeps improving. This suggests the need for designing better loss functions that can target harder examples more effectively. We also find that AVH has a statistically significant correlation with human visual hardness. Finally, we demonstrate the benefit of AVH to a variety of applications such as self-training for domain adaptation and domain generalization. 
 Informative Dropout for Robust Representation Learning: A Shape-bias Perspective 
 Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. Specifically, with inspiration from human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Through extensive experiments, we observe enhanced robustness in various tasks (domain generalization, few-shot classification, robustness against random corruptions and adversarial robustness). Moreover, we show that as a local algorithm, InfoDrop can further improve performance when incorporated with other algorithms for global structure modeling (e.g. Non-Local blocks). To the best of our knowledge, this work is the first attempt to improve different kinds of robustness in a unified model, shedding new light on relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms.
 Provable Smoothness Guarantees for Black-Box Variational Inference 
 Black-box variational inference tries to approximate a complex target distribution through a gradient-based optimization of the parameters of a simpler distribution. Provable convergence guarantees require structural properties of the objective. This paper shows that for location-scale family approximations, if the target is M-Lipschitz smooth, then so is the “energy” part of the variational objective. The key proof idea is to describe gradients in a certain inner-product space, thus permitting the use of Bessel’s inequality. This result gives bounds on the location of the optimal parameters, and is a key ingredient for convergence guarantees.
 Graph Filtration Learning 
 We propose an approach to learning with graph-structured data in the problem domain of graph classification. In particular, we present a novel type of readout operation to aggregate node features into a graph-level representation. To this end, we leverage persistent homology computed via a real-valued, learnable, filter function. We establish the theoretical foundation for differentiating through the persistent homology computation. Empirically, we show that this type of readout operation compares favorably to previous techniques, especially when the graph connectivity structure is informative for the learning problem.
 The Implicit and Explicit Regularization Effects of Dropout 
 Dropout is a widely-used regularization technique, often required to obtain state-of-the-art for a number of architectures. This work observes that dropout introduces two distinct but entangled regularization effects: an explicit effect which occurs since dropout modifies the expected training objective, and an implicit effect from stochasticity in the dropout gradients. We disentangle these two effects, deriving analytic simplifications which characterize each effect in terms of the derivatives of the model and loss. Our simplified regularizers accurately capture the important aspects of dropout: we demonstrate that they can faithfully replace dropout in practice.
 Learning with Bounded Instance- and Label-dependent Label Noise 
 Instance- and Label-dependent label Noise (ILN) widely exists in real-world datasets but has been rarely studied. In this paper, we focus on Bounded Instance- and Label-dependent label Noise (BILN), a particular case of ILN where the label noise rates---the probabilities that the true labels of examples flip into the corrupted ones---have upper bound less than $1$. Specifically, we introduce the concept of distilled examples, i.e. examples whose labels are identical with the labels assigned for them by the Bayes optimal classifier, and prove that under certain conditions classifiers learnt on distilled examples will converge to the Bayes optimal classifier. Inspired by the idea of learning with distilled examples, we then propose a learning algorithm with theoretical guarantees for its robustness to BILN. At last, empirical evaluations on both synthetic and real-world datasets show effectiveness of our algorithm in learning with BILN.
 Maximum-and-Concatenation Networks 
 While successful in many fields, deep neural networks (DNNs) still suffer from some open problems such as bad local minima and unsatisfactory generalization performance. Despite the progresses achieved during the past several years, those difficulties have not been overcome completely and are still preventing DNNs from being more successful. In this work, we propose a novel architecture called Maximum-and-Concatenation Networks (MCN) to try eliminating bad local minima and improving generalization ability as well. MCN is a multi-layer network concatenated by a linear part and the maximum of two piecewise smooth functions, and it can approximate a wide range of functions used in practice. Remarkably, we prove that MCN has a very nice property; that is, \emph{every local minimum of an $(l+1)$-layer MCN can be better than, at least as good as, the global minima of the network consisting of its first $l$ layers}. In other words, via increasing the network depth, MCN can autonomously improve the goodness of its local minima. What is more, \emph{it is easy to plug MCN into an existing deep model to make it also have this property}. Finally, under mild conditions, we show that MCN can approximate certain continuous functions arbitrarily well with \emph{high efficiency}; that is, the covering number of MCN is much smaller than most existing DNNs such as deep ReLU. Based on this, we further provide a tight generalization bound to guarantee the inference ability of MCN when dealing with testing samples. Experiments on the CIFAR datasets confirm the effectiveness of MCN.
 Non-separable Non-stationary random fields 
 We describe a framework for constructing non-separable non-stationary random fields that is based on an infinite mixture of convolved stochastic processes. When the mixing process is stationary and the convolution function is non-stationary we arrive at expressive kernels that are available in closed form. When the mixing is non-stationary and the convolution function is stationary the resulting random fields exhibit varying degrees of non-separability that better preserve local structure. These kernels have natural interpretations through corresponding stochastic differential equations (SDEs) and are demonstrated on a range of synthetic benchmarks and spatio-temporal applications in geostatistics and machine learning. We show how a single Gaussian process (GP) with these kernels can computationally and statistically outperform both separable and existing non-stationary non-separable approaches such as treed GPs and deep GP constructions. 
 Hierarchical Verification for Adversarial Robustness 
 We introduce a new framework for the exact pointwise ℓp robustness verification problem that exploits the layer-wise geometric structure of deep feed-forward networks with rectified linear activations (ReLU networks). The activation regions of the network partition the input space, and one can verify the ℓp robustness around a point by checking all the activation regions within the desired radius. The GeoCert algorithm (Jordan et al., NeurIPS 2019) treats this partition as a generic polyhedral complex to detect which region to check next. Instead, our LayerCert framework considers the nested hyperplane arrangement structure induced by the layers of the ReLU network and explores regions in a hierarchical manner. We show that, under certain conditions on the algorithm parameters, LayerCert provably reduces the number and size of the convex programs that one needs to solve compared to GeoCert. Furthermore, the LayerCert framework allows one to incorporate lower bounding routines based on convex relaxations to further improve performance. Experimental results demonstrate that LayerCert can significantly reduce both the number of convex programs solved and the wall-clock time over the state-of-the-art.


 Kernel Methods for Cooperative Multi-Agent Learning with Delays 
 Cooperative multi-agent decision making involves a group of agents collectively solving individual learning problems, while communicating over a (sparse) network with delays. In this paper, we consider the kernelised contextual bandit problem, where the reward obtained by an agent is an arbitrary linear function of the contexts' images in the related reproducing kernel Hilbert space (RKHS), and a group of agents must cooperate to collectively solve their unique decision problems. We propose Coop-KernelUCB that provides near-optimal bounds on the per-agent regret in this setting, and is both computationally and communicatively efficient. For special cases of the cooperative problem, we also provide variants of Coop-KernelUCB that provides optimal per-agent regret. In addition, our algorithm generalizes several existing results in the multi-agent bandit setting. Finally, on a series of both synthetic and real-world multi-agent network benchmarks, our algorithm significantly outperforms existing clustering or consensus-based algorithms, even in the linear setting.
 Optimal Robust Learning of Discrete Distributions from Batches 
 Many applications, including natural language processing, sensor networks, collaborative filtering, and federated learning, call for estimating distributions from data collected in batches, some  of which may be untrustworthy, erroneous, faulty, or even adversarial.

Previous estimators for this setting ran in exponential time, and for some regimes required a suboptimal number of batches. We provide the first polynomial-time estimator that is optimal in the number of batches and achieves essentially the best possible estimation accuracy.
 Adding seemingly uninformative labels helps in low data regimes 
 Evidence suggests that networks trained on large datasets generalize well not solely because of the numerous training examples, but also class diversity which encourages learning of enriched features. This raises the question of whether this remains true when data is scarce - is there an advantage to learning with additional labels in low-data regimes? In this work, we consider a task that requires difficult-to-obtain expert annotations: tumor segmentation in mammography images. We show that, in low-data settings, performance can be improved by complementing the expert annotations with seemingly uninformative labels from non-expert annotators, turning the task into a multi-class problem. We reveal that these gains increase when less expert data is available, and uncover several interesting properties through further studies. We demonstrate our findings on CSAW-S, a new dataset that we introduce here, and confirm them on two public datasets.
 Interpretable Off-Policy Evaluation in Reinforcement Learning by Highlighting Influential Transitions 
 Off-policy evaluation in reinforcement learning offers the chance of using observational data to improve future outcomes in domains such as healthcare and education, but safe deployment in high stakes settings requires ways of assessing its validity. Traditional measures such as confidence intervals may be insufficient due to noise, limited data and confounding. In this paper we develop a method that could serve as a hybrid human-AI system, to enable human experts to analyze the validity of policy evaluation estimates. This is accomplished by highlighting observations in the data whose removal will have a large effect on the OPE estimate, and formulating a set of rules for choosing which ones to present to domain experts for validation. We develop methods to compute exactly the influence functions for fitted Q-evaluation with two different function classes: kernel-based and linear least squares. Experiments on medical simulations and real-world intensive care unit data demonstrate that our method can be used to identify limitations in the evaluation process and make evaluation more robust.
 Stochastically Dominant Distributional Reinforcement Learning 
 We describe a new approach for managing aleatoric uncertainty in the Reinforcement Learning paradigm. Instead of selecting actions according to a single statistic, we propose a distributional method based on the second-order stochastic dominance (SSD) relation. This compares the inherent dispersion of random returns induced by actions, producing a more comprehensive and robust evaluation of the environment's uncertainty. The necessary conditions for SSD require estimators to predict quality second moments. To accommodate this, we map the distributional RL problem to a Wasserstein gradient flow, treating the distributional Bellman residual as a potential energy functional. We propose a particle-based algorithm for which we prove optimality and convergence. Our experiments characterize the algorithm performance and demonstrate how uncertainty and performance are better balanced using SSD action selection than with other risk measures. 	

 Attacks Which Do Not Kill Training Make Adversarial Learning Stronger 
 Adversarial training based on the minimax formulation is necessary for obtaining adversarial robustness of trained models. However, it is conservative or even pessimistic so that it sometimes hurts the natural generalization. In this paper, we raise a fundamental question---do we have to trade off natural generalization for adversarial robustness? We argue that adversarial training is to employ confident adversarial data for updating the current model. We propose a novel approach of friendly adversarial training (FAT): rather than employing most adversarial data maximizing the loss, we search for least adversarial (i.e., friendly adversarial) data minimizing the loss, among the adversarial data that are confidently misclassified. Our novel formulation is easy to implement by just stopping the most-adversarial data searching algorithms such as PGD (projected gradient descent) early, which we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier question negatively---adversarial robustness can indeed be achieved without compromising the natural generalization.
 Robust Black Box Explanations Under Distribution Shift 
 As machine learning black boxes are increasingly being deployed in real-world applications, there has been a growing interest in developing post hoc explanations that summarize the behaviors of these black box models. However, existing algorithms for generating such explanations have been shown to lack robustness with respect to shifts in the underlying data distribution. In this paper, we propose a novel framework for generating robust explanations of black box models based on adversarial training. In particular, our framework optimizes a minimax objective that aims to construct the highest fidelity explanation with respect to the worst-case over a set of distribution shifts. We instantiate this algorithm for explanations in the form of linear models and decision sets by devising the required optimization procedures. To the best of our knowledge, this work makes the first attempt at generating post hoc explanations that are robust to a general class of distribution shifts that are of practical interest. Experimental evaluation with real-world and synthetic datasets demonstrates that our approach substantially improves the robustness of explanations without sacrificing their fidelity on the original data distribution.
 Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels 
 Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and real-world label noise has never been studied in the controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real label noise (obtained from image search). This new benchmark will enable us to study the image search label noise in a controlled setting for the first time. The second contribution is a simple but highly effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, methods, and training settings. We will release our data and code on GitHub.
 Up or Down? Adaptive Rounding for Post-Training Quantization 
 When quantizing neural networks, assigning each floating-point weight to its nearest fixed-point value is the predominant approach. We find that, perhaps surprisingly, this is not the best we can do. In this paper, we propose AdaRound, a better weight-rounding mechanism for post-training quantization that adapts to the data and the task loss. AdaRound is fast, does not require fine-tuning of the network, and only uses a small amount of unlabelled data. We start by theoretically analyzing the rounding problem for a pre-trained neural network. By approximating the task loss with a Taylor series expansion, the rounding task is posed as a quadratic unconstrained binary optimization problem. We simplify this to a layer-wise local loss and propose to optimize this loss with a soft relaxation. AdaRound not only outperforms rounding-to-nearest by a significant margin but also establishes a new state-of-the-art for post-training quantization on several networks and tasks. Without fine-tuning, we can quantize the weights of Resnet18 and Resnet50 to 4 bits while staying within an accuracy loss of 1%.
 Improving the Gating Mechanism of Recurrent Neural Networks 
 Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate easily through depth or time. However, their saturation property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono-initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.
 On the Noisy Gradient Descent that Generalizes as SGD 
 The gradient noise of SGD is considered to play a central role in the observed strong generalization abilities of deep learning. While past studies confirm that the magnitude and the covariance structure of gradient noise are critical for regularization, it remains unclear whether or not the class of noise distributions is important. In this work we provide negative results by showing that noises in classes different from the SGD noise can also effectively regularize gradient descent. Our finding is based on a novel observation on the structure of the SGD noise: it is the multiplication of the gradient matrix and a sampling noise that arises from the mini-batch sampling procedure. Moreover, the sampling noises unify two kinds of gradient regularizing noises that belong to the Gaussian class: the one using (scaled) Fisher as covariance and the one using the gradient covariance of SGD as covariance. Finally, thanks to the flexibility of choosing noise class, an algorithm is proposed to perform noisy gradient descent that generalizes well, the variant of which even benefits large batch SGD training without hurting generalization.
 Do We Need Zero Training Loss After Achieving Zero Training Error? 
 Overparameterized deep networks have the capacity to memorize training data with zero training error.  Even after memorization, the training loss continues to approach zero, making the model overconfident and the test performance degraded.  Since existing regularizers do not directly aim to avoid zero training loss, they often fail to maintain a moderate level of training loss, ending up with a too small or too large loss.  We propose a direct solution called \emph{flooding} that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the \emph{flooding level}.  Our approach makes the loss float around the flooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the flooding level.  This can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers.  With flooding, the model will continue to ``random walk'' with the same non-zero training loss, and we expect it to drift into an area with a flat loss landscape that leads to better generalization.  We experimentally show that flooding improves performance and as a byproduct, induces a double descent curve of the test loss.

 Learning Fair Policies in Multi-Objective (Deep) Reinforcement Learning with Average and Discounted Rewards 
 As the operations of autonomous systems generally affect simultaneously several users, it is crucial that their designs account for fairness considerations. In contrast to standard (deep) reinforcement learning (RL), we investigate the problem of learning a policy that treats its users equitably. In this paper, we formulate this novel RL problem, in which an objective function (generalized Gini index of utility vectors), which encodes a notion of fairness that we formally define, is optimized.
For this problem, we provide a theoretical discussion where we examine the case of discounted rewards and that of average rewards. During this analysis, we notably derive a new result in the standard RL setting, which is of independent interest: it states a novel bound on the approximation error with respect to the optimal average reward of that of a policy optimal for the discounted reward.
Since learning with discounted rewards is generally easier, this discussion further justifies finding a fair policy for the average reward by learning a fair policy for the discounted reward. Thus, we describe how several classic deep RL algorithms can be adapted to our fair optimization problem. Finally, we validate our approach with extensive experiments in three different domains.
 Evaluating the Performance of Reinforcement Learning Algorithms 
 Performance evaluations are critical for quantifying algorithmic advances in reinforcement learning. Recent reproducibility analyses have shown that reported performance results are often inconsistent and difficult to replicate. In this work, we argue that the inconsistency of performance stems from the use of flawed evaluation metrics. Taking a step towards ensuring that reported results are consistent, we propose a new comprehensive evaluation methodology for reinforcement learning algorithms that produces reliable measurements of performance both on a single environment and when aggregated across environments. We demonstrate this method by evaluating a broad class of reinforcement learning algorithms on common benchmark tasks.
 Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization 
 We consider the setting of distributed empirical risk minimization where multiple machines compute the gradients in parallel and a centralized server updates the model parameters. In order to reduce the number of communications required to reach a given accuracy, we propose a preconditioned accelerated gradient method where the preconditioning is done by solving a local optimization problem over a subsampled dataset at the server. The convergence rate of the method depends on the square root of the relative condition number between the global and local loss functions. We estimate the relative condition number for linear prediction models by studying uniform concentration of the Hessians over a bounded domain, which allows us to derive improved convergence rates for existing preconditioned gradient methods and our accelerated method. Experiments on real-world datasets illustrate the benefits of acceleration in the ill-conditioned regime.
 Generalization and Representational Limits of Graph Neural Networks 
 We address two fundamental questions about graph neural networks (GNNs).  First, we prove that several important graph properties cannot be discriminated by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node.  Our treatment includes a novel graph-theoretic formalism. 

Second, we provide the first data dependent generalization bounds for message passing GNNs.  This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.
 A Graph to Graphs Framework for Retrosynthesis Prediction 
 A fundamental problem in computational chemistry is to find a set of reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction. Existing state-of-the-art methods rely on matching the target molecule with a large set of reaction templates, which are very computational expensive and also suffer from the problem of coverage. In this paper, we propose a novel template-free approach called G2Gs by transforming a target molecular graph into a set of reactant molecular graphs. G2Gs first splits the target molecular graph into a set of synthons by identifying the reaction centers, and then translates the synthons to the final reactant graphs via a variational graph translation framework. 
Experimental results show that G2Gs significantly outperforms existing template-free approaches with up to 63% improvement in terms of the top-1 accuracy and is close to the performance of state-of-the-art template-based approaches, but does not require domain knowledge and is much more scalable. 
 Interferometric Graph Transform: a Deep Unsupervised Graph Representation 
 We propose the Interferometric Graph Transform (IGT), which is a new class of deep unsupervised graph convolutional neural network for building graph representations. Our first contribution is to propose a generic, complex-valued spectral graph architecture obtained from a generalization of the Euclidean Fourier transform. We show that our learned representation consists of both discriminative and invariant features, thanks to a novel greedy concave objective. From our experiments, we conclude that our learning procedure exploits the topology of the spectral domain, which is normally a flaw of spectral methods, and in particular our method can recover an analytic operator for vision tasks. We test our algorithm on various and challenging tasks such as image classification (MNIST, CIFAR-10), community detection (Authorship, Facebook graph) and action recognition from 3D skeletons videos (SBU, NTU), exhibiting a new state-of-the-art in spectral graph unsupervised settings.
 Adaptive Sketching for Fast and Convergent Canonical Polyadic Decomposition 
 This work considers the canonical polyadic decomposition (CPD) of
tensors using proximally regularized sketched alternating least squares algorithms. First, it establishes a sublinear rate of convergence for proximally regularized sketched CPD algorithms under two natural conditions that
are known to be satisfied by many popular forms of sketching. Second, it demonstrates that the iterative nature of CPD algorithms
can be exploited algorithmically to choose more performant sketching rates. This is
accomplished by introducing CPD-MWU, a proximally-regularized sketched alternating least squares algorithm that adaptively selects the sketching rate at each iteration. On both synthetic and real data we observe that
for noisy tensors CPD-MWU produces decompositions of comparable accuracy to the standard CPD decomposition in less time, often half the time; for ill-conditioned tensors, given the same time budget, CPD-MWU produces decompositions with an order-of-magnitude lower relative error. For a representative real-
world dataset CPD-MWU produces residual errors on average 20% lower than CPRAND-MIX and 44% lower than SPALS, two recent sketched CPD algorithms.
 Circuit-Based Intrinsic Methods to Detect Overfitting 
 The focus of this paper is on intrinsic methods to detect overfitting. These rely
only on the model and the training data, as opposed to traditional extrinsic methods
that rely on performance on a test set or on bounds from model complexity. We
propose a family of intrinsic methods called Counterfactual Simulation (CFS)
which analyze the flow of training examples through the model by identifying and
perturbing rare patterns. By applying CFS to logic circuits we get a method that
has no hyper-parameters and works uniformly across different types of models
such as neural networks, random forests and lookup tables. Experimentally, CFS
can separate models with different levels of overfit using only their logic circuit
representations without any access to the high level structure. By comparing lookup
tables, neural networks, and random forests using CFS, we get insight into why
neural networks generalize. In particular, we find that stochastic gradient descent in
neural nets does not lead to “brute force" memorization, but finds common patterns
(whether we train with actual or randomized labels), and neural networks are not
unlike forests in this regard. Finally, we identify a limitation with our proposal that
makes it unsuitable in an adversarial setting, but points the way to future work on
robust intrinsic methods.
 Neural Datalog Through Time: Informed Temporal Modeling via Logical Specification 
 Learning how to predict future events from patterns of past events is difficult when the set of possible event types is large. Many of the patterns detected in the data by training an everything-affects-everything model will be spurious. To exploit known structure, we propose using a deductive database to track facts over time, where each fact has a time-varying state—a vector computed by a neural net whose topology is determined by the fact’s provenance and experience. The possible events at any time correspond to structured facts, whose probabilities are modeled along with their states. In both synthetic and real-world domains, we show that neural models derived from concise Datalog programs achieve better generalization by encoding appropriate domain knowledge into the model architecture.
 Budgeted Online Influence Maximization 
 We introduce a new budgeted framework for online  influence  maximization,   considering the total cost of an advertising campaign instead  of  the  common  cardinality  constraint on a chosen influencer set. Our approach models better the  real-world  setting  where  the cost of influencers varies and advertizers want to find the best value for their overall social advertising budget. We propose an algorithm assuming  an  independent  cascade  diffusion model  and  edge- level  semi-bandit  feedback, and provide both theoretical and experimental results.  Our analysis is also valid for the cardinality-constraint  setting  and  improves the state of the art regret bound in this case.
 Parametric Gaussian Process Regressors 
 The combination of inducing point methods with stochastic variational inference has enabled approximate Gaussian Process (GP) inference on large datasets. Unfortunately, the resulting predictive distributions often exhibit substantially underestimated uncertainties. Notably, in the regression case the predictive variance is typically dominated by observation noise, yielding uncertainty estimates that make little use of the input-dependent function uncertainty that makes GP priors attractive. In this work we propose two simple methods for scalable GP regression that address this issue and thus yield substantially improved predictive uncertainties. The first applies variational inference to FITC (Fully Independent Training Conditional; Snelson et. al. 2006). The second bypasses posterior approximations and instead directly targets the posterior predictive distribution. In an extensive empirical comparison with a number of alternative methods for scalable GP regression, we find that the resulting predictive distributions exhibit significantly better calibrated uncertainties and higher log likelihoods--often by as much as half a nat per datapoint.
 Rigging the Lottery: Making All Tickets Winners 
 Compared to dense networks, sparse neural networks are shown to be more parameter efficient, more compute efficient and have been used to decrease wall clock inference times.  There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations.  We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.
 Learning Robot Skills with Temporal Variational Inference 
 In this paper, we address the discovery of robotic options from demonstrations in an unsupervised manner. Specifically, we present a framework to jointly learn low-level control policies and higher-level policies of how to use them from demonstrations of a robot performing various tasks. By representing options as continuous latent variables, we frame the problem of learning these options as latent variable inference. We then present a temporally causal variant of variational inference based on a temporal factorization of trajectory likelihoods, that allows us to infer options in an unsupervised manner. We demonstrate the ability of our framework to learn such options across three robotic demonstration datasets.
 Optimization Theory for ReLU Neural Networks Trained with Normalization Layers 
 The current paradigm of deep neural networks has been successful in part due to the use of normalization layers. Normalization layers like Batch Normalization, Layer Normalization and Weight Normalization are ubiquitous in practice as they improve the generalization performance and training speed of neural networks significantly. Nonetheless, the vast majority of current deep learning theory and non-convex optimization literature focuses on the un-normalized setting. 
We bridge this gap by providing the first global convergence result for 2 layer non-linear neural networks with ReLU activations trained with a normalization layer, namely Weight Normalization. The analysis shows how the introduction of normalization layers changes the optimization landscape and in some settings enables faster convergence as compared with un-normalized neural networks. 

 Amortized Finite Element Analysis for Fast PDE-Constrained Optimization 
 Optimizing the parameters of partial differential equations (PDEs), i.e., PDE-constrained optimization (PDE-CO), allows us to model natural systems from observations or perform rational design of structures with complicated mechanical, thermal, or electromagnetic properties.  However, PDE-CO is often computationally prohibitive due to the need to solve the PDE---typically via finite element analysis (FEA)---at each step of the optimization procedure. In this paper we propose amortized finite element analysis (AmorFEA), in which a neural network learns to produce accurate PDE solutions, while preserving many of the advantages of traditional finite element methods. This network is trained to directly minimize the potential energy from which the PDE and finite element method are derived, avoiding the need to generate costly supervised training data by solving PDEs with traditional FEA. As FEA is a variational procedure, AmorFEA is a direct analogue to popular amortized inference approaches in latent variable models, with the finite element basis acting as the variational family. AmorFEA can perform PDE-CO without the need to repeatedly solve the associated PDE, accelerating optimization when compared to a traditional workflow using FEA and the adjoint method.
 Stochastic Regret Minimization in Extensive-Form Games 
 Monte-Carlo counterfactual regret minimization (MCCFR) is the state-of-the-art algorithm for solving sequential games that are too large for full tree traversals. It works by using gradient estimates that can be computed via sampling. However, stochastic methods for sequential games have not been investigated extensively beyond MCCFR. In this paper we develop a new framework for developing stochastic regret minimization methods. This framework allows us to use any regret-minimization algorithm, coupled with any gradient estimator. The MCCFR algorithm can be analyzed as a special case of our framework, and this analysis leads to significantly-stronger theoretical guarantees on convergence, while simultaneously yielding a simplified proof. Our framework allows us to instantiate several new stochastic methods for solving sequential games. We show extensive experiments on three games, where some variants of our methods outperform MCCFR.
 Random Hypervolume Scalarizations for Provable Multi-Objective Black Box Optimization 
 Single-objective black box optimization (also known as zeroth-order
optimization) is the process of minimizing a scalar objective $f(x)$, given evaluations at adaptively chosen inputs $x$. In this paper, we
consider multi-objective optimization, where $f(x)$ outputs a vector of
possibly competing objectives and the goal is to converge to the Pareto frontier. Quantitatively, we wish to maximize the standard \emph{hypervolume indicator} metric, which measures the dominated hypervolume of the entire set of chosen inputs. In this paper, we introduce a novel scalarization function, which we term the \emph{hypervolume scalarization}, and show that drawing random scalarizations from an appropriately chosen distribution can be used to efficiently approximate the \emph{hypervolume indicator} metric. We utilize this connection to show that Bayesian optimization with our scalarization via common acquisition functions, such as Thompson Sampling or Upper Confidence Bound, provably converges to the whole Pareto frontier by deriving tight \emph{hypervolume regret} bounds on the order of $\widetilde{O}(\sqrt{T})$. Furthermore, we highlight the general utility of our scalarization framework by showing that any provably convergent single-objective optimization process can be converted to a multi-objective optimization process with provable convergence guarantees. 
 Learning to Rank Learning Curves 
 Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model configurations. In this work, we present a new method that saves computational budget by terminating poor configurations early on in the training. In contrast to existing methods, we consider this task as a ranking and transfer learning problem. We qualitatively show that by optimizing a pairwise ranking loss and leveraging learning curves from other data sets, our model is able to effectively rank learning curves without having to observe many or very long learning curves. We further demonstrate that our method can be used to accelerate a neural architecture search by a factor of up to 100 without a significant performance degradation of the discovered architecture. In further experiments we analyze the quality of ranking, the influence of different model components as well as the predictive behavior of the model.
 Infinite attention: NNGP and NTK for deep attention networks 
 There is a growing amount of literature on the relationship between wide neural networks (NNs) and Gaussian processes (GPs), identifying equivalence between the two for a variety of NN architectures.
This equivalence enables, for instance, accurate approximation of the behaviour of wide Bayesian NNs without MCMC or variational approximations, or characterisation of the distribution of randomly initialised wide NNs optimised by gradient descent without ever running an optimiser.
We provide a rigorous extension of these results to NNs involving attention layers,
showing that unlike single-head attention, which induces non-Gaussian behaviour, multi-head attention architectures behave as GPs as the number of heads tends to infinity. 
We further discuss the effects of positional encodings and layer normalisation, and propose modifications of the attention mechanism which lead to improved results for both finite and infinitely wide NNs.
We evaluate attention kernels empirically, leading to a moderate improvement upon the previous state-of-the-art on CIFAR-10 for GPs without trainable kernels and advanced data preprocessing.
Finally, we release code allowing applications of NNGP/NTK models, with and without attention, to variable-length sequences, with an example on the IMDb reviews dataset.
 Smaller, more accurate regression forests using tree alternating optimization 
 Regression forests (ensembles of regression trees) have long been recognized as the leading off-the-shelf method for regression, where the task is to predict a continuous scalar or vector output. The main approaches are based on bagging, where individual trees are trained independently on bootstrap samples of the data; or on boosting, where individual trees are trained sequentially on the whole data but with adaptively weighted instances. However, both approaches rely on a greedy top-down procedure such as CART to learn an axis-aligned tree, where each decision node tests for a single feature. We instead use the recently proposed Tree Alternating Optimization (TAO) algorithm. This is able to learn an oblique tree, where each decision node tests for a linear combination of features, and which has much higher accuracy than axis-aligned trees. We show that using TAO with the bagging approach produces much better forests than random forests, Adaboost or gradient boosting in every dataset we have tried across a wide range of input and output dimensionality and sample size. The resulting forest has significantly lower test regression error while using shallower trees with fewer parameters and lower inference time overall. This result has an immense practical impact and advocates for the power of optimization in ensemble learning.

 T-GD: Transferable GAN-generated Images Detection Framework 
 Recent advancements in Generative Adversarial Networks (GANs) enable generating realistic images, which can be possibly misused. Detecting GAN-generated images (GAN-images) become more challenging because of the significant reduction of underlying artifacts and specific patterns. The absence of such traces can hinder detection algorithms to detect GAN-images and transfer knowledge in detecting other types of GAN-images. In this work, we present a robust transferable framework to effectively detect GAN-images, called Transferable GAN-images Detection framework (T-GD). T-GD is composed of a teacher and a student model, which can both iteratively teach and evaluate each other to improve the detection performance. First, we train the teacher model on the source dataset and use it as a starting point for learning the target dataset. For training the student model, we inject noise by mixing up both the source and target dataset, but constrain the weights variation for preserving the starting point. Our approach is a self-training method, but is different from prior approaches by focusing on improving the transferability over GAN-images detection. T-GD achieves a high performance on source dataset, overcoming catastrophic forgetting as well as effectively detecting state-of-the-art GAN-images with only a small volume of data without any metadata information.
 On Efficient Low Distortion Ultrametric Embedding 
 A classic problem in unsupervised learning and data analysis is to find simpler and easy-to-visualize representations of the data that preserve its essential properties. A widely-used method to preserve the underlying hierarchical structure of the data while reducing its complexity is to find an embedding of the data into a tree or an ultrametric. The most popular algorithms for this task are the classic "linkage" algorithms (single, average, or complete).  However, these methods exhibit a quite prohibitive running time of $\Theta(n^2)$.

In this paper, we provide a new algorithm which takes as input a set of points $P$ in $R^d$, and for every $c\ge 1$, runs in time $n^{1+O(1/c^2)}$ to output an ultrametric $\Delta$ such that for any two points $u,v$ in $P$, we have $\Delta(u,v)$ is within a multiplicative factor of $5c$ to the distance between $u$ and $v$ in the "best" ultrametric representation of $P$. Here, the best ultrametric is the ultrametric $\Delta^*$ that minimizes the maximum distance distortion with respect to the $\ell_2$ distance, namely that minimizes $\max_{u,v \in P} \Delta^*(u,v)/||u-v||_2$."

We complement the above result by showing that under popular complexity theoretic assumptions, for every constant $\epsilon>0$, 
no algorithm with running time $n^{2-\epsilon}$ 
can distinguish between inputs that admit
isometric embedding and inputs that can incur a
distortion of 3/2 in L∞ -metric.


Finally, we present empirical evaluation on classic machine learning datasets and show that the output of our algorithm is comparable to the output of the linkage algorithms while achieving a much faster running time.
 Network Pruning by Greedy Subnetwork Selection 
 Recent works on network pruning show that large deep neural networks are often highly redundant and one can find much smaller subnetworks with much lower computational cost without a significant drop of accuracy. Most existing methods of network pruning are based on eliminating unnecessary neurons from the large networks. In this work, we study a greedy forward selection approach following the opposite direction, which starts from an empty network, and gradually adds good neurons from the large network.  Theoretically, we show that the small networks pruned using our method achieve provably lower loss than small networks trained from scratch with the same size. It implies that the learned weight of large networks is important to the small pruned models. Practically, for architectures in mobile setting, we find that fine-tuning networks pruned using our method outperforms training them from scratch. Our method improves all the prior arts on learning compact networks, using architectures such as ResNet, MobilenetV2, MobileNetV3 and ProxylessNet on ImageNet. Our theory and empirical results highlight the benefits of fine-tuning networks from large models over training from scratch, which is different from the findings of Liu et al. (2019b). 
 Generalization to New Actions in Reinforcement Learning 
 A fundamental trait of intelligence is the ability to achieve goals in the face of novel circumstances. However, standard reinforcement learning typically assumes a fixed set of actions to choose from. Completing tasks with a new action space then requires time-consuming retraining. The ability to seamlessly utilize novel actions is crucial for adaptable agents. We take a step in this direction by introducing the problem of learning to generalize decision-making to unseen actions, based on action information acquired separately from the task. To approach this problem, we propose a two-stage framework where the agent first infers action representations from acquired action observations and then learns to use these in reinforcement learning with added generalization objectives. We demonstrate that our framework enables zero-shot generalization to new actions in sequential decision-making tasks, such as selecting unseen tools to solve physical reasoning puzzles and stacking towers with novel 3D shapes.
 Structural Language Models of Code 
 We address the problem of any-code completion - generating a missing piece of source code in a given program without any restriction on the vocabulary or structure. We introduce a new approach to any-code completion that leverages the strict syntax of programming languages to model a code snippet as a tree - structural language modeling (SLM).
SLM estimates the probability of the program's abstract syntax tree (AST) by decomposing it into a product of conditional probabilities over its nodes.
We present a neural model that computes these conditional probabilities by considering all AST paths leading to a target node.
Unlike previous techniques that have severely restricted the kinds of expressions that can be generated in this task, our approach can generate arbitrary code in any programming language.
Our model significantly outperforms both seq2seq and a variety of structured approaches in generating Java and C# code.
Our code, data, and trained models are available at http://github.com/tech-srl/slm-code-generation/.
An online demo is available at http://AnyCodeGen.org.
 Educating Text Autoencoders: Latent Representation Guidance via Denoising 
 Generative autoencoders offer a promising approach for controllable text generation by leveraging their learned sentence representations.
However, current models struggle to maintain coherent latent spaces required to
perform meaningful text manipulations via latent vector operations.
Specifically, we demonstrate by example that neural encoders do not necessarily map similar sentences to nearby latent vectors. A theoretical explanation for this phenomenon establishes that high-capacity autoencoders can learn an arbitrary mapping between sequences and associated latent representations.
To remedy this issue, we augment adversarial autoencoders with a denoising objective where original sentences are reconstructed from perturbed  versions (referred as DAAE).
We prove that this simple modification guides the latent space geometry of the resulting model by encouraging the encoder to map similar texts to similar latent representations.
In empirical comparisons with various types of autoencoders, our model provides the best trade-off between generation quality and reconstruction capacity.
Moreover, the improved geometry of the DAAE latent space enables \emph{zero-shot} text style transfer via simple latent vector arithmetic.\footnote{Our code will be publicly released after the review process.}
 Tails of Lipschitz Triangular Flows 
 We investigate the ability of popular flow models to capture tail-properties of a target density by studying the increasing triangular maps used in these flow methods acting on a tractable source density. We show that the density quantile functions of the source and target density provide a precise characterization of the slope of transformation required to capture tails in a target density. We further show that any Lipschitz-continuous transport map acting on a source density will result in a density with similar tail properties as the source, highlighting the trade-off between the importance of choosing a complex source density and a sufficiently expressive transformation to capture desirable properties of a target density. Subsequently, we illustrate that flow models like Real-NVP, MAF, and Glow as implemented lack the ability to capture a distribution with non-Gaussian tails. We circumvent this problem by proposing tail-adaptive flows consisting of a source distribution that can be learned simultaneously with the triangular map to capture tail-properties of a target density. We perform several synthetic and real-world experiments to complement our theoretical findings. 
 Distance Metric Learning with Joint Representation Diversification 
 Distance metric learning (DML) is to learn a representation space equipped with a metric, such that examples from the same class are closer than examples from different classes with respect to the metric. The recent success of deep neural networks motivates many DML losses that encourage the intra-class compactness and inter-class separability. However, overemphasizing intra-class compactness may potentially cause the neural networks to filter out information that contributes to discriminating examples from unseen classes, resulting in a less generalizable representation. In contrast, we propose not to penalize intra-class distances explicitly and use a Joint Representation Similarity (JRS) regularizer that focuses on penalizing inter-class distributional similarities in a DML framework. The proposed JRS regularizer diversifies the joint distributions of representations from different classes in multiple neural layers based on cross-covariance operators in Reproducing Kernel Hilbert Space (RKHS). Experiments on three well-known benchmark datasets (Cub-200-2011, Cars-196, and Stanford Online Products) demonstrate the effectiveness of the proposed approach.
 Structured Linear Contextual Bandits: A Sharp and Geometric Smoothed Analysis 
 Bandit learning algorithms typically involve the balance of exploration and exploitation. However, in many practical applications, worst-case scenarios needing systematic exploration are seldom encountered. In this work, we consider a smoothed setting for structured linear contextual bandits where the adversarial contexts are perturbed by Gaussian noise and the unknown parameter $\theta^*$ has structure, e.g., sparsity, group sparsity, low rank, etc. We propose simple greedy algorithms for both the single- and multi-parameter (i.e., different parameter for each context) settings and provide a unified regret analysis for $\theta^*$ with any assumed structure. The regret bounds are expressed in terms of geometric quantities such as Gaussian widths associated with the structure of $\theta^*$. We also obtain sharper regret bounds compared to earlier work for the unstructured $\theta^*$ setting as a consequence of our improved analysis. We show there is implicit exploration in the smoothed setting where a simple greedy algorithm works.
 Strength from Weakness: Fast Learning Using Weak Supervision 
  We study generalization properties of weakly supervised learning. That is, learning where only a few ``strong'' labels (the actual target of our prediction) are present but many more ``weak'' labels are available. In particular, we show that having access to weak labels can significantly accelerate the learning rate for the strong task to the fast rate of $\mathcal{O}(\nicefrac1n)$, where $n$ denotes the number of strongly labeled data points. This acceleration can happen even if by itself the strongly labeled data admits only the slower  $\mathcal{O}(\nicefrac{1}{\sqrt{n}})$ rate. The actual acceleration depends continuously on the number of weak labels available, and on the relation between the two tasks. Our theoretical results are reflected empirically across a range of tasks and illustrate how weak labels speed up learning on the strong task.
 Scalable and Efficient Comparison-based Search without Features 
 We consider the problem of finding a target object t using pairwise comparisons, by asking an oracle questions of the form “Which object from the pair (i,j) is more similar to t?”. Objects live in a space of latent features, from which the oracle generates noisy answers. First, we consider the non-blind setting where these features are accessible. We propose a new Bayesian comparison-based search algorithm with noisy answers; it has low computational complexity yet is efficient in the number of queries. We provide theoretical guarantees, deriving the form of the optimal query and proving almost sure convergence to the target t. Second, we consider the blind setting, where the object features are hidden from the search algorithm. In this setting, we combine our search method and a new distributional triplet embedding algorithm into one scalable learning framework called Learn2Search. We show that the query complexity of our approach on two real-world datasets is on par with the non-blind setting, which is not achievable using any of the current state-of-the-art embedding methods. Finally, we demonstrate the efficacy of our framework by conducting a movie actors search experiment with real users.
 Bayesian Sparsification of Deep C-valued Networks 
 With continual miniaturization ever more applications of deep learning can be found in embedded systems, where it is common to encounter data with natural representation in the complex domain. To this end we extend Sparse Variational Dropout to complex-valued neural networks and verify the proposed Bayesian technique by conducting a large numerical study of the performance-compression trade-off of C-valued networks on two tasks: image recognition on MNIST-like and CIFAR10 datasets and music transcription on MusicNet. We replicate the state-of-the-art result by Trabelsi et al. (2018) on MusicNet with a complex-valued network compressed by 50-100x at a small performance penalty.
 Adaptive Estimator Selection for Off-Policy Evaluation 
 We develop a generic data-driven method for estimator selection in off-policy policy evaluation settings. We establish a strong performance guarantee for the method, showing that it is competitive with the oracle estimator, up to a constant factor. Via in-depth case studies in contextual bandits and reinforcement learning, we demonstrate the generality and applicability of the method. We also perform comprehensive experiments, demonstrating the empirical efficacy of our approach and comparing with related approaches. In both case studies, our method compares favorably with existing methods.
 LP-SparseMAP: Differentiable Relaxed Optimization for Sparse Structured Prediction 
 Structured predictors require solving a combinatorial optimization problem over a large number of structures, such as dependency trees or alignments. When embedded as structured hidden layers in a neural net, argmin differentiation and efficient gradient computation are further required. Recently, SparseMAP has been proposed as a differentiable, sparse alternative to maximum a posteriori (MAP) and marginal inference. SparseMAP returns an interpretable combination of a small number of structures; its sparsity being the key to efficient optimization. However, SparseMAP requires access to an exact MAP oracle in the structured model, excluding, e.g., loopy graphical models or logic constraints, which generally require approximate inference. In this paper, we introduce LP-SparseMAP, an extension of SparseMAP addressing this limitation via a local polytope relaxation. LP-SparseMAP uses the flexible and powerful language of factor graphs to define expressive hidden structures, supporting coarse decompositions, hard logic constraints, and higher-order correlations. We derive the forward and backward algorithms needed for using LP-SparseMAP as a structured hidden or output layer. Experiments in three structured tasks show benefits versus SparseMAP and Structured SVM.


Text

  Democratic presidential nominee Joe Biden finally disclosed the roster of his biggest fundraisers on Saturday, unveiling the names of the 820 people who have helped him build a big-money juggernaut.
The list includes Biden surrogates like former South Bend, Indiana, Mayor Pete Buttigieg and Rep. Adam Schiff (D-CA); Hollywood filmmakers like Lee Daniels and Jeffrey Katzenberg; and Silicon Valley billionaires like Reid Hoffman and Ron Conway. The campaign did not specify how much these individuals raised for Biden beyond that it was more than $100,000.
The release on Saturday evening came at the last possible moment: Election Day is on Tuesday, and more than 90 million Americans have already voted, having done so without clarity on who Biden’s largest fundraisers are or what influence they may have had on his candidacy. Biden’s last-minute disclosure was a sharp departure from precedent in the Democratic Party, whose presidential candidates have regularly disclosed their so-called “bundlers” in a nod to transparency.
That’s why campaign-finance reformers were concerned that Biden had not yet followed his predecessors Barack Obama and Hillary Clinton’s lead in releasing his bundlers for the general election. 
Biden’s campaign declined to answer inquiries about their bundlers until last week, when it told the New York Times that it would release their names by the end of October. Both Obama and Clinton released updates on the list of people helping them raise big money at consistent intervals; Biden’s only prior update came on a Friday evening just after Christmas in 2019 during the Democratic primary with about 230 names.
“Congratulations on clearing an artificially low bar they set for themselves that defeats the entire purpose of transparency — allowing voters to know who is funding the campaigns asking for their support before casting their ballots,” said Tyson Brody, a Democratic operative who worked for Sen. Bernie Sanders and backs Biden, but is critical of the influence of large campaign contributors.
It makes strategic sense that the Biden campaign would not draw attention to the bundlers who have helped him turn a lagging fundraising operation into a surprising powerhouse. Biden has worked to position himself as the candidate with the interest of the working and middle classes in mind, giving himself the nickname “Middle-Class Joe,” and casting the general election “as a campaign between Scranton and Park Avenue.”
The Biden campaign has tried to draw focus to its small-dollar, online fundraising operation, rather than the celebrities, Silicon Valley billionaires, and Wall Street executives whose support undercuts some of the campaign’s messaging. That’s an especially important task for Biden given that many of these characters are prone to draw the scorn of the left, which is already skeptical of Biden and wants to see big campaign contributors play a smaller role in politics.
And the Trump campaign hasn’t been in much of a place to argue for transparency. President Trump hasn’t released any information about his own bundlers.
So there’s been limited scrutiny. The upshot is that the 90 million people who have already cast ballots ended up voting with incomplete information about the people who helped the campaigns raise the money that may have influenced those very votes.
The debate over bundler disclosure reflects a key campaign question of the Trump era: Should Trump’s own tactics set the standard for his Democratic rivals? Or should Democrats — who claim to prioritize reducing the role of money in politics — aspire to a higher, or at least the pre-Trump, standard?
Campaigns are only legally required to disclose bundlers who are registered lobbyists — everything else is voluntary. Trump and his GOP predecessor at the top of the ticket, Sen. Mitt Romney, declined to share any additional information. But prior to their campaigns, there had been a bipartisan tradition of at least offering some information in order to help voters understand who carried unofficial influence in their campaign; that was done by both the late Sen. John McCain and former President George W. Bush, who pioneered the modern bundling system and made being a bundler into something of a bragging right. 
Bundlers do the often painstaking work of soliciting their networks for high-dollar campaign contributions: inviting their business associates to campaign events, making introductions to campaign staffers, and recruiting more bundlers to serve alongside them. Bundling can often end up being fiercely competitive, with campaigns closely tracking how much individuals have raised, and bundlers sometimes finding themselves in competition for positions on leaderboards.
Although Biden released just a single tier of information on the amounts that his bundlers raised, the campaign privately has six different levels of membership for its finance committee: ranging from a “Protector” who helps the campaign raise $50,000 to a “Biden Victory Partner” who brings home $2.5 million, according to a campaign document seen by Recode. Mementos that Biden has sent to that top-level bundler include a gold-and-blue pin.


The Biden campaign sends out these buttons to the bundlers who have raised over $2.5 million for his bid.A reminder of how the Joe Biden big-money machine works. pic.twitter.com/GRIYE0Otsl— Teddy Schleifer (@teddyschleifer) October 26, 2020



Despite his preference to talk about his low-dollar fundraising operation, Biden has built an impressive big-money machine.



  

    
  
    
    
      
        
  


  






      
    
    
  
  



With less than a week until Election Day, Facebook has admitted to a glitch in the system that handles political ads on its platform. “Technical flaws” related to a new transparency effort that restricted new political ads from appearing on Facebook in the week before the election caused an unstated number of old political ads to not appear at all. The Biden and Trump campaigns both say some of their ads were among them. 
This looks bad for Facebook. While the company says it has mostly fixed the problem — and that the issue had nothing to do with partisanship — the situation highlights a growing distrust in Facebook’s ability to manage political content on its platform. Facebook says the moratorium on new political ads that led to the glitch was part of its “efforts to ensure maximum transparency.” The Biden campaign says Facebook let them down.
“We have no sense of the scale of the problem, who it is affecting, and their plan to resolve it,” Biden’s digital director Rob Flaherty said in a statement Thursday night. “It is abundantly clear that Facebook was wholly unprepared to handle this election despite having four years to prepare.”
This is just the latest in a series of episodes that raise questions about Facebook’s commitment to transparency in its handling of political ads, including objectionable content found in and the opaque targeting of its political ads. It’s also not clear to many users how they’re being targeted by such ads. Facebook has previously taken steps to block ad tracking tools, including one built by New York University researchers, one from ProPublica, and another from Mozilla. So some worry that Facebook’s stated commitment to ad transparency is an empty promise and that the platform has failed to moderate itself successfully.
“Every week, there is new bad stuff that gets through Facebook’s own monitoring and screening,” Laura Edelson, a researcher at NYU studying political ads, told Recode. “The real danger is that Facebook says it can do this job themselves, but they can’t.” 
Last month, Edelson and her colleagues at NYU launched a project called the Ad Observatory that, in part, allows users to download a browser extension designed to record information about the political ads they saw on the platform. The browser extension, which is called Ad Observer, “allows journalists and researchers to better understand the political misinformation and manipulation that spreads daily on your platform,” the group said. 
But on October 16, Facebook sent the NYU project a cease-and-desist letter, demanding that they stop before the end of November. This led a slew of organizations led by Mozilla to call that Facebook withdraw its letter and work with the researchers on improving political ad transparency. 
Facebook claims that it provides transparency with its Ad Library, which the company built in response to demand for information about promoted campaigns on its platform. This searchable database shows information about active and inactive ad campaigns being run on Facebook, including the amount spent as well as the ages, genders, and locations of people who end up seeing an ad. However, the conflict between Facebook and researchers at the Ad Observatory project suggests that users don’t know much about why they see certain political ads. Technical problems with the Ad Library are also the reason why an unstated number of previously approved political ads did not run the week before the election.
As for the Ad Observer tool itself, Facebook says the browser extension engages in bulk data collection, which is a violation of the company’s terms of service. The cease-and-desist letter also said that if the researchers don’t shut down the tool voluntarily, they “may be subject to additional enforcement action.” In fact, the company says it told the researchers months earlier that such a tool would go against its rules. It also demanded that all the data collected by the project be deleted.
“We informed NYU months ago that moving forward with a project to scrape people’s Facebook information would violate our terms,” Facebook spokesperson Joe Osborne said in a statement to Recode. “Our Ad Library, which is accessed by more than 2 million people every month, including NYU, already provides more transparency into political and issue advertising than TV, radio or any other digital ad platform.” 
But while the Ad Library does reveal some general details about impressions — like where ads ended up being shown and the gender breakdown of those who saw an ad — researchers say that’s not enough. Many argue that the tool lacks pivotal details about how those ads were actually targeted, and some claim that not all political ads make it into Facebook’s library tool. 
The recent back-and-forth with Facebook has actually led to a surge in participation in the NYU project. Since people learned that Facebook had sent a cease-and-desist letter to the researchers, thousands more volunteers have downloaded the Ad Observer browser extension. The number of participants is now more than 13,000, which is double the 6,500 who had signed up before Facebook’s cease-and-desist letter.
Meanwhile, the NYU researchers say the tool does not collect personally identifiable information and that the data of all the participants is anonymized and combined. “No personal information from volunteers is collected,” says the Ad Observatory’s website, which specifies that its tool does not collect “anything personally identifying,” including names, birthdays, friend lists, or ad interactions.
Some employees at Facebook have suggested that the tool is not safe. Facebook did not share whether it had any plans to reveal any new information about targeting. 
Sen. Amy Klobuchar (D-MN) asked Facebook CEO Mark Zuckerberg during a recent Senate hearing on Section 230 about political advertisements on the platform. This is a topic Klobuchar has been following as a co-sponsor of the Honest Ads Act, which would require tech companies to reveal more information about how political ad targeting works. While the law hasn’t passed, she’s asked Zuckerberg to meet its standards for fully disclosing which groups are targeted by particular political ads. 
In a statement to Recode, Klobuchar told Recode that technology companies, including Facebook, have not met those standards, and she condemned recent reports about the company trying to squash research.  
“As we face threats to our democracy, we need more transparency, not less,” Klobuchar said.

Other social platforms have recently become more scrupulous about political ads. Major platforms like Twitter, Nextdoor, and TikTok have banned all political ads. But Facebook has doubled down on its controversial policies by refusing to fact-check political ads. There were some related political ad controversies earlier this year, including the company allowing the Trump campaign to run hundreds of misleading ads related to the census as well as ads that contained Nazi imagery. Facebook has since added the option for users to turn off political ads.
In an email, NYU’s Edelson told Recode that she fully supported Facebook’s recent political ad policy changes, like banning new political ads in the week before Election Day.  
“However, it’s now clear that not only has the communication about these policies been haphazard and confusing, but the implementation has been as well,” Edelson said. “If Facebook wants to rebuild trust with both users and advertisers, they need to be much more transparent about how political advertising works on their systems.”
The most recent hiccups in Facebook’s political ad system show that some — from academics to presidential campaigns — remain concerned about the company’s transparency efforts. In a sense, Facebook is making changes that have led to more problems and unintended confusion. So it’s worth wondering, days before a pivotal election that the company has known about for years, why Facebook still doesn’t seem prepared. 
Update October 30, 1:10 ET: This piece has been updated with an additional comment from Edelson.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.


  
  
  





  Despite a pandemic that’s shocked the entire economy and impending antitrust lawsuits, Big Tech is doing rather well. Amazon, Apple, Google, and Facebook raked in a huge amount of money last quarter: $38 billion in profits on nearly $240 billion in revenue. For the most part, this represents growth over what these companies made last year, despite the worst recession in the United States since World War II.  
These numbers are striking not just for the tremendous amount of money these four companies are making but also because Amazon, Apple, Google, and Facebook seem to be defying this moment in history. Earlier this month, a long-awaited congressional report accused the companies of anti-competitive behavior, and some politicians are asking that they be broken up. Meanwhile, unemployment is double the rate it was in the beginning of the year, and numerous industries are struggling to stay afloat. 
But as many of us have been stuck at home, Big Tech’s services have been more important than ever, becoming the primary way many of us interact with the outside world. So what’s been bad for restaurants, airlines, and countless other industries has been good for the world’s biggest tech companies. 
In the quarter ending September 30, Amazon’s profits rose nearly 200 percent from a year earlier. Google’s profit grew about 60 percent and Facebook’s 30 percent. Even Apple, whose profits were down slightly, brought in a healthy $12.7 billion in profit. 
  
  
    
    
      
        
  


  






      
    
    
  
  


Revenue was up for each of these companies. Amazon, which has seen its dominance rise especially sharply during the pandemic as people’s shopping habits shifted online, saw record revenue of $96 billion, a 37 percent increase compared with last year. 
  
  
    
    
      
        
  


  






      
    
    
  
  


As a result of these earnings, Big Tech’s stocks are at or near all-time highs. This is a notable milestone, since Google, Facebook, Amazon, and Apple stock all took a big hit back in March. But unlike many other companies still suffering from that blow, these four companies’ stock prices have now more than recovered. On average, their market cap is up about 50 percent since then. Meanwhile, the Dow Jones Industrial Average is up about 5 percent (Apple is included in the index, helping buoy the whole average). 
  
  
    
    
      
        
  


  






      
    
    
  
  


These massive numbers don’t mean much to the average person, since many Americans don’t have a real stake in the stock market. Instead, shareholders are the ones who benefit — as well as the companies themselves, who are able to reinvest these profits to become even bigger and make competition even harder. As the government ramps up its antitrust cases against Google, Apple, Amazon, and Facebook, their outsize profits are going to become more important.

  
  
  
 



  Silicon Valley is spending far more money to oust Donald Trump in 2020 than it did in 2016, a testament to the new political muscle that the tech industry has flexed over the last four years. And the money is not just from its billionaires.
The tech industry did spend big to support Hillary Clinton in 2016. But Trump was merely a candidate then, without a track record of tangible policy changes on immigration, climate change, or other issues that concern the tech industry. And Silicon Valley did not have the years of preparation to start new groups, raise big money, and mobilize its energy in the sophisticated ways that it has had in the runup to 2020.
And so this time around, Silicon Valley — led by this billionaire class and its captains of industry — has plunged even deeper into the world of partisan campaigning, according to a Recode analysis of extensive campaign-finance data. The exact amount depends on how you define Silicon Valley, but more money has been marshaled to back Joe Biden than was raised to back Clinton, no matter how you measure it.
Ken Duda, a software executive who has spent millions of dollars on this election, said he has spent three times as much as he did in 2016 to beat Trump this cycle. Duda described himself as politically moderate and not a news obsessive but said he was deeply concerned because he believes Trump is leading the country into an “autocracy.”
“I would be very happy to go back to ignoring politics like I did before 2016,” he told Recode. “I hope to put Twitter away after this election, and my political donations will go away along with that. That’s my hope.”
This rise in Democratic giving is all happening against a backdrop of tension between the party and its donors from the tech industry who increasingly fund it. The Democratic Party has gotten far tougher on tech companies and its leaders over this four-year period — even debating a potential breakup of these giants — and despite being the beneficiary of its money, Biden himself has said that he’ll keep on scrutinizing Silicon Valley.
Coming up with a distinct definition for what qualifies as “Silicon Valley” — whether it’s a physical place, an industry, both, or something more thematic — is challenging. So for this analysis, Recode worked with data-analysis outfitter GovPredict to run three different analyses on three different (even if all imperfect) windows into total Silicon Valley donations:

Contributions by people who live in the San Francisco Bay Area zip codes
Contributions by people who describe themselves as a “software engineer” or working in “venture capital”
Contributions by people who describe themselves as working for Facebook, Amazon,  Microsoft, Netflix, Apple, or Alphabet (or its subsidiaries, Google or YouTube)

All of these analyses looked at the total donations to the Biden, Clinton, and Trump campaigns; the Democratic and Republican National Committees; joint fundraising committees between their campaigns and their parties; and major super PACs supporting their campaigns. All contributions from the beginning of the year before the election and up to three weeks before Election Day were included.
To some extent, Silicon Valley is doing nothing unusual. 2020 is by far the most expensive election cycle, adjusted for inflation — costing more than twice as much as the runner-up, the 2016 race. But the new money reflects how Silicon Valley is increasingly turning its financial power into political power that could persist after Election Day.
Bay Area
  
  
    
    
      
        
  


  






      
    
    
  
  


People who live in the nine counties considered to be in the San Francisco Bay Area gave 22 percent more to Democrats in 2020 than they did in 2016, a jump from about $163 million to $199 million. (Those figures include money given in both cycles to super PACs by Democratic megadonor and San Franciscan Tom Steyer, who is not in tech but who donated tens of millions in both 2016 and 2020.)
Gifts to the GOP from the Bay Area, where Republicans are few and far between, rose more dramatically, albeit from a far smaller base: After giving $800,000 to Republicans in 2016, Bay Area residents gave $22 million to boost Trump in 2020, a haul that came from figures like Oracle CEO Safra Catz.
Occupation
  
  
    
    
      
        
  


  






      
    
    
  
  


If you look at tech by choosing two common job descriptions — venture capitalist and software engineer — you can also see the new energy on the left.
This group gave $7.2 million to Democrats in 2016. Four years later, that sum had almost tripled to $19 million. Republican donations from this slice of Silicon Valley also grew by about threefold but once again from a smaller base — from almost $700,000 to $2 million.
Big Tech companies
  
  
    
    
      
        
  


  






      
    
    
  
  


Lastly, one easy, simple way to measure “Silicon Valley” is to look at its biggest, most iconic companies, including Google, Apple, Facebook, Amazon, Microsoft, and Netflix.
Big Tech employees are giving far more in the Trump-Biden race than they did in the Trump-Clinton race. Donations to Democratic efforts jumped from about $8.5 million to about $14 million, growing by nearly 70 percent. Meanwhile, donations to back Trump from Big Tech employees almost quintupled — from just about $180,000 to $850,000. That’s despite Trump’s frequently blasting these donors’ employers, including in the final days of the campaign.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



As though hospitals across America didn’t have enough to handle with the recent resurgence of Covid-19 causing overflows and straining their resources, they’re now the possible targets of a new onslaught of ransomware attacks.
An alert from the FBI, the Department of Health and Human Services (HHS), and the Cybersecurity and Infrastructure Security Agency (CISA) said on Wednesday that there is an imminent threat of ransomware attacks on American hospitals and health care providers. Ransomware is malware that locks up a system’s computers and data until a ransom is paid. The alert didn’t specify who the agencies thought might be responsible for the attacks, but HHS has said in the past that the ransomware associated with the current threat is linked to Russian criminal groups. The alert also didn’t say how many — if any — health care institutions had already been affected, but Reuters reports that there were attacks in New York, Oregon, and Washington state.
The threat identified by the FBI, CISA, and HHS comes from the “Ryuk” ransomware, which emerged in mid-2018 and has cost companies and municipalities at least tens of millions of dollars in ransom payments, in addition to whatever costs were incurred for IT fixes and lost business. 
“Ryuk is a relatively young ransomware family that was discovered in August 2018 and has made significant gains in popularity in 2020,” Dmitriy Ayrapetov, of internet security company SonicWall, said in a statement to Recode. “The increase of remote and mobile workforces appears to have increased its prevalence, resulting not only in financial losses, but also impacting health care services with attacks on hospitals.”
Ryuk is believed to be behind the recent ransomware attack on Universal Health Services (UHS), which owns 400 facilities across the United States and the United Kingdom. The company was forced to take down systems across all 250 of its American facilities. UHS said the attack didn’t harm any of its patients, but employees told the Associated Press that it delayed getting crucial information about patient care and communication with other health professionals.
A new report from SonicWall blamed Ryuk for a third of all known ransomware attacks identified in the last year, and there’s been a significant increase in ransomware attacks in general over the last several months. Hackers have taken advantage of the coronavirus pandemic in other ways, too, sending phishing emails from spoofed addresses relating to health organizations or addresses that closely mimic those organizations.
Hospitals make good targets for ransomware because victims are more likely to pay the ransom as quickly as possible given the possible consequences of any delay in accessing their systems. A 2017 ransomware attack on the UK’s National Health Service cost tens of millions of dollars, and nearly 20,000 patient appointments had to be canceled while the system was offline, compromising their care. An attack on a German hospital in September of this year is believed to have caused a woman’s death, the first known death linked to ransomware (somewhat ironically, the attackers only meant to shut down the university associated with the hospital and not the hospital itself). 
Chris Wysopal, co-founder and chief technology officer of cybersecurity software company Veracode, told Recode back in January that hospitals and local governments are good “soft targets” for ransomware attacks because they often don’t have the money or dedicated personnel needed to sufficiently protect their systems from hackers.
There have also been reports of hacking attempts from China, Russia, and Iran on institutions and companies developing coronavirus vaccines and doing other virus-related work, but in those cases it is more likely the countries are hoping to steal the research for themselves.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  Six days ahead of a historic presidential election whose outcome is expected to shape the future of US democracy and as Congress delays passing a new economic stimulus deal to aid millions of unemployed Americans affected by the ongoing pandemic, the Senate spent more than three hours talking about something else: how social media companies handle controversial speech on the internet.
Wednesday’s hearing was supposed to focus on nuanced reforms to a landmark internet law — Section 230 — which shields tech companies from being sued for content users post on their platforms. Both Democrats and Republicans have been calling for years to reform this law, arguing that it is outdated considering how large and powerful these tech giants have become. That’s why the Senate Committee on Commerce, Science, and Transportation subpoenaed Facebook CEO Mark Zuckerberg, Google CEO Sundar Pichai, and Twitter CEO Jack Dorsey to face questioning.
But instead of talking about reforming the actual law, most Republican Senators — with notable exceptions such as Sen. Shelley Moore Capito (R-WV) and Sen. Deb Fischer (R-NE) — used their time to press the CEOs about specific content moderation decisions that have been controversial with Republicans. Namely, Twitter blocking an unverified story in the New York Post making damning accusations about Hunter Biden earlier this month, or why the company fact-checks Trump more often than Iran’s or the Chinese Communist Party’s leaders.
Some Democrats at the hearing — and many outside observers — have written off the hearing as political theater orchestrated by conservatives days ahead of the election to intimidate these companies so they avoid fact-checking Trump or conservative disinformation campaigns.
But Republicans argued that allegations of bias are critical and valid, and that they need to be swiftly addressed.
Many Senators used assumptions and cherry-picked evidence to try to prove their points. And in response, the tech CEOs effectively skirted more serious discussions about their actual shortcomings in content moderation. 
Here are the fact-checks on the five most head-scratching claims senators — and tech CEOs — made at the hearing.
1) Though Republicans say social media companies are broadly censoring conservative speech, the evidence doesn’t support the claim.
Many conservative lawmakers, encouraged by President Trump, have long alleged that tech companies are censoring Republicans on social media. And today’s hearing was no exception. 
Citing social media companies’ handling of the Hunter Biden New York Post story, as well as Google’s threats to ban conservative news website the Federalist over allegedly racist content, Senate Commerce Committee Chairman Sen. Roger Wicker (R-MS) said in his opening remarks: “These recent incidents are only the latest in a long trail of censorship and suppression of conservative voices on the internet.”
While it’s true that Twitter and Facebook have made some controversial and at times questionable decisions to limit false or unverified speech by conservative politicians and news outlets (Twitter reversed its stance on blocking the Hunter Biden story, Facebook did not), these are individual examples. 
On the whole, data shows that conservative content thrives on social media. Conservative pundits like Dan Bongino and Ben Shapiro consistently rank among the most shared news sources on Facebook based on the company’s data aggregation tool, CrowdTangle. And despite all the hoopla about Twitter’s alleged censoring of Trump, the president still uses the platform every day to reach tens of millions more followers than Joe Biden does. 
In fact, Trump himself has repeatedly stated that, without social media, he wouldn’t be able to “get the word out” to the people. 
Republican senators asked why tech companies haven’t fact-checked high-profile Democratic leaders like Biden as much as they have Trump, but they ignored the very obvious answer: that Trump, unlike Biden, has more frequently promoted false and misleading statements on social media. If Biden were to attack mail-in voting or the basic science behind Covid-19, as Trump has, he would likely face the same kind of moderation.
To Republicans’ credit, an underlying tension here is that many people who work at tech companies lean liberal (more on that later). And, back in 2016, Gizmodo reported that those political beliefs sometimes trickled into low-level employee content moderation decisions via the disastrous “Facebook Trending” section. But a lot has changed since then (for one, Facebook has done away with that its trending section entirely). If anything, the evidence now seems to suggest that Facebook has adjusted in the other direction to please Republicans and fend off claims of anti-conservative bias. According to recent reporting from BuzzFeed News, NBC News, and the Wall Street Journal, the company has at times overridden its fact-checking system and tweaked its algorithms to favor conservative publications over liberal ones like Mother Jones.
2) Ted Cruz claimed social media companies are the biggest threat to free speech in the US. That’s not at all clear. 
Sen. Ted Cruz (R-TX) came in hot to the hearing, announcing his intent to grill Dorsey in a wrestling-match style flyer he (ironically) tweeted out the night before the session, all in the name of defending free speech on the internet.


TOMORROW10am ET / 9am CTLivestream will be available here --> https://t.co/Gl9mQQkvd8#StopTheCensorship pic.twitter.com/1Pu9OYFV5v— Senator Ted Cruz (@SenTedCruz) October 27, 2020



“The three witnesses we have before the committee today collectively pose the single biggest threat to free speech in America and the greatest threat we have to free and fair elections,” Cruz said, speaking about Dorsey, Zuckerberg, and Pichai. 
Cruz is entitled to his opinion, of course, but it’s not at all objectively clear that the biggest threat to free speech or election integrity in this country is when Facebook, Twitter, or Google fact-check politicians like Trump. 
In fact, if you asked this same question of leading free speech advocates and human rights organizations, many would say a bigger concern is Trump’s sustained and increasingly vitriolic attacks on the free press since his first day in office. If social media companies do pose a threat to free speech, they say, it has less to do with how they handle conservative voices and more do with the extremist hate speech that spreads on their platforms, and which has a chilling effect on women, minorities, and other marginalized groups by shutting them out of online public discourse. 
It’s true that social media companies now rival governments in the scope of their power and influence, and free speech defenders of all political persuasions demand that these companies provide more transparency and accountability about what content they do and don’t allow.
But for Cruz and some of his Republican colleagues to support free speech only when it suits their political needs (in an extreme example of this, Sen. Marsha Blackburn (R-TN) denounced Google for allegedly censoring Republicans while simultaneously calling for the company to fire a rank-and-file employee who publicly criticized her) is a hypocritical one at best.
3) Dorsey told Cruz that Twitter doesn’t impact elections. It does.
Despite Cruz’s largely theatrical political showboating, he did get into one important exchange with Dorsey that highlighted an issue originating with the tech platforms themselves: their refusal to acknowledge that they are more than just neutral platforms. 
At one point, Cruz asked Dorsey whether Twitter has influence over the elections, and Dorsey said no.
Cruz retorted, “If you do not believe you have the power to influence elections, why do you block anything?” 
Dorsey’s response was that Twitter blocks content to reduce harassment and make everyone feel included on its platform. Facebook and Google have similarly asserted that they aim to be neutral platforms for people to communicate, with exceptions to protect their users from harm. But that’s just part of the picture.
The reality is that Twitter, Facebook, Google, and every other social media platform make decisions every day about what kind of political speech is and isn’t allowed on their platforms. Moreover, the algorithms underpinning these platforms dictate which topics go viral and reach the masses instantly, and which ones get seen by a much smaller number of people. And because these sites are the main way tens of millions of Americans primarily consume their daily news, what is and isn’t allowed on them can of course impact how someone votes in an election. 
The fact that Dorsey — as well as Zuckerberg and Pichai — wouldn’t admit this basic fact was telling of tech CEOs’ lack of candor about the political power they’ve amassed through their companies. 
4) Senators suggested tech companies’ liberal employee majorities are a problem. But that’s neither illegal nor the government’s job to police.
First of all, let’s be clear that most tech employees at Google, Twitter, and Facebook lean liberal. That’s reflective of the demographics where these companies are based and the skills they hire for: largely college-educated workers in major urban areas like San Francisco, New York, and Seattle. 
In Wednesday’s hearing, several Republican Senators questioned tech CEOs about the political makeup of their workforce as if there was something shameful about this.
The insinuation is that because these companies have a liberal-leaning workforce that they as a default are stifling conservative speech.
But as we mentioned earlier, there isn’t any real proof of that systematic suppression. And even if there were, the solution wouldn’t necessarily be to mandate that everyone who works for Facebook or Twitter pass some kind of political litmus test. 
Congress has a dark history of blacklisting citizens from gainful employment due to their political beliefs. While it’s fair to question the unparalleled political power of tech companies and try to regulate that problem, it’s dangerous for lawmakers to misleadingly frame the issues at hand as being connected to employees’ personal politics. 
5) Senators kept pronouncing Google CEO Sundar Pichai’s name incorrectly. It’s pronounced “Pitch-eye.” 
Senators across the aisle repeatedly butchered the name of Google CEO Sundar Pichai. The soft-spoken Pichai, who was born and raised in India and worked his way up at the search giant from a product manager to its chief executive, refrained from correcting his questioners.
The fact that members of Congress were mispronouncing the name of one of the most important business leaders in the US was an embarrassing slip many observers immediately noted on Twitter. Especially as it was Pichai’s third time being questioned in front of Congress.
And while getting Pichai’s name right is a less important point in the scope of the broader issues at stake around social media, it’s not insignificant, either. In recent months, Trump and some Republican lawmakers have repeatedly mocked Democratic vice presidential nominee Kamala Harris over the pronunciation of her name. It seems more likely that in this case, senators were botching Pichai’s name out of ignorance rather than malice. But as BuzzFeed News pointed out, Congress hasn’t had any trouble pronouncing other hard-to-pronounce names in the past. In the year 2020, there’s really no excuse for elected officials not to at least try and correctly pronounce the name of a global tech titan.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



CEOs from two of the biggest companies in the world (and Twitter) testified in front of the Senate Commerce Committee on Wednesday in a hearing that was billed as a deliberation over Section 230. It ended up being more about castigating social media platforms both for censoring voices too much and for not censoring them enough.
Facebook’s Mark Zuckerberg, Google’s Sundar Pichai, and Twitter’s Jack Dorsey appeared before an almost entirely virtual panel of legislators, none of whom seemed particularly thrilled with the CEOs’ work. But their complaints differed depending on their political party. Republicans generally used the hearing to scold the companies for censoring conservative voices to the extent that they may influence the outcome of the election in Biden’s favor. Democrats objected to having a hearing at all and asked the CEOs what they were doing to suppress violent extremism and election interference on their platforms. 
If nothing else, the hearings showed a bipartisan dislike and mistrust of social media platforms and a desire to do something about them.
The hearing was titled “Does Section 230’s Sweeping Immunity Enable Big Tech Bad Behavior?” — that bad behavior being, to its vocal conservative opponents, social media platforms censoring political viewpoints with which they disagree. But the law is much bigger than that. Section 230 allows websites to host content from users without being liable for it. For instance, you can sue a Twitter user for a defamatory tweet, but you can’t sue Twitter itself. This is what enables these sites to exist in the first place. Without immunity from lawsuits over third-party content, platforms wouldn’t allow it at all. 
The law also allows platforms to moderate user content as they see fit without losing that immunity, a fact that has become a major sticking point for conservatives who feel that platforms are unfairly censoring them. While Attorney General Bill Barr and several Republican legislators want to change Section 230 to require websites to be “politically neutral” in moderation decisions, President Trump has called for an outright repeal of the law. In fact, the president repeated that demand while Wednesday’s hearing was still in progress:


The USA doesn’t have Freedom of the Press, we have Suppression of the Story, or just plain Fake News. So much has been learned in the last two weeks about how corrupt our Media is, and now Big Tech, maybe even worse. Repeal Section 230!— Donald J. Trump (@realDonaldTrump) October 28, 2020



Democrats have their own issues with Section 230 and Big Tech in general. But in the hearing, those concerns took a back seat to their grievances against its timing and the pro-Trump messages they believed committee Republicans were using it to convey. 
Most Republicans barely mentioned Section 230 and instead focused on social media moderation and a perceived silencing of conservative voices, the oft-noted examples of such being President Trump’s fact-checked tweets and the New York Post’s story on Hunter Biden, which Twitter and Facebook initially limited the spread of. There were also several questions about the political ideologies of employees and people who make moderation decisions, with the implication being that very few of them are conservative.
Sens. Ted Cruz (R-TX) and Ron Johnson (R-WI) were especially emphatic about these points. Cruz, a frequent critic of Section 230, even advertised his appearance at the hearing on Twitter and Facebook the night before, calling it a “Free Speech Showdown,” complete with custom art that resembled a poster for a boxing match. Cruz opened his questions by saying the CEOs testifying before him “collectively pose the single greatest threat to free speech in America and the greatest threat we have to free and fair elections.” His use of his time didn’t really get better from there.
“Mr. Dorsey, who the hell elected you and put you in charge of what the media are allowed to report and what the American people are allowed to hear, and why do you persist in behaving as a Democratic Super PAC?” Cruz demanded. Dorsey responded that he is not in charge of those things. Cruz then retweeted several news articles containing his quote as well as posting his own video of it, indicating that his question to Dorsey was meant more for political theater than anything else.
Johnson tried to nail the CEOs down on how many of their employees are liberal and how many are conservative. In response, Dorsey said his company doesn’t keep track of employees’ politics, Pichai said he believed his employees have many different viewpoints, and Zuckerberg said he didn’t know for sure but assumes Facebook skews liberal — which was the only answer Johnson seemed to believe.
Several Republicans also pointed out that Twitter let untrue or violent tweets from other world leaders stay up while punishing Trump for his tweets, even though, as Sen. Roger Wicker (R-MS) said, they are “true.” One example was a series of tweets from Iranian Ayatollah Ali Khamenei that seemed to promote violence against Israel, which are still on the platform. 
“We did not find those to violate our terms of service because we consider them sabre-rattling, which is part of the speech of world leaders in concert with other countries,” Dorsey said. “Speech against our own people or a country’s own citizens, we believe, is different and can cause more immediate harm.”
Some Democrats used their time to criticize the timing and subject of the hearing, calling it part of a coordinated Republican effort to bully platforms into keeping conservative-leaning content up, even if it violates their policies, as well as to amplify the New York Post’s story to try to influence the outcome of the election. Others brought up how social media platforms have facilitated violent extremist groups to meet and organize and foreign powers to influence the elections. They asked the CEOs what they plan to do to prevent or squelch this content on their respective platforms as the election approaches, and whether they would pledge to stop or prevent election interference on their platforms. Dorsey, Pichai, and Zuckerberg all pledged to do so.
Sen. Amy Klobuchar (D-MN) noted that Facebook makes more money when people spend more time on it, and divisive political content has been shown to contribute to that engagement. 
“Does that bother you, what it’s done to our politics?” she asked.
Zuckerberg said the platform is designed to show users the content that’s most important to them.
“Most of the content on the systems is not political, it’s things like making sure you can see when your cousin had her baby,” he said.
“That’s not what I’m talking about, the cousins and the babies here,” Klobuchar said. “I’m talking about conspiracy theories … I think it’s been corrosive.”
But some senators actually took the time to ask the CEOs seemingly genuine questions about their moderation policies and algorithms as well as how Section 230 could be re-written to provide more clarity to users. Sen. Shelley Capito (R-WV) asked if giving Section 230’s “otherwise objectionable” rule for the type of content platforms are allowed to moderate more specific guidelines would be a solution. Zuckerberg noted that having to spell out which content is objectionable and which is not would limit their ability to moderate bullying or harassment. Dorsey and Zuckerberg said multiple times during the hearing that they would be open to increased transparency on their platforms with regard to moderation decisions.
Sen. Brian Schatz (D-HI), who proposed a bipartisan bill about Section 230, said he hoped to have “good faith” discussion about the law after the election. A future hearing about Section 230 is certainly possible regardless of the election’s outcome, as Trump’s feelings are well-known and Biden has said he is in favor of revoking the law — a stance that a campaign official told Recode hasn’t changed.
The co-author of Section 230, Sen. Ron Wyden (D-OR) is not a member of the Commerce Committee and so wasn’t at the hearing. But he wasn’t quiet about it, issuing a statement along the lines of many Democrats’ complaints.
“After watching the hearing today, I don’t believe my Republican colleagues have read the First Amendment, let alone Section 230,” Wyden said. “Their obsession with forcing private companies to print misinformation, lies and hate speech is unconstitutional and lays bare how little this is about Section 230 and how much it is a transparent attempt to work the refs a week before the election.” 
Wyden added, “Today’s sad spectacle shows how far this body is from having a rational debate about how to make the internet a better place.”
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  You’d be forgiven for thinking that now, maybe more than ever, it’s easy to run a small business. That where there’s a will (and stable wifi), you can source, produce, and automate your way to residual income. It’s what my Instagram Explore page seemed to suggest one day, surfacing a Reel announcing the “sold-out launch” of something — you never see what — from someone who seemed optimized for the platform: excited for their own success, unburdened by the intensive labor it must’ve taken to get there.
It amounts to an ad for a suite of integrated marketing tools for merchants on Instagram, a platform that I’ve personally come to rely on for my own small business. It has taken me, a millennial, years to wrap my head around the scope of this work — how to do it, then do it better, then realize my “creative class” ass has never taken a business course and can’t balance a checkbook. (Do people still balance checkbooks?) Global quarantine measures and the resulting surge in online sales might seem like a boon at first glance, but they present a host of additional challenges for small-business owners.
According to data from Empire State Development, small businesses — those with less than 100 employees — represent 98 percent of all businesses in New York and generated more than $1.5 billion in “economic impact” statewide in 2019. In an acknowledgement of the potential difficulty of migrating that impact to our new online-only reality, New York state recently launched the Empire State Digital initiative, billed as “an invitation for small businesses to explore the benefits of expanding beyond their physical brick and mortar location to an online marketplace.” (Never mind the fact that for many business owners navigating Covid-19 protocols, this “invitation” is a do-or-die proposition.)
The partners, including Etsy, Square, Shopify, and Clearbanc, offer perks such no fees (up to a certain amount), a free 90-day trial, and free listing credits, among others. Most of what they offer assistance with — branding, marketing, photography, SEO — is an entire department at any well-structured organization. The program assumes both digital literacy and technological access but doesn’t guarantee them. For the most part, migrating a brick-and-mortar business online is still easier said than done.
To highlight the specific challenges Covid-19 presents, I spoke with four small-business owners with varying digital footprints about what it’s been like to handle the surge in online shopping over the past few months.
The 20-person bookstore with a new dependency on data
With two brick-and-mortar retail locations in Brooklyn and Jersey City, New Jersey, WORD is nothing if not a neighborhood institution. Founded 14 years ago by Christine Onorati, the stores’ day-to-day operations were being handled by her husband, Vincent Onorati, when Covid-19 struck.
“[E-commerce] was never a huge part of our business. It was something we did because we had to,” Vincent Onorati says. At the onset of quarantine, there “were no boxes coming in or out” of either store for at least a month. Major revenue streams like school book fairs and author events — a 1,000-person launch for Jim Carrey’s memoir was scheduled in April — dried up entirely.
The business prioritized fulfilling orders from existing stock before turning to a distribution partner (Ingram Content Group) or sending customers elsewhere. Bookshop.org, which launched at the beginning of lockdown, helped independent booksellers most as an Amazon alternative for affiliate-link revenue. “There was a lot of goodwill in those first few months. It was a big ‘aha moment’ for a lot of people, and we gained a lot of customers,” says Vincent Onorati. “Then the anti-racism books became super-popular in mid-summer, and we couldn’t even keep up with the demand — the publishers had to reprint. But now what worries us is [that] people are falling into their old patterns again.”
“As sensitive as people are [about] where their political vote goes, they have to realize that where they choose to shop has as much significance”
Although WORD was classified as an essential business, no more than one person at a time was allowed inside the store, all the way through June. “We went from having maybe 10 online orders a day to 200 or 300,” he says. As quarantine measures were relaxed, WORD diversified its offerings by creating “mystery boxes” and care packages: greeting-card bundles pegged to Mother’s Day, sock bundles because “no one is leaving the house.”
“I couldn’t believe the amount of puzzles we sold those first few months,” Vincent Onorati says. Still, WORD sales look to be down between 30 and 50 percent year over year.
“There’s this insistence to lump Amazon and bookstores,” he says. “[Books] are inconsequential to them, really. Even people in my family, they don’t buy books there but they get all this other stuff. That’s worse. Independent business doesn’t just mean the bookstore — it means the hardware shop, it means the bodega. I have a friend who was ordering almonds on Amazon. I’m like, ‘You live in Greenpoint, you throw a rock you’ll hit almonds.’ It’s just this conditioning. As sensitive as people are [about] where their political vote goes, they have to realize that where they choose to shop has as much significance.”

The 10-person candle company with a massive increase in direct sales
Boy Smells co-founder Matthew Herman, who launched the brand in 2016 with his partner, David Kien, was taking press appointments in Paris when it became clear that the spread of Covid-19 was about to shut down the global economy. Herman safely made it back stateside, and “by March 17, we were up 1,500 percent in direct-to-consumer candle sales,” he remembers. “It was a weird time where we were trying to figure out what was going on, just as human beings, and then also everyone is like, ‘I’m stuck inside, I guess I need a candle.’”
At the end of 2019, just 25 percent of Boy Smells’ business was direct-to-consumer (DTC). Although they’d brought on a digital marketing agency to scale that part of the business, quarantine mercilessly hastened the shift on its own terms. “We had these huge supply-chain dilemmas because some component parts [for the candles] come from China and they were shut down, then the paper for our boxes comes from northern Italy, of all places, and the wax and fragrance comes from the USA, and then we shut down,” says Herman. “We had to work from home for a long while — we were diversifying our supply chain like crazy. Someone we knew who used to work in candle production was making 2,000 units a week for us out of their garage; me and David and our staff were pouring candles from home, trying to do 1,000 units a week that way.”
The ship has since righted itself to some degree, with China reopening for production “in the nick of time,” and a number of wholesale accounts were added back into the fold. (The brand was 100 percent DTC for a period during lockdown.) The now-10-person company invested in a planner and a CFO, launched a new collection in September, and remains structured for further growth in 2021. “Everyone I talk to is surprised by how well retail has bounced back,” says Herman. “I know I’ve spent more online this year than I did last year, [because] I’m not eating out, I’m not traveling — I’m not spending the way that I used to.”

The two-person vintage store with a huge following (but no e-commerce)
Brandon Veloria Giordano and Collin James Weber, the creative couple behind the five-year-old vintage business James Veloria, went into the pandemic in a precarious situation — at least on paper. Their website, a makeshift storefront hosted on Squarespace, badly needed updating. They didn’t have access to their retail space (or the inventory inside) during lockdown; their landlord, who collected each month’s full rent, didn’t let them back inside the building until the beginning of July.
But the duo’s business flourished on Instagram, where their personalities took center stage in compelling (and shoppable) Instagram stories. Brandon would model designer pieces in the couple’s living room, and shoppers would claim their favorites via direct message; the shopping has since migrated to the website via swipe-up functionality. (Direct messages admittedly got a bit messy.)
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Courtesy of Brandon Veloria Giordano and Collin James Weber
      
    
  



The couple “used every inch of space” to shoot products in their living room while also processing and fulfilling orders right off screen; this way of working allowed the store to stay afloat but also required a lot of physical, logistical, and, at times, emotional maneuvering. “[The stories] are fun and community building, but I was basically doing them to pay our rent,” adds Giordano. “I felt like I needed to be on all the time.”
Only time will tell if their newly launched website can sustain the same growth as Instagram without Brandon “putting on a miniskirt and rolling around on the floor” to draw shoppers in. “We try to make special collections that go up every week, plus daily stories where you can see new stock,” says Weber. “It’s just a lot of work that wasn’t part of our schedule before.” The pandemic, in essence, transformed the couple’s business model from brick-and-mortar retail into that of a social media influencer. Trade shows, for which the couple would travel up to eight times a year with upward of 200 pieces of inventory, didn’t happen at all in 2020.
“Compared to last year at this time, we aren’t that far off,” says Weber, “but it was a lot more work to get here.”
The one-woman retailer with a “pandemic-proof” business model
Monica Khemsurov launched her smoking accessories company Tetra five years ago. At the time she had two partners in the operation; now it’s just her, and the entire business is automated. Her store runs on Shopify, and a fulfillment partner is responsible for all shipping and logistics for a monthly fee. “It’s expensive,” she tells me. “As a small business with low order volume, the issue is, can your margins sustain it?”
Machine automation doesn’t free Khemsurov from labor so much as it creates new kinds of work for her to focus on so that she can achieve scale. Her sales doubled during lockdown, which means she’s had to shave more off her profit margins to spend money on not only marketing to retain those customers but also on product development, production, logistics, and inventory management.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Courtesy of Tetra
      
    
  


As a cannabis-related company, Khemsurov can’t run ads on Facebook or Instagram, though she’s sent product to photographers to create original smoking-related content for Tetra’s social media accounts. “The really hard part during Covid was that so many businesses moved online, so I had to decide where the cutoff point was [with wholesale accounts]. At a certain point, if you have too many people online then retailers are competing with each other for the same Google searches.”
Despite the influx of new competition for keyword buys, Khemsurov continues to post to her brand blog to help with the site’s ranking and offers a 10 percent discount for newsletter sign-ups, as well as free shipping on orders over $150. These are common practices for online businesses, but again: You need the margins to sustain them. “I’m just gonna do what I can,” says Khemsurov. “If I get three people and one person buys something, that’s one person I didn’t have yesterday. I lost so much work during Covid that I’ve been able to focus on Tetra, and like everyone else I’m not spending very much money. I’m looking at this time as an investment.”
ntribute today from as little as $3.


  
  
  
      




  I write about the internet for a living, so it may seem strange to report that I’m no longer very fond of it.
This summer, I found myself without regular internet access. Ahead of the presidential elections, with the manic news cycle, widespread disinformation, and what feels like the constant need to refresh social media, I sometimes wish I were without it again.
About a week after I gave birth to my baby in June, my partner and I moved into our mostly built house in upstate New York. That’s when we learned it would take the local internet provider an astounding month and a half to install internet. Located on a mountain in the Catskills, there was no cell service to tide me over, either. 
I was on maternity leave, so it was no longer vital to have internet for work, but I still really wanted it. To be fair, if I really needed it, I could walk five minutes down the road and steal my neighbor’s wifi, but I’d have to contend with the mosquitoes, the elements, and the baby. So in practice, I didn’t do so very often. 
On average, Americans spend about four and a half hours a day online, according to data from the research firm Zenith. People like me who work online spend even more time there. In normal times, not only would I spend the whole workday online, I’d spend a lot of my time before and afterward there, too. I was always up to date on every news story, idle meme, and silly Twitter controversy. People who were less online — most of them — could tell me nothing I hadn’t at least seen flash across my screen.
At first, not having the internet felt like a real hassle. I felt like I was missing out or that something important might happen without my knowing. I called the provider every day hoping for an earlier installation date. 
But over time, not having internet was a blessing. My sojourns onto the web were brief and more intentional. Every few days I’d go to town or to my neighbor’s house and check Twitter and Instagram, where a relatively interesting backlog of posts and messages had time to build up, and then read a few important news stories. Pretty quickly I’d turn to more practical matters that I kept track of through memos on my phone: “to look up,” “to buy,” “to do.” 
I got really into reference books, where I looked and failed to identify the bee-like flies that attacked me while I was briefly outside using the internet. For baby information, instead of turning up every disparate answer in the world online, I had to sate myself with the conflicting guidance of just two parenting books. The baby survived.
I tweeted and read Twitter less. My tweets were better. My mind was better. My attention span seemed to grow longer. I sat with my feelings without reprieve, and I feel like that made me emotionally stronger.
At the time, we were in the throes of the pandemic. The death toll from the coronavirus in the US had recently surpassed 100,000. Americans were taking to the streets to protest police brutality against Black Americans, who were also being disproportionately ravaged by the coronavirus. I had felt like I needed to bear witness to it all, in order to show that I cared.
But not being online all the time didn’t mean I wasn’t informed about these very important news stories. I would catch up after the fact, when there was more conclusive information, but I wasn’t reading each incremental development. 
What it did was remind me what was within my power and what was not. Reading every last tweet and news story about the coronavirus or police brutality or presidential election malfeasance only gives you the illusion of control. Bearing witness is important, but so is your mental health. Infinite scrolling can feel like a stand-in for real action. It’s not. 
What I could actually do was donate money to causes I believe in, register to vote in my new home, and try my best to be present in my own life.
People blame technology for a whole spate of ills from depression to decreased productivity — though there’s little definitive proof it’s actually the cause. What’s certain to me, at least, is that continually pulling down on my phone to refresh Twitter or Instagram or the latest dire machinations of the president does not feel good.
What I learned was that things that seem important in the moment tend to become less so with time. Have an email that’s really haunting you but where your response isn’t mandatory? Try waiting till it’s no longer relevant. Feel like you need to be on Slack at all hours? Log off and await what your coworkers can do without your presence. (Of course, I’m fortunate enough to have a job where, although people might Slack me at all hours, I’m not usually required to respond. Not everyone has that.)
I found out about news hours or days later, when I was too late to weigh in. No one needed my takes. Did they ever? I read and listened to more books. I completed a variety of construction projects around my new house. I ate whole meals without opening my phone. I learned how to parent.
Looking back, the best part of all this was getting to spend uninterrupted time with my baby and partner. My baby’s first smile wasn’t missed because I was looking through a screen to somewhere else (though I guess I did take a lot of pictures). I was constantly, boringly present. 
By the time the internet installation date drew near in August, I found myself dreading being online again. It felt like this newly calm portion of my life was coming to an end. I would eventually have to go back to work. But I also felt a little restored and better able to deal with the hazards of the outside world. I hadn’t realized how much the online world had burdened me.
Now, months later, I’m working online and out of my bubble. I wish I could say I’m more cautious in my internet usage, but that’s mostly untrue. The lure of the internet and social media apps is strong, especially during this dramatic election year.
What we can do is try to recognize when something really is important, and when there really is something we can do about it. Ahead of this election, vote. Then consider logging off. The events of the world will unfold whether or not we are watching them every step of the way. 


  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  




Searches for changing one’s vote did not trend following the recent presidential debate, and just a few states appear to have processes for changing an early vote. But that didn’t stop President Trump from wrongly saying otherwise on Tuesday. 
In early morning posts, the president falsely claimed on Twitter and Facebook that many people had Googled “Can I change my vote?” after the second presidential debate and said those searching wanted to change their vote over to him. Trump also wrongly claimed that most states have a mechanism for changing one’s vote. Actually, just a few states appear to have the ability, and it’s rarely used. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Twitter did not attach a label to Trump’s recent tweet.
      
      
        Twitter
      
    
  


Trump’s claim about what was trending on Google after the debate doesn’t hold up. Searches for changing one’s vote were not among Google’s top trending searches for the day of the debate (October 22) or the day after. Searches for “Can I change my vote?” did increase slightly around the time of the debate, but there is no way to know whether the bump was related to the debate or whether the people searching were doing so in support of Trump.


  Help Vox’s reporting
Vox wants to hear from you about your 2020 voting experience. To share, fill out this short Google form or email crowdsource@vox.com. 


It was only after Trump’s posts that searches about changing your vote spiked significantly. It’s worth noting that people were also searching for “Can I change my vote?” during a similar period before the 2016 presidential election. 
Google declined to comment on the accuracy of Trump’s post.  
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Google Trends shows that a big spike in searches for changing one’s vote came after Trump’s post. 
      
      
        Google Trends
      
    
  



Trump also claimed that these results indicate that most of the people who were searching for how to change their vote support him. But the Google Trends tool for the searches he mentioned does not provide that specific information.
Perhaps the most egregiously false claim in Trump’s recent posts is about “most states” having processes for changing your early vote. In fact, only a few states have such processes, and they can come with certain conditions. For instance, in Michigan, voters who vote absentee can ask for a new ballot by mail or in person until the day before the election. 
The Center for Election Innovation’s David Becker told the Associated Press that changing one’s vote is “extremely rare.” Becker explained, “It’s hard enough to get people to vote once — it’s highly unlikely anybody will go through this process twice.” 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Trump’s post on Facebook was accompanied by a link to Facebook’s Voting Information Center.
      
      
        Facebook
      
    
  


At the time of publication, Trump’s false claims had drawn about 84,000 and 187,000 “Likes” on Twitter and Facebook, respectively. Trump’s posts accelerated searches about changing your vote in places like the swing state of Florida, where changing one’s vote after casting it is not possible. Those numbers are a reminder of the president’s capacity to spread misinformation quickly. 
On Facebook, the president’s post came with a label directing people to Facebook’s Voting Information Center, but no fact-checking label. Twitter had no annotation on the president’s post. Neither company responded to a request for comment. 
That Trump is willing to spread misinformation to benefit himself and his campaign isn’t a surprise. He does that a lot. Still, just days before a presidential election in which millions have already voted, this latest episode demonstrates that the president has no qualms about using false claims about voting to cause confusion and sow doubt in the electoral process.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  Donald Trump has poked the bears that are Silicon Valley billionaires.
Over the last four years, the tech industry’s very richest have gotten far more political than they have ever been, channeling their money and energy into the world of partisan campaigning. They’ve hired full-time political aides to manage their investments. They’ve traded notes and organized to pool their money for maximum impact. And they’ve become vocal public critics of the president, so incensed by the Trump presidency that they’ve eschewed the longstanding Silicon Valley tradition of staying out of politics. 
Some of the biggest Silicon Valley celebrities are indeed staying out of the race, but here are the 15 Democrats of Silicon Valley who are most responsible for the current political awakening. Recode reviewed all public federal campaign contributions this cycle through October 15.
While they are backing different groups, one striking commonality is how little they had donated prior to Trump’s 2016 run. It’s new territory for almost all of them: Prior to then, these 15 people together had donated about $7 million in total federal campaign contributions. Over the last two years? That figure is over $120 million.
Prior to the 2016 race, these 15 people together had donated about $7 million in total federal campaign contributions. Over the last two years? That figure is over $120 million.
Some caveats to this list: Determining who qualifies as “Silicon Valley” is more subjective than you’d think (Does it apply to everyone who physically lives in the San Francisco Bay Area, no matter the industry? What about tech leaders who live in New York or Seattle?) but we focused on people whose money principally comes from founding or investing in tech companies.
This list also doesn’t tally all political donations. It doesn’t include gifts to state or local candidates. And, most importantly, the sums don’t include the tens of millions of dollars — likely even hundreds of millions — that these donors are spending on outside groups that aren’t required to disclose their backers. So Silicon Valley megadonors’ true contributions to ousting Trump are impossible to assess in total, meaning it is also impossible to assess the scale of their influence in American democracy.
That influence could pay off in a Biden administration that will have to wrestle with how aggressively to regulate the tech companies that have helped create these fortunes. 
Karla Jurvetson: $27.5 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Karla Jurvetson has become one of the nation’s most generous female campaign donors.
      
      
        Randy Vazquez/MediaNews Group/The Mercury News via Getty Images
      
    
  


The psychiatrist has come out of nowhere over the last four years to be one of the ascendant Democratic megadonors of the Trump era. The former wife of tech mogul Steve Jurvetson, she has focused her donations on gifts to female candidates; she almost single-handedly financed a super PAC that spent big to support Elizabeth Warren’s presidential bid in its final weeks, pumping $15 million into that last-ditch effort. And in a sign of her power, she was the one chosen to host Barack Obama when he made his sole Silicon Valley fundraising trip of the cycle last fall.
Dustin Moskovitz: $25 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Dustin Moskovitz helped found Facebook with Mark Zuckerberg.
      
      
        Horacio Villalobos/Corbis via Getty Images
      
    
  


Moskovitz is one of the most thoughtful figures in Silicon Valley in terms of his philanthropy, and his political giving is similar. A founder of Facebook alongside Harvard classmate Mark Zuckerberg, Moskovitz and his political team have scoured the academic literature to try to deduce where megadonors can get the greatest possible “return” on their investments. And his brainy dive into political science research has led him to Future Forward, a super PAC that is focusing on last-minute television ads just before voters head to the polls. Moskovitz has been closely associated with the group for much of the calendar year, and recent reports disclosed that he has put at least $22 million into the little-known group.
Reid Hoffman: $14.1 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Reid Hoffman is one of the country’s biggest and most controversial donors.
      
      
        Phillip Faraone/Getty Images for WIRED25
      
    
  


No megadonor has become more controversial in the Democratic Party than Hoffman, the billionaire founder of LinkedIn. Hoffman has been trying to move the Democratic Party into the digital age, and to do so has been willing to fund unorthodox projects that push the envelope in ways that some other Democratic donors find discomfiting. One of the most unusual expenses from Hoffman has been the $4.5 million that he has spent on his own to create anti-Trump memes. He and his aides have also become the port of call for other major tech donors, building a full-scale political operation that has made Hoffman a powerful figure in Silicon Valley politics. Operatives consider getting on Hoffman’s list of recommended groups to be a major coup. In recent weeks, Hoffman has been emailing his network to encourage them to donate to Biden transition efforts, according to messages seen by Recode.
Jeff and Erica Lawson: $8.2 million
Jeff Lawson, the co-founder and CEO of the $45 billion software company Twilio, and his wife, Erica, had only given about $1,000 to federal candidates before the 2016 race. But in a reflection of how Trump has energized Silicon Valley, the Lawsons soon after his election started cutting checks to dozens of Democratic congressional candidates and state parties. This fall, they started really digging deep, including giving $6 million between them to Future Forward.
Connie Ballmer: $7.6 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Connie Ballmer and her husband, former Microsoft CEO Steve Ballmer, at the White House for a state dinner in 2011.
      
      
        Brendan Smialowski/Getty Images
      
    
  


Ballmer is of Seattle, but she’s the wife of former Microsoft CEO Steve Ballmer — one of the richest people in the world. Ballmer’s total comes almost entirely from the $7 million she donated to Everytown for Gun Safety, a gun-control group started by Mike Bloomberg.
Jeff Skoll: $7.4 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Jeff Skoll speaks onstage in 2017.
      
      
        Matt Winkelmeyer/Getty Images for Sundance Film Festival
      
    
  


Like Lawson, Skoll — the first full-time employee at eBay — had never given more than a few thousand bucks before Trump was elected. But this year, the billionaire philanthropist started funneling his fortune into Democratic efforts, including $4.5 million into Senate Majority PAC, the main Democratic super PAC aiming to retake the Senate.
Eric Schmidt: $6 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Eric Schmidt is a longtime Democratic powerbroker.
      
      
        Lee Jin-man/AP
      
    
  


Schmidt is the consummate Democratic powerbroker, having helped Google curry favor with the Barack Obama administration back when he was Google’s CEO. So, unlike others on this list, Schmidt is not new to this. He has put millions into groups like Future Forward in addition to hosting fundraisers for the Biden campaign directly. It will be interesting to see what role Schmidt may play in a Biden administration if Biden wins.
Sam Bankman-Fried: $5.6 million
Bankman-Fried is one of the most unusual megadonors of the cycle. A 28-year-old cryptocurrency trader who would often sleep in his office overnight on a bean bag, Bankman-Fried, like Moskovitz, identifies as an “effective altruist.” That means he’s trying to use his money for the greatest possible good — which likely has led him to donate to Future Forward.
Patty Quillin and Reed Hastings: $5.3 million
Hastings, the founder of Netflix, has long been involved in politics — he has been a major funder of education reform efforts, and he helped raise money for Pete Buttigieg during the primary. He and his wife, Quillin, are now funding more than ever, including $2 million to Senate Majority PAC. And that $5.3 million figure doesn’t even include the millions more that the couple is spending this year on California ballot initiatives and local politics, which have long been a political priority for them.
Jessica Livingston: $5 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Jessica Livingston (middle) helped found Y Combinator.
      
      
        Noam Galai/Getty Images for TechCrunch
      
    
  


Livingston is one of the co-founders of Y Combinator, the iconic Silicon Valley startup accelerator, alongside her husband Paul Graham. And she cut the biggest check by far of her career this fall when she gave $5 million to a group called Tech For Campaigns, a digital- and tech-focused Democratic group.
Michael Moritz: $3.9 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Venture capitalist Michael Moritz onstage at the Vanity Fair New Establishment Summit on October 19, 2016, in San Francisco, California.
      
      
        Michael Kovac/Getty Images for Vanity Fair
      
    
  


Moritz, a legendary venture capitalist at Sequoia Capital, had only donated $70,000 in his life to politics before Trump was elected. But since 2016, Moritz has gotten heavily involved with Acronym, a liberal group focused on digital anti-Trump advertising, and he and groups associated with him have donated over $1.5 million to its affiliated super PAC. Moritz has also emailed his associates in Silicon Valley to encourage them to support Acronym. A fun fact: Moritz’s longtime co-leader at Sequoia, Doug Leone, is one of the few big donors from tech to Trump, which should make for some interesting conversations between the two of them.
Ken Duda: $3.7 million
Probably the least well-known person on this list, Duda founded a public software company called Arista. He gave $2 million over the last year to Acronym.
Vinod Khosla: $3.1 million
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Vinod Khosla at an awards ceremony in 2015.
      
      
        MediaNews Group/Bay Area News via Getty Images
      
    
  


A billionaire perhaps more widely known for his quixotic campaign to maintain his private access to a Bay Area beach, Khosla has given most of his donations to American Bridge, a Democratic super PAC running anti-Trump ads.

  
  
  
      




  The 2020 election is different from those that have come before in many ways, including the ways in which we’re Googling. 
Waves of Interest, a new collaboration between Google News Initiative and information design firm Truth & Beauty, looks at how Americans’ internet searches have changed over the course of five presidential election years, from 2004 to 2020. The series of interactive data visualizations looks at the relative popularity of a range of popular political concepts, garnered from search trends as well as Pew Research Center election surveys, across election years. So far, 2020’s data goes through September and will be updated when each month is complete.
As Google data editor Simon Rogers told Recode earlier this year, “You’re never as honest as you are with your search engine.” And this year, that honesty has resulted in a snapshot of the biggest concerns and questions Americans have ahead of arguably the most important election of our lifetime. It also suggests which issues might have more bearing on the election’s outcome.


  .woi-embed {
    position: relative;
    padding-bottom: 100%;
    height: 0;
    margin: 1em auto;
  }

  .woi-embed.year {
    padding-bottom: 620px;
  }

  @media (max-width: 600px){
    .woi-embed.year {
        padding-bottom: 0;
        height: 80vh;
      }
  }

  @media (max-width: 480px){
    .woi-embed {
        padding-bottom: 0;
        height: 80vh;
      }
  }





  



In 2020, a number of the popular searches — postal voting, unemployment, vaccine — relate to the coronavirus pandemic as well as this administration’s response. There’s also increased interest in terms like “fact checking” that have to do with misinformation, whether that comes from foreign interference or from the president himself. 
While some issues, like electoral fraud, are common from election to election, others vary widely by year. In the last election, the Second Amendment, minimum wage, and affordable housing were big concerns. (Visit the site for a more comprehensive version of the visualization.) Back in 2004, there was an especially high level of search around same-sex marriage and terrorism.


  .woi-embed {
    position: relative;
    padding-bottom: 100%;
    height: 0;
    margin: 1em auto;
  }

  .woi-embed.year {
    padding-bottom: 620px;
  }

  @media (max-width: 600px){
    .woi-embed.year {
        padding-bottom: 0;
        height: 80vh;
      }
  }

  @media (max-width: 480px){
    .woi-embed {
        padding-bottom: 0;
        height: 80vh;
      }
  }




  


The visualizations also show where in America these searches are more pronounced, highlighting regional differences over time. Abortion, for example, is of perennial importance in the Midwest and South. Gun control has outsized interest in the West, especially in Wyoming, in every election year. Affordable housing appears to always be just as important in the Northeast as the Southwest. Earlier this century, health insurance was a concern nationwide, but lately, it’s more prevalent in the Northeast. 
These charts present a fascinating window into our major concerns going into each election — and 2020’s concerns are particularly novel. Nevertheless, we can’t predict the future based on what people are searching for. But perhaps, thanks to the data the search engine is collecting, we can better understand the past.

  
  
  
      




  Last month, US federal authorities indicted six e-commerce consultants and former Amazon employees in a $100 million bribery scheme in which insiders allegedly accepted payments to help certain Amazon merchants on the platform and hurt others. In the close-knit world of top Amazon sellers and consultants, this was big news. But it was not a surprise. Rumors of such behavior are common in industry circles, and Amazon itself admitted in 2018 that it was investigating employees for reportedly leaking internal marketplace data to outsiders in exchange for cash.
In the weeks since the indictments, a half dozen top-earning Amazon sellers told Recode that the consultants and former employees indicted should face the legal consequences if they broke the law. But these same top sellers also argued that the problem is much bigger than a few bad apples and that Amazon deserves scrutiny for creating the fertile ground for bribery schemes to blossom. The reason? Amazon’s inability or refusal to consistently offer adequate support to its 1.7 million sellers when they have issues, especially when it comes to suspensions that Amazon hands down with little explanation and sometimes no warning. On top of these issues, it’s not uncommon for sellers to languish in Amazon purgatory for weeks or months trying to reinstate their business, either on their own or with the help of an ecosystem of consultants — some of whom prey on merchants’ desperation. 
“With Amazon, you are guilty until proven innocent,” said Eytan Wiener, the co-founder and chief operating officer of a large Amazon seller, whose firm was once suspended by Amazon in the UK and who has helped other suspended sellers in the US.
This reality — that Amazon sellers can have their livelihood snatched from them at any time — can benefit Amazon, too. The company introduced a premium seller program in 2018 that charges thousands of dollars a month to assign a dedicated Amazon representative that a seller can easily contact. While these reps do not oversee account suspensions and can’t directly reinstate a merchant, some large Amazon sellers pay the fee mainly so they have a person they can get on the phone in the event of a suspension or other severe penalty.
Amazon often boasts that the small and mid-sized merchants who help stock the virtual shelves of The Everything Store account for 60 percent of the company’s e-commerce sales worldwide. It has emphasized this as the company’s treatment of its sellers has come under scrutiny in the past few years, most recently during Jeff Bezos’s congressional testimony in July and in the wake of a House antitrust investigation report that followed. The tech giant has certainly created an efficient way for small merchants to build global e-commerce businesses with small teams and little overhead in ways that wouldn’t have been possible even 10 years ago. At the same time, as Amazon has aggressively recruited more sellers to its platform, it has been plagued by complaints about the support it offers them — most notably, the company’s penchant for suspending sellers from doing business on the site with no warning and nary an explanation.


  Do you work at, or sell on, Amazon and have thoughts on this topic? Please email Jason Del Rey at jasondelrey@protonmail.com to reach him confidentially. His phone number and Signal number are available upon request by email.


This was the situation that Jacqueline Tatelman, the co-founder of the backpack brand State Bags, faced earlier this year. Her startup brand does most of its business through its own website but decided to start selling on Amazon earlier this year in order to expand its online presence before the back-to-school shopping season this fall. The company’s plans were delayed when the pandemic hit the US in March and Amazon prioritized essential goods in its warehouses. Then, as the company prepared to finally start selling on the platform in May, a shocking email from Amazon landed in its inbox:
“We have discovered information that indicates your Amazon seller account has engaged in deceptive, fraudulent, or illegal activity. ... As a result, we have closed your Amazon seller account to prevent harm to our customers, other selling partners, and our store.”
The email ended by saying that the brand should email Amazon if they thought the decision was an error. And State Bags did, repeatedly. The only thing that had changed between the time the company set up its Amazon storefront and when it received the suspension notice was a change of the business credit card on file, which was information Amazon had requested.
Amazon responded once more with a generic note saying that an internal case had been opened. Then silence, for months. No reason given, and no updates, as State Bags inventory with a retail value of $60,000 sat unsold in Amazon warehouses, with no guidance from the tech giant on if or how the brand could get the inventory out. 
“It’s not ethically okay to do these things to these small brands who are in the middle of the ocean, in a storm, with a tiny boat and a broken paddle”
“It’s not ethically okay to do these things to these small brands who are in the middle of the ocean, in a storm, with a tiny boat and a broken paddle,” Tatelman told Recode. “That’s how small brands feel.” 
After Recode notified Amazon of the State Bags suspension, Amazon spokesperson Cecilia Fan said the company would reach out to the brand “to address their concerns.” A day later, Amazon granted State Bags access to its storefront for the first time since May.
And the State Bags story is not a one-off. Talk to any Amazon seller that does significant business on the site and there’s a good chance they have a similar tale or know a peer who does. Just this August, a LinkedIn post detailing a similar suspension problem that was written by the founder of a car dashboard camera retailer was widely read in the Amazon seller community. Titled “Please Help! An Open Letter to Jeff Bezos, Amazon Leadership, and the Seller Performance Team,” it described how Amazon had suspended the seller out of nowhere and for six months had claimed that the seller forged documentation. Two days after the president of The Dashcam Store, Andrew Aboudaoud, posted the cautionary tale, Amazon reinstated his selling account with a boilerplate email. No explanation or apology was given.
“The way Amazon currently handles their relationships with third-party sellers is fundamentally broken (in a variety of ways),” Aboudaoud wrote in a message to Recode, “although it does appear Amazon is slowly taking steps to overhaul this system.” 
Fan, the Amazon spokesperson, emailed Recode a statement that read in part: “Amazon’s Selling Partner Support team handled more than 51 million contacts from selling partners in 2019, and we strive to respond to and resolve every contact expeditiously.”
“Selling Partner Support responded to more than 90% of emails in under 12 hours; answered more than 96% of phone calls in under 90 seconds; answered more than 90% of chats in under 90 seconds; and fully resolved more than 85% of all seller issues in under 24 hours,” the statement added. Using that 85% figure, if every one of Amazon’s 1.7 million sellers has an issue at one point, that still means 255,000 of them — or 15% — could have to wait more than 24 hours for a resolution. I’ve been interviewing Amazon sellers for years, and it’s not uncommon for these cases to remain unresolved for weeks, if not months.
Amazon sellers acknowledge that scaling seller support at Amazon’s size is a challenge, and some say the seller support experience has gotten modestly better in recent years. 
“They’ve done better and are a bit more explanatory, but still have a long way to go” 
“They’ve done better and are a bit more explanatory, but still have a long way to go,” Wiener, the co-founder of a large Amazon seller, told Recode. “But they should get some credit for what they’ve done because it’s a very hard task.” 
Still, sellers argue that Amazon’s platform is so big, even a few percentage points of cases that slip through the cracks or are the result of technical errors can wreck many small businesses and the lives of the people who run and work for affected merchants.
Some sellers have turned to the paid seller support program that Amazon offers as a type of insurance against unexpected problems. Amazon charges a monthly fee of between $1,600 and $5,000 for the program, depending on the size of the seller’s business. Fan, the Amazon spokesperson, said the optional paid program provides sellers with “business guidance on topics such as inventory management, merchandising, and global expansion,” but “does not provide expedited account resolution” when Amazon suspends an account or takes other serious action against a merchant.
Sellers who use the program or are familiar with it agreed that the paid internal Amazon reps do not directly resolve suspensions or other major account issues. But these sellers said many merchants pay the fee simply so they have a specific person they can get on the phone to point them in the right direction when something big goes wrong.
“They’ve set up the exact same business model as Tony Soprano: pay us to help protect you ... from us,” said one Amazon merchant whose business sells more than $10 million of goods annually on Amazon. “It’s messed up.”
This seller doesn’t pay for the program today but said he probably would in the near future as he hears more firsthand stories of merchants getting penalized or shut down for reasons they say are unfair or unclear. Referencing one claim in the recent indictment of Amazon consultants, which alleged a large cash bribe delivered via an Uber, the seller added: 
“If my business got shut down overnight and I could send $100,000 in an Uber to fix it, I’m probably not going to do it but I’m certainly going to consider it.” 

  
  
  
      




  Last month, US federal authorities indicted six e-commerce consultants and former Amazon employees in a $100 million bribery scheme in which insiders allegedly accepted payments to help certain Amazon merchants on the platform and hurt others. In the close-knit world of top Amazon sellers and consultants, this was big news. But it was not a surprise. Rumors of such behavior are common in industry circles, and Amazon itself admitted in 2018 that it was investigating employees for reportedly leaking internal marketplace data to outsiders in exchange for cash.
In the weeks since the indictments, a half dozen top-earning Amazon sellers told Recode that the consultants and former employees indicted should face the legal consequences if they broke the law. But these same top sellers also argued that the problem is much bigger than a few bad apples and that Amazon deserves scrutiny for creating the fertile ground for bribery schemes to blossom. The reason? Amazon’s inability or refusal to consistently offer adequate support to its 1.7 million sellers when they have issues, especially when it comes to suspensions that Amazon hands down with little explanation and sometimes no warning. On top of these issues, it’s not uncommon for sellers to languish in Amazon purgatory for weeks or months trying to reinstate their business, either on their own or with the help of an ecosystem of consultants — some of whom prey on merchants’ desperation. 
“With Amazon, you are guilty until proven innocent,” said Eytan Wiener, the co-founder and chief operating officer of a large Amazon seller, whose firm was once suspended by Amazon in the UK and who has helped other suspended sellers in the US.
This reality — that Amazon sellers can have their livelihood snatched from them at any time — can benefit Amazon, too. The company introduced a premium seller program in 2018 that charges thousands of dollars a month to assign a dedicated Amazon representative that a seller can easily contact. While these reps do not oversee account suspensions and can’t directly reinstate a merchant, some large Amazon sellers pay the fee mainly so they have a person they can get on the phone in the event of a suspension or other severe penalty.
Amazon often boasts that the small and mid-sized merchants who help stock the virtual shelves of The Everything Store account for 60 percent of the company’s e-commerce sales worldwide. It has emphasized this as the company’s treatment of its sellers has come under scrutiny in the past few years, most recently during Jeff Bezos’s congressional testimony in July and in the wake of a House antitrust investigation report that followed. The tech giant has certainly created an efficient way for small merchants to build global e-commerce businesses with small teams and little overhead in ways that wouldn’t have been possible even 10 years ago. At the same time, as Amazon has aggressively recruited more sellers to its platform, it has been plagued by complaints about the support it offers them — most notably, the company’s penchant for suspending sellers from doing business on the site with no warning and nary an explanation.


  Do you work at, or sell on, Amazon and have thoughts on this topic? Please email Jason Del Rey at jasondelrey@protonmail.com to reach him confidentially. His phone number and Signal number are available upon request by email.


This was the situation that Jacqueline Tatelman, the co-founder of the backpack brand State Bags, faced earlier this year. Her startup brand does most of its business through its own website but decided to start selling on Amazon earlier this year in order to expand its online presence before the back-to-school shopping season this fall. The company’s plans were delayed when the pandemic hit the US in March and Amazon prioritized essential goods in its warehouses. Then, as the company prepared to finally start selling on the platform in May, a shocking email from Amazon landed in its inbox:
“We have discovered information that indicates your Amazon seller account has engaged in deceptive, fraudulent, or illegal activity. ... As a result, we have closed your Amazon seller account to prevent harm to our customers, other selling partners, and our store.”
The email ended by saying that the brand should email Amazon if they thought the decision was an error. And State Bags did, repeatedly. The only thing that had changed between the time the company set up its Amazon storefront and when it received the suspension notice was a change of the business credit card on file, which was information Amazon had requested.
Amazon responded once more with a generic note saying that an internal case had been opened. Then silence, for months. No reason given, and no updates, as State Bags inventory with a retail value of $60,000 sat unsold in Amazon warehouses, with no guidance from the tech giant on if or how the brand could get the inventory out. 
“It’s not ethically okay to do these things to these small brands who are in the middle of the ocean, in a storm, with a tiny boat and a broken paddle”
“It’s not ethically okay to do these things to these small brands who are in the middle of the ocean, in a storm, with a tiny boat and a broken paddle,” Tatelman told Recode. “That’s how small brands feel.” 
After Recode notified Amazon of the State Bags suspension, Amazon spokesperson Cecilia Fan said the company would reach out to the brand “to address their concerns.” A day later, Amazon granted State Bags access to its storefront for the first time since May.
And the State Bags story is not a one-off. Talk to any Amazon seller that does significant business on the site and there’s a good chance they have a similar tale or know a peer who does. Just this August, a LinkedIn post detailing a similar suspension problem that was written by the founder of a car dashboard camera retailer was widely read in the Amazon seller community. Titled “Please Help! An Open Letter to Jeff Bezos, Amazon Leadership, and the Seller Performance Team,” it described how Amazon had suspended the seller out of nowhere and for six months had claimed that the seller forged documentation. Two days after the president of The Dashcam Store, Andrew Aboudaoud, posted the cautionary tale, Amazon reinstated his selling account with a boilerplate email. No explanation or apology was given.
“The way Amazon currently handles their relationships with third-party sellers is fundamentally broken (in a variety of ways),” Aboudaoud wrote in a message to Recode, “although it does appear Amazon is slowly taking steps to overhaul this system.” 
Fan, the Amazon spokesperson, emailed Recode a statement that read in part: “Amazon’s Selling Partner Support team handled more than 51 million contacts from selling partners in 2019, and we strive to respond to and resolve every contact expeditiously.”
“Selling Partner Support responded to more than 90% of emails in under 12 hours; answered more than 96% of phone calls in under 90 seconds; answered more than 90% of chats in under 90 seconds; and fully resolved more than 85% of all seller issues in under 24 hours,” the statement added. Using that 85% figure, if every one of Amazon’s 1.7 million sellers has an issue at one point, that still means 255,000 of them — or 15% — could have to wait more than 24 hours for a resolution. I’ve been interviewing Amazon sellers for years, and it’s not uncommon for these cases to remain unresolved for weeks, if not months.
Amazon sellers acknowledge that scaling seller support at Amazon’s size is a challenge, and some say the seller support experience has gotten modestly better in recent years. 
“They’ve done better and are a bit more explanatory, but still have a long way to go” 
“They’ve done better and are a bit more explanatory, but still have a long way to go,” Wiener, the co-founder of a large Amazon seller, told Recode. “But they should get some credit for what they’ve done because it’s a very hard task.” 
Still, sellers argue that Amazon’s platform is so big, even a few percentage points of cases that slip through the cracks or are the result of technical errors can wreck many small businesses and the lives of the people who run and work for affected merchants.
Some sellers have turned to the paid seller support program that Amazon offers as a type of insurance against unexpected problems. Amazon charges a monthly fee of between $1,600 and $5,000 for the program, depending on the size of the seller’s business. Fan, the Amazon spokesperson, said the optional paid program provides sellers with “business guidance on topics such as inventory management, merchandising, and global expansion,” but “does not provide expedited account resolution” when Amazon suspends an account or takes other serious action against a merchant.
Sellers who use the program or are familiar with it agreed that the paid internal Amazon reps do not directly resolve suspensions or other major account issues. But these sellers said many merchants pay the fee simply so they have a specific person they can get on the phone to point them in the right direction when something big goes wrong.
“They’ve set up the exact same business model as Tony Soprano: pay us to help protect you ... from us,” said one Amazon merchant whose business sells more than $10 million of goods annually on Amazon. “It’s messed up.”
This seller doesn’t pay for the program today but said he probably would in the near future as he hears more firsthand stories of merchants getting penalized or shut down for reasons they say are unfair or unclear. Referencing one claim in the recent indictment of Amazon consultants, which alleged a large cash bribe delivered via an Uber, the seller added: 
“If my business got shut down overnight and I could send $100,000 in an Uber to fix it, I’m probably not going to do it but I’m certainly going to consider it.” 

  
  
  
      





  Joe Biden’s campaign has signed off on a fundraiser on tech issues being hosted by a series of prominent critics of Big Tech, including Sen. Elizabeth Warren. The event serves as the latest clue in the challenge to figure out how a Biden administration would govern the tech industry, suggesting that those critics would at least have a line into his White House.  
Warren and a half-dozen other big names are set to hold an October 27 event on “Advancing Innovation, Competition, and Prosperity in the American Tech Sector,” according to a copy of the invitation obtained by Recode. The event is one of the closest linkages yet between the most pro-regulation voices in the Democratic Party and the Democratic nominee, who has not publicly pinned down his precise positions on tech regulation.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Biden Victory Fund
      
    
  


The speakers at the event include David Cicilline, the Congress member who just led the congressional investigation into Big Tech companies; Tish James, New York’s state attorney general who is leading the states’ own investigations; and leading proponents of a tech breakup such as early Facebook investor Roger McNamee and influential Columbia Law School professor Tim Wu. And Warren, who made Big Tech issues a cornerstone of her presidential campaign.
Biden, who will not be there, does not necessarily endorse every position espoused by speakers at fundraisers held on his behalf. But presidential campaigns vet and debate potential fundraiser hosts — and, in this case, it is not just that one or two hosts have a personal, private opinion about the conversation topic and they happen to be organizing a broader pro-Biden event. Here the Biden campaign is blessing an entire event organized around one point of view — that the tech giants are too menacing and stifling out competition — featuring a lineup of the most high-profile leaders on the issue.
The Biden campaign didn’t return requests for comment.
One reason it is particularly revealing is that Biden has proven to be elusive on tech issues. Activists on the left hope that he will govern in a less tech-friendly way than Barack Obama did. But while Biden has made scattered critical comments about Facebook in particular, he has not made tech regulation a campaign priority and has left both the activists and the tech companies themselves largely speculating about what a Biden administration would mean for them.
Another interpretation of that, though, is that Biden is malleable on the issue — and therefore can be influenced by political assets such as campaign fundraising potential. One objective of the October 27 event hosted by Warren and others may be to raise a large amount and show Biden that there is money to be made — and that there’s political upside more broadly — by allying with the tech-breakup crowd. Tickets range from $250 to $100,000.
Because Biden certainly knows that there is big money on the other side of the issue. Some Biden supporters who are critical of Big Tech are concerned about Biden’s ties to tech elites, such as former Google CEO Eric Schmidt. And in a sign of just how enigmatic Biden has proven on tech regulation, Biden’s campaign has also hosted technology-focused fundraisers with Schmidt, a leading voice against the breakup of companies like his former employer.
So all told, the question these fundraisers pose is: Who will have more influence in Joe Biden’s administration when it comes to Big Tech: Eric Schmidt or Elizabeth Warren? 

  
  
  
      




  The US early voting total in 2020 has already exceeded the number of early votes cast in 2016 — and there are still 11 more days to go until Election Day.
More than 51 million people have voted early, either in person or by mail, as of Friday morning, according to the US Elections Project, which is run by the University of Florida’s Michael McDonald.
That means at least 4 million more people have voted early so far this year, compared to all early voting in 2016. And the early vote totals in Texas (6.4 million votes), North Carolina (2.7 million votes), California (5.8 million votes) already exceed the total number of people who voted for Donald Trump in those same states in 2016. So far in 2020, the country has already cast 37 percent of the total votes counted in the 2016 general election.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Rani Molla/Vox
      
    
  


The numbers are unprecedented, though maybe not surprising. There has been an expected increase in the number of voters planning to cast their ballots by mail because of the Covid-19 concerns, as well as greater access to both mail and early in-person voting this year.
Still, the numbers are a sign — though an incomplete one — that voter turnout may be on pace to be the highest in a century. McDonald, of the US Elections Project, has previously predicted turnout to be around 65 percent of the voting-eligible population, or about 150 million voters. (Turnout was about 60 percent in 2016, at about 137 million people.) FiveThirtyEight is predicting turnout of about 154 million people, based on polls of voter enthusiasm and other data.
There is a record level of early voting, but also a lot we don’t know
Of the more than 51 million American voters who’ve already cast their ballots, about 35 million returned mail ballots, and about another 15 million have voted in person. These totals are imperfect, as not all states break out the two separately.
In 2016, about 1 in 5 voters cast mail-ballots, but given concerns around the Covid-19 pandemic, that number has been expected to double, especially as many states adjusted vote-by-mail rules this year to make it easier to send in a ballot.
But in the lead-up to the election, President Donald Trump’s false attacks on vote-by-mail helped generate a partisan divide around it, with Democrats being more likely to support it and choose that as a voting option, compared to Republicans. 

  
    Related
  

  
    What’s the same and what’s different about the polls this time
  

This is borne out in some of the early voting data available. So far, almost twice as many Democrats have voted early compared to Republicans, based on data from the 19 states that make party registration data available, compiled by the US Elections Project. Where state-level data is available, more than 10 million registered Democrats have returned mail ballots, compared to 4.5 million registered Republicans. But the in-person early vote count currently favors registered Republicans, but by the narrowest of margins — which as of Friday, was less than 15,000 votes in 10 states were that data was available. 
Overall, the early vote edge goes to Democrats — though it’s very hard to say what that might mean for the 2020 outcome at this point, as Republicans may be more inclined to wait to vote and to do so in person. Remember, early voting numbers also looked really good for Hillary Clinton in 2016. Party registration is not a totally accurate reflection of how someone votes, anyway, and there’s still a lot more voting to go. Another caveat: These are national numbers, and that includes highly populated states like California, which tend to skew blue.  
Ultimately, it all comes down to the battleground states for the presidential election. In Florida, 44 percent of total early votes come from registered Democrats compared to 35 percent of registered Republicans. The same trends nationally also played out there: Democrats returned many more mail ballots, so far, but Republicans actually have an edge with in-person early voting in the Sunshine State. In North Carolina, of total early votes cast, 41 percent were registered Democrats, compared to 29 percent of registered Republicans.
But these numbers are constantly changing, and, experts caution it’s premature to jump to conclusions about what early vote totals mean for the final 2020 outcome. And when it comes to turnout, one thing experts are watching for is if voting is front-loaded in 2020: Basically, a lot more people may be voting early because of the pandemic, or even just because of high voter enthusiasm, but that could begin to taper off, with turnout on Election Day itself a bit lower than normal. 
It may also be the case — as forecasters like Silver are predicting — that high early voter turnout will be mirrored by high turnout on November 3. According to a survey by Democracy Fund, about two-thirds of voters said they planned to vote early or by mail in 2020, which is an increase in the number of people who voted by mail (by about 20 percent) or in-person (also about 20 percent) in 2016. 
Even with that jump in early voting, about 34 percent of voters said in that same survey that they still planned to vote in person, on Election Day. So, America, there’s still a long way to go.


  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



What may be the clearest indication yet of foreign election interference came to light this week when people across the country reported receiving threatening emails ordering them to vote for President Trump. Though many emails appeared to come from a violent group of Trump supporters, the FBI and the Office of the Director of National Intelligence said in a Wednesday press conference that Iran was likely behind them. But they offered few details as to how they came to this conclusion, and Iran has denied any involvement.
This comes as voter intimidation tactics, foreign and domestic, are of growing concern. Russia is believed to have its own plans to disrupt the presidential election, according to the New York Times, including hacking into government computer systems. Trump has encouraged his followers to serve as poll watchers, and there are reports that his supporters are congregating just outside buffer zones around polling locations. All this comes on top of fears that disinformation spread across social media will proliferate in this election as it has in previous years.
What we know about the emails
On Monday and Tuesday, reports emerged that registered Democrats in several states, including but not limited to Alaska and Florida, received threatening emails ordering them to vote for Trump or “we will come after you.” Some of the emails contained the recipient’s personal information, and many came from an address that appeared to be linked to the Proud Boys, a far-right extremist group of so-called “western chauvinists.” The group recently received attention after Trump refused to disavow white supremacy during the first presidential debate and said, “Proud Boys, stand back and stand by.” (Trump later said he didn’t know anything about the Proud Boys and that they should “stand down.”)
Threats of violence against Democrats wouldn’t be completely out of character for the Proud Boys. According to reports, some members are currently engaged in a campaign to “watch” polling stations across America, which has caused concern that their presence will intimidate voters. Meanwhile, Proud Boys chairman Enrique Tarrio has denied having anything to do with the recent voter intimidation email campaign.
The emails do appear to be the work of someone else. The email address that appears as the sender, “info@officialproudboys.com,” was spoofed, and the emails themselves came from servers in Estonia, Saudi Arabia, and the United Arab Emirates. This doesn’t mean the emails came from those countries, just that they were routed through servers in those countries — in which case it would indicate that the senders were trying to obscure their true origin.
Indeed, intelligence officials now blame foreign actors for the campaign. FBI Director Christopher Wray and Director of National Intelligence John Ratcliffe announced on Wednesday that Iran and Russia had obtained voter registration information and would be using it to interfere with the upcoming presidential election.
They offered few details but appeared to reference the emails attributed to the Proud Boys, though not by name. Ratcliffe said Iran was sending “spoofed emails designed to intimidate voters” as well as videos that purported to show how to cast fraudulent ballots — which Ratcliffe said were untrue. He added, “This is not a partisan issue.”
It’s interesting that Ratcliffe, a Trump appointee, would step in so aggressively here when, per his claim, the alleged foreign interference was an attempt to “damage President Trump.” Since 2016, when election security and preventing foreign interference took on an even greater significance, Republicans have traditionally blocked most attempts to prevent it. Ratcliffe himself stopped doing election security briefings in August. Trump is an outspoken opponent of any claims that Russia interfered with the 2016 election and has said little about any Russian efforts to meddle in 2020. There have also been allegations that the Trump administration is downplaying the threat of Russian interference and exaggerating the threat from China and Iran, which are believed to favor Biden.
For whatever it’s worth, Iran has denied it interfered with or had any plans to interfere with the election, saying that “the highest level” of the country was already doing so with its “desperate public attempts to question the outcome of its own elections” — likely alluding to Trump’s frequent attempts to spread disinformation about mail-in voting.
There’s little cause for concern — for now
While it’s always alarming when a country is accused of voter intimidation and election interference, the little that is known about this attempt — Ratcliffe and Wray offered few details — points to an unsophisticated campaign. 
Spoofing email addresses is relatively easy and common, a technique often used by scammers in phishing campaigns. Voter registration information is publicly available; political campaigns, for example, routinely obtain and use such lists, and your details may be available online, depending on which state you live in. Whatever techniques those behind the campaign used to cover their tracks, it seems, couldn’t stand up to even two days’ of investigation from government agencies. Several news organizations quickly sussed out that the emails were likely not from the Proud Boys.
But that doesn’t mean the emails didn’t have their intended effect. A threat accompanied by identifying information and a home address would be an understandably scary email for anyone to receive, no matter how unlikely it is that the threat would be carried out.
At Wednesday’s briefing, Wray urged the American public to view disinformation about voting infrastructure “with a healthy dose of skepticism” and encouraged everyone “to seek election and voting information from reliable sources” — specifically, state election officials.
“We are standing before you now to give you the confidence that we are on top of this, and providing you with the most powerful weapon we have to combat these efforts: the truth, information,” Ratcliffe said. “We ask every American to do their part, to defend against those who wish us harm. The way you do that is quite simple: Do not allow these efforts to have their intended effect. If you receive an intimidating or manipulative email in your inbox, don’t be alarmed and do not spread it.”
It seems that, in 2020, voter disinformation and suppression won’t be limited to social media sites.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.


  
  
  
      




  Facebook’s much-anticipated independent decision-making body, the Facebook oversight board, announced it will start allowing people to submit cases for review beginning today. 
That means that if you post something on Facebook or Instagram and it’s taken down for violating any of Facebook’s ever-changing rules on things like hate speech, nudity, misinformation, or violence — you will soon have the ability to appeal that decision to someone besides Facebook. For now, that option will roll out in waves, and in the next few weeks, Facebook says it’ll be an option for all users. 
Social media experts have long awaited the Board’s launch because it’s expected to serve as the final decision-maker in how Facebook handles complicated and problematic posts, which have plagued the social media company. Look, for example, at how it managed the unsubstantiated New York Post article about Hunter Biden or any of the countless times Facebook has been accused of letting racist hate speech run rampant on its platform. The Board said it will prioritize cases that threaten to harm freedom of expression or human rights, but declined to comment on specific cases it plans to take. 
Facebook’s oversight board is made up of a group of 20 academics, journalists, and international policy experts from around the world, and is set up as a separate company from Facebook, funded by a $130 million independent trust. Its decisions on individual pieces of content are binding, meaning Facebook has agreed to follow whatever decisions the Board makes, and the group can also make broader policy recommendations to Facebook — although those won’t be binding. That means the board has the power to overrule even Facebook CEO Mark Zuckerberg, who has a history of taking stubborn stances in the name of protecting free expression. Zuckerberg allowed President Trump’s “when the looting starts, the shooting starts” post in response to Black Lives Matter protests in Minneapolis, and, until recently, allowed Holocaust denialism on Facebook — even when some of his own employees, civil rights leaders, and others have raised serious concerns. 
“The Board is eager to get to work,” said Catalina Botero Marino, co-chair of the oversight board, in a press statement on Thursday. “We won’t be able to hear every appeal, but want our decisions to have the widest possible value, and will be prioritizing cases that have the potential to impact many users around the world, are of critical importance to public discourse, and raise questions about Facebook’s policies.” 
At a time when Facebook is being criticized by US politicians on both sides of the aisle for how it handles contentious speech on its platform, the Board is meant as an outside check on Facebook’s power. Some, though, have criticized the Board, saying it was too slow in getting started (Facebook CEO Mark Zuckerberg first publicly described the idea two years ago) and too narrow in scope to meaningfully change how Facebook handles hate speech and misinformation. For example, for now, users will only be able to appeal cases where they feel their content is wrongfully taken down, not cases in which they think inflammatory content is wrongfully staying up on the platform (the Board says that latter option will come in the next few months). 
“Facebook was always criticized for moving fast and breaking things. I think we are looking at this as the opposite that,” said oversight board co-chair and former Danish Prime Minister Helle Thorning-Schmidt on a press call with reporters on Thursday.
Critics point out that the oversight board seems unlikely to help Facebook deal with one of the most controversial content moderation challenges it has faced to date: the 2020 US presidential election.
President Trump has been making unsupported assertions on Facebook and Twitter for months now that the election is “rigged,” centering on false claims about mail-in voting — which Facebook has labelled with a generic link to nonpartisan voting information, and Twitter has more aggressively — at times — labeled as “misleading” and fact-checked.
Many anticipate that Trump — or other politicians — could question the results of the election or declare a premature victory on social media before the race is called. In that case, it would be up to Facebook or Twitter to decide how to deal with such a declaration. (Facebook and Twitter have signaled they would fact-check and label such a post or even take it down, depending on what it says.) Whatever decision these companies make will be widely controversial. 
But it seems unlikely the Board will take any cases in time to impact election-night posts or regulate misinformation in the remaining days until the election.
That’s because it will take up to 90 days for the Board to decide on a case — and that’s after the Board even figures out which cases it wants to hear first. Facebook the company can submit a case to the Board for expedited review, but on a press call with journalists on Thursday morning, the company said it will not send any cases to the Board before November 3. 
“We are not going to send something for expedited review before the election,” said head of strategic initiatives at Facebook Brent Harris. “And we have done that because we do not wish to place undue pressure on the board.” 
Last month, a group of 25 experts from academia, civil rights, politics, and journalism announced they were creating an ad-hoc group to scrutinize Facebook’s oversight board, calling themselves “The Real Facebook Oversight Board.” 


Congratulations to Facebook's Oversight Board (@OversightBoard) which opens for business today! We wish you the best of luck. (You'll need it!) But seriously, we consider you our friends & allies & we need all possible efforts in holding Facebook to account... pic.twitter.com/BahanEDGrG— The Real Facebook Oversight Board (@FBoversight) October 22, 2020



Facebook oversight board’s Thorning-Schmidt said she welcomes the feedback.
“We welcome all debate on this,” she said. “Part of the reason why we have joined this course is because we want to debate around content moderation.” 

  
  
  
      





  Quibi was a bad idea, poorly executed. Now it’s dead, just six months after it debuted.
Here’s a quick timeline of its short life:

Hollywood legend Jeffrey Katzenberg raised $1.75 billion to launch a service that promised to deliver subscribers “quick bites” of “premium video” to their phones. Even though there was no shortage of video, premium or otherwise, that people could watch on their phones or any other devices.

Quibi launched in April, during the first swell of the pandemic. And even though every other video service saw a huge leap in engagement during the shutdown, Quibi couldn’t find an audience — let alone paying customers. 
Last month, Katzenberg started trying to sell the company. Today, he’s shutting it down.

It was easy to be skeptical about Quibi before launch because ... see above. The real surprise is that it failed so quickly. And even that surprise is a little bit couched. Once news got out that Katzenberg was trying to sell, the only question was whether he’d find a buyer or have to shutter. As I wrote last month, you don’t try to sell your startup five months after launch if things aren’t going terribly, even though Katzenberg insisted otherwise in sales pitches.
But, that said: I would like to see more Quibis in the future. 
Not the concept or the execution (again, see above) but the model: Running a media business the old-fashioned way, where you ask people to make something, pay them for it, and then try to re-sell that work to someone else. Because there’s another version of running a media business — what YouTube, Twitter, and Facebook do — and I don’t feel great about that one in 2020. 
To recap: Katzenberg and Meg Whitman, the CEO he hired away from Hewlett Packard, paid Hollywood studios, TV networks, and digital shops like Vox Media (which owns this site) to make short videos. Then they tried selling subscriptions to those videos to you.
That’s one way — the old way — to run a media business. 
There are lots of variants, and you can debate the right way to scale those companies and how much money you need to make them work, etc. The model includes everything from your local newspaper (if it still exists) to TV networks to Spotify to Netflix. But they’re all using the same basic playbook. 
There is also the new — and often much more successful way — to run a media business: Get people to give you stuff for free, get people to consume that stuff for free, and sell their attention to advertisers. You may not want to call yourself a media business — for strategic, valuation, or legal reasons — but you are most definitely in the media business. This has worked really, really well for YouTube, Twitter, and Facebook.
But as we spend a lot of time discussing these days, it’s not clear that the model that YouTube, Twitter, and Facebook use — which is dependent on ingesting as much free content as possible, and distributing as widely and quickly as possible, with as little input from the people who run those businesses as possible — is good for the rest of us.
And at the core of all the proposals to fix those businesses is the idea that they should act a lot more like ... traditional media businesses. These proposals call for the people who run these platforms to pay attention to what they distribute, and even make judgment calls about whether that stuff should be distributed. And, yes: It also involves paying people who make some of the stuff they distribute.
I don’t want to belabor this thought, and I don’t want to oversell it. Quibi would have likely struggled using any model because it didn’t have stuff people wanted to see, and it didn’t have the distribution it needed to get it in front of them, anyway. 
And while the Facebooks of the world run on free content, they certainly have to spend money on lots of other stuff. TikTok, for example, spent $1 billion on marketing in a single year in order to get its free videos, uploaded for free by its users, in front of people around the world.
But if you’re going to dunk on Quibi for failing so big, so fast, at least give them this: They failed the old-fashioned way. Which still has an upside.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



The Trump administration is once again trying to force social media platforms to do its bidding. This time, the Federal Communications Commission (FCC) has been tapped to use a law called Section 230 to prevent websites from moderating content in a way that many conservatives believe is biased against them. Despite the law being designed to prevent FCC intervention — and the FCC itself using that as justification not to regulate the internet just a few years ago — it appears the agency is going to try. 
This comes after Trump and many conservatives have called for Twitter and Facebook to be punished after the platforms suppressed links to the New York Post’s questionably sourced story about Hunter Biden. This prompted another all-caps demand from Trump to repeal Section 230 and the Republican-led Senate to prepare to subpoena Twitter CEO Jack Dorsey, accusing the company of election interference.
The very next day, FCC Chairman Ajit Pai announced that his agency would “move forward with a rulemaking to clarify” the meaning of Section 230, which gives internet platforms like Facebook and Twitter immunity from lawsuits over content their users provide. That is to say, if someone defames you in a tweet, you can sue the Twitter user but not Twitter itself. This 25-year-old law is what allows websites that rely on third-party content to exist at all. It also allows those sites to moderate that content as they see fit, which has been a source of ire for conservatives who believe they are being censored when Facebook bans them, YouTube demonetizes them, or Twitter appends fact-checks to their tweets.
Trump has been particularly upset about this in recent months, as platforms have cracked down on the misinformation he spreads. In May, he went so far as to issue an executive order calling for the FCC to come up with rules that would prevent websites from moderating content based on a perceived anti-conservative bias, which is the basis for the FCC’s actions now.
The FCC isn’t the thought police
But legal experts — former FCC commissioners and staff among them — don’t think the FCC is allowed to regulate the internet in this way.
“I don’t think the FCC has the authority to be thought police over platforms,” former FCC Chairman Tom Wheeler, Pai’s predecessor and no fan of Section 230 himself, told Recode. 
Wheeler added: “The Trump administration practices government by performance. They come out and they beat their chests and they say we’re gonna do this on 230, and we’re gonna do that on the digital divide. But it’s chest-pounding, not policy. There’s a difference between showbiz and substance.”
Pai has claimed that the FCC’s general counsel, Thomas M. Johnson Jr., told him that the FCC has the legal authority to interpret Section 230. When asked to elaborate on where that legal authority comes from, FCC spokesperson Brian Hart told Recode, “We don’t have anything to add at this point.”
Johnson later issued a statement citing Section 201(b) of the Communications Act, which says that the FCC may “prescribe such rules and regulations as may be necessary in the public interest to carry out this Act.” Many have argued that 201(b) applies only to common carriers. And the section does, in fact, fall under Title II of the Communications Act, which is titled “Common Carriers” — defined as entities that provide “telecommunications services,” like phone companies.
But Johnson’s interpretation of 201(b) is that it should apply to everything covered by the Act and that this includes social media companies. This is despite the fact that the FCC said in 2017 that it was “misguided and legally flawed” to classify broadband internet as a telecommunications service under Title II. 
“201(b) is inside Title II of the Communications Act, and Pai has gone out of his way to say that ISPs are not subject to Title II,” Wheeler said. “If the ISPs are not subject to Title II, how in the world can you make the stretch that those that transmit over the ISPs are subject to Title II?”
Harold Feld, senior vice president at open internet advocacy group Public Knowledge, said in 2019 — when an executive order commanding the FCC to regulate Section 230 was just a rumor — that this was both a “bad idea” and that he couldn’t see any way that the FCC had the authority to do it. 
“The FCC cannot rewrite acts of Congress to suit its whims,” Kate Ruane, senior legislative counsel at the American Civil Liberties Union, said in a statement. “Section 230 is critical to protecting free speech online and the FCC has no authority to change it, especially not in ways that will undermine free expression.”
Section 230 was designed to prevent FCC interference
“There’s nothing in Section 230 of the Communications Decency Act that gives the FCC authority either to interpret it or, even more importantly, set rules,” said Gigi Sohn, a distinguished fellow at the Georgetown Institute for Technology & Law Policy who was counselor to Wheeler from 2013 to 2016. “In fact, the legislative history is completely to the contrary.” 
The law’s bipartisan co-authors, Sen. Ron Wyden and former Rep. Chris Cox, have said they intentionally wrote the law to prevent the FCC from having this authority in the first place. 


The FCC does not have the authority to rewrite the law, and Ajit Pai can't appoint himself commissioner of the speech police. Read my comments with former-Rep. Cox on why this process is so deeply flawed. https://t.co/7JxSSKNmRR— Ron Wyden (@RonWyden) October 15, 2020



Back in 1995, when the Communications Decency Act was being considered, there was some debate over what the FCC’s role in regulating the internet should be. Wyden and Cox thought FCC oversight would be a barrier to internet innovation and development. 
As Cox said in the House at the time, Section 230 “will establish as the policy of the United States that we do not wish to have content regulation by the federal government of what is on the internet — that we do not wish to have a ‘Federal Computer Commission’ with an army of bureaucrats regulating the internet.”
He continued: “If we regulate the internet at the FCC, that will freeze or at least slow down technology. It will threaten the future of the internet.”
That’s a vision that Pai himself agreed with back in 2017, when the FCC — under his chairmanship — repealed net neutrality, citing Section 230’s provision that the United States’ policy is not to interfere with the growth of the internet with federal or state regulation as justification for taking a “light-touch approach” to “burdensome regulation that stifles innovation and deters investment.”
Because of this, Sohn said, the FCC would essentially have to reverse its own decision — one that has become emblematic of the FCC’s anti-regulatory approach under Pai and the Trump administration — in order to make the case that it has any authority over Section 230 at all. 
To this argument, Johnson said in his statement that “none of these observations bear on the central question” of whether the FCC has the authority to interpret Section 230, and that doing so would not be regulation but “clarifying a legal standard.” Johnson admitted that, while the FCC once used Section 230 to guide its actions when repealing net neutrality, it “need not rely” on it here and can instead use 201(b) as he interprets it. Put simply, Johnson believes that the FCC can pick and choose which statute to apply and which to ignore depending on what it hopes to accomplish.
What happens next
If the FCC does decide to try to make rules for Section 230, it will need the majority of the five-person commission to agree on them. Pai is clearly in favor, and fellow commissioner Brendan Carr is, too. Democratic commissioners Geoffrey Starks and Jessica Rosenworcel are not. That leaves the fifth commissioner as the deciding vote. Right now, that’s Republican Michael O’Rielly, who has signaled that he is not in favor of regulating Section 230 in this way. But it could soon be Nathan Simington, who Trump nominated last month to replace O’Rielly. And Simington appears to be in favor of regulating Section 230 through the FCC, which is likely part of why Trump nominated him in the first place.
After that, it will probably take several weeks or months to issue those rules. It’s already taken nearly five months between Trump’s executive order calling for its action for the FCC to get to this point. Depending on how the election goes, Trump may no longer be in office and the Democrats could control both branches of the legislature, in which case executive orders and FCC interpretations under the Trump administration will almost certainly come to an end. 
But let’s say that Trump does win the election — what then? There’s still no guarantee that what Trump wants to happen will happen, or that it would happen anytime soon. Congress can overturn FCC rules, and it likely would if it has a Democratic majority in both houses. While Democrats have their own problems with Section 230, those have mostly focused on eliminating immunity protections for websites that feature child sex trafficking and child sexual abuse images. Changing Section 230’s content moderation policies has become a partisan issue — Republicans are the ones writing bill after bill opposing Section 230 and decrying perceived censorship on social media platforms — and that makes it much less likely that Democrats will pick up the Republicans’ cause.
If Congress doesn’t reject the FCC’s rules, then it’ll be up to the courts, which has become the norm for an administration that refuses to accede to laws until it absolutely has to. A number of organizations already have sued the Trump administration over the executive order, citing First Amendment and regulatory policy violations. And that litigation will likely come with injunctions that prevent any regulations from taking effect until the courts can rule on them.
“This is going to be tied up in litigation forever and a day,” Sohn said.
Courts have typically ruled in Section 230’s favor, but there’s at least one judge who seems to feel differently: Supreme Court Justice Clarence Thomas recently said that he thinks courts have gotten Section 230 wrong, and the protection from lawsuits its provides has been granted too liberally. But he said this in the court’s denial to hear a case about Section 230, which indicates that the majority of justices aren’t interested in reconsidering Section 230 right now.
So how likely is it that the FCC will be the one to make Trump’s dreams of an internet that doesn’t put fact-checks on his tweets come true? Not very, and certainly not anytime soon. But the administration has already gotten its way just by threatening to do so: Twitter changed its rules a few hours after Pai issued his statement. The next day, it allowed the Post story on the platform.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



The Trump administration is once again trying to force social media platforms to do its bidding. This time, the Federal Communications Commission (FCC) has been tapped to use a law called Section 230 to prevent websites from moderating content in a way that many conservatives believe is biased against them. Despite the law being designed to prevent FCC intervention — and the FCC itself using that as justification not to regulate the internet just a few years ago — it appears the agency is going to try. 
This comes after Trump and many conservatives have called for Twitter and Facebook to be punished after the platforms suppressed links to the New York Post’s questionably sourced story about Hunter Biden. This prompted another all-caps demand from Trump to repeal Section 230 and the Republican-led Senate to prepare to subpoena Twitter CEO Jack Dorsey, accusing the company of election interference.
The very next day, FCC Chairman Ajit Pai announced that his agency would “move forward with a rulemaking to clarify” the meaning of Section 230, which gives internet platforms like Facebook and Twitter immunity from lawsuits over content their users provide. That is to say, if someone defames you in a tweet, you can sue the Twitter user but not Twitter itself. This 25-year-old law is what allows websites that rely on third-party content to exist at all. It also allows those sites to moderate that content as they see fit, which has been a source of ire for conservatives who believe they are being censored when Facebook bans them, YouTube demonetizes them, or Twitter appends fact-checks to their tweets.
Trump has been particularly upset about this in recent months, as platforms have cracked down on the misinformation he spreads. In May, he went so far as to issue an executive order calling for the FCC to come up with rules that would prevent websites from moderating content based on a perceived anti-conservative bias, which is the basis for the FCC’s actions now.
The FCC isn’t the thought police
But legal experts — former FCC commissioners and staff among them — don’t think the FCC is allowed to regulate the internet in this way.
“I don’t think the FCC has the authority to be thought police over platforms,” former FCC Chairman Tom Wheeler, Pai’s predecessor and no fan of Section 230 himself, told Recode. 
Wheeler added: “The Trump administration practices government by performance. They come out and they beat their chests and they say we’re gonna do this on 230, and we’re gonna do that on the digital divide. But it’s chest-pounding, not policy. There’s a difference between showbiz and substance.”
Pai has claimed that the FCC’s general counsel, Thomas M. Johnson Jr., told him that the FCC has the legal authority to interpret Section 230. When asked to elaborate on where that legal authority comes from, FCC spokesperson Brian Hart told Recode, “We don’t have anything to add at this point.”
Johnson later issued a statement citing Section 201(b) of the Communications Act, which says that the FCC may “prescribe such rules and regulations as may be necessary in the public interest to carry out this Act.” Many have argued that 201(b) applies only to common carriers. And the section does, in fact, fall under Title II of the Communications Act, which is titled “Common Carriers” — defined as entities that provide “telecommunications services,” like phone companies.
But Johnson’s interpretation of 201(b) is that it should apply to everything covered by the Act and that this includes social media companies. This is despite the fact that the FCC said in 2017 that it was “misguided and legally flawed” to classify broadband internet as a telecommunications service under Title II. 
“201(b) is inside Title II of the Communications Act, and Pai has gone out of his way to say that ISPs are not subject to Title II,” Wheeler said. “If the ISPs are not subject to Title II, how in the world can you make the stretch that those that transmit over the ISPs are subject to Title II?”
Harold Feld, senior vice president at open internet advocacy group Public Knowledge, said in 2019 — when an executive order commanding the FCC to regulate Section 230 was just a rumor — that this was both a “bad idea” and that he couldn’t see any way that the FCC had the authority to do it. 
“The FCC cannot rewrite acts of Congress to suit its whims,” Kate Ruane, senior legislative counsel at the American Civil Liberties Union, said in a statement. “Section 230 is critical to protecting free speech online and the FCC has no authority to change it, especially not in ways that will undermine free expression.”
Section 230 was designed to prevent FCC interference
“There’s nothing in Section 230 of the Communications Decency Act that gives the FCC authority either to interpret it or, even more importantly, set rules,” said Gigi Sohn, a distinguished fellow at the Georgetown Institute for Technology & Law Policy who was counselor to Wheeler from 2013 to 2016. “In fact, the legislative history is completely to the contrary.” 
The law’s bipartisan co-authors, Sen. Ron Wyden and former Rep. Chris Cox, have said they intentionally wrote the law to prevent the FCC from having this authority in the first place. 


The FCC does not have the authority to rewrite the law, and Ajit Pai can't appoint himself commissioner of the speech police. Read my comments with former-Rep. Cox on why this process is so deeply flawed. https://t.co/7JxSSKNmRR— Ron Wyden (@RonWyden) October 15, 2020



Back in 1995, when the Communications Decency Act was being considered, there was some debate over what the FCC’s role in regulating the internet should be. Wyden and Cox thought FCC oversight would be a barrier to internet innovation and development. 
As Cox said in the House at the time, Section 230 “will establish as the policy of the United States that we do not wish to have content regulation by the federal government of what is on the internet — that we do not wish to have a ‘Federal Computer Commission’ with an army of bureaucrats regulating the internet.”
He continued: “If we regulate the internet at the FCC, that will freeze or at least slow down technology. It will threaten the future of the internet.”
That’s a vision that Pai himself agreed with back in 2017, when the FCC — under his chairmanship — repealed net neutrality, citing Section 230’s provision that the United States’ policy is not to interfere with the growth of the internet with federal or state regulation as justification for taking a “light-touch approach” to “burdensome regulation that stifles innovation and deters investment.”
Because of this, Sohn said, the FCC would essentially have to reverse its own decision — one that has become emblematic of the FCC’s anti-regulatory approach under Pai and the Trump administration — in order to make the case that it has any authority over Section 230 at all. 
To this argument, Johnson said in his statement that “none of these observations bear on the central question” of whether the FCC has the authority to interpret Section 230, and that doing so would not be regulation but “clarifying a legal standard.” Johnson admitted that, while the FCC once used Section 230 to guide its actions when repealing net neutrality, it “need not rely” on it here and can instead use 201(b) as he interprets it. Put simply, Johnson believes that the FCC can pick and choose which statute to apply and which to ignore depending on what it hopes to accomplish.
What happens next
If the FCC does decide to try to make rules for Section 230, it will need the majority of the five-person commission to agree on them. Pai is clearly in favor, and fellow commissioner Brendan Carr is, too. Democratic commissioners Geoffrey Starks and Jessica Rosenworcel are not. That leaves the fifth commissioner as the deciding vote. Right now, that’s Republican Michael O’Rielly, who has signaled that he is not in favor of regulating Section 230 in this way. But it could soon be Nathan Simington, who Trump nominated last month to replace O’Rielly. And Simington appears to be in favor of regulating Section 230 through the FCC, which is likely part of why Trump nominated him in the first place.
After that, it will probably take several weeks or months to issue those rules. It’s already taken nearly five months between Trump’s executive order calling for its action for the FCC to get to this point. Depending on how the election goes, Trump may no longer be in office and the Democrats could control both branches of the legislature, in which case executive orders and FCC interpretations under the Trump administration will almost certainly come to an end. 
But let’s say that Trump does win the election — what then? There’s still no guarantee that what Trump wants to happen will happen, or that it would happen anytime soon. Congress can overturn FCC rules, and it likely would if it has a Democratic majority in both houses. While Democrats have their own problems with Section 230, those have mostly focused on eliminating immunity protections for websites that feature child sex trafficking and child sexual abuse images. Changing Section 230’s content moderation policies has become a partisan issue — Republicans are the ones writing bill after bill opposing Section 230 and decrying perceived censorship on social media platforms — and that makes it much less likely that Democrats will pick up the Republicans’ cause.
If Congress doesn’t reject the FCC’s rules, then it’ll be up to the courts, which has become the norm for an administration that refuses to accede to laws until it absolutely has to. A number of organizations already have sued the Trump administration over the executive order, citing First Amendment and regulatory policy violations. And that litigation will likely come with injunctions that prevent any regulations from taking effect until the courts can rule on them.
“This is going to be tied up in litigation forever and a day,” Sohn said.
Courts have typically ruled in Section 230’s favor, but there’s at least one judge who seems to feel differently: Supreme Court Justice Clarence Thomas recently said that he thinks courts have gotten Section 230 wrong, and the protection from lawsuits its provides has been granted too liberally. But he said this in the court’s denial to hear a case about Section 230, which indicates that the majority of justices aren’t interested in reconsidering Section 230 right now.
So how likely is it that the FCC will be the one to make Trump’s dreams of an internet that doesn’t put fact-checks on his tweets come true? Not very, and certainly not anytime soon. But the administration has already gotten its way just by threatening to do so: Twitter changed its rules a few hours after Pai issued his statement. The next day, it allowed the Post story on the platform.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Data privacy laws are still a work in progress, but one major improvement is coming: Global Privacy Control, which — assuming everything works out — will let you automatically opt out of having your data sold or shared at every website you visit. For now, it doesn’t do much, but it is available if you want to add it to your browser. If nothing else, the recent launch of the new specification is a great opportunity to check out your browser’s privacy options — and your browser options in general.  
Trackers hidden on the vast majority of websites collect as much information about us as possible and try to link that data to our actions online as well as off, typically to send us targeted ads. The idea behind Global Privacy Control would be to place a setting on your browser that tells every site you visit that you don’t want your data to be sold or shared with anyone else, and websites would have to respect your wishes. While some browsers have built-in tools (or available extensions) meant to stop tracking in the first place, they aren’t always effective, and they can’t do anything once your data is collected. And while laws like the California Consumer Privacy Act (CCPA) give users the right to request that businesses not sell their data, those users have to make that request of every site they visit, a process that is too time-consuming for most people. With Global Privacy Control, that request would be automatic, relayed as soon as you visit the site, and, if you’re in a location where it’s legally required — like California — websites would have to abide by your request.
If a browser extension that tells websites your privacy preferences sounds familiar, that’s because something like this has been tried before. Do Not Track, introduced in 2010, was an attempt by the Federal Trade Commission (FTC) to institute a sort of digital equivalent to the Do Not Call list: a browser extension or setting that tells websites you visit that you don’t want to be tracked. The problem with Do Not Track was that websites weren’t legally required to comply with it, so very few of them did.
Ashkan Soltani was part of the Do Not Track effort back in 2010 as a staff technologist at the FTC and has spent most of his career researching and investigating internet privacy and tracking. He’s the co-author of CCPA and an upcoming ballot measure in California called Proposition 24 that would amend it. Unsurprisingly, then, he’s also behind Global Privacy Control. 
Soltani told Recode that he’s pretty optimistic that Global Privacy Control will be able to do what Do Not Track couldn’t. CCPA includes a provision for browser “global privacy controls” regarding data selling and sharing, and a requirement that websites follow them. Do Not Track couldn’t be used for this because “track” means more than just the sale or sharing of data; it’s too broad. Global Privacy Control, however, is more specific and limited to what the law requires. 
“It would have been ideal if the [California attorney general] had adopted Do Not Track as the mechanism, but unfortunately their opinion was that couldn’t be used,” Soltani told Recode. “Hence, Global Privacy Control.” 
The trick now is getting California to approve Global Privacy Control as the global privacy control called for in the law, at which point websites will be legally bound to follow it. In the meantime, a few websites have already agreed to do so voluntarily, including the New York Times and the Washington Post. 
Global Privacy Control may not do much now, but if you want to get it for yourself, it’s available on DuckDuckGo’s mobile browser now and in the process of being added to the Brave’s browser. Or you can get it through the Privacy Badger extension available for most browsers. But, as Wired points out, it could be several years before Global Privacy Control fully goes into effect, and there’s still no guarantee that it will. 
In the meantime, why not take advantage of the web browser privacy options you do have? Some are better than others, as you’ll see, and even the best browser from a privacy standpoint has its downsides. And you should never assume that your web browsing is 100 percent private because data companies come up with new ways to follow you around the internet all the time. With that in mind, here’s a rundown on what’s out there.
Google Chrome: Popular but not very private 
The most popular browser by far is Google’s Chrome, so it’s likely what you’re using to read this article right now. But it’s not the most private. In fact, it’s widely considered to be one of the worst. And no, Incognito Mode will not save you. The fact is, Google isn’t all that inclined to limit tracking on its services: The company has a massive ad business, part of which relies on the data it collects from users, and that data includes what those users do on its browser, including data acquired through the many trackers Google puts on websites. And, if you have a Google account and stay logged in while using Chrome, that will be linked to your account on Google’s other platforms, like Gmail and YouTube. 
Having all of your accounts linked to one company in this way might be part of Chrome’s appeal for you and worth whatever privacy trade-offs you have to make. If you want to keep using Chrome, there are a few things you can do to reduce tracking. Chrome’s privacy settings let you block third-party cookies, and, in the settings for your Google account, you can turn off ad personalization and use activity controls to turn off things like web activity tracking and location history. You can also add browser extensions like Privacy Badger, DuckDuckGo, and Ghostery that block trackers. But why add a bunch of extensions when you can get a browser that already does the job?
Microsoft Edge: The new and improved Internet Explorer?
Microsoft used to dominate the browser market with its problematic Internet Explorer. That’s still around, but it’s quickly being replaced by the company’s recently revamped Edge browser, and Microsoft’s web browser market share is growing. Your default privacy settings block some trackers, and ad personalization should be turned off by default. You can log into your Microsoft account to link Edge with whatever other Microsoft platforms you use, but if the goal here is privacy, that might not be the best idea. If you must, Microsoft does have a privacy dashboard that you can use to control privacy settings across your account, including turning off personalized ads.
A recent study said Edge was one of the worst browsers for privacy — worse than Chrome, even — because it sent an identifier back to Microsoft’s servers. Because the identifier was linked to the device’s hardware, it couldn’t be changed or reset. That should no longer be the case, but it’s something to keep in mind.
Safari: Only for Apple and rather private
Safari is Apple’s native browser, which means it’s also only available for Apple devices (a Windows version is no longer supported). Privacy has long been a selling point for Apple, and it is for Safari as well. Many privacy protections, like blocking third-party cookies, are on by default. Safari also limits the amount of data collected from you and stops trackers from following your activity around the internet, and you can easily find out which trackers are trying to follow you across websites (just click on the little shield icon in the toolbar). It’s always enlightening to give privacy reports like that a read, just to get a sense of who is tracking you, how often, and where.
Firefox: A browser not built by a major tech company
Firefox comes from Mozilla, another company that has made privacy part of its business model — it’s actually part of Mozilla’s manifesto and its nonprofit foundation’s new “Unfck the Internet” campaign — and which continues to roll out improved privacy protections to keep up with the evolving tracking technology ecosystem. It blocks trackers, third-party cookies are off by default, and Mozilla is working on ways to block fingerprinting, which can track you even if you have cookies blocked. 
Browsers built for privacy 
Even more private is Brave, which was built specifically to be a private browsing experience as well as a faster one: it blocks ads by default, along with other trackers. Brave also lets you use Tor in its “private window” feature — more on Tor later. There’s also DuckDuckGo, which is best known as a privacy-first search engine but now offers a mobile browser. Ghostery, which began life as a browser extension that blocked trackers, has also gotten into the mobile browser game.
And then there’s Tor
Tor is as private as it gets. Your traffic is encrypted and routed through several nodes before it reaches its final destination, so not even your ISP will know where you go. And you’re in private (or incognito) mode by default, which means all cookies and site data from your session are deleted as soon as you close the browser. That’s why Tor is known as the browser of choice for people who want to do illegal things on the dark web. But privacy is for everyone, not just criminals. The big trade-off with Tor is that all that encrypted routing means pages take longer to load, and some sites block traffic from the Tor network entirely. 
If you’ve never thought much about any of this before, Tor might seem like a pretty extreme step to take. Fortunately, there are other options out there that will improve your privacy without sacrificing your web browsing experience. And while we wait for privacy laws and tools like Global Privacy Control to become available, they’re a good way to keep your data out of someone else’s hands.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  After a 14-month investigation, the United States government filed a landmark lawsuit against Google on Tuesday, arguing that the search giant used unfair practices to preserve its search and search advertising monopoly.
The Department of Justice and 11 states filed the lawsuit against Google in a federal court, accusing Google of using money it makes from its dominant position in search to pay other companies to help maintain its lead and block out competitors. Google pays Apple billions each year to be the default browser on Safari, for example, and search comes preloaded on devices using Google’s Android operating system.
“Two decades ago, Google became the darling of Silicon Valley as a scrappy startup with an innovative way to search the emerging internet,” states the suit. “That Google is long gone. The Google of today is a monopoly gatekeeper for the internet.” 
In a press briefing, Justice Department officials said the government is stepping in to protect access to a free market for customers and Google’s competitors. They argue that Google has illegally maintained its monopoly through exclusive business deals that put its own search and browser on phones and keep out competitors.
“If the government does not enforce the antitrust laws to enable competition, we could lose the next wave of innovation,” Justice Department spokesperson Marc Raimondi said in a press briefing. “If that happens, Americans may never get to see the next Google.”
The case argues that Google’s anticompetitive practices are harming three key groups: American consumers, who “are forced to accept” its often-controversial privacy practices; advertisers, who have to pay a “toll” to Google to reach their customers; and competing tech companies, who “cannot emerge from Google’s long shadow.”
The announcement unveils the biggest antitrust case against a tech company since the Microsoft antitrust case in 1998. “Google’s practices are anticompetitive under long-established antitrust law,” the new complaint reads. The DOJ likened the situation to Microsoft, which made its internet browser the default on Windows operating systems and made it impossible to delete.
The Justice Department’s suit poses a potential existential threat to Google’s business if it results in breaking off Google’s search engine — which accounted for about $21 billion last quarter, or more than half of its total revenue — from its other lines of business, such as cloud computing and video. 
Google rebutted the basis of the lawsuit, calling it a “dubious complaint” and arguing that consumers can easily use other products besides its own.
“Today’s lawsuit by the Department of Justice is deeply flawed,” a Google spokesperson said in a statement. “People use Google because they choose to — not because they’re forced to or because they can’t find alternatives.”
The suit was filed amid heavy political tension between major tech companies and the US government, with Attorney General Bill Barr reportedly speeding up the timing of the lawsuit so that it would be filed before the presidential election in November.
Here’s a breakdown of the case, its political consequences, and the complicated path ahead for Google and the DOJ.
The case against Google
The DOJ considers Google search to be a monopoly in the US, where nearly 90 percent of internet searches are through Google. The complaint says that Google is illegally trying to maintain its dominance through anticompetitive practices. 
In its lawsuit, the Justice department claims that Google has used exclusive business contracts to limit rival companies’ ability to put their products on Google’s Android mobile devices, and incentivizes device manufacturers like Apple and carriers like Verizon to use Google search instead of other search engines.  
The suit argues these practices violate the century-old Sherman Antitrust Act, which outlaws companies from “every contract, combination, or conspiracy” to monopolize.
And the suit says Google uses its profits from its massive hold on the search industry to maintain that grip by paying companies like Apple, LG, and AT&T to make it the default search engine on their devices, thus making it harder for potential rivals to compete.
“For many years, Google has used anti competitive tactics to maintain and extend its monopolies in the markets for general search services, search advertising, and general search text advertising—the cornerstones of its empire,” the report reads.
Google said its contracts are similar to how other companies promote their products.
“[L]ike countless other businesses, we pay to promote our services, just like a cereal brand might pay a supermarket to stock its products at the end of a row or on a shelf at eye level,” the company wrote in a response to the DOJ complaint.
The DOJ said that Google’s contracts help Google maintain its search monopoly because its scale contributes to its effectiveness: The more user data it has, the better its search results are. Additionally, the more people use Google search, the more advertisers will pay Google to reach them. 
“Google deprives rivals of the quality, reach, and financial position necessary to mount any meaningful competition to Google’s longstanding monopolies,” the report reads. “By foreclosing competition from rivals, Google harms consumers and advertisers.”
The DOJ complaint is also notable because rather than focusing on how Google’s monopoly could raise prices, it focuses on how lack of competition could lower quality, according to Thomas Campbell, former director of the FTC’s antitrust arm and a professor of antitrust law at Chapman University. 
“Normally in antitrust cases the argument is that because of exclusionary conduct, a market is monopolized and the price to the consumer is higher,” Campbell told Recode. In this case, “the main point is the benefit of having a search engine that protects your privacy is lost.” 
The lawsuit is more narrow than a report earlier this month from Congress’s House Judiciary Subcommittee on Antitrust, which also discussed how Google allegedly prioritizes its own search results over competing search platforms, like restaurant reviews on Yelp or flight queries on Expedia. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Rani Molla/Vox
      
    
  


The DOJ case also limits its focus to search, rather than discussing other industries where Google is dominant, including online advertising, smartphone operating systems, and web browsers. Critics say it uses that dominance in each to reinforce its other business lines.
Democratic state attorneys general may address these other issues in future lawsuits. The DOJ could also expand the scope of its lawsuit as the case proceeds. 
The complicated bipartisan politics behind Big Tech regulation
The lawsuit against Google comes at a time when there’s unprecedented public and political opposition to the financial and political power of major tech companies. Lawmakers on both sides of the aisle want to regulate Big Tech, though they disagree on why and how to regulate these giants. 
President Trump, as well as many Republican and Democrat lawmakers, have argued with increasing urgency that major tech companies like Google have amassed far too much market power. They say the companies stifle competition and leave consumers with no choice but to use their services when they go online.
This is a departure from the decades-long prevailing legal attitude in the US government. Historically, the idea was that to break up a company, you have to prove not just that it’s a monopoly but that it’s charging customers more for its products than it would with greater competition.
That makes it harder to go after Google on antitrust grounds because its most popular products — search, email, browser, maps — are all free.
But in the past several years, there’s been a shift in that thinking, thanks in part to the scholarship of a new wave of influential legal academics dubbed the “hipster antitrust” movement, as well as rising bipartisan political opposition to Big Tech’s influence over the American public.
“It’s a major, major change in the government’s orientation toward monopoly power,” said Sally Hubbard, director of enforcement strategy at the Open Markets Institute, an antitrust nonprofit.
Earlier this month, the Democrat-led House Judiciary Committee concluded its year-long investigation into major tech companies, concluding that not just Google, but also Amazon, Facebook, and Apple, use monopoly power to protect their dominant positions in the industry. This investigation has set the stage for lawmakers to introduce new laws regulating the tech giants in the future.
The progressive flank of the Democratic party, such as Sen. Elizabeth Warren (D-MA) and Sen. Bernie Sanders (D-VT), have long argued that the US needs new laws to break up big tech companies, which they say have amassed too much market power and are hurting the American people and economy. 
At the same time, Republicans have been ramping up their attacks on tech for a more specific reason: alleged and unproven “anti-conservative” bias. Facebook’s and Twitter’s recent efforts to fact-check and even block unsubstantiated claims made by Republican politicians and some conservative-leaning news outlets have further fueled these complaints.
Some conservatives, including Trump, are increasingly calling for Congress to repeal Section 230, a landmark internet law that protects social media companies from being sued for what people say on their platforms. Some Democrats, including presidential candidate Joe Biden, have also called for Section 230 reform, although not in the same manner Republicans are demanding.
On a press call with reporters announcing the case on Tuesday, DOJ’s Shores was clear in saying that the lawsuit does not address concerns about Section 230. 
“The antitrust case is very separate from the questions about social media and some other technology issues that are out there about skew or bias that have been the subject, at least for us, with regard to the Section 230 of the Communications Decency Act,” said Shores.
But it’s impossible to separate the timing of its release from these larger talks of tech reform. 
In fact, some have questioned whether the DOJ has rushed out its case against Google in order to file the suit before the election to please Trump, who has long been calling for this suit to move forward as part of his administrations’ larger confrontational policy against Big Tech.
If Biden wins the presidency, his administration’s Department of Justice could pursue the current case, refine the charges, or drop it altogether. Several legal experts told Recode that it’s likely that a potential Biden administration would pursue the case in some form given the bipartisan support for going after tech.
And the American public has also increasingly come to question Big Tech’s power, with roughly half thinking major technology companies should be regulated more than they are now, according to a June poll by Pew Research.
As Recode previously reported, even some Google employees (often anonymously, for fear of punishment by their employer) have argued that the company should be broken up to help Google return to its small, scrappy startup ethos — which they think it needs in order to continue innovating.
What this means for Google’s future
Google and the DOJ have a long and complicated road ahead before we see any meaningful resolution to this suit. 
The case could take several years to play out in court; remember that Microsoft’s DOJ case took several years to come to a settlement. 
Similarly, the Google case is expected to drag on for years — and of course there’s the real possibility that Google may eventually win, or settle with the US government to avoid a breakup, as Microsoft did.
But in the meantime, the mere threat of antitrust action is likely to loom over Google, putting the company in a defensive crouch, potentially slowing down its growth and preventing it from continuing the types of business practices that made it successful — such as buying up companies like YouTube, Android, and DoubleClick.
“When the Microsoft suit came down, we saw them change their behavior and it sent shockwaves through the industry,” said Open Market’s Hubbard, who said that, similarly, this suit could prevent not just Google, but the other tech giants like Amazon, Facebook, and Apple, from being as brazen in buying up competitors or enforcing questionable business contracts that reinforce their market power.
There could also be more force behind the DOJ’s suit if more state attorneys general sign on.
No Democratic state attorneys general have signed on to the case now, but some, such as New York State AG Letitia James, have said they may sign on to the DOJ’s case at a later time after they’ve finished their own independent investigations.
The DOJ, or states, may also file more lawsuits against Amazon, Facebook, and Apple. And as the case unfolds in court, Congress could pass new legislation.
Regardless of the DOJ’s suit’s eventual outcome, its filing marks a clear turning point for Big Tech. Giant companies like Google can no longer expect to continue skirting outdated regulations with impunity as they expand their empires; instead, they will face growing scrutiny and enforcement now that they’re increasingly seen as potentially harmful institutions whose powers need to be checked by the government. 

  
    Related
  

  
    Bill Barr and Elizabeth Warren find a common enemy: Google
  


  
  
  
      




  After a 14-month investigation, the United States government filed a landmark lawsuit against Google on Tuesday, arguing that the search giant used unfair practices to preserve its search and search advertising monopoly.
The Department of Justice and 11 states filed the lawsuit against Google in a federal court, accusing Google of using money it makes from its dominant position in search to pay other companies to help maintain its lead and block out competitors. Google pays Apple billions each year to be the default browser on Safari, for example, and search comes preloaded on devices using Google’s Android operating system.
“Two decades ago, Google became the darling of Silicon Valley as a scrappy startup with an innovative way to search the emerging internet,” states the suit. “That Google is long gone. The Google of today is a monopoly gatekeeper for the internet.” 
In a press briefing, Justice Department officials said the government is stepping in to protect access to a free market for customers and Google’s competitors. They argue that Google has illegally maintained its monopoly through exclusive business deals that put its own search and browser on phones and keep out competitors.
“If the government does not enforce the antitrust laws to enable competition, we could lose the next wave of innovation,” Justice Department spokesperson Marc Raimondi said in a press briefing. “If that happens, Americans may never get to see the next Google.”
The case argues that Google’s anticompetitive practices are harming three key groups: American consumers, who “are forced to accept” its often-controversial privacy practices; advertisers, who have to pay a “toll” to Google to reach their customers; and competing tech companies, who “cannot emerge from Google’s long shadow.”
The announcement unveils the biggest antitrust case against a tech company since the Microsoft antitrust case in 1998. “Google’s practices are anticompetitive under long-established antitrust law,” the new complaint reads. The DOJ likened the situation to Microsoft, which made its internet browser the default on Windows operating systems and made it impossible to delete.
The Justice Department’s suit poses a potential existential threat to Google’s business if it results in breaking off Google’s search engine — which accounted for about $21 billion last quarter, or more than half of its total revenue — from its other lines of business, such as cloud computing and video. 
Google rebutted the basis of the lawsuit, calling it a “dubious complaint” and arguing that consumers can easily use other products besides its own.
“Today’s lawsuit by the Department of Justice is deeply flawed,” a Google spokesperson said in a statement. “People use Google because they choose to — not because they’re forced to or because they can’t find alternatives.”
The suit was filed amid heavy political tension between major tech companies and the US government, with Attorney General Bill Barr reportedly speeding up the timing of the lawsuit so that it would be filed before the presidential election in November.
Here’s a breakdown of the case, its political consequences, and the complicated path ahead for Google and the DOJ.
The case against Google
The DOJ considers Google search to be a monopoly in the US, where nearly 90 percent of internet searches are through Google. The complaint says that Google is illegally trying to maintain its dominance through anticompetitive practices. 
In its lawsuit, the Justice department claims that Google has used exclusive business contracts to limit rival companies’ ability to put their products on Google’s Android mobile devices, and incentivizes device manufacturers like Apple and carriers like Verizon to use Google search instead of other search engines.  
The suit argues these practices violate the century-old Sherman Antitrust Act, which outlaws companies from “every contract, combination, or conspiracy” to monopolize.
And the suit says Google uses its profits from its massive hold on the search industry to maintain that grip by paying companies like Apple, LG, and AT&T to make it the default search engine on their devices, thus making it harder for potential rivals to compete.
“For many years, Google has used anti competitive tactics to maintain and extend its monopolies in the markets for general search services, search advertising, and general search text advertising—the cornerstones of its empire,” the report reads.
Google said its contracts are similar to how other companies promote their products.
“[L]ike countless other businesses, we pay to promote our services, just like a cereal brand might pay a supermarket to stock its products at the end of a row or on a shelf at eye level,” the company wrote in a response to the DOJ complaint.
The DOJ said that Google’s contracts help Google maintain its search monopoly because its scale contributes to its effectiveness: The more user data it has, the better its search results are. Additionally, the more people use Google search, the more advertisers will pay Google to reach them. 
“Google deprives rivals of the quality, reach, and financial position necessary to mount any meaningful competition to Google’s longstanding monopolies,” the report reads. “By foreclosing competition from rivals, Google harms consumers and advertisers.”
The DOJ complaint is also notable because rather than focusing on how Google’s monopoly could raise prices, it focuses on how lack of competition could lower quality, according to Thomas Campbell, former director of the FTC’s antitrust arm and a professor of antitrust law at Chapman University. 
“Normally in antitrust cases the argument is that because of exclusionary conduct, a market is monopolized and the price to the consumer is higher,” Campbell told Recode. In this case, “the main point is the benefit of having a search engine that protects your privacy is lost.” 
The lawsuit is more narrow than a report earlier this month from Congress’s House Judiciary Subcommittee on Antitrust, which also discussed how Google allegedly prioritizes its own search results over competing search platforms, like restaurant reviews on Yelp or flight queries on Expedia. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Rani Molla/Vox
      
    
  


The DOJ case also limits its focus to search, rather than discussing other industries where Google is dominant, including online advertising, smartphone operating systems, and web browsers. Critics say it uses that dominance in each to reinforce its other business lines.
Democratic state attorneys general may address these other issues in future lawsuits. The DOJ could also expand the scope of its lawsuit as the case proceeds. 
The complicated bipartisan politics behind Big Tech regulation
The lawsuit against Google comes at a time when there’s unprecedented public and political opposition to the financial and political power of major tech companies. Lawmakers on both sides of the aisle want to regulate Big Tech, though they disagree on why and how to regulate these giants. 
President Trump, as well as many Republican and Democrat lawmakers, have argued with increasing urgency that major tech companies like Google have amassed far too much market power. They say the companies stifle competition and leave consumers with no choice but to use their services when they go online.
This is a departure from the decades-long prevailing legal attitude in the US government. Historically, the idea was that to break up a company, you have to prove not just that it’s a monopoly but that it’s charging customers more for its products than it would with greater competition.
That makes it harder to go after Google on antitrust grounds because its most popular products — search, email, browser, maps — are all free.
But in the past several years, there’s been a shift in that thinking, thanks in part to the scholarship of a new wave of influential legal academics dubbed the “hipster antitrust” movement, as well as rising bipartisan political opposition to Big Tech’s influence over the American public.
“It’s a major, major change in the government’s orientation toward monopoly power,” said Sally Hubbard, director of enforcement strategy at the Open Markets Institute, an antitrust nonprofit.
Earlier this month, the Democrat-led House Judiciary Committee concluded its year-long investigation into major tech companies, concluding that not just Google, but also Amazon, Facebook, and Apple, use monopoly power to protect their dominant positions in the industry. This investigation has set the stage for lawmakers to introduce new laws regulating the tech giants in the future.
The progressive flank of the Democratic party, such as Sen. Elizabeth Warren (D-MA) and Sen. Bernie Sanders (D-VT), have long argued that the US needs new laws to break up big tech companies, which they say have amassed too much market power and are hurting the American people and economy. 
At the same time, Republicans have been ramping up their attacks on tech for a more specific reason: alleged and unproven “anti-conservative” bias. Facebook’s and Twitter’s recent efforts to fact-check and even block unsubstantiated claims made by Republican politicians and some conservative-leaning news outlets have further fueled these complaints.
Some conservatives, including Trump, are increasingly calling for Congress to repeal Section 230, a landmark internet law that protects social media companies from being sued for what people say on their platforms. Some Democrats, including presidential candidate Joe Biden, have also called for Section 230 reform, although not in the same manner Republicans are demanding.
On a press call with reporters announcing the case on Tuesday, DOJ’s Shores was clear in saying that the lawsuit does not address concerns about Section 230. 
“The antitrust case is very separate from the questions about social media and some other technology issues that are out there about skew or bias that have been the subject, at least for us, with regard to the Section 230 of the Communications Decency Act,” said Shores.
But it’s impossible to separate the timing of its release from these larger talks of tech reform. 
In fact, some have questioned whether the DOJ has rushed out its case against Google in order to file the suit before the election to please Trump, who has long been calling for this suit to move forward as part of his administrations’ larger confrontational policy against Big Tech.
If Biden wins the presidency, his administration’s Department of Justice could pursue the current case, refine the charges, or drop it altogether. Several legal experts told Recode that it’s likely that a potential Biden administration would pursue the case in some form given the bipartisan support for going after tech.
And the American public has also increasingly come to question Big Tech’s power, with roughly half thinking major technology companies should be regulated more than they are now, according to a June poll by Pew Research.
As Recode previously reported, even some Google employees (often anonymously, for fear of punishment by their employer) have argued that the company should be broken up to help Google return to its small, scrappy startup ethos — which they think it needs in order to continue innovating.
What this means for Google’s future
Google and the DOJ have a long and complicated road ahead before we see any meaningful resolution to this suit. 
The case could take several years to play out in court; remember that Microsoft’s DOJ case took several years to come to a settlement. 
Similarly, the Google case is expected to drag on for years — and of course there’s the real possibility that Google may eventually win, or settle with the US government to avoid a breakup, as Microsoft did.
But in the meantime, the mere threat of antitrust action is likely to loom over Google, putting the company in a defensive crouch, potentially slowing down its growth and preventing it from continuing the types of business practices that made it successful — such as buying up companies like YouTube, Android, and DoubleClick.
“When the Microsoft suit came down, we saw them change their behavior and it sent shockwaves through the industry,” said Open Market’s Hubbard, who said that, similarly, this suit could prevent not just Google, but the other tech giants like Amazon, Facebook, and Apple, from being as brazen in buying up competitors or enforcing questionable business contracts that reinforce their market power.
There could also be more force behind the DOJ’s suit if more state attorneys general sign on.
No Democratic state attorneys general have signed on to the case now, but some, such as New York State AG Letitia James, have said they may sign on to the DOJ’s case at a later time after they’ve finished their own independent investigations.
The DOJ, or states, may also file more lawsuits against Amazon, Facebook, and Apple. And as the case unfolds in court, Congress could pass new legislation.
Regardless of the DOJ’s suit’s eventual outcome, its filing marks a clear turning point for Big Tech. Giant companies like Google can no longer expect to continue skirting outdated regulations with impunity as they expand their empires; instead, they will face growing scrutiny and enforcement now that they’re increasingly seen as potentially harmful institutions whose powers need to be checked by the government. 

  
    Related
  

  
    Bill Barr and Elizabeth Warren find a common enemy: Google
  


  
  
  
      




  You have to do something extraordinary to unite Elizabeth Warren and Bill Barr. 
But that’s what Google has done. President Trump’s attorney general and personal fixer has filed an antitrust suit against the trillion-dollar internet titan. And the Massachusetts senator, who despises everything about the Trump administration and has called on Barr to resign, is cheering him on. Sort of.
“Two things can be true at the same time: Bill Barr is a corrupt Trump crony who should not be the Attorney General and the Justice Department has the power to pursue a legitimate, long-time-coming suit against Google for engaging in anti-competitive, manipulative, and often illegal conduct,” Warren said in a statement to Recode.  
This would be a striking alliance at any time. It’s even more so now that we’re days before an election, when you’d think that a Democrat would go out of their way not to support what the Trump administration wants.
But Warren and many others on the left have decided that going after Google, in what may be the most important tech lawsuit since the US government sued Microsoft nearly two decades ago, is worth making common cause with a bitter enemy.
Tuesday’s lawsuit is the most consequential result of the so-called “techlash” that has been brewing over the past few years — a feeling, often hard to quantify, that the big tech companies have grown without any kind of checks or balances from the government, and need to be reined in … somehow.
News organizations now scrutinize Google and other big companies with newfound zeal, and Congress has hauled in tech executives for public hearings. But that hasn’t slowed these companies’ velocity or reduced their power. Even punitive acts like a $5 billion fine levied against Facebook for privacy violations barely qualify as a wrist slap.
And while a landmark congressional report laid out an argument for restraining Google, Facebook, and their peers this month, there’s no guarantee that lawmakers will end up acting on any of that.
Barr’s suit, though, has the potential to fundamentally change the way Google works: A victory for the DOJ — or a settlement in advance of a verdict — could require Google to divest itself of crucial properties. 
Just as crucially, the fact that Google is likely to spend years fighting the federal government in court could slow down the company, forcing it to pass on moves it wants to make, or simply by distracting it from its core business. Which is what some Microsoft executives say happened at that company when it fought a Clinton-era antitrust suit.
“In the case of Microsoft, some say that the trial was the remedy — it created a culture within Microsoft that paralyzed the company from bulldozing into adjacent markets,” says Luther Lowe, who heads public policy at Yelp, the restaurant review company that has complained bitterly about Google for years. “I hope that US v. Google will do the same.”
Make no mistake: Many of Google’s most vocal opponents wish Bill Barr weren’t the one making the case against the company. Barr is overseeing the Google case directly, after his top antitrust lawyer recused himself because of a conflict of interest. It’s possible that his presence in the case could work in Google’s favor. 
Not only is Barr routinely accused of turning the DOJ into Trump’s personal law service, but Trump — and Barr — have made it clear that punishing technology companies is a political act, meant to win points with Trump’s base. 
Trump routinely goes after tech companies in tweets, of course. But more than that, in recent months he’s turned that into action by attempting to ban both TikTok and WeChat, citing national security concerns. And his White House has reportedly told its Republican allies that Trump would like them to “ratchet up scrutiny of social media companies it sees as biased against conservatives” in the runup to next month’s presidential election; and they have obliged. Last week’s flurry of press releases slamming and threatening Facebook and Twitter after those companies’ restrained distribution of a dubious New York Post story aimed at Joe Biden underscores the point.
Barr, meanwhile, has a very long list of critics — not just ideological opponents like Warren, but also former DOJ officials,  a whistleblower who accused Barr of chasing after marijuana companies out of personal animus, and a current federal prosecutor who says Barr “has brought shame on the department he purports to lead.”
And when it comes to the Google suit, there are specific concerns. In September, the New York Times reported that some DOJ officials worried that Barr was rushing to bring a suit before the election, “overrul[ing] career lawyers who said they needed more time to build a strong case against one of the world’s wealthiest, most formidable technology companies.” 
The most damning argument against Barr comes from Barr himself, via a jaw-dropping Fox News interview in June where he claimed there is anti-conservative bias in the tech industry and said one way to fix that would be “through the antitrust laws and challenging companies that engage in monopolistic practices.”
To hammer that home: There’s no substance to conservative complaints of systemic bias among technology companies. But even if there were, no one would argue in court that you would use antitrust laws, which are supposed to protect consumers from economic harm, to fix that.
So why doesn’t this alarm Warren, or anyone else who wants to see Google brought up on charges?
There are a few schools of thought:

The most straightforward argument is that even though Barr may be suspect or corrupt, his career staff at the DOJ is not, and they have been taking the case seriously. Ditto for the staff who work for the attorney general in Texas and other states, who are pursuing their own cases against Google.
Another is that it doesn’t matter what Barr says or does outside of court — what happens in a federal courtroom is what will matter. Which is something we’ve seen more than once in the Trump era. In 2017 and 2018, when the DOJ sued to stop AT&T from buying Time Warner, AT&T’s lawyers argued that the suit was politically motivated because of Trump’s many complaints about the Time Warner-owned CNN. But those arguments didn’t go anywhere. Ditto for opponents of Trump’s first travel ban, who argued, unsuccessfully, that Trump’s first executive order was in fact a ban against Muslims, based on Trump’s own commentary.

But the main argument you hear from people who hate Barr but support his pursuit of Google is that someone needs to do it. And if it has to be Barr, then fine.
“I would much rather Bill Barr not be involved, because it gives Google ammunition to argue that it’s politically motivated,” says Sally Hubbard, director of enforcement strategy at Open Markets, an antitrust nonprofit. “But I’m not thinking about it as Bill Barr’s case.”
Open Markets was one of several left-leaning groups that signed an open letter in September urging the DOJ and state attorneys general to move forward with the case: “[T]he time for this enforcement action to proceed is now. In fact, it was long before now. As days, weeks, months, and years, continue to pass, more and more companies go out of business as Google’s dominance becomes more and more deeply entrenched.”
As Hubbard, Warren, and other Google opponents point out, some of them have been waiting for years for the federal government to move against Google. As the Wall Street Journal has reported, some officials in Barack Obama’s Federal Trade Commission recommended an antitrust lawsuit against the search giant in 2012. Instead, the FTC concluded that Google hadn’t violated antitrust laws, and gave Google what amounted to the most delicate of slaps on the wrist.
And while there is a lot of conversation in Washington about reforming and regulating Google and other big tech companies, none of that is likely to happen anytime soon, and may never happen at all. 
That’s because overhauling antitrust laws — or the laws around Section 230, which gives tech platforms a sort of immunity from prosecution over content they host — would require lawmakers to ... work together to change the laws. That’s a nonstarter in today’s Congress, where Republicans and Democrats aren’t even close to being on the same page.

  
    Related
  

  
    What the FCC can and can’t do to Section 230
  

And even in a theoretical Democratic Congress, which we might see after the November election, lawmakers would have to make tech legislation a priority — and they are going to have a long list of priorities competing for their time.
But while an antitrust suit could take years to play out, it could actually affect the way Google does business — both in the future and in the present — as it spends time and attention fighting the government in federal court. Which is enough for Elizabeth Warren, and everyone else who hates Bill Barr, to root for him, for now.


  
  
  
      




  Ahead of this year’s presidential election, several states including battlegrounds like Texas, Pennsylvania, and Ohio are approaching or passing voter registration records. But don’t break out the champagne to celebrate democracy just yet. Voter registration totals are not a very good or meaningful indicator of how many people will actually vote. There are, however, a number of other indicators that suggest 2020 will be a banner year for voter turnout.
As the country continues to grapple with the pandemic, this election season is proving to be an exceptional one. The ways in which people are voting, for instance, are far from typical. Some 80 million Americans could cast their ballots by mail, and already a record 22 million have voted early. And while the battle between Donald Trump and Joe Biden is attracting the most attention, state and local elections are seeing record levels of fundraising. An influx of new voters would surely mix things up even more.
Still, the voter registration numbers we’re seeing now are more complicated than they look. In the United States, you generally have to register before you vote. And unless you sign up to vote elsewhere or the state is notified of your death or imprisonment, you generally stay registered. So while a state’s voter registration numbers include people who have registered for the first time — young people and first-time voters — those figures can, for a limited time, also include people who have moved and aren’t going to vote in that state. 
The intention of not immediately culling voters, specified in the National Voter Registration Act, is to give people a window of time before they’re removed from the rolls without direct confirmation in order to avoid mistakenly disenfranchising voters. Some states are more aggressive than others in removing voters, so their voter registration totals may be more or less reflective of the voting populace.
“Let’s say you have two states of the same size with a million new voters registered in each, and 500,000 have moved away or died,” Center for Election Innovation & Research founder David Becker explained to Recode. “If state A has diligently cleaned the list, they’d show a net 500,000 increase in voter registration, while state B might show a net million.”
Kevin Morris, a voting rights researcher at NYU’s Brennan Center for Justice, put it another way: “You can’t compare one state to another because different states have different removal practices.” 
Instead, these voting experts suggest looking at new voter registrations, rather than total voter registrations, to get an idea of how many people might turn out to vote. Only some states, however, report this information incrementally, and it’s often lagging. Thanks to the pandemic keeping people away from the DMV and other voter registration places, new voter registrations had been down earlier this year, though the number rebounded this summer
It remains to be seen how much new voter registration grew closer to the election, when new voter registration tends to spike, but in many ways that metric no longer matters. Here are some other reasons to believe that there will be record voter turnout in this election.
Early voting turnout
Election Day is still nearly three weeks away, but 16 percent of those who voted in the 2016 election have already done so this year, according to the US Elections Project, an effort by election expert and University of Florida professor Michael McDonald to make available timely election data. In some states, early voting turnout is even higher: 39 percent in Vermont, 30 percent in Virginia, 27 percent in Texas.
High early voting turnout was evidenced by long lines and crashing online voter registration portals.
This early voting turnout has shattered records. As of last Sunday, early voting numbers were nearly seven times what they were at the same time in 2016. At this rate, for the first time ever, the majority of voters might cast their ballots before Election Day. 
“There’s no analog in American history to compare this to,” Becker said. “This is history.”
Of the states where partisan data is already available, Election Project found that, so far, early votes were twice as likely to be cast by Democrats than Republicans.
Voter enthusiasm is high
A number of polls have shown that voters think this election is more important than ever, meaning they’re more likely to actually vote. 
“In spite of numerous challenges facing voters — foreign interference, divisive partisanship, fear of the pandemic — voters are incredibly motivated to show their enthusiasm in this election,” Becker said.
This summer, the Pew Research Center asked registered voters how much the presidential election “really matters,” and this year a whopping 83 percent said it does — a higher percentage than Pew has recorded in 20 years of collecting data. Similarly, a record 86 percent of registered voters said Trump and Biden have different positions on the issues, suggesting voters perceive a meaningful difference between the candidates. 


!function(){"use strict";window.addEventListener("message",function(a){if(void 0!==a.data["datawrapper-height"])for(var e in a.data["datawrapper-height"]){var t=document.getElementById("datawrapper-chart-"+e)||document.querySelector("iframe[src*='"+e+"']");t&&(t.style.height=a.data["datawrapper-height"][e]+"px")}});window.addEventListener('DOMContentLoaded',function(){var i=document.createElement("iframe");var e=document.getElementById("datawrapper-GB0mR");var t=e.dataset.iframeTitle||'Interactive graphic';i.setAttribute("src",e.dataset.iframe);i.setAttribute("title",t);i.setAttribute("frameborder","0");i.setAttribute("scrolling","no");i.setAttribute("aria-label",e.dataset.iframeFallbackAlt||t);i.setAttribute("title",t);i.setAttribute("height","400");i.setAttribute("id","datawrapper-chart-GB0mR");i.style.minWidth="100%";i.style.border="none";e.appendChild(i)})}()

Americans are also simply saying they’re more enthusiastic about this election than usual, according to Gallup. Some 67 percent of Americans said they are more enthusiastic than usual about voting, tying as the highest rate measured in previous elections.
Fewer undecided voters than the last election
People’s heightened partisanship might also be an indicator of high voter turnout.
The share of undecided voters is about half what it was during the 2016 presidential election, according to a variety of polls, including ones from Reuters, Quinnipiac, and Monmouth. This is another signal that turnout will be higher this year. People who know who they want to vote for are less likely to sit out an election, according to Morris at the Brennan Center.
“There are low rates of undecided voters, who are less likely to vote,” Morris said. “Far more people know who they prefer than normal.”
But even if those undecided voters do vote, it seems they’re just as likely to vote for Trump as Biden, according to Reuters. So the undecideds might not affect the outcome of the presidential election much. Nevertheless, 2020 races are likely to be unpredictable not only at the national level but state and local as well. And just because more people are likely to vote doesn’t mean we’ll know for sure how they’ll vote. 
As Becker put it, “It’s highly likely those new voters will be more representative of the electorate in that state than the existing registration list. How much and how that plays out, we’re still studying.” 

  
  
  
      




  Ahead of this year’s presidential election, several states including battlegrounds like Texas, Pennsylvania, and Ohio are approaching or passing voter registration records. But don’t break out the champagne to celebrate democracy just yet. Voter registration totals are not a very good or meaningful indicator of how many people will actually vote. There are, however, a number of other indicators that suggest 2020 will be a banner year for voter turnout.
As the country continues to grapple with the pandemic, this election season is proving to be an exceptional one. The ways in which people are voting, for instance, are far from typical. Some 80 million Americans could cast their ballots by mail, and already a record 22 million have voted early. And while the battle between Donald Trump and Joe Biden is attracting the most attention, state and local elections are seeing record levels of fundraising. An influx of new voters would surely mix things up even more.
Still, the voter registration numbers we’re seeing now are more complicated than they look. In the United States, you generally have to register before you vote. And unless you sign up to vote elsewhere or the state is notified of your death or imprisonment, you generally stay registered. So while a state’s voter registration numbers include people who have registered for the first time — young people and first-time voters — those figures can, for a limited time, also include people who have moved and aren’t going to vote in that state. 
The intention of not immediately culling voters, specified in the National Voter Registration Act, is to give people a window of time before they’re removed from the rolls without direct confirmation in order to avoid mistakenly disenfranchising voters. Some states are more aggressive than others in removing voters, so their voter registration totals may be more or less reflective of the voting populace.
“Let’s say you have two states of the same size with a million new voters registered in each, and 500,000 have moved away or died,” Center for Election Innovation & Research founder David Becker explained to Recode. “If state A has diligently cleaned the list, they’d show a net 500,000 increase in voter registration, while state B might show a net million.”
Kevin Morris, a voting rights researcher at NYU’s Brennan Center for Justice, put it another way: “You can’t compare one state to another because different states have different removal practices.” 
Instead, these voting experts suggest looking at new voter registrations, rather than total voter registrations, to get an idea of how many people might turn out to vote. Only some states, however, report this information incrementally, and it’s often lagging. Thanks to the pandemic keeping people away from the DMV and other voter registration places, new voter registrations had been down earlier this year, though the number rebounded this summer
It remains to be seen how much new voter registration grew closer to the election, when new voter registration tends to spike, but in many ways that metric no longer matters. Here are some other reasons to believe that there will be record voter turnout in this election.
Early voting turnout
Election Day is still nearly three weeks away, but 16 percent of those who voted in the 2016 election have already done so this year, according to the US Elections Project, an effort by election expert and University of Florida professor Michael McDonald to make available timely election data. In some states, early voting turnout is even higher: 39 percent in Vermont, 30 percent in Virginia, 27 percent in Texas.
High early voting turnout was evidenced by long lines and crashing online voter registration portals.
This early voting turnout has shattered records. As of last Sunday, early voting numbers were nearly seven times what they were at the same time in 2016. At this rate, for the first time ever, the majority of voters might cast their ballots before Election Day. 
“There’s no analog in American history to compare this to,” Becker said. “This is history.”
Of the states where partisan data is already available, Election Project found that, so far, early votes were twice as likely to be cast by Democrats than Republicans.
Voter enthusiasm is high
A number of polls have shown that voters think this election is more important than ever, meaning they’re more likely to actually vote. 
“In spite of numerous challenges facing voters — foreign interference, divisive partisanship, fear of the pandemic — voters are incredibly motivated to show their enthusiasm in this election,” Becker said.
This summer, the Pew Research Center asked registered voters how much the presidential election “really matters,” and this year a whopping 83 percent said it does — a higher percentage than Pew has recorded in 20 years of collecting data. Similarly, a record 86 percent of registered voters said Trump and Biden have different positions on the issues, suggesting voters perceive a meaningful difference between the candidates. 


!function(){"use strict";window.addEventListener("message",function(a){if(void 0!==a.data["datawrapper-height"])for(var e in a.data["datawrapper-height"]){var t=document.getElementById("datawrapper-chart-"+e)||document.querySelector("iframe[src*='"+e+"']");t&&(t.style.height=a.data["datawrapper-height"][e]+"px")}});window.addEventListener('DOMContentLoaded',function(){var i=document.createElement("iframe");var e=document.getElementById("datawrapper-GB0mR");var t=e.dataset.iframeTitle||'Interactive graphic';i.setAttribute("src",e.dataset.iframe);i.setAttribute("title",t);i.setAttribute("frameborder","0");i.setAttribute("scrolling","no");i.setAttribute("aria-label",e.dataset.iframeFallbackAlt||t);i.setAttribute("title",t);i.setAttribute("height","400");i.setAttribute("id","datawrapper-chart-GB0mR");i.style.minWidth="100%";i.style.border="none";e.appendChild(i)})}()

Americans are also simply saying they’re more enthusiastic about this election than usual, according to Gallup. Some 67 percent of Americans said they are more enthusiastic than usual about voting, tying as the highest rate measured in previous elections.
Fewer undecided voters than the last election
People’s heightened partisanship might also be an indicator of high voter turnout.
The share of undecided voters is about half what it was during the 2016 presidential election, according to a variety of polls, including ones from Reuters, Quinnipiac, and Monmouth. This is another signal that turnout will be higher this year. People who know who they want to vote for are less likely to sit out an election, according to Morris at the Brennan Center.
“There are low rates of undecided voters, who are less likely to vote,” Morris said. “Far more people know who they prefer than normal.”
But even if those undecided voters do vote, it seems they’re just as likely to vote for Trump as Biden, according to Reuters. So the undecideds might not affect the outcome of the presidential election much. Nevertheless, 2020 races are likely to be unpredictable not only at the national level but state and local as well. And just because more people are likely to vote doesn’t mean we’ll know for sure how they’ll vote. 
As Becker put it, “It’s highly likely those new voters will be more representative of the electorate in that state than the existing registration list. How much and how that plays out, we’re still studying.” 

  
  
  
      




  Germany gets a lot of favorable Covid-19 press — and for good reason. Its daily new cases per million people have been persistently lower than any of its Western European neighbors, and its death rate, from the beginning of the outbreak, has been among the lowest in Western Europe: currently 0.15 deaths per million people, compared to France’s 1.15 and Spain’s 2.19.
Even as coronavirus cases surge across the continent — the week prior to October 11 saw the largest increase since the beginning of the pandemic — Germany’s latest wave is still small relative to other countries in the region. 


Number of #coronavirus infections per 1 million inhabitants (7 day average). Germany is a giant, comparatively clean island smack in the middle of Europe. via ⁦@Welt⁩ pic.twitter.com/1jYAnrWczh— Bojan Pancevski (@bopanc) October 4, 2020



So what exactly is Germany getting right? 
What’s often cited is an effective deployment of technology, such as a contact tracing app, to fight the pandemic. There’s the frequently praised mass testing program, which rivals South Korea’s, and the oversupply of ICU beds — controversial before the coronavirus, now lauded. It also helps that Angela Merkel has a doctorate in quantum chemistry and heads a country that treats scientists, like the Berlin-based virologist and podcaster Christian Drosten, like superstars. 
Yet this is far from the whole story of Germany’s relative success.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Our World in Data
      
    
  


Over the past few weeks, I talked to doctors, health officials, and researchers in Germany— including some of the country’s first Covid-19 responders — and elsewhere to get a deeper perspective on why Germany has had better-than-average pandemic performance in Europe. 
I heard, again and again, four explanations for the country’s coronavirus success. They had nothing to do with tech, Merkel, or hospital beds. And they’ve been largely overlooked. 
Let’s call them the L’s: luck, learning, local responses, and listening. While the pandemic certainly isn’t over, and Germany is facing a pivotal moment with a record number of new infections, these factors may be the reason Germany bends the curve quickly once again. 
The power of luck
Günter Fröschl, a tropical medicine doctor at Munich University, has been leading Germany’s longest-running Covid-19 testing unit. He’s been at it so long, he swabbed four of the first five coronavirus patients in late January. At that time, his fiancée — another infectious disease specialist — happened to be working in Brescia, Italy, ground zero of Europe’s deadliest Covid-19 outbreak. The two were on the phone every day comparing notes, and Fröschl concluded the only reason the paths of the two countries diverged so widely early on in the pandemic was something both countries had no control over. 
“We had a lot of luck in Germany,” Fröschl says. 
The first known Covid-19 cases in Germany originated in a Munich-area auto parts firm called Webasto. There, an employee from China — who tested positive for the virus after returning home — infected several others during a visit to Munich. When she notified her German counterparts of her positive test result, the company informed its staff, including one employee who, despite not having serious symptoms, sought out a test. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        The Munich headquarters of Webasto, a German car parts supplier and home to the first coronavirus cases in Germany.
      
      
        TF-Images/Getty Images
      
    
  


“The patient came to us and said, ‘I had a common cold for a few days. I’m feeling fine — but we did have a Chinese colleague coming to visit us who tested positive,’” Fröschl recalls. The fact that this patient came forward meant public health officials were able to identify, trace, and isolate other cases, and instead of a large and silent outbreak early on in the pandemic, health authorities stopped the virus from spreading further at that point. 
There was another element of luck involved: The Bundeswehr Institute of Microbiology in Munich is home to a biosafety level 3 lab — the kind that deals with highly infectious and deadly agents that can spread through inhalation, like SARS-CoV-2. When China released the genetic sequence of the new coronavirus in January, Fröschl’s colleagues at the institute got ready with coronavirus PCR tests. That meant the test was available in Munich when the first patients showed up there, and Fröschl was able to use it to quickly diagnose the first cases. “The index patient was meeting this unique situation in Europe,” Fröschl says. “That is luck. It’s not that we were so smart.”  
It wasn’t just Munich that had tests ready. In Berlin, scientists created the test kit the World Health Organization and many countries ended up using even before China released the sequence of the virus. But Fröschl points out that if that first patient had shown up in a less prepared part of the country, the outcome may have been different — perhaps something more like what happened in Italy, where cases went undetected for weeks and then overwhelmed the health system. “I’m always emphasizing,” Fröschl says, “we were just lucky.”
The power of learning
Of course, the key to Germany’s coronavirus management isn’t only about luck. It’s also about learning and acting quickly on new knowledge. After the Webasto cluster came under control, Fröschl and his colleagues got to work applying what they learned from the experience — establishing protocols for diagnosing, isolating, and treating Covid-19 patients safely. 
This meant that by the end of February, when travelers started returning from Austria, Italy, and other countries with outbreaks, they were ready. The Webasto outbreak gave doctors and public health officials “extremely valuable” experience dealing with the virus. “Everything was in place,” Fröschl says. “We had experience of how to treat people and remain calm.”
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        People take part in a demonstration against right-wing extremists with a poster supporting German doctor and coronavirus expert Christian Drosten on October 3, 2020, in Konstanz, Germany. 
      
      
        Andreas Gebert/Getty Images
      
    
  


There was also learning from other countries. “We tried to take the strategy of South Korea, Japan, and Taiwan — all good examples of how a quick and fast response can reduce the number of positive cases,” said Nicolai Savaskan, the chief medical officer of a local health department in Berlin. One part of that fast response: Germany’s mass testing program. While Germany was quick to lock down, it also scaled up testing from the start of the pandemic, and then repeatedly adapted the program to respond to changes in the epidemic dynamics. 
In anticipation of a rise in cases following summer travel, for example, labs across the country scaled up their supply. You can see the result of this in the country’s test positive — or cases divided by tests — rate. This metric tells you whether a country’s testing capacity is rising in step with the demand for testing and growth in real cases. Since the beginning of May, relatively early in the pandemic, Germany’s test positive rate has held steady even though cases have increased, while the rate started to rise in July and August in other European countries currently experiencing the worst outbreaks, including France, Spain, and the UK. 


All of @OurWorldInData's data on COVID-19 testing has just been updated.Latest positive rates in Europe:   Ukraine: 17.2%   Poland: 12.3%   Spain: 10%   France: 7.5%   Italy: 6.3%   United Kingdom: 5.9%   Russia: 2.8%   Germany: 1.4%(Source: https://t.co/z0HnOz4gb5) pic.twitter.com/rsK5lXMWEF— Edouard Mathieu (@redouad) October 14, 2020



“There have been ups and downs in Germany’s [outbreak], but the difference is they managed to scale up testing,” said Edouard Mathieu, the Paris-based data manager of Oxford University’s Our World in Data project. From May to the present, Germany went from around 60,000 tests per day to a staggering 160,000. And even now, Germany is again adapting its testing approach: adding a new rapid, antigen-testing strategy that will launch this week, the Wall Street Journal reported, to increase capacity as cases rise going into winter. 
This also helps explain why outbreaks in the country — or even screw-ups like failing to notify positive cases quickly — haven’t spun out of control yet, as we’ve seen in other countries. “They are testing more people every time they find a case, which means they haven’t lost touch with the epidemic,” Mathieu said. It also means they didn’t waste their early lockdown: They used it to build robust systems that will likely help them control the current uptick, too.  
The power of local responses 
Germany, a federal country made up of 16 states with some 400 municipal health departments, ran a localized coronavirus response. 
Though this has sometimes led to a confusing array of policies, it’s also meant municipal governments could act quickly and tailor pandemic policies to the needs and challenges facing local populations across 16 federal states with 400-plus counties. 
And this may be another reason for Germany’s success compared to neighbors with more centralized systems such as France, Spain, and the UK. 
“The decentralized [approach to] managing the pandemic was maybe a good way to deal with a quickly changing situation,” said Berlin’s Savaskan. He explained that while local health authorities have to report cases to Germany’s national public health agency, the Robert Koch Institute (RKI), they could each tailor their pandemic responses to meet local needs in their region and react quickly whenever problems arose. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Shortly after curfew, folded chairs stand in front of a bar in Neukölln, Berlin. Due to the pandemic, there is a nighttime curfew and stricter contact bans for indoors and outdoors.
      
      
        Annette Riedl/picture alliance via Getty Images
      
    
  


So, for example, while the RKI recommended a 14-day quarantine after contact with an infected person, in Berlin, health authorities decided that was too long to be acceptable for the population and that a seven-day quarantine with a coronavirus test at that point would do. “We could adapt what was recommended by RKI and then implement ... [to] locally fit the needs of the people,” Savaskan said. 
Similarly, early on in the pandemic, in March, Berlin decided to shut down bars, dance halls, and nightclubs ahead of other regions, since they were local sources of contagion. When they reopened in June, municipal health authorities were in constant contact with the industry to encourage them to cooperate in contact tracing. 
“We have a rate of contact tracing higher than 90 percent,” Savaskan said, meaning nearly all the contacts of infected people are being identified and followed up with. 
When we talked at the end of September, Savaskan was heading to meet the health minister in Berlin. Outbreaks in bars and nightclub settings were on the rise again, and politicians wanted to engage local health departments on how to get the situation under control. By October 10, a midnight curfew for bars and clubs went into effect. 
“The narrative so far in Germany concerning the public health departments is that people trust in them — they believe that when they give very detailed information about their life, this is taken very seriously. And I think this is the major impact of the success of the German response,” Savaskan said. It’s also allowed authorities to identify and stop chains of infection at an early stage. 
The power of listening to scientists
There’s one other L that sets Germany apart. It’s the most straightforward of them all — but it’s certainly not being done in many countries, particularly the US. From the moment the coronavirus arrived in Germany, German authorities have been good about listening to scientists, says Clemens-Martin Wendtner, a Munich-based internal medicine doctor. Wendtner would know: He was also part of Germany’s coronavirus front line, overseeing the treatment of the country’s first patients in Munich. 
He, too, didn’t mention Angela Merkel when I asked him how he explains how Germany managed to control the coronavirus. Instead, he said local politicians did something that now seems like a foreign concept in America: They listened to scientists. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Markus Söder, the minister president of Bavaria and one of the contenders to succeed Chancellor Angela Merkel, wearing a mask in the colors of the Bavarian flag.
      
      
        Peter Kneffel/picture alliance via Getty Images
      
    
  


Since February, Wendtner has been texting new findings and insights to the health minister in Bavaria — the German state that’s home to Munich — every week. And during the first weeks of the pandemic, before heading to the hospital, he’d join a 9 am briefing in the office of the health ministry to share his data there, too. 
“Every [piece of] information we had from the hospital, they also had from the political decision side,” he said. 
So that’s why Germany instituted a mandatory mask policy in public spaces in the spring and shut down schools. That’s why Jens Spahn, the federal minister of health, retracted the idea of Covid-19 immunity passports after listening to scientists. “He used the direct approach, just calling me here in my office,” Wendtner said.
As the science evolved and leaders listened to scientists, the policies keep changing. Recently, the Bavarian government decided to invest 50 million euros in hepafilters that deactivate infectious aerosols for use in classrooms across the state. “It’s not reasonable to open the window in Bavaria every 20 minutes” in winter, Wendtner said. So as temperatures drop, filters might help keep schools open at a time when we know the coronavirus can spread through aerosols, especially in poorly ventilated rooms.
Of course, science hasn’t been free of politics in Germany. And in the race to find a successor for Merkel, state politicians have certainly used the pandemic to increase their profile. But the big picture, Wendtner says, is that the public trusted German politicians “because they didn’t lie in the beginning and [they] built up trust,” following science, not denying it. 









  
  
  
      




  Facebook said on Wednesday morning that it’s reducing distribution of a New York Post story containing unconfirmed claims with questionable sourcing about Democratic presidential candidate Joe Biden’s son, Hunter Biden. Shortly after, Twitter said it’s blocking users from posting the story entirely. 

  
    Related
  

  
    Trump team makes a suspicious effort to swing the election with purported Hunter Biden emails
  

Both companies’ moves to limit the reach of a major news publisher are unusual and drastic at a time when many Republican lawmakers, President Trump, and political figures are threatening new regulations and accusing tech companies of censoring conservative political speech. While these companies have taken down or limited the reach of viral political misinformation networks in the past, doing so to a publication as prominent as the New York Post has immediately attracted attention and criticism. Although it is known for its conservative slant and track record of publishing questionable stories, the tabloid is a widely read, mainstream outlet.
Facebook spokesperson Andy Stone announced his company’s decision in a tweet on Tuesday: 


While I will intentionally not link to the New York Post, I want be clear that this story is eligible to be fact checked by Facebook's third-party fact checking partners. In the meantime, we are reducing its distribution on our platform.— Andy Stone (@andymstone) October 14, 2020



Twitter, meanwhile, blocked users from sharing the New York Post article. If someone tries to tweet the article, Twitter returns an error message, warning users that the link has been identified as “potentially harmful.” 
  
  
    
    
      
        
  


  






      
    
    
  
  


“In line with our Hacked Materials Policy, as well as our approach to blocking URLs, we are taking action to block any links to or images of the material in question on Twitter,” wrote Twitter spokesperson Nicholas Pacillo in response to Recode’s questions about why Twitter blocked the article. Twitter posted a longer series of tweets explaining its rationale on Monday evening.
There’s a real argument that Facebook — a primary news source for four in 10 Americans — and Twitter should be trying to slow the spread of an unproven news story whose claims haven’t been corroborated by other major news outlets and whose origins are from questionable sources. Especially since the New York Post has a track record of sometimes promoting viral conspiracy theories.
In April, the New York Post was responsible for helping spread a baseless conspiracy theory promoted by former Trump adviser Roger Stone, purporting that Bill Gates created the coronavirus to “microchip” people. That story went viral on Facebook, according to research by Joan Donovan, the research director of the Shorenstein Center on Media, Politics and Public Policy at the Harvard Kennedy School.
“It’s really a problem for Facebook because they would not enjoy the same kind of audience without the accordances of the platform,” said Donovan. “Stories that ‘outperform’ the normal rate of distribution often do so because of their novelty. They are one of the few outlets reporting on this, so they get rewarded for not abiding by the same standards as more reputable news outlets.”
Wednesday’s action is particularly unusual for Facebook because the company has long taken a laissez-faire approach to curating the endless stream of viral misinformation on its platform, especially about politics. Facebook CEO Mark Zuckerberg has often said he doesn’t want the platform to be an “arbiter of truth.” Now, it appears that Facebook is taking a firmer stance on moderating what goes viral on its platform — raising questions about when and how it decides to step in, weeks ahead of a presidential election.
Facebook did not respond to Recode’s repeated requests to explain why it reduced distribution of the story on its platform. Stone, the Facebook spokesperson, said in a follow-up tweet to his original post announcing reduced distribution of the New York Post story that this was “part of our standard process to reduce the spread of misinformation.” 
But it’s hard to find other examples of Facebook reducing distribution of a major news story before it’s been fact-checked. It did happen last month, when, ahead of the recent presidential debate, Facebook similarly slowed the spread of false stories about Joe Biden having an earpiece. Those stories originated from a New York Post reporter’s tweet containing an anonymously sourced claim, which turned out to be bogus.
Facebook’s move has already drawn the ire of some Republican leaders on social media. Sen. Josh Hawley, a Republican from Missouri, sent Facebook a letter demanding the company answer why it reduced distribution of the New York Post article. Hawley called the action “censorship” and, without showing any evidence, pointed to it as another example of what many conservatives, including President Trump, have called “anti-conservative bias” on social media platforms. 
President Trump also tweeted in response to the social media companies’ decisions to moderate the article on Monday evening, calling for a repeal of Section 230, a landmark internet law which largely shields social media companies from being sued over what its users say on their platforms. 


So terrible that Facebook and Twitter took down the story of “Smoking Gun” emails related to Sleepy Joe Biden and his son, Hunter, in the @NYPost. It is only the beginning for them. There is nothing worse than a corrupt politician. REPEAL SECTION 230!!! https://t.co/g1RJFpIVUZ— Donald J. Trump (@realDonaldTrump) October 14, 2020



Some have questioned whether Facebook is taking firmer action recently against political misinformation and hate speech because the company is trying to curry favor with a potential Democratic leadership in the White House and Congress. While curbing misinformation and hate speech isn’t necessarily a partisan issue, Democrats have long called for Facebook to take a firmer stance.
But long before Democrats were ahead in the polls and appeared likely to take control of Congress and the White House, Facebook executives had warned of a “hack and leak” situation, one in which leaked information of questionable authenticity is released prior to an election, for political purposes, similar to the 2016 Hillary Clinton emails. Facebook has said that it would take extra caution with such information, and the company has gone so far as to caution news outlets and other social media platforms to do the same. 
The New York Post’s Hunter Biden story is a clear example of something that could potentially fit the hack-and-leak description. While it will take time for fact-checkers to thoroughly interrogate the claims in the story, the basic allegations and circumstances behind the story are rather suspect.
The main allegation in the story is that Hunter Biden introduced then Vice-President Biden to an executive of a Ukrainian energy firm. The article claims that this contradicts Biden’s previous statement that he has never spoken with his son about his foreign business deals. It does not, however, prove that Biden actually talked to this business executive; the Biden campaign put out a statement denying any Biden meeting with the executive in official records. A campaign official told the New York Times that a brief encounter was “technically possible” but was very unlikely and that no discussions related to Burisma would have occurred.
The sourcing of the story has also been questioned: It claims that evidence was obtained from a laptop repair store owner who had access to a computer brought in for repair that may have belonged to Hunter Biden. The New York Post claims the repair store owner brought materials to the lawyer of Rudy Giuliani, the former New York City mayor who’s now often described as Donald Trump’s “personal lawyer,” and Giuliani recently shared a copy of them with the paper.
The circumstances behind this leak have drawn scrutiny from political journalists, including New York magazine’s Jonathan Chait. Also called into question is the conservative editorial slant of the New York Post. The newspaper is owned by Rupert Murdoch, who is in a major battle with Facebook in Australia over how much the company pays his media properties for their content.
Facebook’s response to the New York Post’s track record of promoting false news stories appears to be to treat its latest report with more suspicion than a comparable major news outlet. Nevertheless, the company’s refusal to explain its process is only leaving important questions unanswered and room for unsubstantiated accusations of political bias.

  
  
  
      




  Mixtapes forever changed how we listen to music. What if news got the same treatment? Enter “Your News Update,” a new way to listen to Vox on Google smart speakers and Assistant-enabled devices. 
You’ll hear quick daily updates from your favorite Vox podcasts, like Today, Explained, plus new original content from our Politics and Policy, Culture, The Goods, Recode, and tech teams. 
Here’s how it works: The Your News Update algorithm curates a selection of audio stories tailored to you. What you hear each day is ultimately determined by where you live, what you like, what you need to know that day, and the preferred sources you define in the Google Assistant app. By selecting Vox as a preferred news source, the Assistant will regularly surface stories from Vox in your update.
Your News Update is available on all Assistant-enabled devices, including smart speakers, phones, and tablets. Follow the steps below to select Vox as a preferred news source. To get the playlist, ask your Assistant to “Play Your News Update” or “Play Google News.” You  can also find and hear all episodes in Vox Quick Hits. 

How to make Vox a preferred news source
Download either the Google Assistant (free on Android or Apple) or the Google Home app (free on Android or Apple) on your phone or tablet.
For Google Assistant, open the app and tap the icon in the top right corner. Scroll down and select News. Make sure your News playlist format is set to Your News Update. Then search for Vox in the list of sources and tap the star so it turns blue.


How to set Vox as a preferred source on the Google Assistant app. Taylor Maycan/Vox

For Google Home, open the app and tap the icon in the top right corner. Scroll down and select Assistant Settings > News. Make sure your News playlist format is set to Your News Update. Then search for Vox in the list of sources and tap the star so it turns blue.


How to set Vox as a preferred source for Your News Update on the Google Home app. Taylor Maycan/Vox

Once you complete these steps, you’re ready to listen.
Start by telling your Google smart speaker, “Okay, Google, play Google News.” Move through your news feed by asking to “skip” or “play the next story” (but don’t forget to say “Okay, Google” first).
Tip: Shorter stories tend to play first; longer stories come later.
Prefer to listen on a phone or tablet? Open the Google Assistant or the Google Home app, then tap the microphone icon to ask for Your News Update. You can also tap the keyboard icon to type the request.


How to listen to Your News Update on mobile on the Google Assistant app. Taylor Maycan/Vox

You can read more about Your News Update and learn more about how to listen to podcasts and news through the Google Assistant. And, of course, you can listen to Vox’s full slate of podcasts any time on your favorite device.

  
  
  
      




The fake news is coming from inside the White House, and it could influence who lives there next.

Earlier this month, Harvard’s Berkman Klein Center for Internet & Society released a working paper studying mail-in voting disinformation campaigns. Using a quantitative and qualitative study of millions of tweets and tens of thousands of Facebook posts and news stories about mail-in voter fraud — the persistent but debunked idea that people are illegally using mail-in ballots to meaningfully sway elections — the study found that President Trump was largely responsible for spreading that disinformation.

In particular, the study found that the president himself, on Twitter as well as through press conferences and interviews, was the main source of falsehoods about mail-in voter fraud. In turn, right-wing media organizations and media organizations in general abetted the spread of that disinformation by uncritically parroting it without full context.

The intention is to get people to believe mail-in voting is faulty precisely as 80 million people are set to vote by mail this year, due to the coronavirus. Uncertainty about the mail-in voting process has the potential to subdue voter turnout and undermine faith in the outcome of the upcoming election.

This is hardly the only disinformation campaign being led by Trump this year. A recent Cornell study found the president to be the largest driver of coronavirus disinformation as well. In conjunction with lies about mail-in voting, these two campaigns not only jeopardize the health of millions of Americans but also stand to sway the election results.

We spoke with the lead author of the mail-in voting study, Yochai Benkler, about how this disinformation campaign works, why it’s so insidious, and what can be done about it. This interview has been edited for brevity and clarity.

Rani Molla

What’s your main takeaway from mail-in voting disinformation?

Yochai Benkler

This looks like a political- and media elite-driven disinformation campaign by the Republican Party, led by Donald Trump, directly from the media elites to mass media — and then social media sort of chimes in and secondarily amplifies it and circulates it around. But the primary driver is Trump, his campaign, the RNC, and other sorts of Republican leadership. And the primary vector is straight through mass media: Fox media and talk radio on the right, and the rest of the media ecosystem.

Rani Molla

So this sort of contradicts the narrative that misinformation wells up from the dark corners of the internet, from 8chan, QAnon, and things like that. This is from the top down, from the president.

Yochai Benkler

Absolutely. And I want to clarify, though, that I don’t think that because we found that’s true in this critically important area that nothing matters about QAnon or nothing matters about the internet. That’s an easy way to misunderstand what we’re saying. What we found is that in this area — and the truth is it’s also true in many things related to Covid and masks and a variety of others — that is simply not happening.

Rani Molla

You said that the media also perpetuates disinformation, and I get that with Fox News and things like that, but are you also saying that, just by covering it, the media is doing so, too?

“The voter fraud frame has been used by Republicans to set barriers on a background theory that they gain electorally from depressing turnout, particularly depressing turnout in urban and minority populations”
Yochai Benkler

It depends on how significant the intervention is, and it depends even more on how you’ll cover it. So, not every time the president says something it’s news just because he said it. It doesn’t have to be. If yesterday there was a big story about the highest job losses ever and today the president comes out and says something outrageous about cutting funding to states, you shouldn’t fall into the trap of saying, “Oh, there’s a new agenda item, let’s put that in the headline,” and forget about yesterday’s. And sometimes he uses it that way: “There’s bad news on the economy. There’s bad news on Covid. Let me say something outrageous.” And immediately you change the agenda.

Rani Molla

So how should and shouldn’t presidential misinformation be covered?

Yochai Benkler

So if you’re reporting: “On Thursday, the president said that mail-in voter fraud is a major issue, Democrats objected. Republicans said the Democrats are trying to steal the election, etc.” You’re creating a problem.

If you say, “On Thursday, once again, the president falsely stated that mail-in voting is full of fraud. Consensus of all of the studies that have been made independently is that mail-in voting is safe and an important way to vote during a pandemic.” That’s different. Which of the two you do is really what shapes what the people who don’t yet have a view will think about it.

Yes, you have to cover him because he’s the president. No, not every tweet is news. Yes, everything needs 15 to 30 minutes more of thought on how you frame it. You need an editorial equivalent of a four-second lag to figure out what you’re not carrying and why you’re not carrying it. Why is he trying to change the subject if he’s trying to change the subject?

Rani Molla

What is the point of misinformation around mail-in voting?

Yochai Benkler

The voter fraud frame has been used by Republicans to set barriers on a background theory that they gain electorally from depressing turnout, particularly depressing turnout in urban and minority populations.

Rani Molla

And disinformation about mail-in voting dovetails with the misinformation around the coronavirus pandemic.

Yochai Benkler

The president and Republican Party have been trying, have been persuading their followers that Covid-19 is not a big issue. There’s a real gap in personal concern about the disease between Republicans and Democrats, which presumably will translate at some level into who does and doesn’t show up at the polls because they’re afraid to get sick. And so if you’re able to eliminate mail-in voting completely, let’s say for the moment, you have a built-in advantage from the fact that you’ve already propagandized to your followers that Covid-19 is not a big deal, right?

Rani Molla

Why did you focus on mail-in voting disinformation in this study rather than all the other disinformation out there?

Yochai Benkler

I want to distinguish here between narrow things like QAnon — Democrats running a global pedophilia ring, which even if they have tens of thousands, even if they have hundreds of thousands, even if there are 2 million people who believe it, that’s not going to move a 330 million-person democracy one way or the other — and questions of, “Who’s to blame for the economic collapse? Is it directly tied to responsibility for dealing with Covid or not? How poorly are we doing? Are we doing poorly? Or how bad is the disease? And how poorly was it managed?” These are the big things that are weighing at the 100 million-voter level when you look at surveys of what people care about.

Rani Molla

From a historical standpoint, have politicians and their attendant news organizations always spread disinformation at this level, or is this especially bad because we have President Trump who’s so forthright about disinformation? Is this worse than it used to be? Or is this just par for the course?

Yochai Benkler

Ask people in the Middle East about whether weapons of mass destruction were worse or better as a disinformation campaign at a national level. We tend to have such a strong sense of the crisis of the moment. Think of the 1960s, where the president, the leading presidential candidate, and the two major civil rights leaders were assassinated in the span of six years. Yeah, things are bad. But democracy in America has always been attacked in many ways internally.

“There’s an elite that wants power. And it’s using and developing the techniques, the most cutting-edge techniques it can, to control the population.”
Rani Molla

Good point. Let’s try a different tack. The refrain that I keep hearing is that social media makes everything worse since you’re able to spread this disinformation at scale. Do disinformation campaigns last longer, or are they more powerful because of social media?

Yochai Benkler

You think that North Korea is strong on social media? You think that Pravda was social media? The Committee on Public Information in World War I was the origin. This is pre-radio, we’re talking about newspapers and the penny press and posters. As soon as the public, as the masses are invented at the beginning of the 20th century, we see the emergence of propaganda as a discipline. There’s an elite that wants power. And it’s using and developing the techniques, the most cutting-edge techniques it can, to control the population.

Rani Molla

So social media is just the technology of the day with which they’re doing the same thing?

Yochai Benkler

As it turns out, even that’s an overstatement. Because Fox News, if you look at all of the Pew surveys from the last seven or eight months, the group of Republicans who are most on message are the people who say that they only get news from Fox News and talk radio. Anybody who gets news from anything else, which includes online sources, is less single-mindedly loyal to the perspective of the party. So if all you consume is Pravda, that is to say Fox News and talk radio, you believe in the party line. If you get a little bit of samizdat on the side, you’re not quite so sure.

Rani Molla

Recently Facebook banned QAnon and Holocaust denial, and took down a Trump post that incorrectly said the flu is more deadly than Covid. Twitter is noting when the president tweets misinformation and is generally trying to dissuade people from sharing falsehoods. What’s your take on efforts by social media companies to curtail disinformation on their platforms?

Yochai Benkler

On these big campaigns — the economy, Covid, and voter fraud — I think it’s okay for them to do it. They can try, particularly when the people they’re constraining are known elites. I think that’s a place where using powerful corporate power to contain powerful political elites is not too bad.

I doubt that it will be hugely influential if tomorrow you shut down Trump’s Twitter handle. He would not meaningfully lose access to the people he wants to lead because as it is, even on this campaign, he uses his daily press briefings and picks up the phone to Maria Bartiromo or Sean Hannity on the radio and he makes his comment, so he’ll go find a different venue.

Rani Molla

What’s the downside in trying?

Yochai Benkler

I am worried about a handful of very powerful corporations getting legitimacy to navigate public discourse. We are facing such a challenge that we’re at risk of making bad precedent. Just like traditional journalists want to appear neutral, there’s enormous pressure on Facebook and Twitter not to appear biased against the right. So you have completely asymmetric levels of propaganda, which means if you actually had neutral application of the policies, you’d get massively more enforcement against right-wing than left-wing stories, just because that’s the origin of most of the propaganda at the moment. But if you’re trying to actually look even-handed, then suddenly, you’re going to make up some antifa groups that are not antifa groups at all, but happen to have a lefty orientation, you’re going to shut them down. You’re going to look even-handed under conditions that are not actually symmetric and even.

I have a long-term concern about imagining that we can solve really foundational tensions in our democracy by giving more power to a tiny number of extremely powerful companies to shape how we talk about our relations in the society.

Will you help keep Vox free for all?

Millions of people rely on Vox to understand how the policy decisions made in Washington, from health care to unemployment to housing, could impact their lives. Our work is well-sourced, research-driven, and in-depth. And that kind of work takes resources. Even after the economy recovers, advertising alone will never be enough to support it. If you have already made a contribution to Vox, thank you. If you haven’t, help us keep our journalism free for everyone by making a financial contribution today, from as little as $3.

    
  
    
    
      
        
  


  






      
    
    
  
  



In what is fast becoming a pattern for voter registration in the 2020 presidential election, technical difficulties befell another state’s voter registration website on the last day citizens can register. This time, Virginians were locked out of the ability to register online.
For about six hours on Tuesday, visitors to Virginia’s voter registration website were greeted with a message saying it was “temporarily unavailable” due to a “network outage” that could not have come at a worse time. 
  
  
    
    
      
        
  


  






      
    
    
  
  


The cause, according to the state’s Information Technology Agency, was that a Verizon fiber optic cable that provided internet to the state’s Enterprise Solutions Center was somehow cut, taking the voter registration site down with it. 
“A crew working overnight on a roadside utilities project severed fiber cables,” Krys Grondorf of Verizon told Recode. “It was not a Verizon crew that severed the cable — it was a county roadside utilities project.”

  
    Related
  

  
    Everything you need to know about voting in 2020 (but were afraid to ask)
  

The website was back up and running by around 3:30 pm, and Virginia’s attorney general requested a deadline extension before the end of the day.
He had good news on Wednesday morning: Voter registration has been extended, giving Virginians until October 15 at 11:59 pm to register.


 BREAKING  Judge says he will GRANT our request to extend voter registration deadline until 11:59pm on Thursday, October 15. Register to vote now!!— Mark Herring (@MarkHerringVA) October 14, 2020



There is precedent for this extension: In 2016, Virginia’s site crashed just before the deadline due to too much traffic. A federal judge ordered the state to extend the deadline by 36 hours to make up for it.
Last week, Florida’s voter registration website crashed in the final hours of registration. While the Florida secretary of state initially said the outage was brief, it turned out to last for several hours, potentially preventing tens of thousands of registrations. The state ended up extending registration for seven hours the next day, albeit in the middle of the day and with little notice.
Update, October 14, 10 am ET: Updated with deadline extension news. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  Two college students started Morning Brew five years ago. Now they’re in talks to sell their business newsletter company to Business Insider, according to sources familiar with the two companies.
It’s unclear how much Business Insider intends to pay for Morning Brew, which says it will turn a profit on revenue of $20 million this year. But people who have talked to the company’s founders believe they expect to sell it for more than $50 million, and possibly much more; the Wall Street Journal reports that the deal could be worth more than $75 million.
This is an interesting deal, if it gets completed. Business Insider is a digital publisher that got its start with a mix of high-volume clickbait and the occasional scoop, but has recently made a push into more sober journalism it wants to sell via subscriptions. Morning Brew is a business-focused publisher that reassembles news into bite-sized chunks for its millennial audience
You can imagine the logic behind this one: Business Insider gets a company with 2 million subscribers to its free newsletter, which it can try to convert into paying subscribers. And Morning Brew’s team of 50 people gets more resources to help it build out more iterations of its newsletter and other products, like a podcast arm.
A deal could be a huge windfall for Austin Rief and Alex Lieberman, Morning Brew’s co-founders, who started the company as undergrads at the University of Michigan. The two men said they’ve only raised $750,000 from friends and family over the course of the company’s history, which likely means they would keep the majority of the proceeds for themselves.
“I can’t confirm anything, but speaking hypothetically, we’d be happy to be in talks with them,” said Business Insider CEO Henry Blodget, via text message. “Alex and Austin are amazing entrepreneurs, and it’s a terrific company.”
The deal would also underscore the media industry’s current fascination with email newsletters, which are a very old distribution model that’s once again in favor. 
For instance: Axios, the politics-focused startup that launched in 2017, is reportedly on track to do $58 million in revenue this year, largely on the backs of its popular newsletters. And Substack, a venture-backed company that helps individual writers launch and run their own newsletters, has generated a lot of media buzz and has brought several high-profile writers into its stable. Two of them — Andrew Sullivan and Casey Newton — used to work for Vox Media, which owns Recode.
Business Insider was founded in 2007 by Henry Blodget, who had previously made a name for himself as a Wall Street analyst during the dot-com boom but was later charged with securities fraud (Blodget settled with financial regulators without admitting or denying the charges). In 2015, he sold the company to German publisher Axel Springer in a deal that valued his company at more than $440 million. (Disclosure: I worked for Blodget at Silicon Alley Insider, a predecessor to Business Insider, and made money when he sold the company.)
In February, Axel Springer told investors that Business Insider “expects significant growth in revenues,” and that in addition to ad revenues, its three-year-old subscription business would be a “key driver of revenue growth.” The company also said it would make make “extensive investments ... especially in the areas of journalism and product & technology.”

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  




Kyle Potter, the editor of the travel deals website Thrifty Traveler, is used to flying a lot. But things haven’t been the same since the pandemic. Back in June, when he flew for the first time since Covid-19 hit the United States, it was an eerie experience. 
“Pretty much everyone just really feels on edge. I think in a lot of cases, people feel kind of guilty about being on planes right now,” Potter told Recode. “It just kind of all adds up into an unfamiliar experience.” 
The notion of being stuck in a metal tube filled with lots of other people and recycled air seems scary in a pandemic, which is why we’re seeing so many car vacations this year. Plus, many people don’t have the money or motivation to travel long distances. There are those who must travel for whatever reason, and making that experience as tolerable as possible is essential to keep the airlines in business.
Experts estimate it could be years before people fly as often as they did. This bodes very poorly for the flight industry. Months after air travel plummeted at the dawn of the pandemic, money from the government bailout is drying up. That means airlines are now laying off or furloughing tens of thousands of employees, while others are being asked to take pay cuts. 
That’s left many airlines and airport-related businesses scrambling for ways to convince travelers that it’s safe — and somewhat easy — to take to the skies. Now some are providing their own Covid-19 testing to travelers, while others are already looking for vaccination-related business opportunities. Overall, these moves hint that more aggressive measures to combat contagion could become a permanent fixture in the future of flying. 
Airlines and airports are incorporating Covid-19 testing into travel 
In the US, airlines including United, American Airlines, and Hawaiian Airlines are offering options for Covid-19 testing to passengers traveling to the state of Hawaii. These will include at-home tests, drive-through testing, and in-person tests at the airport. With a negative result in hand, travelers will be exempted from the state’s two-week quarantine requirements for new arrivals. American Airlines plans to roll out similar programs for some people traveling to Jamaica and Costa Rica. 
Potter expects this trend to continue as international travel slowly resumes. “For all of the countries that are off-limits to Americans — which, to be honest, is most of them — that is going to be … the condition for allowing travel to resume,” he told Recode. “Anywhere you can think of, with very few exceptions, that is going to become the norm, especially for international travel.”

  
    Related
  

  
    The pandemic could change air travel forever
  

But what about the privacy of people who test positive? Aaron McMillan, United Airlines’ operations policy and support managing director, told Recode that the airline itself wouldn’t keep records of a traveler’s personal health information. But United would get notified from a testing partner “that said customer wouldn’t be able to travel that day” and the airline “would make the necessary arrangements,” he said. Health experts at the airport’s testing site would then provide that United traveler with next steps for receiving care.
For some international travelers, in-airport tests have already become a norm as they travel between different countries. Airports in the US, including Tampa International Airport and Oakland Airport, are beginning to open Covid-19 testing sites of their own, too, and more are set to join. 
Even XpresSpa, a firm that normally provides in-terminal spa treatments for travelers, announced this month that, in Newark Airport and JFK, it will offer Covid-19 tests, which cost between $75 and $200 depending on the test or tests you get. Travelers can walk in or make scheduled appointments that take place in the airports’ terminals. The company has also said it’s in conversation with “health passport apps” and looking into creating so-called air bridges between specific cities. The company’s CEO, Doug Satzman, told Recode that, in the future, he even imagines XpresSpa helping with vaccinations for Covid-19 or other illnesses.  
The Transportation Security Administration has also made some changes. Instead of a human officer confirming the validity of someone’s identification card, TSA is now piloting a self-service checkpoint using a biometrics-based algorithm designed to confirm that a live photo of a person matches their ID. This builds on the agency’s previous pre-pandemic tests of facial recognition technology, which has also worried privacy experts over potential racial bias, inaccuracy, and surveillance applications. 
Then there’s Clear, a company that’s best known for allowing people to bypass TSA screening and head straight toward their carry-on baggage inspection. In the pandemic, Clear has started to offer Health Pass, a biometric-based way of tracking peoples’ health status that was used, notably, to help conduct the National Hockey League playoffs in Canada. (In the NHL case, at least, there have been no recent confirmed Covid-19 cases stemming from the bubble the league created.) Despite the concerns of privacy experts and some members of Congress, Clear has now announced that it will integrate Covid-19 test lab results from LabCorp and Quest into Health Pass.
Clear told Recode that it’s in discussion with airlines and airports about integrating Health Pass into existing security screening processes for the general public. Clear employees are already using the health-based version of the tech at the airport; United, one of the company’s partners, told Recode that it might support Clear’s plans for Covid-19 testing. Amid the pandemic, Clear has taken the opportunity to expand its business model, transitioning from a service that made flying more hassle-free to a business built around confirming peoples’ identities everywhere, as OneZero recently reported. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Amid the pandemic, Clear has expanded from a service mainly focused on air travel to offer a new tool called Health Pass.
      
      
        Robyn Beck/AFP via Getty Images
      
    
  


But the air travel industry having business opportunities with Covid-19 tests reflects the broader failure of the US approach to the public health crisis, argues Kenneth Goodman, director of the Institute for Bioethics at the University of Miami’s Miller School of Medicine. 
“The very idea that some people with an extra 200 or 150 — whatever it costs — dollars can buy a test is a sad sign of where our country and where the world is,” Goodman said. “This was not a business opportunity. This was an attempt to try and stop there being too many dead people. And we’ve made a total and complete hash of it.” 
Goodman also emphasized that testing negative on a Covid-19 test is by no means an assurance that you haven’t just contracted the illness or that you won’t contract it at the airport or on a plane. 
Air travel Covid-19 tests could be a peek at what’s to come
Still, these changes highlight the new normal for air travel anxieties. Potter, of the Thrifty Traveler, says that before the pandemic, fliers were concerned about two primary things: the price of a ticket and in-flight services. Now, many in-flight services are gone, and airlines are most concerned with convincing fliers that it’s safe to be aboard. 
The Centers for Disease Control and Prevention, which has issued guidance for air travel, estimated last month that close to 11,000 people were exposed to Covid-19 on flights, according to the Washington Post. As the Mayo Clinic notes, airplanes have sophisticated air filtration systems that can limit the spread of the virus, but a few studies have suggested that Covid-19 can be spread not just on flights themselves but also in security lines and airport terminals. 
“To really boil it down, airlines have focused almost entirely on convincing people to get back on planes, convincing them that it’s safe to do so, and trying to get them on and off the plane and through the airport as fast and as safely as possible,” he told Recode. 
Airlines and airports have made a point of showing off their cleaning procedures, including litanies of electrostatic sprayers, robot cleaners, and questionable fever-detection tech. 
Another move is toward more contactless and biometric technologies. The Australian company Elenium, for instance, has debuted contactless Covid-19 airport kiosks that can hear peoples’ voices and respond to their head movements. These kiosks are now installed in airports in the Middle East, the US, and Australia.
“Even when the Covid crisis dissipates, when there is a vaccine, I think that both the public and business will be sensitive to the impact of future health events, even bad flu seasons,” Elenium CEO Ilya Gutin told Recode in an email. He noted that the company received more leads in the four weeks after announcing its Covid-19-focused kiosk than it had in the past four years. 
As airlines begin incorporating Covid-19 testing, they’re also considering what their plans would look like when a vaccine finally becomes available to the public. McMillan, from United Airlines, told Recode that the company was considering how technology might be able to tie information about whether someone was healthy enough to fly to their boarding passes. “We see that evolving into vaccination records,” he said. 
“Much like 9/11 changed the security process at the airport, this pandemic will change how people travel with health documentation going forward for sure,” McMillan added, echoing what many airline industry experts told Recode earlier this year. 
But Potter is skeptical that people will be flying with the same enthusiasm anytime soon. Even a vaccine, he said, wouldn’t be enough to make flying feel normal again. 
“It was really one of the first major industries to feel the pain of this,” he told Recode. “I think it’s probably going to be one of the last to feel the relief, whenever that happens.”
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.


  
  
  
      





  Twitter announced on Friday — less than 30 days ahead of the US election — that it’s enacting a series of significant changes in order to make it harder to spread election misinformation on its platform. It’s one of the most aggressive series of actions any social media company has taken yet to stop the spread of misinformation on their platforms.
The changes include prompting people not to retweet without adding their own commentary, turning off automatic recommendations for other people’s tweets, and adding more context to its Trending section. Twitter will also start putting more warning labels on misleading tweets by US politicians and accounts with more than 100,000 followers, and block users from “liking” or replying to those tweets. And if a politician declares premature victory before it’s verified by independent sources, Twitter will label the tweet and direct users to its voter information page.
Taken as a whole, the moves represent the sort of significant systemic change that some misinformation experts say is necessary to slow the spread of viral lies on the platform, especially those about the election process and results. Facebook has similarly tried to limit voting misinformation, but its most recent action to ban political ads following the election has received less praise than Twitter’s new policies. But the true test will be whether Twitter and Facebook can execute on their promises, and if the changes rolled out just a few weeks ahead of the election will actually be effective.
“As always, the big question for both platforms is around enforcement,” wrote Evelyn Douek, a researcher at Harvard Law School studying the regulation of online speech, in a message to Recode. “Will they be able to work quickly enough on November 3 and in the days following? So far, signs aren’t promising.” 
Twitter already has a policy of adding labels to misleading content that “may suppress participation or mislead people” about how to vote. But in recent cases when President Trump has tweeted misleading information about voting, it’s taken the platform several hours to add such labels. Facebook has similarly been criticized for its response time.
Already, President Trump has criticized Twitter for its new policy, with a campaign spokesperson calling it “attempting to silence voters and elected officials to influence our election” in a statement to the Washington Post. And the move comes at a time when Trump and Republican lawmakers have threatened to repeal a critical law, Section 230, that shields internet platforms like Twitter from legal liability over concerns of alleged, and unproven, anti-conservative bias.

  
    Related
  

  
    The Trump administration’s flawed plan to destroy the internet as we know it
  

Douek said that platforms “need to be moving much quicker and more comprehensively on actually applying their rules.” But, she added, if “introducing more friction is the only way to keep up with the content, then that’s what they should do.”
The concept of “friction” to which Douek is referring is the idea of slowing down the spread of misinformation on social media to give fact-checkers more time to correct it. It’s also an ideal that many misinformation experts have long advocated. Overall, misinformation experts, including Douek, lauded Twitter for introducing friction by nudging users to think twice before sharing misleading content. 
Twitter’s changes also target its fact-checking efforts on users who have real influence: the “blue checkmark” public figures, politicians, and power users with more than 100,000 followers. It’s a lot more effective to curb misinformation by concentrating on those users rather than ones with smaller audiences, experts say.
“I think there are a lot of positive things in these policy changes,” said Renee DiResta, a researcher on misinformation ecosystems at the Stanford Internet Observatory. “Of course, you’re never going to fix misinformation online or get rid of it all — people have always been wrong on the internet since the internet appeared. But this is more of a matter of, can you mitigate the directly harmful challenges associated with virality.”

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Florida’s track record on voting rights isn’t great, and Monday night provided another example for why that is. The state’s voter registration website crashed for several hours just before the deadline to vote in the general election. Although the state reopened voter registrations for seven hours to compensate for those missed during the crash, advocates say that wasn’t enough.
It is, however, all they have, as a federal district court judge on Friday denied their request to extend registration hours even longer — though not without some harsh words for the state and its long-standing issues with running elections.
“Notwithstanding the fact that cinemas across the country remain closed, somehow, I feel like I’ve seen this movie before,” Judge Mark Walker, of the Northern District of Florida court, wrote. “Just shy of a month from election day, with the earliest mail-in ballots beginning to be counted, Florida has done it again.”
Florida has one of the earliest voter registration deadlines in the country: October 5 by 11:59 pm. Florida’s voter registration website, RegisterToVoteFlorida.gov, crashed in the hours preceding that deadline, likely preventing tens of thousands of Floridians from being able to register. The state then extended the voter registration deadline to October 6 by 7 pm. 
According to a statement from Florida Secretary of State Laurel M. Lee, the site had “an unprecedented 1.1 million requests per hour,” and the enormous volume caused the crash. Lee added that her office would be “working with our state and federal law enforcement partners to ensure this was not a deliberate act against the voting process,” though later said in a statement to Recode that “we have not identified any evidence of interference or malicious activity impacting the site.”
During the crash, Lee’s tweets indicated that the interruption was brief and had been fixed by 6 pm, but several responses to those tweets said this was not the case. People complained that they had been trying for hours to register, and a Florida state representative tweeted just before 10 pm that the site still didn’t work. According to the Orlando Sentinel, the 7 pm deadline, announced at noon, was chosen to match how many hours the system was down. 
This is not the first time a Florida government website has crashed at the time it’s most needed. The state’s unemployment website, overwhelmed by the surge in traffic, was notoriously glitchy in the first months of the pandemic, leaving many Floridians unable to apply for benefits or having to wait weeks to do so. Though a recent audit had found hundreds of system errors, no action was taken to fix them before the pandemic.
This isn’t even the first time Florida’s voter registration website has failed. The Florida Democratic Party accused the state’s Republican leaders of “blatant voter suppression” and pointed to website crashes before the 2020 primaries, a routine maintenance downtime the weekend before National Voter Registration Day in 2019, and site glitches a few days before the general election deadline in 2018. The Florida ACLU also noted that the website has a history of crashes that, it said, the state has made no effort to repair.
“During every major election cycle, Florida’s voter registration website falters,” Gaby Guadalupe, deputy communications director at the ACLU of Florida, told Recode. “Advocates warned that this would happen again during this critical election. The reality is the state needs to own this mistake, find a more permanent solution, and fix the online registration system.”
It’s also not the first time Florida has been accused of disenfranchisement. Until recently, the state restricted anyone convicted of a felony from voting, even after they’d finished their sentence. This barred as many as 1.4 million people — 10 percent of the state’s population — from the right to vote in the state, and disproportionately affected Black people. In 2018, an amendment to the state’s constitution to restore voting rights to ex-felons won the popular vote, only for the Republican-led legislature to pass a law that required ex-felons to pay any financial penalties from their convictions before they could be allowed to vote. Activists have scrambled to help ex-felons pay those penalties and re-enfranchise as many of them as possible. The state’s attorney general has called for an investigation into that effort, citing “potential violations of election laws.”
Florida does, at least, make it relatively easy to vote by mail. Voters don’t need to provide an excuse when they request their ballot, they are able to track their ballot once they mail it back, and if their ballot is rejected, they must be told and given the opportunity to fix the ballot so it can be counted. That said, Florida also has a history of rejecting an excessive number of mail-in ballots.
Several state voting rights groups sued Gov. Ron DeSantis and Lee to extend the registration deadline by two more days, saying the state had plenty of time and warning to adequately prepare the website to handle anticipated increased traffic but didn’t do so. Judge Walker seemed to agree, castigating the state throughout his decision.
“In this case, potential voters attempted to perform their civic duty, to exercise their fundamental right, only to be thwarted, once again, by a state that seemingly is never prepared for an election,” Walker wrote. “This case is about failure on the part of a civil servant, whose responsibility is to run an election system, that will cost thousands of potential voters their fundamental right to vote in the upcoming election.”
Even so, Walker found, avoiding the “chaos” that another registration deadline extension would cause outweighed the harm done to those disenfranchised citizens.
The good news is that 50,000 Floridians registered to vote during the extended hours, mitigating some of the damage. A reliable website that didn’t crash when it was most needed, of course, would have mitigated that damaged completely.
“Floridians shouldn’t face unnecessary obstacles to their right to vote because of inadequate state software,” Guadalupe said.
Update, October 9, 1:30 pm ET: Updated to add court decision denying the deadline extension.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.


  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



On Tuesday, Facebook announced that it would be expanding its ban on QAnon, and has since begun a purge of Groups and Pages that reference the fringe, far-right conspiracy theory. 
The announcement came less than a week after the United States House of Representatives voted in favor of a resolution condemning QAnon that urged Americans to seek “information from authoritative sources and to engage in political debate from a common factual foundation.” The text of the bill referenced some of the worst parts of the QAnon theory, including its anti-Semitism and its undermining of legitimate child safety efforts, while also censuring “all other groups and ideologies that encourage people to destroy property and attack law enforcement.” 
Though 17 Republicans and one independent voted against the measure, the resolution ultimately passed with widespread bipartisan support. The condemnation on Capitol Hill was seemingly bolstered when Facebook announced that it would amp up its efforts to fight the conspiracy theory, and delete all Groups and Pages associated with it. But some wonder if Facebook has done enough.
The representative who co-sponsored the QAnon resolution, Rep. Tom Malinowski, says that while Facebook’s move is worthwhile, the company has not addressed the underlying problem that allowed the conspiracy theory to grow. QAnon, which the FBI has deemed a domestic terrorism threat, has already come into the mainstream after all. Malinowski, a Democrat from New Jersey, has faced death threats from the conspiracy theory’s supporters. Those attacks came in response to Malinowski’s support for the resolution and to an advertising campaign that’s been released by the National Republican Congressional Committee (and been flagged by fact-checkers as false). 
There’s disagreement about the best way to handle the social media presence of conspiracy theories and related groups. While Facebook has focused on content moderation, others have said that the focus on removing violating posts risks missing how social media algorithms can increasingly promote and polarize people into bubbles of conspiracy theory-focused content. 
Now, Malinowski tells Recode that the next step is changing the algorithms that helped QAnon gain a social media following in the first place, even if that means companies like Facebook end up making less money. He’s already working on legislation that would give Congress some power in regulating the algorithms themselves. 
A transcript of our interview follows, lightly edited for clarity and length.
Rebecca Heilweil
Can you start by explaining how you came to work on this resolution and why?
Rep. Tom Malinowski
I’ve been concerned about rising extremism in our country for a while and QAnon, particularly this year, as we saw evidence that recruitment to this conspiracy-mongering cult was growing exponentially. I thought, among other things that we need to be doing, that it would be helpful to demonstrate overwhelming bipartisan condemnation. Given President Trump’s apparent approval or, at least, non-disapproval of QAnon. I thought it would be good to show that both Republicans and Democrats are united against this nonsense.
Rebecca Heilweil
There are several extremist groups that seem to be gaining more attention in this country. What about QAnon particularly stood out to you?
Tom Malinowski
To be clear, I’m worried about many groups. I’m worried about white supremacist groups that are responsible for most violent terrorist attacks in our country over the last few years. I’m worried about some groups on the far left as well. But most of these other groups are not mass-membership organizations. They have caused extreme acts of violence, but they don’t have millions of people obsessively following them.
I think what’s particularly pernicious about QAnon is that it is immunizing millions of Americans against reality by teaching them not to trust objective sources of information — science and government experts — and to replace all of those things with an assortment of crazy conspiracy theories that have, at their heart, the old anti-Semitic blood libel, which tries to explain everything that happens in the world as a conspiracy of powerful people —many of whom happen to the Jewish — who are trying to control the world and are kidnapping your children. 
Rebecca Heilweil
What’s your reaction to news that Facebook is going to expand its enforcement against QAnon and remove related groups and pages — regardless of whether or not they appear to be promoting violence?
Tom Malinowski
So it’s a good move. I’m happy that they did it. I think it’s particularly powerful coming in the wake of the congressional condemnation. It’s a good one-two punch. Obviously, I’m interested in how effectively they’re going to implement it and whether it’s even possible to identify and crack down on all of the evasion tactics that people in the QAnon community are likely going to use, like changing their vocabulary and their means while delivering the same message.

  
    Related
  

  
    Facebook and Twitter said they would crack down on QAnon, but the delusion seems unstoppable
  

But I’m glad Facebook did this. My concern is that it’s not enough. I’ve never believed that it’s enough to just take down content and groups. ... I think the deeper problem is the design of the social network itself. This is like a farmer who suddenly discovers his fields are overgrown with noxious weeds. So he pulls out all the weeds, which is good, but never stops to consider how they spread in the first place.
So, on the one hand, it’s like they’re playing whack-a-mole with extremists without necessarily being willing to change the design of the social network that’s built to elevate extremism. If that remains the case, it’s a strike, it’s a very powerful blow against what we now call QAnon. But the phenomenon of disinformation and conspiracy theories and extremism spreading online will continue.
Rebecca Heilweil
Do you think that there’s a role for Congress or for legislators in regulating the kind of algorithms that these companies are producing?
Tom Malinowski
I am going to try to take that on. We are developing legislation right now that aims to take that challenge on. 
Rebecca Heilweil
What about people who saw this Facebook announcement and just said, “This is way too late”? It’s not necessarily meaningless, but it’s close to meaningless in the sense that, according to some, this was a problem that Facebook helped to create.
Tom Malinowski
That’s part of what I’m saying, but that’s the farmer analogy, right? You let the weeds grow. You may even have sown the seeds for the weeds, and then you’re ripping them out. I’m not going to criticize them for ripping out the weeds. They are doing now what they should be doing: taking these groups down. 
You know, we’re not monitoring this in real time, but there are researchers who do. And what we’ve heard in the last day or so is that Facebook is being very aggressive in taking these groups down, and Gab and Parler are now trying to welcome all the Q adherents being pushed away by Facebook. 
That’s a good sign, by the way, like the most avid believers make it to sites like that. But I think the “Save the Children soccer moms” are not likely to give up Facebook for an extreme platform. … So again, this is good. I don’t want to disparage it. 
But I don’t think it’s enough. It’s still the easier thing for Facebook to do. They can take down millions of Pages and Groups and posts without changing the design of their social network. And naturally, they don’t want to change the design of an incredibly profitable thing that they created. It may not be possible to really solve this problem without accepting that Facebook has to make less money.
Rebecca Heilweil
You had the goal of this being a bipartisan effort in working on this resolution. It seems, to a large extent, that it was. But not everyone ended up voting for it. I’m curious what you think of your colleagues who did not vote in favor of this?
Tom Malinowski
I was very satisfied with the final vote. ... We were hoping for something overwhelmingly bipartisan, and we got that. Given what’s going on in the Republican Party, I’m not surprised that 17 people couldn’t bring themselves to condemn an anti-Semitic conspiracy-mongering cult. But it’s still a disturbing sign of the times that that’s the case.
Rebecca Heilweil
In response to this resolution, it seems like you were attacked by QAnon. What are you taking away from that experience and what that has been like for you — as a person but also a member of Congress?
Tom Malinowski
I was attacked, not just in response to the resolution but in response to a Republican television ad that the NRCC has been running in my district now for three weeks. It falsely accuses me of doing exactly what QAnon suspects that powerful government officials are doing. I believe that they were deliberately playing to the paranoia that QAnon promotes, and the Q community obviously noticed, because the Q entity — whatever that is — posted a Republican’s press release about their ad to these millions of QAnon adherents. The result was immediate death threats to my office. 
That’s not the sort of thing you want to wake up to. It doesn’t change anything I’m going to do. I think the most disturbing part about it is that the NRCC expressed no shame, and has, in fact, doubled down.  
Rebecca Heilweil
What has this shown you about QAnon? What has it shown you about the extent of this problem?
Tom Malinowski
I guess it shows that QAnon is big enough that less responsible political operatives in the Republican Party see it as an opportunity, not a threat.
Rebecca Heilweil
If there are any takeaways you have from this resolution or from this Facebook ban about things that we should be thinking about given the upcoming election? 
Tom Malinowski
That’s a good question. The Facebook ban is very well timed. Given the risk of misinformation before and even more important immediately after the election, anything that tamps down the hysteria out there is particularly needed.
Rebecca Heilweil
What’s next in fighting this? After the ban, you hinted at coming legislation. Is there anything else that you think we need to address?
Tom Malinowski
My focus is going to be on the social media companies, and in particular, on the way in which their algorithms have elevated and amplified content that allows extremist groups to grow.
The algorithms know what you’re afraid of, and they send more of it your way. They know what you’re angry at, and they send more of it your way. The companies understand that the fear and anger are what keep us glued to our screens. And so it’s in their interest to give us more and more extreme versions of what we’re scared of and angry about. And as long as that continues to happen, we’re gonna have this problem.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.


  
  
  
      




    
    
  

  Facebook is going to temporarily ban all political ads … but only after the 2020 election, a move that solves neither its organic content problem nor the problematic political ads appearing on its platform prior to voting.
On Wednesday, the social media giant announced that it will temporarily stop running social, electoral, and political ads in the United States after the polls close on Election Day, November 3. The measure is an effort “to reduce opportunities for confusion and abuse,” wrote Guy Rosen, Facebook’s vice president of integrity, in a blog post announcing the decision. The company will notify advertisers once it lifts the policy post-election, but it didn’t indicate when that would be. In early September, Facebook said it would ban new political ads the week before the election, but ads that have already been in the mix prior to then will continue to appear in News Feeds.
Also on Wednesday, Facebook said it would ban and remove posts that seek to intimidate voters, including ones that encourage poll watching “when those calls used militarized language or suggest that the goal is to intimidate, exert control, or display power over election officials and voters.”
These announcements are the latest in a series of small, slow, and iterative measures Facebook has introduced in recent months related to US politics and elections. And while they’re better than doing nothing, they are also too little, too late.
There’s a lot a political ad ban doesn’t do — it doesn’t stop politicians from lying in ads in the days leading up to the election, and it doesn’t stop giving political campaigns the ability to hyper-target ads to tiny groups of voters with very specific messaging. (Microtargeting makes it super easy to precisely target negative and misleading ads to certain voters, and it makes it harder for opponents and other groups to know those ads are out there and counter them.) Plus, banning political ads after the election doesn’t solve the, you know, before-the-election problem. 
Some political strategists also argue that clamping down on political ads online hurts small campaigns more than it does the big ones. Facebook ads are a lot less expensive to run than television commercials — which means campaigns with big budgets can go to TV, while campaigns with small budgets can’t. 
Also: Ads are just a small part of the equation. Facebook’s role in presenting voters with political information, disinformation, and conspiracy theories stretches far beyond advertising, and focusing too much on advertising allows it and other tech platforms to avoid the bigger problem: organic content. That means the type of stuff that goes on the platform for free — such as a false story in 2016 claiming Pope Francis had endorsed Donald Trump, or an unsubstantiated claim made by the president about mail-in-voting over the summer. 
Misleading and polarizing organic content spreads fast and far on the platform all the time because social media thrives on engagement, and what engages people is content that evokes strong emotions. A political campaign doesn’t need to pay for a political ad to spread lies claiming Elizabeth Warren wasn’t born in the US or Marco Rubio has six secret love children — they can just post it. 
The dangerous, preposterous QAnon conspiracy movement, which has shifted from the fringes to the mainstream, is a perfect example of social media’s failures. It has flourished on places like Facebook, Instagram, and Twitter — not because of ads, but because of organic content. Facebook finally banned QAnon this week, but it has already reached far and wide on its platform, as Recode’s Shirin Ghaffary recently explained:
The theory continues to grow online, in both the number of followers and the strength of its political influence in the Republican Party. The growing political clout of the movement is especially worrisome for misinformation researchers who say QAnon is potentially becoming one of the largest networks of extremism in the United States. QAnon is gaining broad appeal not just with the extremely online, male-dominated, 4chan message board crowd, where QAnon was first born; it’s also increasingly popular with suburban moms and yoga-loving wellness gurus on Instagram and Twitter.
Misinformation about voter fraud and the election is spreading — and it’s not relying on paid ads to do it. While Facebook tries to catch misinformation and put warnings on it, falsities travel a lot faster than its content moderators. In a hypothetical post-election scenario where Joe Biden wins the election but President Trump refuses to concede or insists the election was rigged, he doesn’t need an ad to spread that sort of lie — again, he can just do it in a post. Facebook says it will attach an “informational label” to content seeking to delegitimize the outcome of the election. However, it seems unlikely it would take down such posts or outright fact-check them, given how reluctant it’s been to take such actions in the past. 
Will it work? When I consider my own anecdotal experience, I doubt it. I spent hours browsing the Facebook pages of anti-maskers for a story over the summer and encountered multiple people who had such labels on content they shared. They just dismissed them by claiming Facebook was censoring them or hiding the truth. They had also often developed their beliefs because of content they saw on Facebook or other online platforms.
In a September 3 post, Facebook CEO Mark Zuckerberg wrote that the 2020 election is “not going to be business as usual.” But the iterative measures Facebook has introduced so far seem to be exactly that — business as usual.

  
  
  
      




  Facebook is going to temporarily ban all political ads … but only after the 2020 election, a move that solves neither its organic content problem nor the problematic political ads appearing on its platform prior to voting.
On Wednesday, the social media giant announced that it will temporarily stop running social, electoral, and political ads in the United States after the polls close on Election Day, November 3. The measure is an effort “to reduce opportunities for confusion and abuse,” wrote Guy Rosen, Facebook’s vice president of integrity, in a blog post announcing the decision. The company will notify advertisers once it lifts the policy post-election, but it didn’t indicate when that would be. In early September, Facebook said it would ban new political ads the week before the election, but ads that have already been in the mix prior to then will continue to appear in News Feeds.
Also on Wednesday, Facebook said it would ban and remove posts that seek to intimidate voters, including ones that encourage poll watching “when those calls used militarized language or suggest that the goal is to intimidate, exert control, or display power over election officials and voters.”
These announcements are the latest in a series of small, slow, and iterative measures Facebook has introduced in recent months related to US politics and elections. And while they’re better than doing nothing, they are also too little, too late.
There’s a lot a political ad ban doesn’t do — it doesn’t stop politicians from lying in ads in the days leading up to the election, and it doesn’t stop giving political campaigns the ability to hyper-target ads to tiny groups of voters with very specific messaging. (Microtargeting makes it super easy to precisely target negative and misleading ads to certain voters, and it makes it harder for opponents and other groups to know those ads are out there and counter them.) Plus, banning political ads after the election doesn’t solve the, you know, before-the-election problem. 
Some political strategists also argue that clamping down on political ads online hurts small campaigns more than it does the big ones. Facebook ads are a lot less expensive to run than television commercials — which means campaigns with big budgets can go to TV, while campaigns with small budgets can’t. 
Also: Ads are just a small part of the equation. Facebook’s role in presenting voters with political information, disinformation, and conspiracy theories stretches far beyond advertising, and focusing too much on advertising allows it and other tech platforms to avoid the bigger problem: organic content. That means the type of stuff that goes on the platform for free — such as a false story in 2016 claiming Pope Francis had endorsed Donald Trump, or an unsubstantiated claim made by the president about mail-in-voting over the summer. 
Misleading and polarizing organic content spreads fast and far on the platform all the time because social media thrives on engagement, and what engages people is content that evokes strong emotions. A political campaign doesn’t need to pay for a political ad to spread lies claiming Elizabeth Warren wasn’t born in the US or Marco Rubio has six secret love children — they can just post it. 
The dangerous, preposterous QAnon conspiracy movement, which has shifted from the fringes to the mainstream, is a perfect example of social media’s failures. It has flourished on places like Facebook, Instagram, and Twitter — not because of ads, but because of organic content. Facebook finally banned QAnon this week, but it has already reached far and wide on its platform, as Recode’s Shirin Ghaffary recently explained:
The theory continues to grow online, in both the number of followers and the strength of its political influence in the Republican Party. The growing political clout of the movement is especially worrisome for misinformation researchers who say QAnon is potentially becoming one of the largest networks of extremism in the United States. QAnon is gaining broad appeal not just with the extremely online, male-dominated, 4chan message board crowd, where QAnon was first born; it’s also increasingly popular with suburban moms and yoga-loving wellness gurus on Instagram and Twitter.
Misinformation about voter fraud and the election is spreading — and it’s not relying on paid ads to do it. While Facebook tries to catch misinformation and put warnings on it, falsities travel a lot faster than its content moderators. In a hypothetical post-election scenario where Joe Biden wins the election but President Trump refuses to concede or insists the election was rigged, he doesn’t need an ad to spread that sort of lie — again, he can just do it in a post. Facebook says it will attach an “informational label” to content seeking to delegitimize the outcome of the election. However, it seems unlikely it would take down such posts or outright fact-check them, given how reluctant it’s been to take such actions in the past. 
Will it work? When I consider my own anecdotal experience, I doubt it. I spent hours browsing the Facebook pages of anti-maskers for a story over the summer and encountered multiple people who had such labels on content they shared. They just dismissed them by claiming Facebook was censoring them or hiding the truth. They had also often developed their beliefs because of content they saw on Facebook or other online platforms.
In a September 3 post, Facebook CEO Mark Zuckerberg wrote that the 2020 election is “not going to be business as usual.” But the iterative measures Facebook has introduced so far seem to be exactly that — business as usual.

  
  
  
      




  Much of the United States woke up last Friday to the news that President Trump had tested positive for the coronavirus. Many turned to social media with theories about how Trump was faking it or had been targeted by an assassination plot. The most popular conspiratorial idea, however, was rooted in a familiar hoax: hydroxychloroquine.
Mentions of hydroxychloroquine spiked in the hours and days following the president’s Covid-19 diagnosis, according to data from the media intelligence firm Zignal Labs, which monitors misinformation on social media, traditional media, and other online sources. The majority of these mentions involved people calling for Trump to be treated with hydroxychloroquine, a malaria drug that’s been widely shown as ineffective as a Covid-19 treatment. Trump has been treated with an experimental antibody treatment developed by Regeneron, as well as remdesivir and dexamethasone. 
The prevalence of online discussions around hydroxychloroquine highlights how misinformation about the drug — much of which was promoted by Trump himself — has become deeply ingrained in the public consciousness. And the durability of the hydroxychloroquine myth suggests that the endless stream of false and misleading claims from Trump that downplay the severity of Covid-19 will have a lasting negative impact. As a Cornell University study recently found, the president is the “single largest driver of misinformation around Covid.” It now appears that this so-called infodemic is also shaping public conversation around Trump’s own illness.
The volume of online discussion about Trump and hydroxychloroquine versus other misinformation wasn’t even close. From October 1 to 5, there were 336,286 mentions of hydroxychloroquine related to Trump’s diagnosis, and 106,000 specifically included calls for Trump to be treated with hydroxychloroquine or links to articles that called for that treatment, according to Zignal Labs. A smaller number of those mentions pointed out that Trump received several treatments but that hydroxychloroquine was not one of them. Mentions of all these phrases were almost nonexistent in the days leading up to Trump’s diagnosis.
There were newer conspiracy theories, too. The next most popular misinformation topic were allusions to the idea that Trump was faking the illness, of which there were 86,977. Conspiracy theories about a plot to kill the president — including claims that Trump was intentionally infected at the debate and that teams of “deep state” assassins had pursued the president — came next, with about 33,000 mentions. There were also 13,768 mentions about how masks don’t help to prevent transmission of the virus, which pointed to senators who were infected but had been seen wearing masks.

!function(){"use strict";window.addEventListener("message",(function(a){if(void 0!==a.data["datawrapper-height"])for(var e in a.data["datawrapper-height"]){var t=document.getElementById("datawrapper-chart-"+e)||document.querySelector("iframe[src*='"+e+"']");t&&(t.style.height=a.data["datawrapper-height"][e]+"px")}}))}();


While Trump’s ongoing battle with Covid-19 is bound to fuel speculation about how and why the president got infected, the immediate surge of discussion about treating him with hydroxychloroquine shouldn’t come as a huge surprise. As early as March, some experts said the drug was an effective treatment for the disease, a claim that Trump promoted to his supporters for weeks. Trump’s doctors even said in early June — months before he tested positive for the coronavirus — that the president underwent a two-week course of hydroxychloroquine “safely and without side effects.” Less than two weeks later, the Food and Drug Administration (FDA) said that the drug was “unlikely to produce an antiviral effect.” Speculation about hydroxychloroquine has nevertheless continued on mainstream conservative media outlets like Breitbart, Fox News, and the Federalist.
“To be clear, because hydroxychloroquine got so much attention from the president, and was subsequently studied so heavily, it’s probably one of the things in the pandemic where the science is the most settled,” Kellie Owens, a health researcher at Data & Society, told Recode. “The most recent meta-analysis of randomized controlled trials that I’ve seen showed no benefit to hydroxychloroquine, and suggests potential increases in mortality.”
As far as the public knows, Trump has not received hydroxychloroquine since he tested positive for the coronavirus. And why would he? It’s a potentially dangerous treatment. 
Why hundreds of thousands of people would call for the president to receive the drug is not quite as simple, but the situation ultimately boils down to the amount of time and attention hydroxychloroquine has gotten in the press and on social media. Consider late July, when Trump and his son Donald Trump Jr. shared a video of Houston doctor Stella Immanuel claiming that hydroxychloroquine could help cure Covid-19 and that “you don’t need to wear a mask.” After being promoted by Breitbart and the Trumps, the video went viral in a matter of hours but was quickly discredited and removed from Facebook and Twitter for spreading false claims about Covid-19. Don Jr. actually got suspended from Twitter for sharing it, and Immanuel skyrocketed to social media stardom. Previously unknown, Immanuel now has nearly 200,000 Twitter followers. (Immanuel also says she’s under investigation by the Texas Medical Board.)
Now we know that thousands of people online continue to cling to the false hope that hydroxychloroquine cures Covid-19. On October 2, the day that Trump announced his positive test result, one of the most popular posts on Facebook, according to engagement data from the social media analytics firms CrownTangle and NewsWhip, highlights a tweet in which Immanuel offered to prescribe hydroxychloroquine to President Trump if his own doctors wouldn’t. The post had over 623,000 interactions. 


Everyone in the White House get on HCQ twice a month. With daily vit C, D, and zinc. If your doctors will not prescribe it I will. I can see you via telehealth emergency licensed in DC. Use https://t.co/jTDgb5Et5p and I will hook you guys up. POTUS, FLOTUS don’t wait to get sick.— Stella Immanuel MD (@stella_immanuel) October 2, 2020



It’s hard to identify exactly why the hydroxychloroquine myth remains so popular. Some say early observational studies that suggested the drug had some benefit made some people believe the deadly pandemic could end with a miracle drug. Trump’s promotion of such a fantasy just expanded its reach.
“Often, you have to dig down to find a small grain of truth that can be used to promote that sort of misinformation narrative,” said John Gregory, a senior analyst at NewsGuard. He explained that the narrative about hydroxychloroquine as a cure had many months to take hold as it circulated through mainstream media sources and on social media.
“It became more ingrained in the audience that this was already a proven cure, and they’re going to reject the new evidence,” Gregory said.
Again, with this context in mind, it’s chilling to consider the long-term implications of Trump’s most recent statements about the Covid-19. Since being diagnosed and hospitalized with the disease, the president has said that Americans must “not be afraid of” the coronavirus or “let it dominate your lives.” He’s also continued to suggest, recklessly, that Covid-19 is less severe than the flu. These statements fuel a frightening and dangerous narrative about the pandemic that distrusts science and stands to shape the public’s response to a deadly pandemic.
And for now, due in part to Trump’s history of spreading misinformation, the hydroxychloroquine saga continues, even if the drug doesn’t work against Covid-19.

  
  
  
      




  A long-awaited report from top Democratic congressional lawmakers about the dominance of the four biggest tech giants had a clear message on Tuesday: Amazon, Apple, Facebook, and Google engage in a range of anti-competitive behavior, and US antitrust laws need an overhaul to allow for more competition in the US internet economy. 
“To put it simply, companies that once were scrappy, underdog startups that challenged the status quo have become the kinds of monopolies we last saw in the era of oil barons and railroad tycoons,” the report’s introduction states.
The 400-plus page report, written by the majority staff of the Democratic members of the House Judiciary Subcommittee on Antitrust, is the result of a 16-month investigation into whether these corporate giants abuse their power, and whether the country’s antitrust laws need to be reworked to rein them in. The report released Tuesday cites numerous examples of each tech titan engaging in acts that the lawmakers believe have hurt innovation and impede competition. While the anti-competitive behaviors cited vary from company to company, they are all linked by the allegation that the four giants abuse their gatekeeper status in various internet industries to secure and grow their market power in those sectors and others. 
So what’s the solution? The report recommends creating new laws that would potentially break up tech companies and make it harder for them to pursue acquisitions; it also calls for clarifying existing antitrust laws with the goal of making them easier to enforce, particularly for tech companies. For now, the report’s recommendations are only high-level guidance to Congress for potential future legislation; it won’t lead to immediate action against these companies.
The release of the report was complicated on Tuesday by news that the Republican lawmakers in the House Antitrust Committee refused at the last minute to sign the report with their Democratic colleagues. Instead, Rep. Ken Buck (R-CO) and Jim Jordan (R-OH) each plan to release their own reports. Buck’s report, a draft of which Politico published on Monday, largely agrees with the Democrats’ conclusion that the big four tech firms have amassed too much power. But he disagrees with Democrats on how to fix the problem: Instead of creating new laws, Buck’s memo calls on Congress to fund and empower regulatory agencies and government departments like the Federal Trade Commission (FTC) and Department of Justice (DOJ) to go after Big Tech under existing laws. Jordan’s report hasn’t yet been released, but Reuters coverage indicates it will focus on so-far unproven claims of tech companies’ supposed anti-conservative bias, which he has shouted over his colleagues about in previous hearings.
These partisan divides are somewhat besides the point: Regardless of the specifics of how they advise to go after Big Tech, the fact that Republicans and Democrats agree that these companies pose a threat to the free market is significant.
“This is the first time since the 1970s that a congressional committee has devoted this kind of attention to dominant firms … and changing the structure of a major American industry,” said William Kovacic, the Republican former chairman of the FTC.
Here’s a breakdown of some of the key claims the report makes about each of the four major tech giants:
Amazon
With Amazon accounting for nearly 40 percent of all e-commerce sales in the US — making it more than seven times larger in this arena than No. 2 Walmart — the Democrats’ report argues that the tech giant has used its powerful position in anti-competitive ways. (The report also alleges that Amazon’s US e-commerce market share is closer to 50 percent or more in the country, rather than the near-40 percent figure commonly cited based on estimates from the research firm eMarketer). The report argues that the company unfairly gleans data and information from its third-party sellers that it uses to strengthen the retail side of its business, including favoring its own product brands over those of competitors, giving this merchandise exclusive merchandising space on its virtual shelves, and prioritizing it in search results. 
Another criticism is that Amazon can charge sellers ever-increasing fees because of its dominant position, and that most sellers and brands have practically no negotiating power due to their reliance on the Amazon sales channel. Amazon also penalizes sellers for selling their merchandise for lower prices on other retail sites.
Amazon released a company blog post in response to Tuesday’s report, calling it “flawed thinking” that Amazon is engaging in anti-competitive business practices, and that antitrust regulatory action “would have the primary effect of forcing millions of independent retailers out of online stores.”
“All large organizations attract the attention of regulators, and we welcome that scrutiny. But large companies are not dominant by definition, and the presumption that success can only be the result of anti-competitive behavior is simply wrong,” reads the post.
Facebook
Tuesday’s report argues that Facebook has expanded its monopolistic power in the social media industry by using a “copy, acquire, kill” strategy against its competitors, and by unfairly hurting rival companies like Instagram (which the company purchased in 2012). 
Specifically, the report argues that Facebook’s acquisition of Instagram was a blatant attempt to “neutralize a nascent competitive threat.” The report alleges that after Facebook bought Instagram, it intentionally stymied the photo-sharing app’s success so that it wouldn’t compete with Facebook internally. 
The report cites a slew of internal emails, memos, and testimony from senior-level Facebook employees, including CEO Mark Zuckerberg, which support the argument that Facebook crushed Instagram by exerting monopoly power.
In one email, Zuckerberg told Facebook’s former CTO that “ that he had “been thinking about ... how much [Facebook] should be willing to pay to acquire mobile app companies like Instagram ... that are building networks that are competitive with our own.” The report argues this proves that Zuckerberg had anti-competitive interests from the beginning.
The report also cites a former senior-level Instagram employee who told Congress that Zuckerberg oversaw “brutal infighting between Instagram and Facebook” after the acquisition, with Zuckerberg slowing down Instagram’s natural growth to benefit Facebook proper. The Instagram whistleblower went so far as to call it “collusion, but within an internal monopoly. … It’s unclear to me why this should not be illegal.” 
As part of their investigation, the subcommittee found an internal Facebook document called “The Cunningham Memo,” written in 2018 by Thomas Cunningham, a senior data scientist and economist at Facebook, which allegedly shows that Facebook has knowingly “tipped” its company toward becoming a monopoly, acknowledging that social media apps have tipping points where “either everyone uses them, or no-one uses them,” according to the memo. This memo was a key part of Zuckerberg’s acquisition strategy ahead of the Instagram purchase, according to internal documents and an interview the subcommittee conducted with a former Facebook employee involved with the project. 
In a statement to Recode on Tuesday, Christopher Sgro, a spokesperson for Facebook, disagreed with the report’s conclusions. “Facebook is an American success story. We compete with a wide variety of services with millions, even billions, of people using them. Acquisitions are part of every industry, and just one way we innovate new technologies to deliver more value to people. Instagram and WhatsApp have reached new heights of success because Facebook has invested billions in those businesses. A strongly competitive landscape existed at the time of both acquisitions and exists today. Regulators thoroughly reviewed each deal and rightly did not see any reason to stop them at the time,” Sgro wrote.
Google
The Democrats’ report argues that Google has a monopoly in the online search and marketing industry, creating an “ecosystem of interlocking monopolies” — which it has maintained through anti-competitive practices in two key ways.
The first is by launching an “aggressive campaign to undermine” what the report calls “vertical search providers” — which are search engines for specific topics, such as Yelp for restaurants, or Expedia for travel. The report says Google uses its dominance to “boost Google’s own inferior” content over some of these other companies’ content in its search results.
The second major way that Google has demonstrated anti-competitive behavior, the report argues, is through “a series of anti competitive contracts” that pushed people to rely on Google search when using phones with the Android operating system (Google purchased Android in 2005).
“Documents show that Google required smartphone manufacturers to pre-install and give default status to Google’s own apps,” the report states.
Unsurprisingly, Google told Recode it disagreed with Tuesday’s reports, saying they “feature outdated and inaccurate allegations from commercial rivals about Search and other services.”
Americans simply don’t want Congress to break Google’s products or harm the free services they use every day,” read a statement in part from Julie McAlister, a spokesperson for Google.
Apple
According to Tuesday’s report, Apple exerts monopoly power through its oversight of software that’s downloaded on half of all mobile phones in the US. That’s a direct reference to Apple’s App Store — if you have an iPhone, you can only use apps that you download from the company’s tightly controlled store. The subcommittee staff investigating Apple say in the report that the company has exploited its dominance to exclude some rivals from its store, unfairly favor its own apps, and charge fees that some app developers told the subcommittee are “exorbitantly high.”
Such a battle between Apple and developers over in-app fees exploded into public spotlight earlier this year when the maker of Fortnite, Epic Games, told its users they could buy the game’s virtual currency directly from Epic rather than through the Apple iOS version of the app. The reason? Epic wanted to avoid the 30 percent fee Apple charges for such in-app purchases. Dueling lawsuits ensued, and Apple even banned the game from the App Store. This is just one example of many cases like this that the report cites.
Apple, of course, refuted the conclusions in Democrats’ report, telling Recode in a statement, “Our company does not have a dominant market share in any category where we do business. ... Last year in the United States alone, the App Store facilitated $138 billion in commerce with over 85% of that amount accruing solely to third-party developers. Apple’s commission rates are firmly in the mainstream of those charged by other app stores and gaming marketplaces.” 
So what’s next?
Depending on the results of the November election, Democrats may not need Republicans’ support on antitrust legislation — if Democrats sweep Congress and win the White House. (The latest polls show Democrats and Biden currently have an edge, but poll-based predictions are far from certain.)
If Joe Biden does win the presidency, “this [report] is a roadmap for how you would tackle this under a President Joe Biden … administration,” a staff member for a Democratic member of the subcommittee told Recode.
Rep. Pramila Jayapal (D-WA), a member of the subcommittee, told Recode in an interview on Tuesday, “I do anticipate ... that we will have signed pieces of legislation pass the House of Representatives next year.” The bipartisan subcommittee will meet later this year to debate and potentially amend the report.
Tuesday’s congressional reports are just the beginning of upcoming antitrust regulatory proceedings against Big Tech. The DOJ is imminently expected to file a lawsuit against Google for anti-competitive business practices, which several state attorneys general may sign on to. Separately, the FTC is also investigating the business practices of the tech giants over antitrust concerns. 

  
  
  
      




  Facebook said in a Tuesday press release that it “will remove any Facebook Pages, Groups and Instagram accounts representing QAnon” from its platforms. Although it’s unclear how Facebook is defining affiliations with QAnon accounts, this announcement appears to be one of the broadest bans Facebook has ever imposed. 
The new ban expands on the social network’s previous actions against the conspiracy theory and its followers. In August, Facebook announced that it had removed hundreds of QAnon Facebook Groups and Pages for “discussions of potential violence.” The company now says it will remove such Pages and Groups “even if they contain no violent content.” The announcement also comes after Facebook’s announcement last week that it will promote credible information about child safety, after QAnon hijacked related hashtags like #SaveTheChildren. 
“We’ve been vigilant in enforcing our policy and studying its impact on the platform but we’ve seen several issues that led to today’s update,” the company explained in a Tuesday statement. “For example, while we’ve removed QAnon content that celebrates and supports violence, we’ve seen other QAnon content tied to different forms of real world harm, including recent claims that the west coast wildfires were started by certain groups, which diverted attention of local officials from fighting the fires and protecting the public.”
Recode has contacted Facebook for comment. 

  
    Related
  

  
    Facebook and Twitter said they would crack down on QAnon, but the delusion seems unstoppable
  

Facebook has in the past struggled to enforce its rules against accounts that promote the QAnon conspiracy theory. However, in today’s press release, Facebook noted that QAnon frequently changes its messaging strategies in order to evade content moderators and that it will take time for the social network to fully scale up enforcement of this latest policy update.
“We expect renewed attempts to evade our detection, both in behavior and content shared on our platform, so we will continue to study the impact of our efforts and be ready to update our policy and enforcement as necessary,” said the statement. The New York Times reported that, as of Tuesday night, almost 100 QAnon accounts had already been affected, according to an analysis from the Facebook-owned tool CrowdTangle. 
Facebook’s announcement also comes less than a week after members of the US House of Representatives voted to formally condemn the conspiracy theory. As the November 3 election draws ever closer, the social media company has faced growing criticism over its handling of misinformation and potentially violent groups that can organize on its platforms. QAnon has factored into some of those concerns, as it’s been linked to violence in the past and its adherents have at times spread dangerous misinformation, including about the recent West Coast wildfires.
Some feel that Facebook has a role in controlling the spread of QAnon but that the company’s updated policy comes way too late. As Recode’s Shirin Ghaffary has reported, before its latest announcement, Facebook seemed unwilling to take simple steps to remove Groups and Pages that promoted QAnon, meaning people could easily find content about the conspiracy theory. 
Others question whether Facebook’s updated approach will have much impact. Adam Enders, a political scientist at the University of Louisville who studies conspiracy theories, says it’s not clear how many followers of QAnon there actually are, since many who might be sympathetic to QAnon are generally sympathetic to other vague conspiratorial thoughts about the “deep state.”
“There’s not much movement on the needle to be had,” Enders told Recode. “The people that are part of these groups that are down the rabbit hole — true believers — they’re just going to find a different platform.”  
“Removing some QAnon groups that people are pretty unlikely to be incidentally exposed to in the first place isn’t going to impact those people very much,” he added. “It’s just going to be, probably, a minor annoyance to the actual true believer.” 
Other groups that have been critical of Facebook, including Accountable Tech, Media Matters, and the Anti-Defamation League (which helped organized the advertising boycott of Facebook earlier this year), emphasized that the impact of the move will depend on how effective the company is at finding and removing QAnon content.
“Their announcement acknowledged several important truths — that enforcement at the individual post level cannot counter hate and disinformation; that content need not explicitly support violence to bring about real-world harms; and that without aggressive deterrence, these platforms will continue to serve as critical organizing and recruitment tools for extremist movements,” said Accountable Tech’s co-founder Jesse Lehrich in a statement. 
It will take time for the impact of this shift in Facebook’s policies around QAnon to come into focus. In the meantime, many doubt the company is truly invested in or would be able to clamp down on the theory, especially with a high-stakes election just weeks away. 
Update, October 6, 8 pm: Updated to include additional commentary from experts and advocacy groups.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



You may have never heard of it, but Section 230 of the Communications Decency Act is the legal backbone of the internet. The law was created almost 30 years ago to protect internet platforms from liability for many of the things third parties say or do on them. And now it’s under threat by one of its biggest beneficiaries: President Trump. In combination with an executive order issued in May, the Justice Department’s proposed law, released in September, could ensure that the president can say whatever he wants on social media and address the accusation that platforms are biased against conservatives. (Evidence suggests the contrary.)
Section 230 says that internet platforms that host third-party content — think of tweets on Twitter, posts on Facebook, photos on Instagram, reviews on Yelp, or a news outlet’s reader comments — are not liable for what those third parties post (with a few exceptions). For instance, if a Yelp reviewer were to post something defamatory about a business, the business could sue the reviewer for libel, but it couldn’t sue Yelp. Without Section 230’s protections, the internet as we know it today would not exist. If the law were taken away, many websites driven by user-generated content would likely go dark. 
After Facebook and Twitter deleted posts containing factual inaccuracies about the risks of Covid-19, Trump issued his loudest demand yet to repeal the law:


REPEAL SECTION 230!!!— Donald J. Trump (@realDonaldTrump) October 6, 2020



This follows efforts from the Trump administration and Republican legislators to chip away or change the law, including an executive order, various bills, and, most recently, a proposal from Attorney General Bill Barr to Congress to create legislation that would require social media platforms to moderate users consistently or else lose immunity from lawsuits that Section 230 provides. Trump, going one step further, is now calling for a repeal of the law.
Trump’s attacks on Section 230 began in earnest in May after Twitter put a warning label and fact-check on two of his tweets that contained misinformation about mail-in voting, but the ramifications of changing the law would extend far beyond a few tweets. Here’s a look at how Section 230 went from an amendment to a law about internet porn to the pillar of internet free speech to Trump’s latest weapon against perceived anti-conservative bias in the media.
Section 230’s salacious origins
In the early ’90s, the internet was still in its relatively unregulated infancy. There was a lot of porn floating around platforms like AOL and the World Wide Web where anyone, including our nation’s impressionable children, could see it. This alarmed some lawmakers. In an attempt to regulate this situation, in 1995 lawmakers introduced a bipartisan bill called the Communications Decency Act which would extend to the internet laws governing obscene and indecent use of telephone services. This would also make websites and platforms responsible for any indecent or obscene things their users posted. 
In the midst of this was a lawsuit between two companies you might recognize: Stratton Oakmont and Prodigy. The former is featured in The Wolf of Wall Street, and the latter was a pioneer of the early internet. But in 1995, Stratton Oakmont sued Prodigy for defamation after an anonymous user claimed on a Prodigy bulletin board that the financial company’s president engaged in fraudulent acts. As the New York Times explains the court’s decision:
The New York Supreme Court ruled that Prodigy was “a publisher” and therefore liable because it had exercised editorial control by moderating some posts and establishing guidelines for impermissible content. If Prodigy had not done any moderation, it might have been granted free speech protections afforded to some distributors of content, like bookstores and newsstands.
Fearing that the Communications Decency Act would stop the burgeoning internet in its tracks and mindful of the court’s decision, then-Rep. (now Sen.) Ron Wyden and Rep. Chris Cox authored an amendment that said that “interactive computer services” were not responsible for what their users posted, even if those services engaged in some moderation of that third-party content. The internet companies, in other words, were mere platforms, not publishers.
“What I was struck by then is that if somebody owned a website or a blog, they could be held personally liable for something posted on their site,” Wyden explained to Vox’s Emily Stewart last year. “And I said then — and it’s the heart of my concern now — if that’s the case, it will kill the little guy, the startup, the inventor, the person who is essential for a competitive marketplace. It will kill them in the crib.”
Section 230 also allows those services to “restrict access” to any content they deem objectionable. In other words, the platforms themselves get to choose what is and what is not acceptable content, and they can decide to host it or moderate it accordingly. That means the free speech argument frequently employed by people who are suspended or banned from these platforms — that the Constitution says they can write whatever they want — doesn’t apply, no matter how many times Laura Loomer tries to test it. As Harvard Law professor Laurence Tribe points out, the First Amendment argument is also generally misused in this context:


1. The First Amendment limits only the Government, not private entities like Twitter.2. Anyway, Twitter’s tagging of Trump’s claims about write-in voting is itself absolutely protected under the First Amendment as an expression of opinion.— Laurence Tribe (@tribelaw) May 27, 2020



Wyden likens the dual nature of Section 230 to a sword and a shield for platforms: They’re shielded from liability for user content, and they have a sword to moderate it as they see fit.
The Communications Decency Act was signed into law in 1996. The indecency and obscenity provisions, which made it a crime to transmit such speech if it could be viewed by a minor, were immediately challenged by civil liberty groups. The Supreme Court would ultimately strike them down, saying they were too restrictive of free speech. Section 230 stayed, and the law that was initially meant to restrict free speech on the internet instead became the law that protected it.
This protection has allowed the internet to thrive. Think about it: Websites like Facebook, Reddit, and YouTube have millions and even billions of users. If these platforms had to monitor and approve every single thing every user posted, they simply wouldn’t be able to exist. No website or platform can moderate at such an incredible scale, and no one wants to open themselves up to the legal liability of doing so.
That doesn’t mean Section 230 is perfect. Some argue that it gives platforms too little accountability, allowing some of the worst parts of the internet — think 8chan or sites that promote racism — to flourish along with the best. Simply put, internet platforms have been happy to use the shield to protect themselves from lawsuits, but they’ve largely ignored the sword to moderate the bad stuff their users upload.
Recent challenges
In recent years, Section 230 has come under threat. In 2018, two bills — the Allow States and Victims to Fight Online Sex Trafficking Act (FOSTA) and the Stop Enabling Sex Traffickers Act (SESTA) — were signed into law, which changed parts of Section 230. Now, platforms could be deemed responsible for prostitution ads posted by third parties. These were ostensibly meant to make it easier for authorities to go after websites that were used for sex trafficking, but they did this by carving out an exception to Section 230. The law was vulnerable.
Amid all of this was a growing public sentiment that social media platforms like Twitter and Facebook were becoming too powerful. In the minds of many, Facebook even influenced the outcome of the 2016 presidential election by offering up its user data to shady outfits like Cambridge Analytica. There were also allegations of anti-conservative bias. Right-wing figures who once rode the internet’s relative lack of moderation to fame and fortune were being held accountable for various infringements of hateful content rules and kicked off the very platforms that helped created them. Alex Jones and his expulsion from Facebook and other social media platforms is perhaps the most illustrative example of this.
Republican Sen. Ted Cruz, demonstrating a profound misunderstanding of Section 230, claimed in a 2018 op-ed that the law required the internet platforms it was designed to protect to be “neutral public forums.” Lawmakers have tried to introduce legislation that would fulfill that promise ever since. 
Republican Rep. Louie Gohmert introduced the Biased Algorithm Deterrence Act in 2019, which would consider any social media service that used algorithms to moderate content without the user’s permission or knowledge to be legally considered a publisher, not a platform, thereby removing Section 230’s protections. (Remember the Stratton Oakmont v. Prodigy case? This bill would have hearkened back to that era.) Later that year, Republican Sen. Josh Hawley introduced the Ending Support for Internet Censorship Act, which would require that, in order to be granted Section 230 protections, social media companies would have to show the Federal Trade Commission (FTC) that their content moderation practices were politically neutral.
Neither of those bills went anywhere, but the implications were obvious: Emboldened by FOSTA-SESTA, the two sex-trafficking bills from 2018, lawmakers not only wanted to chip away at Section 230 but were actively testing out ways to do it. 
More likely to succeed is a bipartisan bill introduced in March called the Eliminating Abusive and Rampant Neglect of Interactive Technologies (EARN IT) Act, from Sens. Lindsey Graham and Richard Blumenthal. Here, the lawmakers used the prevention of child pornography as an avenue to erode Section 230 by requiring companies to follow a set of “best practices” developed by a newly established commission or else lose their Section 230 immunity from civil lawsuits over child pornography postings. Some privacy advocates fear that the proposed law would extend to requiring tech companies to provide law enforcement with access to all user content. The law has bipartisan support, with Hawley and Democrat Dianne Feinstein among its cosponsors. At the end of September, a House version of EARN IT was introduced by Reps. Sylvia Garcia and Ann Wagner, a Democrat and Republican, respectively, paving the way for EARN IT to get a House vote as well.
Trump’s executive order
President Trump, who has benefited greatly from social media, is trying to dial back Section 230’s protections through an executive order. Back in May, Trump signed his “Executive Order on Preventing Online Censorship” roughly 48 hours after Twitter applied a new policy of flagging potentially false or misleading content to two of the president’s tweets. At the signing ceremony, Trump referred to Twitter’s actions as “editorial decisions,” and Barr referred to social media companies as “publishers.”
“They’ve had unchecked power to censure, restrict, edit, shape, hide, alter virtually any form of communication between private citizens or large public audiences,” Trump said at the time. “We cannot allow that to happen, especially when they go about doing what they’re doing.”
The order says that platforms that engage in anything beyond “good faith” moderation of content should be considered publishers and therefore not entitled to Section 230’s protections. It also calls on the Federal Communications Commission (FCC) to propose regulations that clarify what constitutes “good faith”; the FTC to take action against “large internet platforms” that “restrict speech”; and the attorney general to work with state attorneys general to see if those platforms violate any state laws regarding unfair business practices.
While the order talks a big game, legal experts don’t seem to think much — or even any — of it can be backed up, citing First Amendment concerns. It’s also unclear whether or not the FCC has the authority to regulate Section 230 in this way, or if the president can change the scope of a law without any congressional approval.
Barr’s proposals
Barr is not a fan of Section 230, and his Department of Justice has been looking into the law and how he believes it allows “selective” removal of political speech. This has included a set of recommendations from the Justice Department in June and the legislation proposal sent to Congress on Wednesday. The proposal includes the addition of a “good faith” section requiring platforms to spell out their moderation rules, follow them to the letter, explain any moderation decisions to the user whose content is being moderated, and provide the user with the chance to respond. There are also additional carve-outs that would remove civil lawsuit immunity for material that violates anti-terrorism, child sex abuse, cyberstalking, and antitrust laws. 
“For too long Section 230 has provided a shield for online platforms to operate with impunity,” Barr said in a statement. “Ensuring that the internet is a safe, but also vibrant, open and competitive environment is vitally important to America. We therefore urge Congress to make these necessary reforms to Section 230 and begin to hold online platforms accountable both when they unlawfully censor speech and when they knowingly facilitate criminal activity online.”
It’s not clear how Barr determined that platforms are “unlawfully” censoring speech, as First Amendment protections do not extend to private businesses.
Trump and Barr also recently met with some Republican state attorneys general to discuss ways state laws can be used to further dictate how and when social media platforms can moderate their users’ speech.
Needless to say, Section 230’s creator isn’t thrilled.
“As the co-author of Section 230, let me make this clear: There is nothing in the law about political neutrality,” Wyden said. “It does not say companies like Twitter are forced to carry misinformation about voting, especially from the president. Efforts to erode Section 230 will only make online content more likely to be false and dangerous.”
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  Facebook deleted one of Trump’s posts today for violating the company’s policies against harmful health misinformation. This was only the second time Facebook has taken down a post from the president for violating its misinformation policies.
On Tuesday morning, President Trump — fresh out of the hospital after contracting Covid-19 — posted to his Facebook account, falsely stating that the flu can be more deadly than Covid-19. After several hours, Facebook took down the post for violating its health misinformation policies against Covid-19, as CNN first reported. Twitter left an identical post by Trump up on its platform but added a warning label on the tweet for spreading misleading information. 
Shortly after Facebook and Twitter moderated his posts, Trump posted “REPEAL SECTION 230!!!” on Facebook and Twitter. That’s a reference to an increasingly controversial piece of legislation, Section 230, which largely protects social media platforms from being sued for what people post on their platforms. Trump has repeatedly threatened to repeal Section 230 because he and some other Republicans claim, without any proof, that these firms have an “anti-conservative bias.” If Section 230’s protections for web platforms are rolled back, it would severely restrict social media companies’ ability to operate as usual.
This isn’t the first time that Facebook has deleted a Trump post. 
In August, Facebook and Twitter deleted a post by Trump depicting a Fox News interview in which he falsely stated that children are “almost immune” to the coronavirus. That isn’t true; children are not immune to the virus.
But it’s notable that Facebook, which has been frequently criticized for not taking action against Trump’s misleading posts about mail-in voting, or for his posts that seem to encourage violence at Black Lives Matter protests is — at least in this instance — holding firm on enforcing its Covid-19 misinformation policies.
Facebook CEO Mark Zuckerberg has positioned Covid-19 as an area where his company will take stronger action against misinformation than it does on posts related to politics or other thornier topics. Part of the company’s thinking is that health information is more clear-cut than other politically contested areas. But as today’s episode is showing, everything — even basic facts about Covid-19 after six months of a global pandemic — is politically contested by President Trump. 
  
  
  
      




  Amazon has long opposed the idea of its warehouse employees forming a union, though much of its anti-union strategies have stayed under wraps. But a confidential Amazon internal memo viewed by Recode reveals how the company is making significant investments in technology to track and counter the threat of unionization.
The 11-page document, dated February 2020, describes Amazon’s plans to spend hundreds of thousands of dollars to better analyze and visualize data on unions around the globe, alongside other non-union “threats” to the company related to factors like crime and weather. Out of 40 or so data points listed in the memo, around half of them were union-related or related to employee issues, like mandatory overtime and safety incidents. The memo requested staffing and funds to purchase software that would specifically help consolidate and visually map data from three different Amazon groups, led by employee relations (which is part of human resources), along with Amazon’s Global Intelligence Unit and Global Intelligence Program. 
The new technology system — called the geoSPatial Operating Console, or SPOC — would help the company analyze and visualize at least around 40 different data sets, the memo says. Among them are many related to unions, including “Whole Foods Market Activism/Unionization Efforts,” “union grant money flow patterns,” “and “Presence of Local Union Chapters and Alt Labor Groups.” Additionally, one of the potential use cases for the tool is described in the memo as “The Union Relationship Map,” though no other details are provided.
The memo offers evidence of how Amazon is dedicating significant time and resources to reduce the likelihood of unionization among its front-line workforce, which totaled nearly 1.4 million people across Amazon and Whole Foods from March through September 19, counting every employee who worked for the companies for any period of time. This is not a complete surprise: Amazon is now the second-largest private-sector employer in the US, only behind Walmart, and scrutiny of its labor practices has increased as it has rapidly expanded its warehousing network, even during the Covid-19 pandemic. 
“In my opinion, it’s definitely part-strategy of union avoidance,” said a former Amazon senior HR manager familiar with past company union-avoidance tactics, and with whom Recode shared the memo. The former manager requested anonymity for fear of retribution. “The tool could be used for things like factoring in the financial strength of the closest unions [and] success rate of local unions (how many campaigns have resulted in [collective bargaining agreements]).” The former HR manager added that, as the internal memo shows, the tool could also be useful for mitigating non-union-related risks to employees, like rerouting customer orders during severe storms so warehouse workers can stay home until dangerous weather passes.
Amazon spokesperson Jaci Anderson said in a statement that Amazon respects “employees’ right to join, form or not to join a labor union or other lawful organization of their own selection, without fear of retaliation, intimidation or harassment. Across Amazon, including in our fulfillment centers, we place enormous value on having daily conversations with each associate and work [to] make sure direct engagement with our employees is a strong part of our work culture.”


  Do you work at Amazon and have thoughts on what’s going on? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com or Jason Del Rey at jasondelrey@protonmail.com to reach them confidentially. Signal numbers available upon request by email. 




While some Amazon warehouse employees belong to unions in European countries like Germany and Italy, no union has ever had success organizing any of Amazon’s workforce in the United States. Amazon closed down a call center in 2001 that was the focus of a unionization attempt, and a small group of warehouse technicians in Delaware staged a union vote in 2014, but a majority of the employees voted against.
More recently, Amazon has faced increased scrutiny from labor activist groups, progressive politicians, and a handful of outspoken employee organizers over the company’s treatment of its massive warehouse workforce. Earlier in the pandemic, some warehouses employees argued that Amazon’s safety measures came too late or were inconsistently enforced. Amazon fired several of them, citing violations of company policies. The employees all believe they were fired for speaking out. Amazon on Thursday said that nearly 20,000 of its Amazon and Whole Foods employees have contracted Covid-19, which represents 1.44 percent of its front-line employees. Nationwide, more than 2 percent of Americans have tested positive for the virus. But Amazon’s calculation included employees who worked for the company “at any point” between the beginning of March and September 19, potentially inflating the denominator in that equation because it includes workers who may have only been employed for short stretches.
Either way, Amazon is not alone among large retailers in its opposition to unions. Walmart is perhaps the most infamous for its anti-union tactics. The brick-and-mortar titan has been known to show anti-union videos to new hires and supply store managers with “Labor Relations Training.” But Amazon’s technical expertise and data-focused DNA potentially positions it to develop a more sophisticated tracking and union avoidance system than its peers. And the SPOC memo sheds more light on that sophistication.
“I think it’s Amazon acting like most employers in that they have a very strong preference for remaining non-union and will put significant resources at any attempt to cut off organizing before it can get off the ground,” Rebecca Givan, a labor professor at Rutgers University, told Recode. “The difference is that Amazon is a company that uses data collection, surveillance, analytics in everything it does, from workflow in its warehouses to consumer preferences to product placement on its website. So they’re extremely well-positioned to apply that expertise.”
Amazon’s public case against unionization has included some typical anti-union employer talking points, like wanting a direct line of communication between management and employees. But former executives, one of whom previously called unionization “likely the single biggest threat to the business model,” have told Recode that the company is also concerned that its warehouse operations would become less efficient if employees unionized. Translation: If its workers were to successfully unionize, it’s unlikely the company could continue pushing front-line staff as hard on performance goals, which can involve stowing or grabbing several hundred items per hour. An Amazon spokesperson hinted at this when Amazon defeated the last union drive in one of its US warehouses in 2014. “Amazon’s culture and business model are based on rapid innovation, flexibility and open lines of direct communication between managers and associates,” the statement at the time read in part.
Simply compiling the union-related datasets mentioned in the memo is unlikely to be illegal, said Wilma Liebman, a former chairman of the National Labor Relations Board under President Barack Obama.
“But once employees become aware of such surveillance of union and other ‘concerted’ activities, there are potential legal issues presented,” she wrote in an email to Recode. “Open surveillance is illegally coercive even if managers do not directly threaten to retaliate or take action based on the information obtained. There is an implied message that the company people will be rewarded and the union adherents will suffer.”
The SPOC document made waves among Amazon corporate staff last week after some employees came across the memo and distributed it within the company.
These employees first found the memo while they were investigating why an unfamiliar email address linked to the employee relations team subscribes to dozens of employee mailing lists related to worker issues and diversity groups.
Employees then discovered that the unfamiliar email was linked to the larger SPOC project and feared it was part of an HR surveillance program to quietly monitor employee conversations that could lead to labor organizing.  
Anderson, the Amazon spokesperson, previously told Recode that the SPOC program monitors all types of external activity that impacts the safety and well-being of its employees, from weather events to power outages, and is not intended to favor or target one type of external threat — such as unionization — over another.
But several Amazon employees who viewed the memo, and who are involved in employee activism, believe the document shows evidence of an anti-union surveillance program. The SPOC memo discovery also comes just a month after a report drew attention to two Amazon job listings posted in August seeking “intelligence analysts” to research “labor organizing threats against the company.” The tech giant deleted the postings amid fierce criticism from labor activists and the media attention that followed.
“I (correctly) thought I knew that Amazon had sophisticated intelligence operations surveilling their workers, but to see it written out so blatantly — it hits a little different; makes it a little more real,” one employee, who was granted anonymity because they were talking in violation of Amazon’s external communications policy, told Recode. 
Givan, the Rutgers labor professor, said that if there’s one takeaway for employees from the memo, it’s that you are being watched.
“From an organizing perspective, the employees should assume everything they are talking about is being monitored,” she said, “and everything that they’re writing is feeding the algorithm.”

  
  
  
      





  While most of America is focused on the presidential vote, Californians have another important decision to make at the polls this November. They’re being asked to approve what will likely become the internet privacy law for the United States.
Proposition 24, also known as the California Privacy Rights and Enforcement Act of 2020 (CPRA), is supposed to expand a landmark California privacy law that passed two years ago; there’s a good chance Californians will approve this one, too. It’s framed as legislation that will better protect their privacy — in particular, sensitive data such as Social Security numbers, race, religion, and health information. 
And while the proposed law technically governs the use and sale of data for Californians, California has an enormous impact on the tech industry, which means CPRA will become the de facto law for all of the US.
Which should sound like a good thing for most people. Among other impacts of the proposed law, it makes a point of protecting young people by mandating triple fines for infringements against consumers under age 16. It will allow consumers to restrict the use of geolocation data by third parties, effectively ending practices like sending targeted ads to people who’ve visited a rehab center or a cancer clinic. And it will fund the creation of an agency to protect consumer privacy.
For news publishers, though, any new data regulation can create problems, and news publishers already have plenty of well-documented problems. But I think the proposed enhancements will actually help the news industry.
Fighting the Google/Facebook duopoly
From targeted advertising to personalization, data does a lot of work online. Unfortunately, two companies dominate data collection and therefore digital advertising. One big question about any privacy laws is whether they actually create more advantages for Google and Facebook instead of leveling the playing field for smaller competitors.
We’ve seen this happen before. In Europe, which began enforcing a new privacy law in May 2018, big tech companies have been able to effectively neuter the law by implementing half-measures and exploiting loopholes while enforcement lags.
The good news for consumers and news publishers alike is that CPRA seeks to close any  loopholes in the previous privacy law the state passed two years ago.
For starters, the law is supposed to more clearly limit data collection and use for third parties — companies you don’t expect to get access to your data when you visit a news site — while allowing publishers to continue to use data they generate on their own sites.
That makes sense. As we have noted for years, consumers generally expect an app or website to collect data about them to help improve the service, recognize them as return visitors, or to recommend content. But they don’t expect unknown third parties to collect data about them to build profiles and serve targeted advertising on unrelated sites or apps. 
That unbridled data surveillance by some big tech companies outside of their own user-facing services — that is, Google and Facebook’s ability to track you even when you’re not on their properties — has undermined consumer trust in the entire digital economy. Giving consumers the ability to control their own data should help restore some of that trust.
Privacy and subscriptions can work together
News publishers are also increasingly interested in trying to sell subscriptions instead of relying on digital ads. CPRA can help there by letting publishers offer subscriptions to consumers who opt out of having their data shared with other parties.
Some CPRA critics think this provision puts a price on “privacy.” I would argue that it gives news publishers the flexibility to decide on their own business model, and gives consumers an opportunity to understand how content gets funded. If they do not find it compelling enough, they are likely to seek out a competitive news service elsewhere. News publishers feel this tension every day. That’s why I think they will see healthy competition for consumers at various price points.
Third parties and liability
Lastly, and maybe most importantly, the CPRA closes loopholes that could be exploited by big tech platforms. One aspect of this is what we’re calling “the switch language,” which clearly aligns the obligations of third parties to serve the interests of consumers. It notes that when a consumer exercises their opt-out rights and a publisher passes their choice along to all the companies with which it works (third parties), those companies must stop reusing that consumer’s data for any other purpose. This essentially forces those companies to revert to the role of a service provider. The “switch language” also prevents any wiggle room by not allowing contracts to override this requirement. As publishers experienced in Europe, platforms like Google and Facebook often use their unbalanced negotiating leverage to force publishers to sign over these data rights, so this section is hugely important for individual publishers that do not have the leverage to force Google or Facebook to stop mining data off their properties. 
Finally, CPRA clarifies that publishers are not responsible for third parties that violate the previous section as long as they do not have actual knowledge of the violation. Taken together, these provisions reflect a thoughtful understanding of how data flows in the digital economy. They also put the onus squarely on big tech companies to tailor their data collection practices in accordance with consumer preferences.
Privacy laws are imperfect yet unavoidable
CPRA isn’t perfect, but it’s well-intentioned. And while you might hear tech giants warning that it will hurt publishers, you should consider the source of those warnings, and the motivations behind them.
Consumer expectations are evolving; policy, and our industry, must follow. Yes, there may be some short-term problems as advertisers get used to working with less data and lower the price for the ads they buy. But those playing the long game will be prepared for a world where more value is placed on publishers’ direct relationships — and consumer trust.
Jason Kint is the chief executive of Digital Content Next, a trade association that represents digital content companies, including Vox Media.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



It may have gotten off to a slow start, but digital Covid-19 contact tracing apps are finally picking up steam in the United States — and may have surmounted one of their biggest obstacles to widespread use.
States are now able to create contact tracing apps that work with those from other states, thanks to a national server that works with the exposure notification tool developed by Apple and Google and stores information about potential exposures from all states that use it. This gives us the potential for a nationwide digital contact tracing system. Some states are already incorporating this technology into their regional efforts: the latest are New York and New Jersey, which unveiled new apps on Thursday.
While the national server has been around for a few months, the fact that a growing number of states are using it is a big deal. Because America’s federal government refuses to create a national contact tracing app, each state has so far needed to create its own, leading to a patchwork of apps across the country that used different technologies and couldn’t communicate with each other. Any given state app became effectively useless as soon as the device it was on crossed a state line.
This is no longer the case. Some states — New Jersey, New York, Pennsylvania, and Delaware — have teamed up to form a regional alliance of apps with a common codebase. That means if you had the New York version of the app and traveled to New Jersey for a large, crowded superspreader event where you were exposed to the coronavirus, you’d still get a notification (assuming the infected person used one of those states’ apps and reported their positive results to the proper health authorities).
The public health authorities behind these new digital contact tracing apps are using the Apple-Google exposure notification tool, which uses Bluetooth to exchange anonymous “keys” with any devices that come within a certain proximity (and have opted into the tool). If someone tests positive for the coronavirus, their keys are uploaded to a server. Individual devices check the server every day, and if any keys on the server match keys received by the device, it will alert its owner that they may have been exposed to the virus. 
Apple and Google worked together to make the tool interoperable with iOS and Android operating systems, and iOS 14 has the exposure notification tool baked into the system itself and notifies users about the feature, creating a relatively seamless process to enable it. While a contact tracing app doesn’t have to use the Apple-Google tool — in fact, some states and countries have elected to go their own way here — those apps have had technical and privacy issues, and can’t work with each other. 
The Association of Public Health Laboratories launched the new national key server in July specifically for the Apple-Google tool, which means any state apps that use the tool and this national server will basically work together. If someone with Wyoming’s app tests positive, their keys are uploaded to a server and can be accessed by devices using apps from Delaware, New Jersey, New York, North Carolina, North Dakota, Pennsylvania, and parts of California that are participating in a pilot program. 
Several other states, including Alabama, Nevada, Virginia, and the territory of Guam, are also using the Apple-Google exposure notification tool in their contact tracing apps, but they are not using the national key server. A few more — Colorado, Connecticut, Maryland, Oregon, Washington state, and Washington, DC — have announced plans to come out with their own apps powered by the Apple-Google tool.
New Jersey and New York are the newest states to roll out their apps. Along with Pennsylvania and Delaware, they’re touting a sort of regional effort at contact tracing. They’re using the national server, and the four apps are all very similar, from their name (“COVID Alert [state]”) to their source code: The apps were built using an open source codebase called COVID Green, which was initially developed for Ireland’s app. That gives all four apps a uniform look and function, and it means the process of building them was relatively fast compared to the time it would have taken for each state to build their own app from scratch. 
The states were also assisted by the Linux Foundation Public Health (LFPH) initiative, which works with public health authorities to use open source software to fight the coronavirus. 
“We’re really focused on making sure to build out an ecosystem around exposure notification, and making sure that there’s open source options in order to get these apps out more quickly,” Jenny Wanger of LFPH told Recode. “And to make sure that they’re secure and trustworthy, as well as building a community for everybody who’s actually implementing these apps.”
One testament to how the attitude toward digital contact tracing and the Apple-Google exposure notification tool in particular has changed: In May, Pennsylvania told Recode it was using a different digital contact tracing tool to assist human contact tracing efforts, while New York told Recode it was focusing on building its manual contact tracing program. 
Apple and Google also recently announced an “exposure notification express” option that allows states to enable exposure notifications without having to build a standalone app. Several states that are part of the so-called Western States Pact — Washington, Oregon, and California — are currently piloting projects based on the exposure notifications express technology.
The new apps and region-wide efforts come at a time when there’s an increased awareness of and interest in contact tracing, and when infection rates are relatively low but could be on the verge of a fall spike — meaning contact tracing could come in handy very soon. As always, the effectiveness of the apps depends on how many people use them. With states working together, new technology allowing apps to work across state lines, and the device operating system updates in place that facilitate their use, we’re as close as we’ve ever been to having a nationwide digital contact tracing effort.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  Donald Trump is a liar. 
Before he became president, Trump lied about everything from his personal wealth to his TV ratings to the number of floors in his condo towers. Once in the White House, he started out lying about the size of his inauguration crowd, and super-sized his lies from there. 
So now that Trump has tested positive for the coronavirus, who are we going to trust for information about the state of his health?
For the very near-term, we are triangulating: There are the official pronouncements from Trump and his White House circle, and there are reports from outlets with good access to Trump’s orbits. We can combine the two — and add in what we know about Covid-19 — and get a decent idea of what might actually be happening, for the moment.
Earlier Friday morning, NBC, the New York Times and other outlets reported that Trump was experiencing “mild symptoms” from the disease; shortly after that, White House chief of staff Mark Meadows said the same thing, on the record. After Meadows spoke, Trump’s wife Melania tweeted that she had the same symptoms.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        President Trump and first lady Melania Trump walk toward Marine One on September 29.
      
      
        Andrew Caballero-Reynolds/AFP via Getty Images
      
    
  


But that status — where we’re able to share a common reality, shaped by a combination of official pronouncements melded with independent reporting — won’t last long, if at all. 
Part of it is because we haven’t had a common reality for some time now. Americans who follow the news get their news from different sources, which shapes their perception of basic facts. Most Americans don’t follow the news at all, and an alarming number of people get their understanding of the world from the internet, where very sharp armchair experts sit side by side with deranged conspiracy theorists.
And some of it is because Trump himself has conditioned us not to believe a single thing that he, or anyone in his orbit, says. 
This is the scenario — a fast-moving, potential catastrophe where we need real faith in federal leadership — that we’ve been worrying about since the first days of the Trump presidency, when then-White House press secretary Sean Spicer hectored reporters and insisted that they had falsely reported on the size of the crowd at Trump’s inauguration. 
It was a petty claim, and one that was chilling because it was so easy to debunk. If you start your presidency lying about something so transparently false, what does that mean when you talk about stuff we can’t see with our own eyes?

  
    Related
  

  
    This is what Trump did in the days before his coronavirus test, in photos
  

And it has continued through then, at more or less a daily rate. Trump and his circle lie reflexively. They lie about enormously consequential stuff, like Trump’s repeated assurances that the coronavirus wasn’t anything to worry about, though he was privately acknowledging that it was “deadly stuff.” Most recently, the president has repeatedly lied about the threat of election fraud in a transparent attempt to sow doubts about the results of November’s election.
And they lie about the smallest things. This week, Trump’s press secretary Kayleigh McEnany falsely claimed that Amy Coney Barrett, Trump’s nominee for the Supreme Court, was a Rhodes scholar (she didn’t receive the prestigious Rhodes Scholarship, but instead graduated from Rhodes College).
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        President Trump held a press conference addressing news that the New York Times obtained years of his tax returns. Sitting alongside the president were former New Jersey Gov. Chris Christie, former New York City Mayor Rudy Giuliani, and White House press secretary Kayleigh McEnany on September 27.
      
      
        Brendan Smialowski/AFP via Getty Images
      
    
  


Getting accurate information about the health of the president of the United States has always been a problem, both because presidents and their advisers weren’t eager to tell anyone that America’s leader may be unwell, and because reporters around them often kept quiet.  
Franklin Delano Roosevelt, for instance, asked photographers not to publish images of him struggling to walk because of polio-induced paralysis. Reporters like Lesley Stahl kept concerns about Ronald Reagan’s mental fitness to themselves; years after leaving office, Reagan announced that he had Alzheimers, but he never indicated whether it affected him at the time.
Those kinds of questions about a president’s health would be nearly impossible to keep quiet today. We’re in a much different media environment, with a much more aggressive press, and much more access to information. 
But even now, we know that we’ve known very little about Trump’s health. Recall, for instance, the letter Trump’s private doctor released in 2015 announcing that if Trump was elected he would “be the healthiest individual ever elected to the presidency” — which turned out to be dictated, word for word, by Trump himself. Or more ominously, Trump’s unplanned and still-unexplained visit to Walter Reed hospital nearly a year ago.
But past presidential health concerns — including Trump’s dissembling about his own status — were also long-term problems that didn’t necessarily have to be grappled with immediately. 
Now, though, we have a real-time crisis: We’re well aware that Trump has a disease that is particularly deadly for older, overweight men, but we have no reason to trust anything the White House says about the state of his health. What happens if Trump is truly incapacitated, or worse? Who will we trust to relay that information?
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        President Trump boards Air Force One on his way to Bedminster, New Jersey, for a fundraising event on October 1.
      
      
        Mandel Ngan/AFP via Getty Images
      
    
  


The one saving grace about the Trump administration’s attempt to conceal the truth from the world, about everything, is that it has been terrible about it. Some of the lies, like Spicer’s crowd-size fiction, can be debunked on the spot; others are quickly surfaced by the many leakers in and around the White House, who relay different versions than the Trump-dictated reality to reporters. And some become clearer over time, like Bob Woodward’s recent book Rage, which meticulously details Trump’s lies about the early months of the pandemic using taped, on-the-record conversations with Trump himself as the primary source.
But even top medical professionals with state-of-the-art equipment and unlimited resources — the ones who will be caring for Trump now — run up against the limits of knowledge when trying to assess someone’s health. And that’s even more true with a virus that we’re still learning about, less than a year after it surfaced in China. 
And it certainly isn’t something the general public can assess on its own. Even if Trump appears in public at some point to assure us, we won’t have any idea what’s actually happened or happening to him. 
So we can only hope that Trump tells us the truth, but there’s no reason to think that will happen. We can also hope that reporters in and around the White House can provide a more accurate understanding of what people in and around the White House think is happening. But fundamentally, we’re going to be in a haze, hoping it all works out. It’s a terrible place to be.

  
  
  
      




  After months of working with Facebook to safeguard the 2020 election, several national Democratic Party leaders say the company has failed to meet promises to stem the tide of misinformation spreading on its platform. If the election’s results are contested after November 3 — which is an acute concern since a surge in mail-in ballots is expected to delay the count and President Trump has refused to commit to a peaceful transition of power — they worry that Facebook is utterly unprepared to prevent people from using its platform to spread chaos.
Recode spoke with four sources with direct knowledge of ongoing monthly private conversations about election misinformation between several senior Democratic party committee leaders and senior members of Facebook’s policy team, including VP and director-level staff. These sources spoke on the condition of anonymity for fear of repercussions for discussing private talks. 
Democratic sources told Recode that monthly calls with Facebook that started in May have been “maddening” and have left party members “banging their head against the wall.” They say Facebook employees in these meetings — while seemingly well-intentioned — have failed to stop the spread of misinformation attacks against Democratic candidates, been reluctant to share information about extremist groups encouraging voter suppression, and appeared “flat-footed” in their plan for dealing with conflicting information about the election results.
Facebook spokesperson Tom Reynolds said that Facebook has been proactive in working with party leaders on both sides of the aisle about stopping abuse of its platform ahead of the election and that this is a top priority for the company.
“Elections have changed and so have we. Since 2016, we’re the only tech platform to build a global fact-checking network of over 70 fact-checkers. We have disrupted more than 100 coordinated inauthentic behavior networks and have hired 35,000 people to work on enforcing our policies and keeping the people on Facebook safe,” said Reynolds.
In recent months, Facebook has taken several steps to try to limit election chaos on its platform. It’s banned political ads in the week leading up to the election and promised to take down specific misrepresentations about voting or about when to vote. It’s launched a hub, its Voter Information Center, which the company says has so far helped 2.5 million Americans register to vote. And the company has tightened its rules around who can purchase political ads, though Facebook still doesn’t offer voters much insight into why they’re targeted with political ads on its platform. 
Facebook is “essentially slapping Band-Aids on wounds that require emergency room stitches”
But some Democrats believe these moves aren’t enough to stop the barrage of half-truths, lies, and violent rhetoric about the election process that continue to spread on the platform. 
“They’re essentially slapping Band-Aids on wounds that require emergency room stitches,” said one Democratic source familiar with ongoing discussions with Facebook.
While Democrats want to see Facebook more aggressively remove misinformation relating to the election, Facebook faces opposing pressure from Trump and leading Republican lawmakers who claim (without evidence) that Big Tech companies are biased against conservatives when they moderate their platforms. So far, it’s the Republicans that have been accusing Facebook and other tech companies of political bias; now Democrats, including Joe Biden and top Democratic lawmakers, are starting to get louder in their criticism, too. 
Four years after 2016, Facebook is grappling with similar problems
In 2016, Russian state operatives exploited Facebook by using fake accounts to sow political division in the US, posting ads and other content that reached some 146 million Americans on Facebook and Instagram. (Twitter also reported that tens of thousands of Russian-government-linked accounts reached around 670,000 Americans leading up to the race.) After initially calling it “crazy” to think that Facebook influenced the outcome of the election, Facebook CEO Mark Zuckerberg later acknowledged Facebook’s role in spreading disinformation, apologized, and promised to do better.


  Do you work at Facebook and want to talk? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.




As part of its promise to improve, the company has worked more closely with both Democratic and Republican Party leaders, along with outside researchers and US intelligence agencies to police misinformation on the platform. 
Even if Democrats are motivated at least in part by their party’s own interests, their complaints about Facebook raise concerns for voters on both sides of the political aisle. What’s at stake is ensuring Americans have accurate information about how to vote.
“Facebook constantly says they’re working on it. We understand these are developing threats, but at the same time, elections have been on the calendar for a while.”
On two different occasions in September, Trump encouraged people in North Carolina to vote twice, by mail-in ballot and in person. On the first occasion, Trump said “send it in early, and then go and vote, and if it’s not tabulated, you vote, and the vote is gonna count.” Facebook took down instances of the video for violating its policies against voter fraud.
On the second occasion, in a Facebook post, Trump encouraged double-voting with more nuance. He once again cast doubt on the reliability of voting by mail and encouraged voters to go to polls and check if their mail-in ballot had been counted or not, and if not, to vote in person. Facebook left this post up and added a label under it linking to its Voter Information Center and asserting that voting by mail is trustworthy.  
Earlier this summer, Trump falsely asserted that every Californian will automatically receive an absentee ballot, which will lead to mass voter fraud (in fact, only registered voters will receive ballots, and there is no evidence of mass voter fraud). Facebook left this post up.
Looking ahead, Democratic Party leaders worry that Trump — or his supporters — could prematurely declare a false victory via social media on election night and that Facebook is ill-equipped to handle the situation.
“Facebook constantly says they’re working on it,” said another source. “We understand these are developing threats, but at the same time, elections have been on the calendar for a while, and it’s unsettling that we have less than 50 days to go.”
Facebook has made progress, but it’s not enough
According to sources, Democrats hoped that monthly meetings between campaign committee leaders and Facebook’s policy team would be an opportunity to talk through substantive solutions to problems around misinformation on Facebook’s platform. Instead, several sources said that Facebook largely used the meetings to run a PR charm offensive, trying to convince Democrats that Facebook is working hard to fix its problems and that Facebook CEO Mark Zuckerberg is a “principled person.”
Democratic leaders told Recode they’ve seen some progress overall from Facebook since 2016: The company has a more direct line of communication with Democrats than four years ago, when these monthly meetings weren’t even happening. They acknowledge Facebook has gotten better at identifying malicious foreign actors. 
But a lot has changed in the US political landscape since 2016, and domestic actors, including some Republican Party leaders and extremist groups, have started using disinformation tactics also used in foreign interference. But Facebook hasn’t adapted, Democratic leaders say. 
The company has hesitated and at times outright refused to strike down politicians’ false claims about voting, particularly when President Trump makes such claims, and it’s been slow and insufficient when dealing with extremist groups such as QAnon. 
“Across the board, progressives, Democratic operatives, and civil rights leaders have been increasingly more and more frustrated, more exasperated, and less hopeful that Facebook has any desire to do the right thing,” said Jesse Lehrich, a foreign policy spokesperson in Hilary Clinton’s 2016 campaign who now leads the social media policy nonprofit Accountable Tech. Lehrich was not present in Democratic Party leaders’ recent meetings with Facebook. 
“People have spent a lot of time in good faith to improve some of these things, and there was just a tipping point where everyone thinks it’s a waste of your time to work with these people [Facebook] because they just string you along,” Lehrich told Recode.
Now, Democratic sources say time is running out — it’s 33 days until the election.
“It feels like we’re just continuing to just throw things into the void and yell off of a rooftop, and it’s falling on deaf ears,” said one Democratic operative with direct knowledge of the meetings between Facebook’s policy team and Democratic campaign leaders. 
Several other top Democratic operatives who haven’t been present in the monthly meetings called out two Facebook leaders, besides Zuckerberg, as sources of frustration: Facebook COO Sheryl Sandberg and VP of Public Policy Joel Kaplan. Neither Sandberg nor Kaplan are present in the monthly meetings. Sandberg has long been the public face and behind-the-scenes reputation manager for Facebook when dealing with Democrats, civil rights leaders, and other liberal groups, which hasn’t always gone well. Earlier this summer, Sandberg and Zuckerberg had a disastrous call with leaders of civil rights groups such as Color of Change and the NAACP Legal Defense and Educational Fund over concerns about hate speech on Facebook.
Kaplan has long been scrutinized both inside and outside the company because of his conservative ties, including his public support of Brett Kavanaugh during the Supreme Court Justice’s contentious confirmation hearings. In August, BuzzFeed News reported that Kaplan had intervened to question a fact-check on a Facebook page belonging to right-wing figure Charlie Kirk. 
Democrats feel “in the dark” when it comes to how Facebook is handling misinformation
One key complaint Democrats have is that Facebook leaves it up to them to flag examples of political misinformation on its platform, according to several sources. Party leaders expected to see the social media giant take more responsibility for cleaning up its own messes. Facebook disputed this characterization and said the company regularly presents its strategy for attacking misinformation with both Democratic and Republican Party leaders.
“They [Facebook] say ‘let us know if you see anything, you should be monitoring for this,’” said one Democratic source with direct knowledge of the conversations. “Our approach is that we are a resource-stretched political committee; they are a multibillion-dollar company. They should be able to get this right and put in the necessary resources.”
Facebook employees have also been tight-lipped about the extent of the company’s monitoring of extremist groups in monthly meetings, according to three sources familiar with the misinformation discussions.
In one recent meeting, Democratic campaign committee leaders asked Facebook to share which extremist groups on the platform the company was monitoring that could cause physical harm during the election cycle, sources said. Facebook representatives declined to name any, according to three sources with direct knowledge of the meeting. 
“I’m not asking for Facebook to share specific data, but they wouldn’t even name five or 10 of the extremist groups that they were most closely watching,” said one source. 
“It didn’t seem like something they were very much on top of,” said another source.
Facebook’s Reynolds said the company takes extremist threats seriously and prioritizes this work. In recent months, the platform has tightened its policies against groups like QAnon, the boogaloo movement, and others — but it has also received criticism for not being quick or comprehensive enough. 
The dreaded “post-election, pre-results” period
Democrats, civil rights leaders, bipartisan policy leaders, and Facebook itself have warned about the dangerous so-called “post-election, pre-results” period when election polls have closed but the vote totals aren’t finalized yet. In 2020, that’s expected to be a bigger issue than usual because many more people are expected to vote by mail due to Covid-19. It’s a time when political candidates and bad actors could try to undermine Americans’ faith in the electoral process, causing civil unrest.
Trump has been declaring for months that he expects the election to be “rigged” because of the expected increase in mail-in ballots — setting the stage for a contested result.
“We’re not saying you should do this in order to help Democrats win. That is not fair. It’s to protect voters.”
Sources familiar with Facebook’s discussions with Democrats say Facebook doesn’t seem prepared enough for this scenario.
“It was brought up on a recent call, about what happens post-election if there is not a result, what happens between Election Day and results day,” said one Democratic source familiar with the discussions. “And they essentially had nothing.”
Another Democratic Party source familiar with the discussions said that Facebook assured Democrats that the company was working on “tabletop exercises” and “strategizing behind the scenes” to prepare for election night. When pressed on specific scenarios, however, such as Trump contesting the election results, the company wouldn’t go into details.
“With the hypotheticals we would bring up, they [Facebook] would never have any interest in discussing them,” said another Democratic Party source familiar with the discussions. “They would say, ‘We can’t really deal with hypotheticals, we don’t know necessarily how President Trump would respond.’” 
Since Facebook first started its monthly talks with Democrats, it has announced it will work with respected outside sources like Reuters to call the elections, and it will mark up any premature posts about election results with a link to those trusted sources. It also announced it will ban political ads that falsely declare a premature victory. Sources said these are welcome announcements, but not enough.
In order to truly safeguard rhetoric around the elections, party leaders say, Facebook needs to go further and actually plan to mark up posts that dispute election results with a clear fact-check, similar to how Twitter has in the past for some of Trump’s posts. Or delete the posts altogether. 
Facebook declined to comment on further plans for how it will handle disputed election results.
“I really don’t think that any of these asks have been partisan,” said one Democratic Party source. “We’re not saying you should do this in order to help Democrats win. That is not fair. It’s to protect voters.”
All of this leaves Democratic leaders deeply concerned about what might unfold on Facebook in the days leading up to the election, and during the still-undetermined time period before votes are counted and the election is called. 
Experts estimate it might take weeks until there is a final result, and in the presidential debate earlier this week, Trump suggested it could take months before he accepts the result of the election.
“There is just a lack of urgency when it comes to the election part of this. We’re talking about just a handful of weeks at that point out from Election Day, and they’re still unable to give us timelines,” said one source with knowledge of the meetings. “There’s a lot of talk, but not really much transparency.”

  
  
  
      




  After months of working with Facebook to safeguard the 2020 election, several national Democratic Party leaders say the company has failed to meet promises to stem the tide of misinformation spreading on its platform. If the election’s results are contested after November 3 — which is an acute concern since a surge in mail-in ballots is expected to delay the count and President Trump has refused to commit to a peaceful transition of power — they worry that Facebook is utterly unprepared to prevent people from using its platform to spread chaos.
Recode spoke with four sources with direct knowledge of ongoing monthly private conversations about election misinformation between several senior Democratic party committee leaders and senior members of Facebook’s policy team, including VP and director-level staff. These sources spoke on the condition of anonymity for fear of repercussions for discussing private talks. 
Democratic sources told Recode that monthly calls with Facebook that started in May have been “maddening” and have left party members “banging their head against the wall.” They say Facebook employees in these meetings — while seemingly well-intentioned — have failed to stop the spread of misinformation attacks against Democratic candidates, been reluctant to share information about extremist groups encouraging voter suppression, and appeared “flat-footed” in their plan for dealing with conflicting information about the election results.
Facebook spokesperson Tom Reynolds said that Facebook has been proactive in working with party leaders on both sides of the aisle about stopping abuse of its platform ahead of the election and that this is a top priority for the company.
“Elections have changed and so have we. Since 2016, we’re the only tech platform to build a global fact-checking network of over 70 fact-checkers. We have disrupted more than 100 coordinated inauthentic behavior networks and have hired 35,000 people to work on enforcing our policies and keeping the people on Facebook safe,” said Reynolds.
In recent months, Facebook has taken several steps to try to limit election chaos on its platform. It’s banned political ads in the week leading up to the election and promised to take down specific misrepresentations about voting or about when to vote. It’s launched a hub, its Voter Information Center, which the company says has so far helped 2.5 million Americans register to vote. And the company has tightened its rules around who can purchase political ads, though Facebook still doesn’t offer voters much insight into why they’re targeted with political ads on its platform. 
Facebook is “essentially slapping Band-Aids on wounds that require emergency room stitches”
But some Democrats believe these moves aren’t enough to stop the barrage of half-truths, lies, and violent rhetoric about the election process that continue to spread on the platform. 
“They’re essentially slapping Band-Aids on wounds that require emergency room stitches,” said one Democratic source familiar with ongoing discussions with Facebook.
While Democrats want to see Facebook more aggressively remove misinformation relating to the election, Facebook faces opposing pressure from Trump and leading Republican lawmakers who claim (without evidence) that Big Tech companies are biased against conservatives when they moderate their platforms. So far, it’s the Republicans that have been accusing Facebook and other tech companies of political bias; now Democrats, including Joe Biden and top Democratic lawmakers, are starting to get louder in their criticism, too. 
Four years after 2016, Facebook is grappling with similar problems
In 2016, Russian state operatives exploited Facebook by using fake accounts to sow political division in the US, posting ads and other content that reached some 146 million Americans on Facebook and Instagram. (Twitter also reported that tens of thousands of Russian-government-linked accounts reached around 670,000 Americans leading up to the race.) After initially calling it “crazy” to think that Facebook influenced the outcome of the election, Facebook CEO Mark Zuckerberg later acknowledged Facebook’s role in spreading disinformation, apologized, and promised to do better.


  Do you work at Facebook and want to talk? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.




As part of its promise to improve, the company has worked more closely with both Democratic and Republican Party leaders, along with outside researchers and US intelligence agencies to police misinformation on the platform. 
Even if Democrats are motivated at least in part by their party’s own interests, their complaints about Facebook raise concerns for voters on both sides of the political aisle. What’s at stake is ensuring Americans have accurate information about how to vote.
“Facebook constantly says they’re working on it. We understand these are developing threats, but at the same time, elections have been on the calendar for a while.”
On two different occasions in September, Trump encouraged people in North Carolina to vote twice, by mail-in ballot and in person. On the first occasion, Trump said “send it in early, and then go and vote, and if it’s not tabulated, you vote, and the vote is gonna count.” Facebook took down instances of the video for violating its policies against voter fraud.
On the second occasion, in a Facebook post, Trump encouraged double-voting with more nuance. He once again cast doubt on the reliability of voting by mail and encouraged voters to go to polls and check if their mail-in ballot had been counted or not, and if not, to vote in person. Facebook left this post up and added a label under it linking to its Voter Information Center and asserting that voting by mail is trustworthy.  
Earlier this summer, Trump falsely asserted that every Californian will automatically receive an absentee ballot, which will lead to mass voter fraud (in fact, only registered voters will receive ballots, and there is no evidence of mass voter fraud). Facebook left this post up.
Looking ahead, Democratic Party leaders worry that Trump — or his supporters — could prematurely declare a false victory via social media on election night and that Facebook is ill-equipped to handle the situation.
“Facebook constantly says they’re working on it,” said another source. “We understand these are developing threats, but at the same time, elections have been on the calendar for a while, and it’s unsettling that we have less than 50 days to go.”
Facebook has made progress, but it’s not enough
According to sources, Democrats hoped that monthly meetings between campaign committee leaders and Facebook’s policy team would be an opportunity to talk through substantive solutions to problems around misinformation on Facebook’s platform. Instead, several sources said that Facebook largely used the meetings to run a PR charm offensive, trying to convince Democrats that Facebook is working hard to fix its problems and that Facebook CEO Mark Zuckerberg is a “principled person.”
Democratic leaders told Recode they’ve seen some progress overall from Facebook since 2016: The company has a more direct line of communication with Democrats than four years ago, when these monthly meetings weren’t even happening. They acknowledge Facebook has gotten better at identifying malicious foreign actors. 
But a lot has changed in the US political landscape since 2016, and domestic actors, including some Republican Party leaders and extremist groups, have started using disinformation tactics also used in foreign interference. But Facebook hasn’t adapted, Democratic leaders say. 
The company has hesitated and at times outright refused to strike down politicians’ false claims about voting, particularly when President Trump makes such claims, and it’s been slow and insufficient when dealing with extremist groups such as QAnon. 
“Across the board, progressives, Democratic operatives, and civil rights leaders have been increasingly more and more frustrated, more exasperated, and less hopeful that Facebook has any desire to do the right thing,” said Jesse Lehrich, a foreign policy spokesperson in Hilary Clinton’s 2016 campaign who now leads the social media policy nonprofit Accountable Tech. Lehrich was not present in Democratic Party leaders’ recent meetings with Facebook. 
“People have spent a lot of time in good faith to improve some of these things, and there was just a tipping point where everyone thinks it’s a waste of your time to work with these people [Facebook] because they just string you along,” Lehrich told Recode.
Now, Democratic sources say time is running out — it’s 33 days until the election.
“It feels like we’re just continuing to just throw things into the void and yell off of a rooftop, and it’s falling on deaf ears,” said one Democratic operative with direct knowledge of the meetings between Facebook’s policy team and Democratic campaign leaders. 
Several other top Democratic operatives who haven’t been present in the monthly meetings called out two Facebook leaders, besides Zuckerberg, as sources of frustration: Facebook COO Sheryl Sandberg and VP of Public Policy Joel Kaplan. Neither Sandberg nor Kaplan are present in the monthly meetings. Sandberg has long been the public face and behind-the-scenes reputation manager for Facebook when dealing with Democrats, civil rights leaders, and other liberal groups, which hasn’t always gone well. Earlier this summer, Sandberg and Zuckerberg had a disastrous call with leaders of civil rights groups such as Color of Change and the NAACP Legal Defense and Educational Fund over concerns about hate speech on Facebook.
Kaplan has long been scrutinized both inside and outside the company because of his conservative ties, including his public support of Brett Kavanaugh during the Supreme Court Justice’s contentious confirmation hearings. In August, BuzzFeed News reported that Kaplan had intervened to question a fact-check on a Facebook page belonging to right-wing figure Charlie Kirk. 
Democrats feel “in the dark” when it comes to how Facebook is handling misinformation
One key complaint Democrats have is that Facebook leaves it up to them to flag examples of political misinformation on its platform, according to several sources. Party leaders expected to see the social media giant take more responsibility for cleaning up its own messes. Facebook disputed this characterization and said the company regularly presents its strategy for attacking misinformation with both Democratic and Republican Party leaders.
“They [Facebook] say ‘let us know if you see anything, you should be monitoring for this,’” said one Democratic source with direct knowledge of the conversations. “Our approach is that we are a resource-stretched political committee; they are a multibillion-dollar company. They should be able to get this right and put in the necessary resources.”
Facebook employees have also been tight-lipped about the extent of the company’s monitoring of extremist groups in monthly meetings, according to three sources familiar with the misinformation discussions.
In one recent meeting, Democratic campaign committee leaders asked Facebook to share which extremist groups on the platform the company was monitoring that could cause physical harm during the election cycle, sources said. Facebook representatives declined to name any, according to three sources with direct knowledge of the meeting. 
“I’m not asking for Facebook to share specific data, but they wouldn’t even name five or 10 of the extremist groups that they were most closely watching,” said one source. 
“It didn’t seem like something they were very much on top of,” said another source.
Facebook’s Reynolds said the company takes extremist threats seriously and prioritizes this work. In recent months, the platform has tightened its policies against groups like QAnon, the boogaloo movement, and others — but it has also received criticism for not being quick or comprehensive enough. 
The dreaded “post-election, pre-results” period
Democrats, civil rights leaders, bipartisan policy leaders, and Facebook itself have warned about the dangerous so-called “post-election, pre-results” period when election polls have closed but the vote totals aren’t finalized yet. In 2020, that’s expected to be a bigger issue than usual because many more people are expected to vote by mail due to Covid-19. It’s a time when political candidates and bad actors could try to undermine Americans’ faith in the electoral process, causing civil unrest.
Trump has been declaring for months that he expects the election to be “rigged” because of the expected increase in mail-in ballots — setting the stage for a contested result.
“We’re not saying you should do this in order to help Democrats win. That is not fair. It’s to protect voters.”
Sources familiar with Facebook’s discussions with Democrats say Facebook doesn’t seem prepared enough for this scenario.
“It was brought up on a recent call, about what happens post-election if there is not a result, what happens between Election Day and results day,” said one Democratic source familiar with the discussions. “And they essentially had nothing.”
Another Democratic Party source familiar with the discussions said that Facebook assured Democrats that the company was working on “tabletop exercises” and “strategizing behind the scenes” to prepare for election night. When pressed on specific scenarios, however, such as Trump contesting the election results, the company wouldn’t go into details.
“With the hypotheticals we would bring up, they [Facebook] would never have any interest in discussing them,” said another Democratic Party source familiar with the discussions. “They would say, ‘We can’t really deal with hypotheticals, we don’t know necessarily how President Trump would respond.’” 
Since Facebook first started its monthly talks with Democrats, it has announced it will work with respected outside sources like Reuters to call the elections, and it will mark up any premature posts about election results with a link to those trusted sources. It also announced it will ban political ads that falsely declare a premature victory. Sources said these are welcome announcements, but not enough.
In order to truly safeguard rhetoric around the elections, party leaders say, Facebook needs to go further and actually plan to mark up posts that dispute election results with a clear fact-check, similar to how Twitter has in the past for some of Trump’s posts. Or delete the posts altogether. 
Facebook declined to comment on further plans for how it will handle disputed election results.
“I really don’t think that any of these asks have been partisan,” said one Democratic Party source. “We’re not saying you should do this in order to help Democrats win. That is not fair. It’s to protect voters.”
All of this leaves Democratic leaders deeply concerned about what might unfold on Facebook in the days leading up to the election, and during the still-undetermined time period before votes are counted and the election is called. 
Experts estimate it might take weeks until there is a final result, and in the presidential debate earlier this week, Trump suggested it could take months before he accepts the result of the election.
“There is just a lack of urgency when it comes to the election part of this. We’re talking about just a handful of weeks at that point out from Election Day, and they’re still unable to give us timelines,” said one source with knowledge of the meetings. “There’s a lot of talk, but not really much transparency.”

  
  
  
      




  On September 18, 2020, President Trump signed an executive order to ban Chinese-owned apps TikTok and WeChat. The next day, my mother called me and told me my beloved grandfather, who lives in Beijing, China, had a recurrence of colon cancer. Immediately my heart dropped to the floor, imagining that I only had 60 days to chat with my grandfather in what is likely his final months. Without WeChat, how were we going to talk? 
As Trump escalates his rhetoric against China, much of the media has focused on Trump’s ban on TikTok, the video-creation app popular among teens. But less attention has been paid to WeChat, an app little known to most Americans, but which acts as the lifeblood of the internet for Chinese citizens as well as Chinese living overseas. Because so many foreign websites are banned in China, WeChat functions as Facebook, Instagram, Zoom, Venmo, bill pay, and online banking all rolled into one app. 
Trump’s ban, which would have prevented new downloads on US app stores as well as overseas payments between users, would have resulted immediately in a loss of functionality of WeChat. Although the ban has been temporarily blocked, my family is waiting anxiously for the day that our communications are cut off. There’s no immediate alternative to WeChat because so many communication apps are blocked for Chinese users by the Chinese government. 
The day after Trump’s executive order, US Magistrate Judge Laurel Beeler of Northern California issued a preliminary injunction on the ban, asserting that it violated First Amendment rights. As of now, the fate of WeChat — and TikTok — in America is uncertain. But one thing is for sure: If Trump’s WeChat ban is put into effect, which he is still threatening to do, Chinese Americans will be cut off from their families back home during an already stressful worldwide pandemic.  
I use WeChat every day. My entire extended family, with the exception of my mother and sister, live in Mainland China. I’m part of multiple big 朋友群 (messaging groups) — this is analogous to a non-Chinese family’s iMessage — with different members of my family, from my grandparents to my most distant cousins. I receive messages constantly, from trivial little updates, like my aunt displaying her rations — the many bags of rice and gallons of oil she received that day from her company (a common practice in China) — my grandparents’ schedules of their daily walks, or even just pictures of the family cat. Because of the time difference, I often wake up to a barrage of messages, such as my grandparents and uncle discussing when he will pick the grandparents up from their Beijing apartment. It makes my family, who mostly live some 7,000 miles away from my apartment in Brooklyn, feel like we live together in the same house. 
I also use WeChat to call my grandparents. I’m notified of when my mother starts a group call for our messaging group, and if I want to, I can easily join and talk to all of them at once. It’s nice being able to see their faces on my small iPhone screen, even if my grandmother puts her phone camera right under her nostrils. When I wrote an article about affirmative action in Chinese that was published on WeChat, I also posted it on my WeChat Stories to share with my family and Chinese friends. But my absolute favorite thing about WeChat is when I send any message after 9 pm, and my grandmother responds with a sticker of flowers with the phrase “早点睡觉”— “go to sleep earlier.”
It makes my family, who mostly live some 7,000 miles away from my apartment in Brooklyn, feel like we live together in the same house
I didn’t always have such a close relationship with my relatives. I was born in the United States to my Chinese mother and father. My parents immigrated here in the early ’90s as students and medical refugees, and we were the only family members who lived in the United States. I was raised for the first two years of my life in Beijing by my grandparents as a satellite baby. When I eventually went back to the United States, I barely saw the people who helped raise me. 
I was always jealous of the experiences with family that my non-immigrant classmates would casually mention. During Christmas and New Year’s, other children visited their relatives while I would sit at home with my mother and father. When our school would tell us students to sell wrapping paper and magazine subscriptions to family members for a fundraiser, I felt totally left out. I was raised without knowing what it was like to have family close by, and I often felt like I barely knew my Chinese family. We didn’t call often because it was expensive — before WeChat was founded in 2011, my parents would spend hours researching the cheapest long-distance phone plan before dialing into my grandmother’s home phone in China. 
WeChat has helped close that distance. I’m able to talk to my grandparents face to face virtually, and I got to know my third cousins in rural Hubei. Because I made a Chinese bank account and linked it to my WeChat account, my relatives have no excuse for not sending me digital red envelopes of money during Chinese New Year. WeChat has also been essential during the Covid-19 pandemic. I had to cancel my trip to Wuhan for Chinese New Year, which would have been my first time spending such an important holiday with relatives in China. When I read the news about how dire it was in Wuhan, I was able to get in touch on WeChat with my relatives there who confirmed that they had not been infected. It was a huge relief.
Beyond communication with family overseas, there are other reasons why WeChat is essential for members of the Chinese diaspora. WeChat has a translation function, including a speech to text translation function, which helps elderly and low-English-proficiency Chinese live and function in English-speaking communities. My friends who teach English to Chinese people on Zoom rely on WeChat’s payment function for their weekly salary. My mother and other Chinese Americans have formed WeChat mutual aid groups, helping migrant Chinese workers who are infected with Covid-19 get medicine and shelter. In addition, there are groups of Chinese Americans who are actively writing articles to other Chinese Americans on WeChat through initiatives like the WeChat Project, as a way to organize Chinese Americans to vote and support Black Lives Matter.  
I understand that Chinese-owned apps like WeChat come with privacy concerns because of government monitoring. I have concerns, too. I notice if I try to send a meme with Xi Jinping’s face over WeChat it “mysteriously” does not get sent to the receiver, and I know that my metadata — the call length, the location of the user — and the content I talk about is being monitored and saved. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        The writer’s grandfather and grandmother outside of Beijing Mining University in Beijing, China in 2019.
      
      
        Courtesy of Rita Wenxin Wang
      
    
  



Chinese people are all too aware we are being watched, but we have no choice. The WeChat app is all-encompassing and is impossible to substitute — most other messaging apps including WhatsApp and Line are banned by the Great Firewall, and Chinese-branded smartphones like Huawei are still more popular and less expensive for the average Chinese than the American Apple iPhones. 
I also understand that the Trump administration is using its ban as a ploy to ease data concerns of Chinese apps. But I didn’t ask to be born to a family that lives under an authoritarian regime with incredibly limited access to Western, English-speaking media. That is the reality of Chinese Americans whose loved ones live in China. We’re not spies using WeChat to steal government secrets; we’re ordinary people trying to stay in touch with our families abroad. 
The modern transnational family has been closer than ever because of technology, but Sinophobia and escalating political tensions will drive my family apart in ways we never wanted to be and never asked to be. I’m now worried about not being able to communicate with my family, especially as my grandfather could be in his last few months. My relationship with my him, once so distant, has strengthened over the years, in large part because of WeChat. He loves to hear what I am writing about or the books I am reading, and we spent hours on WeChat talking about Daoism for a high school World History assignment. After my mother told me my grandfather’s cancer had recurred, I spent every call, every message we sent to each other, wondering if that was the last time we would speak. The pandemic has only made it worse — as a US citizen, I am unable to travel to China to see him. 	
We’re not spies using WeChat to steal government secrets; we’re ordinary people trying to stay in touch with our families abroad
Anticipatory grief is the grief that occurs before a loved one’s death rather than after. It’s an “in-between” emotional space that can lead to a roller coaster of emotions, from guilt to anger. But crucially, it may be a step toward closure that people who lose loved ones suddenly do not have. With Trump’s executive order to ban WeChat, a ban that looms over my head as my family comes to terms with my grandfather’s diagnosis, I wonder if I’ll be able to get the closure, however little, that I need. 
I — and all other Chinese Americans — am put into the impossible position of weighing between country and family, between data privacy concerns and the desire to talk to the people who raised me. I’m an American, but I’m willing to take the data privacy risk to call my grandparents, who I see in person perhaps once every three years. 
Wouldn’t you do that, too?
Rita Wenxin Wang is a freelance writer based in Brooklyn. She is passionate about Asian American diaspora and leftist politics. 

  
  
  
      




    
    
  

  The nation’s very richest people are sitting out the presidential race. Not a single one of the 10 wealthiest people in the US — almost all of them tech billionaires — have shared who they’re voting for in November.
That, in part, reflects two simultaneous dynamics that undergird Silicon Valley:


Public animosity toward the ultra-wealthy, particularly those leading Big Tech companies, has been rising in recent years, making an endorsement from one of these ultra-wealthy people as much of a liability as an asset for a candidate.
Even if candidates wanted their approval, tech billionaires are politically beleaguered these days, especially those who are running companies and must traipse across an antitrust minefield and unproven accusations of political bias. So they aren’t exactly rushing to extend themselves even more by taking sides on President Donald Trump’s reelection.

So it nets out in a convenient way for all sides: The world’s richest people aren’t offering their endorsements. And candidates aren’t asking.
These high-profile figures don’t typically endorse presidential candidates. But most of the tech giants and their leaders are consistently at odds with Trump’s White House on issues ranging from immigration to climate change. 
Many tech leaders have consolidated around former Vice President Joe Biden, especially over the summer after he wrapped up the nomination. People like Reid Hoffman, Eric Schmidt, and Laurene Powell Jobs — billionaires, yes, but not household names — are investing their money and energy to oust Trump.
But at the tippy top of America’s billionaire rankings — the rich who are prominent outside of Silicon Valley, who measure their net worth not by the billions but by the tens of billions, the few who have celebrity cachet — there is silence. That’s true also for the single non-tech billionaire in the top 10, Warren Buffett, who is similarly declining to weigh in after pushing hard to beat Trump in 2016.
Many of these tech billionaires have gotten even more fantastically wealthy during the Trump era as their companies’ stock prices surged. But on the whole, they have positioned themselves as inclusive, progressive, and civic-minded leaders who take principled stands when the moment demands it, like when Trump pulled out of the Paris climate accord.
It doesn’t exactly seem like cultivating the public support of these leaders has been a priority for Biden. In a sign of how political campaigns see Silicon Valley as a partial liability these days, the Biden campaign has not unveiled a list of endorsements from business and tech leaders. That is a departure from 2016, when Hillary Clinton’s campaign announced a list of endorsements from people like Netflix CEO Reed Hastings, Facebook chief operating officer Sheryl Sandberg, and the three billionaire founders of Airbnb. (None of them have offered formal endorsements this year, though some have made donations to Biden.)
One person who signed that list in 2016 said they weren’t aware of any efforts to organize a similar list four years later. There are some less-developed efforts affiliated with the Biden campaign to privately organize tech leaders for fundraising purposes, but there’s no flashy public endorsement rollout.
Much has changed since 2016, though. The Democratic Party is locked in a debate about whether these Silicon Valley billionaires and their companies have too much power in American society, a debate that was on the margins during the 2016 campaign. The Biden campaign has been particularly critical of Facebook for not policing Trump’s posts more aggressively — so it would be quite cacophonous for it to boast about an endorsement from Sandberg in 2020, for instance.
And for some of the billionaires themselves, it likely boils down to a business decision. The world’s richest person, Jeff Bezos, has recently been more willing to make donations to political candidates, and, as the owner of the Washington Post, has frequently tangled with Trump. Mark Zuckerberg, the world’s fourth-richest person and an advocate for criminal justice reform, has said that he has been “disgusted” by Trump’s rhetoric on race. But neither of them have endorsed Biden in the presidential race, which would risk exposing them to Trump’s wrath.
It could also endanger the efforts that tech leaders have taken to portray their companies as politically neutral. Trump and his allies frequently accuse Big Tech companies of having an anti-conservative bias, and the Silicon Valley giants often prove skittish about giving that largely unproven argument even the perception of credibility.
And then Elon Musk, whose net worth has skyrocketed in 2020 under Trump but who pitches himself as an environmentalist, said in an interview published on Monday that he was an undecided voter and cast doubt on Biden’s mental sharpness, which has come into question and is an aspect that Trump has recently sought to exploit.
Even those who are not public company CEOs are skittish about becoming partisan combatants. Bill Gates, Microsoft’s retired founder and the second wealthiest American, has emerged as a civic leader during the Covid-19 pandemic and has been unsparingly critical in his comments about how Trump is handling the crisis. But despite being ‘tempted” to issue an endorsement, he and his wife declined to do so because of their desire to remain steadfastly nonpartisan (they have a policy to not issue endorsements).
The co-founders of Google, Larry Page and Sergey Brin, rank as the eighth and ninth richest people in the US. But they hardly make public statements of any kind these days, even though Brin has quietly made some donations to progressive groups and even protested the Trump administration’s immigration ban at San Francisco’s airport the week after Trump’s inauguration. Neither has weighed in on Trump vs. Biden, although Brin’s wife made a small donation to Biden this summer. (A Brin spokesperson didn’t return a request for comment on whether Brin shared her thinking.)
The two mega billionaires who have flirted with making their views clear have been Oracle founder Larry Ellison and former Microsoft CEO Steve Ballmer. Ellison has emerged this year as a rare Trump ally in Silicon Valley, hosting a fundraiser for Trump that collected $7 million for his reelection bid. But Ellison did not attend the event, and has insisted that it should only be read as his patriotic desire to root for any incumbent president, not a formal endorsement. Ballmer’s wife, Connie, gave half a million dollars to a pro-Biden super PAC earlier this year, although Ballmer himself has not.
Asked if Steve Ballmer was backing Biden, a spokesperson said he was “nonpartisan” and declined to comment on his politics.
It doesn’t take much guesswork to surmise who each of these tech titans is choosing as Election Day draws near. But not saying it out loud says something about Silicon Valley’s popularity in 2020.



  
  
  
      





  Amazon accounts for nearly 40 percent of e-commerce sales in the US today, and it takes a cut of even more online shopping by selling payments services and other technologies to external shopping sites. Now, the online retail giant is making a play to grab a piece of brick-and-mortar shopping, too — and it wants customers to literally lend a hand to do it.
Amazon on Tuesday is unveiling a new biometric technology called Amazon One that allows shoppers to pay at stores by placing their palm over a scanning device when they walk in the door or when they check out. The first time they register to use this tech, a customer will scan their palm and insert their payment card at a terminal; after that, they can simply pay with their hand. The hand-scanning tech isn’t just for Amazon’s own stores — the company hopes to sell it to other retailers, including competitors, too.
The technology will be available at the entrance of two of the company’s Amazon Go cashierless convenience stores in Seattle, Washington, starting Tuesday, and will roll out to the rest of the chain’s 20-plus stores in the future, Amazon Vice President Dilip Kumar told Recode in an interview Monday. Recode reported in December that Amazon had filed a patent application for such a hand-payment technology.
The technology could also show up in Whole Foods stores, with Amazon hinting in a press release that it will introduce palm payments in the coming months at its other stores beyond its Amazon Go locations. Kumar wouldn’t comment on a potential Whole Foods implementation, though the New York Post reported a year ago that such a plan was in the works.
But the Amazon executive did make it clear that the company expects to sell the technology to other retailers, like it began doing earlier this year with its “Just Walk Out” technology — the cocktail of cameras, sensors, and computer vision software that powers Amazon Go stores. Kumar said the Amazon One pitch to other retailers is straightforward: reduce friction for your customers at checkout, thereby shortening lines and increasing how many shoppers are served along the way. 
Amazon’s plan to license these two homegrown technologies to other retailers, whether competitor or not, is the real story: Amazon isn’t satisfied with e-commerce dominance; it wants to earn a cut of more transactions in the physical retail world, where 80-something percent of commerce still takes place in the US. So it’s building out a futuristic suite of services to court other retailers, while showcasing the technology in its own stores as case studies. 
One obvious question is whether retailers, many of which consider Amazon a competitor of one sort or another, will want to do business with the tech giant. Kumar pointed to Amazon Web Services, the company’s $40 billion division that leases computing power, data storage, and myriad software capabilities to internet companies big and small, as an example of an Amazon offering that attracts competitors. 
Amazon will collect data on where Amazon One customers shop when they use the payment option, but it will not know what shoppers purchase or how much they spend inside third-party retail stores. An Amazon spokesperson said the company has “no plans to use transaction information from third-party locations for Amazon advertising or other purposes,” and shoppers can sign up for the service without linking it to an Amazon customer account if they choose.
Another question is whether enough people will be willing to hand over scans of their hands to Amazon in order to save a bit of time at checkout. It’s true that a no-touch payment method might be more attractive today, during the Covid-19 pandemic, than even a year ago. But new payment methods often face steep adoption challenges, and that’s even when biometrics aren’t involved. Biometric tracking poses a host of privacy concerns, including the potential of targeted hacking or a mass data breach.
Kumar, the Amazon executive, said the more locations where Amazon can introduce the technology, the more valuable customers will find it and be willing to give it a try. That’s why the company plans to pitch other use cases beyond payments. Kumar also said Amazon is discussing with potential partners the idea of linking palm scans with building IDs to replace office ID cards, or with event tickets for stadiums or arenas — two settings that don’t sound especially appealing during a global health crisis, but may be in the future when gathering in a crowd won’t pose serious health risks.
The executive added that Amazon chose palm scans over other biometric options for a few reasons. One, he said, is that it’s not easy for a bad actor to identify a person by simply viewing an image of their hand, if that material ever leaked. Another is the uniqueness of each person’s hand. “Even identical twins have many differences in their palm structure,” he said. A spokesperson added that the images are encrypted when scanned, and then “sent to a highly secure area we custom-built in the cloud for analysis and storage.”
To some, the upside still won’t be worth it. “How lazy are people that they will hand over their handprints so they don’t have to take out their wallet?” my wife asked when I mentioned the new technology to her in an embargoed dinner-table discussion. But Apple’s Touch ID fingerprint-scanning tech and its Face ID face-scanning tech also seemed a little crazy at first — until they weren’t. 
And if enough customers trust Amazon with the trade-off, physical retailers will face an interesting dilemma: chase the future by aligning with the most powerful tech company in retail, or stick to the present and hope their customers don't stray as a result.


  
  
  
      





    
    
  


  Apple and Facebook have called a temporary truce in one of several fights between the two tech giants. But they’re still not pleased with each other.
The short version: Apple has backed off — for now — on its demand for a 30 percent cut of all revenue generated by online events that Facebook is letting small businesses sell via its iPhone app. The temporary change also affects other companies including ClassPass and Airbnb, which have been selling online workout classes and “experiences” during the pandemic.
The takeaway: If you’re a yoga instructor who wants to sell an online class for $10 via Facebook, you’ll keep all $10 every time someone pays for a class using an iPhone, instead of losing $3 of that to Apple’s App Store.   
And here are some buts: Apple says it will reinstate its 30 percent fee at the end of the year. And Facebook is complaining that Apple won’t remove its 30 percent tax for gamers who want to sell online access to their events.
Zoom out and the bigger picture is that Facebook is one of several tech companies that is complaining — or doing more than complaining — about Apple’s policies around the use of its giant iOS platform. It’s a long-simmering fight that has gotten much more public in the past few months as US lawmakers and regulators have mulled whether Apple should be reined in by antitrust restrictions. But it’s unclear whether Apple is going to give much ground.
Facebook’s fight with Apple over online events, for instance, is just one of three tiffs between the two companies that have cropped up in the past two months: Facebook has complained about the way Apple is treating a Facebook gaming app and is unhappy about new Apple privacy policies that cut into Facebook’s ad business. 
Other tech companies publicly complaining about Facebook include Microsoft, which is also upset about the way Apple is treating a gaming app it wanted to distribute via iOS; and a newly announced “coalition for app fairness,” in which several companies accuse Apple of using “carefully crafted anti-competitive policies.”
Two of the companies in that coalition are doing more than public grousing. Spotify filed an antitrust complaint with European regulators last year, prompting an investigation. And Epic, the gaming company behind Fortnite, has publicly defied Apple’s app store policy, prompting Apple to kick Fortnite out of its App Store, which triggered an antitrust suit from Epic; a key hearing in that suit is scheduled for next week.
More details about Facebook’s online events push: Facebook says it will use its own Facebook Pay system to process payments for online events, which it is positioning as an effort to help small businesses affected by the pandemic. The company says it won’t take any cut from those sales until “at least August” of next year. 
Meanwhile, Facebook is going out of its way to poke publicly at Apple while announcing the change. Here’s a statement attributed to Facebook PR rep Joe Osborne: “This is a difficult time for small businesses and creators, which is why we are not collecting any fees from paid online events while communities remain closed for the pandemic. Apple has agreed to provide a brief, three-month respite after which struggling businesses will have to, yet again, pay Apple the full 30% App Store tax.” 
And here’s one from Vivek Sharma, VP of Facebook’s games unit: 
Apple’s decision to not collect its 30% tax on paid online events comes with a catch: gaming creators are excluded from using Facebook Pay in paid online events on iOS. We unfortunately had to make this concession to get the temporary reprieve for other businesses. For any Facebook Gaming creator who wants to use paid online events, we are not collecting any fees for purchases on Facebook desktop until at least August 2021. We know times are tough, and we’ll continue to help our gaming creator community wherever we can.
Apple execs, meanwhile, say they are giving Facebook (along with ClassPass and Airbnb) a temporary reprieve from the tax because of the pandemic, as their customers and vendors struggle to figure out how to survive in a remote economy.
But the company says it won’t change its stance on Facebook’s online gaming events, since those existed before the coronavirus pandemic and have always been a virtual experience. Here, for the record, is the company’s public statement: “The App Store provides a great business opportunity for all developers, who use it to reach half a billion visitors each week across 175 countries. To ensure every developer can create and grow a successful business, Apple maintains a clear, consistent set of guidelines that apply equally to everyone.”
One last bit of context: Plenty of people complain about both Apple and Facebook, but the two companies have tangled with each other for years. A recent example: Apple CEO Tim Cook, in a 2018 MSNBC interview, took pains to criticize both Mark Zuckerberg and the company he founded. As I wrote then, when Facebook was being pummeled for the Cambridge Analytica fiasco:

Cook has made a point of criticizing Facebook for both the Cambridge Analytica affair and its overall approach to consumer privacy in recent days. But it’s not a new stance for him or the company: He made similar comments about Facebook and Google in 2015, and his predecessor Steve Jobs went out of his way to contrast Apple’s privacy stance with rivals like Google in 2010.
...[Kara] Swisher posed a question for Cook: What would he do if he were Facebook CEO Mark Zuckerberg? His answer: “I wouldn’t be in this situation.”





 
  
  
  
      





  Some Amazon employees are furious after they discovered the company’s HR department appears to be quietly monitoring a subset of listservs dedicated to employees who are minorities and those who are involved in activism. 
Earlier this week, a group of Amazon employees discovered that an email alias affiliated with Amazon’s HR team had subscribed to 78 listservs at Amazon, the majority of which are related to underrepresented employees and employee activism issues, such as climate change, Black employee networking, and Muslim employees. Amazon has thousands of internal emailing lists where employees discuss common interests and projects, so the dozens of listservs that the alias was subscribed to are a small subset of the total groups that exist. 
Amazon denies that its HR teams were tracking emails to monitor organizing, and told Recode that it subscribed to the groups to monitor employee feedback on company culture. 
Some of the listservs that Amazon HR appears to be monitoring have been used for employee organizing around controversial issues at the company in recent months, such as Amazon warehouse worker rights, corporate carbon emissions, and military technology. Others are less political, such as groups for women in engineering and employees who are parents. 
Recode spoke with six Amazon employees who said they’re alarmed by the email monitoring when they consider how Amazon has been increasingly cracking down on worker organizing. These employees spoke on the condition of anonymity because of Amazon’s policies against speaking to the press without management’s approval.  
They told Recode that dozens of other colleagues have also posted messages on Amazon’s internal forums expressing concern about the monitoring, and many more are likely also upset but too scared to speak out publicly. 
“Most people would just read and go quiet,” said one employee. “It doesn’t seem smart to engage when we’re being told that we are being tracked.”
Just last month, Bloomberg News reported that Amazon had posted a job listing (which it later removed) for analysts to research “labor organizing threats against the company.”  
“If this is what it looks like ... then this is a specific targeting of nonwhite male groups as potential threats to be observed,” said one employee. “It means that the people responsible for that at Amazon believe that women and people of color are suspicious and threats to the company.”
After discovering the alias, an Amazon employee sent a message to all 78 listservs informing them that the company seemed to be watching their activities. Vice first reported on an Amazon employee warning their colleagues about the monitoring.


  Do you work at Amazon and have thoughts on what’s going on? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com or Jason Del Rey at jasondelrey@protonmail.com to reach them confidentially. Signal numbers available upon request by email. 




“Good day! If you are a moderator or a user of this list, please note that it is being explicitly watched,” started the email. It goes on to state, “Without editorialising, it is difficult to read this project and its initial posting date without also considering the context of the recent job posting for which Amazon has come under fire.” (That’s a reference to the “labor organizing threats” analyst position.) 
The email noted that while the Muslim employee listserv was on the list of groups that HR was watching, the Christian one was not.
Amazon spokesperson Jaci Anderson said the practice was intended to gather employee feedback to help improve company practices, and that Amazon does not link feedback from the emails to individual employees. She added that the company chooses which listservs to monitor based on the size and level of activity of the employee group, and for no other reason. 
“We continually work to improve the Amazon employee experience, and with hundreds of thousands of employees located around the world, we use several methods to gather feedback at scale,” Amazon spokesperson Jaci Anderson said in a statement. “The anonymized feedback that is sometimes shared from these open email forums has helped us improve our employee benefits, further strengthen our COVID-19 procedures, and improve the overall Amazon employee experience.”
One of the Amazon employees who appears to be linked to the email alias is a data analyst in the employee relations division of HR. And because some employee relations roles at Amazon involve mitigating the risk of unionization in its massive warehouse network, this detail has fueled corporate employees’ concerns about the listserv monitoring.  
According to documents reviewed by Recode, the email account subscribed to these groups also appears to be linked to a larger data visualization project run by Amazon’s employee relations team called “SPOC” (geoSPatial Operating Console), which involves monitoring threats to Amazon’s operations — including unionization. Anderson, the Amazon spokesperson, said the program monitors all types of external activity that impacts the safety and well-being of its employees, from weather events to power outages, and is not intended to favor or target one type of external threat over another. 
Shortly after an Amazon employee emailed documents about the broader SPOC monitoring project to dozens of employee listservs, those documents were deleted from Amazon’s internal network that’s broadly available to employees.
In April, Business Insider reported that Amazon was tracking Whole Foods workers’ unionization efforts in a geographic heat map. And Recode has previously reported how as far back as the early 2000s, Amazon has previously tracked worker organizing at its warehouses before using excel to make heat maps.  
“It’s disappointing but not surprising,” said one Amazon engineering manager about HR tracking listserv emails and broader anti-unionization monitoring efforts. “We are working in corporate America at one of the largest and most technically advanced companies in the world. Always assume big brother is watching.”
This employee said they wish the company were as employee-obsessed as it is customer-obsessed, but also acknowledged that the company “overall takes care of their [corporate] employees very well.”
Amazon has faced growing tensions within both its corporate and blue-collar workforces in recent years. Tensions peaked during the Covid-19 pandemic as many of its warehouse workers complained about working conditions and pay — and some started nascent talks about unionization. The company came under serious scrutiny when it fired Christian Smalls, a Staten Island warehouse worker who was organizing his colleagues for safer working conditions at the beginning of the pandemic, and after a report emerged that a top Amazon lawyer called Smalls, who is Black, “not smart, or articulate” in an executive meeting. Amazon said it fired Smalls for violating social distancing rules.
The Smalls case, among other factors, prompted many of the companies’ tech employees to advocate for greater workplace protections for Amazon warehouse and delivery workers. The company responded by providing some incremental benefits, such as temporarily raising pay for fulfillment center employees and offering more time off. At the same time, Amazon also fired several corporate employees leading internal activism efforts, who had organized their tech colleagues in solidarity with Smalls and other warehouse workers.
“After firing a number of employees organizing for better Covid-19 protections, and getting caught calling a Black organizer ‘not articulate,’ it seems like Bezos decided it was time to form an internal anti-worker police force to frighten employees into shutting up,” one Amazon software engineer told Recode. “That open feeling I had as a tech worker able to freely discuss ideas with coworkers has vanished so fast it’s made my head spin. There’s a real culture shift happening, and it sucks.” 
In the months since Smalls’s firing, Amazon employee activism has quieted down, at least publicly. But workers’ fears about the company targeting and monitoring minority and activist employees show that tensions still run deep at the company, and, if anything, its workers’ trust in their employer continues to erode.


  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Amazon has announced a new way for consumers to surveil their own homes: a camera-equipped drone that connects to Ring security systems. Ring, which Amazon owns, has a history of enabling controversial levels of surveillance in homes and neighborhoods. So the addition of a flying camera that can venture into new nooks and crannies is, at best, unsettling.
The Ring Always Home Cam is designed to fly around different areas of someone’s home every so often, capturing footage before landing back in its dock. The device is meant to stay indoors and fly autonomously based on pre-programmed flight paths that navigate between the walls of a house, a Ring spokesperson told Recode. The announcement comes after Amazon last year won a patent for a home surveillance drone; it’s also worth mentioning that the Federal Communications Commission (FCC) has not yet authorized the sale of this device.
Amazon says its new Ring surveillance drone is scheduled to go on sale in 2021 for $249 (once FCC authorization is obtained), and the company says it built in the product with “privacy in mind.” In a live blog of the virtual announcement event, Amazon said the Always Home Cam “only records when in flight; when it’s not in use it sits in a dock and the camera is physically blocked.” The company added that the drone is “loud enough so you hear when it’s in motion.” This is illustrated in a promotional video from Ring that shows a hypothetical robbery in which a burglar breaks into a man’s bedroom while the man is not home. The drone then chases off the burglar while the man anxiously watches the action through a smartphone app.

On its face, the new Ring drone might seem neat and futuristic, but it also serves as a reminder of the company’s checkered history with privacy and surveillance. Ring has long faced intense criticism over its existing security products, and those concerns only grew after Amazon acquired the company for $1 billion in 2018. One particularly sensitive issue is Ring’s vast and somewhat secretive network of police partnerships, which allow law enforcement to request footage collected by Ring cameras. Ring’s Neighbors app has also been accused of exacerbating racism and capitalizing on fear of crime. Meanwhile, many, including some members of Congress, are worried that the company will soon incorporate facial recognition into the Ring platform.
“The introduction of a roving drone security camera inside your own home potentially upends the idea of the very idea of home as a private place,” Matthew Guariglia, a policy analyst at the Electronic Frontier Foundation, told Recode in email. “Amazon’s new products certainly have the potential to extend what was already an invasive surveillance system into the realm of the absurd.” 
While a Ring spokesperson told Recode that the Always Home camera footage cannot be requested by police, Amazon has not made a formal commitment not to allow police to request this footage in the future. 

  
    Related
  

  
    Jeff Merkley and Bernie Sanders have a plan to protect you from facial recognition
  

The announcement of the Ring drone arrives at a time when Amazon is also attempting to expand its products’ functionality in private and public spaces with an update to its Sidewalk project. As Amazon explains on its website, Sidewalk aims to create a shared network that could connect a suite of Amazon’s connected products for the home, like some of its Ring devices and Echo voice assistants. The effort is also meant to operate at a larger scale, potentially connecting devices throughout a neighborhood. For example, Amazon says that Sidewalk would enable certain Ring products to continue sending certain alerts even in the absence of a wifi connection. Eventually, the platform will promote “smart security” and even help find pets and valuables, the company said in a blog post on Monday. 
“The Sidewalk Project has the potential to extend what is supposed to be home surveillance into community and neighborhood surveillance,” Guariglia said. “With all of these technologies, the individuals who purchase this equipment often are not asking how their neighbors feel about technology that could potentially extend the reach of networked smart devices, including those created for the purpose of recording and tracking well outside of their own property and into public spaces.”
So despite the advertised benefits of Amazon’s growing network of gadgets, the company is also setting itself up for more criticism over how these products also seem invasive or even Orwellian, especially as lawmakers face more pressure to regulate surveillance products and limit the technological capabilities of law enforcement. 
Basically, it seems as though Amazon wants to be everywhere, and it’s working hard to get there. So even while every Ring product might seem useful — even cool — on its own, considered in the aggregate, Amazon is producing a constellation of connected products that could be repurposed to record and surveil us, whether through its microphone- and camera-equipped devices, like the Echo and the Echo Show, or the new Ring cameras for cars Amazon also announced on Thursday. And with each new device, Amazon seems to hold more of the cards, collecting not only data about what’s happening in our homes but in our neighborhoods, too. That may not be the future we want.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      






  Facebook’s much-anticipated independent oversight board — a group that will be able to overrule Facebook’s leaders, even CEO Mark Zuckerberg, about whether controversial posts should stay up or be removed — announced its plans to start making decisions on contested content by mid to late October. That means the board may be called on to make decisions about important Facebook posts related to the US presidential election. 
In recent months, some have criticized the long-awaited board for not moving quickly enough to deal with issues around misinformation, hate speech, and extremism on the platform, and doubted whether it would be functional before the November election. 
But as long as internal testing of its technical systems goes well, the board says it will start accepting contested content cases around mid to late October. That means that if President Trump or any other candidate declares a premature victory on Facebook on election night, the board could potentially take on that case and decide whether that post should stay up or come down. While the board is still determining the specific criteria for how it will prioritize cases, it generally will take on “difficult, significant and globally relevant” cases “that can inform future policy,” according to its website. 
“The go-live date is not connected to any specific case that the board is seeking or not seeking to take,” Facebook oversight board’s director of administration Thomas Hughes told Recode. “That said, the type of case you just described [in which a politician declares a premature election victory], would be in scope, and could be referred to the board by Facebook, or potentially in time, referred by a user.”
Here’s how the board will work once it goes live: It will take on cases from users and from Facebook itself. Facebook the company can refer to the board any kind of contentious post it wants an outside opinion on, and the board will have 90 days (or 30 days if the case is expedited) to rule on the decision. For Facebook users, they can only go to the board if something they personally posted was taken down and they want to dispute it. In later months, the board plans to expand its purview and allow users to request for other people’s content to be taken down if they believe it violates Facebook’s policies against things like hate speech or harmful misinformation.
At a time when Facebook is being attacked by both Republicans and Democrats for how it’s been handling politically contentious speech in the US, the board is meant to add oversight to the company’s decision-making. But it won’t solve the lion’s share of Facebook’s problems around how to deal with hate speech and misinformation. For one thing, the board will only take a small number of cases a year, likely “tens or hundreds” according to Hughes, out of the tens of thousands of annual cases that are expected to come its way.
And it won’t be all about the US, either.
Facebook’s oversight board is made up of 20 lawyers, academics, journalists, and policy experts from all over the world —  collectively, its members speak 27 different languages and have lived in 29 different countries.
“Obviously, the US election has an enormous impact on the world,” said Hughes, “But there will be quite a broad range of things that the board I think would be very keen to get stuck into early on.”
Facebook first floated the idea of an independent oversight board back in 2018, as it was facing scrutiny for its handling of Russian interference on the platform during the 2016 US election. Almost two years later, the board in January announced its governing rules, and in May announced its members.
Ruling on specific controversial posts is one thing, but actually getting Facebook to rethink its policies is another challenge. Some social media researchers have questioned the power of the board to dictate Facebook’s policy, and how much the company will listen to its recommendations.
Now, the election could turn out to be the first big test of how impactful this oversight board will truly be in practice. In fact, whether or not the board accepts a case related to controversial election content is a test in and of itself.


  
  
  
      









  
For the past five years, nearly every one of Walmart’s big moves has seemingly had one goal: to narrow Amazon’s embarrassingly huge lead in e-commerce.
From Walmart’s $3 billion acquisition of Jet.com in 2016 to its $16 billion deal for India’s Flipkart in 2018, as well as its recent launch of the Walmart+ membership program, Amazon and its Prime membership service seemed to always be at the forefront of Walmart’s corporate mind. 
For the first time in a long time, though, the giant brick-and-mortar retailer is expected to make a big bet that isn’t solely about chasing the Seattle-based tech giant founded by Jeff Bezos: a multibillion-dollar investment in, and multifaceted business partnership with, TikTok, the global sensation and short-form video-sharing app. 
Whether the partnership with TokTok turns out to be a brilliant chess move for Walmart or a disastrous distraction, it still shows the traditional retailer is trying to invent a digital future where it will be a leader of Amazon rather than a follower. If successful, Walmart could become a leader in online video commerce — an area of retail that has been a hit in Asia, but is still nascent in the US. 
Some background on the dramatic geopolitical story that set Walmart up for this opportunity: In August, President Donald Trump cited security concerns over TikTok being owned by the China-based company ByteDance when he signed an executive order that would essentially ban TikTok in the US unless it sold itself to US entities. 
Over the weekend, Trump said he would approve a deal that would set up a new entity, called TikTok Global, of which Walmart and the database software company Oracle would own a combined 20 percent. ByteDance investors — some of which are US-based firms — would own the rest for now. As of Monday morning, though, there was still confusion about whether the deal would result in TikTok Global being majority-owned by US businesses or China’s ByteDance instead, so Trump once again threatened to scuttle the deal if US firms did not gain majority control.
Assuming the deal gets done, Walmart says it will “provide our ecommerce, fulfillment, payments and other omnichannel services to TikTok Global.” The details on these arrangements are scarce, but the mentions of “e-commerce,” “fulfillment,” and “payments” suggest that Walmart could help TikTok incorporate shopping features into its app. 
That means TikTok users could buy merchandise created or promoted by their favorite TikTok artists without leaving the app, and TikTok could potentially use Walmart’s existing warehouse and “fulfillment” services to deliver the merchandise. At the moment, when an influencer or a consumer brand advertises a product on TikTok, users almost always have to click through to an external shopping website to make a purchase. 
Sure, Walmart could provide incentives for TikTok or its most popular creators to link to Walmart.com in those instances, but a deeper integration into the app could be the longer-term vision. Last month, for example, TikTok experimented for the first time with allowing a popular creator to sell goods through a pop-up page within the app.
The Chinese version of TikTok, called Douyin, is an example of what Walmart and TikTok could do together. Douyin has a deeper e-commerce integration for shopping than TikTok currently does, including strength in livestreaming video commerce, which allows app users to watch live as their favorite personalities or brands show off new merchandise and then have an easy way to buy what they see. Such online shopping trends — a more interactive version of QVC for the digitally savvy — are already popular in Asia. 
No US company, not even Amazon, has figured out how to make this idea mainstream in the states, but many are trying. Amazon has dabbled in video commerce via its Twitch streaming service and on Amazon.com. But it’s early enough stateside that if this kind of consumption becomes popular in the US, a TikTok relationship could help Walmart lead the way among retailers. 
Walmart’s recent statement also says the commercial agreements with TikTok would “grow our third-party marketplace, fulfillment and advertising businesses.” Like Amazon, Walmart allows outside merchants to sell goods through Walmart.com  — on its “marketplace” — so that the retailer can offer Walmart.com visitors a larger variety of goods for sale than it can in its physical stores. The language in Walmart’s statement implies that Walmart might work out a deal: Some of those influencers or brands that sell goods through TikTok might make those items for sale directly through Walmart through its marketplace. 
Walmart could also introduce features that let its existing marketplace merchants more easily advertise or sell their goods through TikTok. Today, the selection on Amazon’s third-party marketplace is much bigger than Walmart’s. But a TikTok-branded storefront, or a direct link for Walmart merchants to advertise through the video app, could help the retailer attract a new set of younger consumers that might otherwise look to Amazon or other e-commerce websites as their main shopping destination. 
The move wouldn't be to directly match Amazon’s sheer marketplace size, but to give TikTok users and/or Walmart marketplace merchants access to goods or customers that they can’t get anywhere else. 
To be clear, a potential Walmart investment and partnership with TikTok could still turn out to be a costly distraction for the retailer, and even for the app. Walmart’s leadership team is already busy with the launch of its Amazon Prime competitor Walmart+ in the US, which requires an expensive expansion of grocery delivery to combat Amazon and Instacart along the way, as well as a brutal and costly battle in India with Amazon (via Flipkart) for the future of one of the fastest-growing e-commerce markets in the world. 
Walmart also has to deal with the day-to-day logistical complexities of handling the rapid growth in online shopping that the pandemic has spurred. There’s healthy skepticism about whether the world’s largest brick-and-mortar retailer can achieve this lofty vision even if it gets the chance.
But for once, Walmart seems to be looking past Amazon while making a big bet, instead of simply aiming for Jeff Bezos’s back. 

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  




If you’ve ever used the internet (which I have to assume includes everyone reading this article on a news website), you’ve probably noticed that the things you do on one website tend to follow you around on others, or that certain social media platforms know a whole lot more about you than you thought you revealed. Meanwhile, you likely have no idea who knows what about you, or how they got that information. Data collection is the backbone of the internet ecosystem, but it’s largely invisible to you, the average user, until you see its end result: an ad so uniquely targeted to you and your interests that you swear Facebook must be listening to your conversations through your phone (it probably isn’t).
Several companies and organizations are trying to make that world a little less opaque to users like you. One of them is The Markup, a nonprofit investigative news site. It just released a tool called Blacklight, and it’s designed to present all of this information in a way that’s easy to understand. If you want to know how the ad technology that knows everything about you works, it’s a great place to start. If you just want to know who might find out that you visited a potentially embarrassing or deeply personal website before you go there, it’s good for that, too.
There are a few similar tools — Apple’s newly released Safari 14 browser update, for example, will tell you which trackers are on a website you visit. But with Safari, you have to actually visit the site first, and its list of trackers doesn’t include context about which companies are associated with which trackers and what those companies do. For instance, Safari will tell you that Vox has a tracker called “agkn.com,” but Blacklight will tell you agkn.com is owned by Neustar, which specializes in “accurate targeting” based on a “wide range of attributes” gleaned from your behavior both on- and offline. And now that you know Neustar exists, you can make an informed decision to opt out of being tracked by it.
Blacklight serves more as an information tool than something you’d use in real time as you browse the internet because you have to go to Blacklight’s site and enter your desired website address in the prompt. Blacklight then scans the site and tells you how many trackers are on it, what they do, and who they’re potentially sending your data to. Some of those names you might recognize, like Oracle and Verizon. Others you likely won’t, like LiveRamp or Criteo. But it’s safe to say that all of them know a lot about you. 
I tried Blacklight out for myself to see what websites might be telling those companies about me. Vox, the site you’re reading right now, is largely ad-supported. Perhaps unsurprisingly, Blacklight found a lot of ad trackers (31) and third-party cookies (54) on it. Vox also uses Facebook’s Pixel and Google’s analytics trackers, which tell those platforms that your device visited Vox. Facebook and Google trackers in particular are very common on websites, and allow Facebook and Google to connect your behavior across all of those sites to your user profile on their platforms, giving them lots of data about you and your interests for ad targeting purposes.
Vox is not unique in this regard. Its tracker load is comparable to what Blacklight found on other ad-supported national news sites, including Slate (38 trackers, 6 cookies, Facebook), Mashable (24 trackers, 33 cookies, Facebook and Google), and Politico (33 trackers, 60 cookies, Facebook). 
Some sites have more advanced tracking technology. On Breitbart, for example, Blacklight found 26 trackers, 15 cookies, Facebook and Google trackers, as well as a script that enables what’s called “canvas fingerprinting,” which can be used to track you even if you block cookies. Time magazine’s site has 14 trackers, 25 cookies, Facebook and Google trackers, and, Blacklight found, it uses a session recorder that can detect things like mouse cursor movements, clicks, keystrokes, and page scrolls while you browse the site. That might sound creepier than it actually is: Websites can use session trackers to get granular data about their visitors’ behavior on their site to improve how the site itself looks and works. But they can also watch a specific user’s interactions on their site and attach it to identifying information, if they have it, to make inferences about that user. (The Markup, which is a nonprofit and relies on donations rather than ads for support, doesn’t have any trackers.)
Maybe you don’t care if a national news website knows what you’re looking at and when, but you might feel differently when it’s a site that deals with more sensitive information. On WebMD, Blacklight found 26 trackers, 31 cookies, and a Facebook tracker. A website for a medication for autoimmune diseases sent data to a variety of companies, including Facebook. A site that sells STD testing kits had 13 ad trackers, 25 cookies, Facebook and Google trackers, and a session recorder. Even if you trust those sites to respect and maintain your privacy, you’re also trusting the third parties they allow to collect your data on their website, and you’re trusting whatever companies those third parties might sell your data to. You also probably have no idea who those companies even are. 
The Markup pointed Recode to Airbnb and M&Ms’ websites as examples of major websites with potentially concerning tracking behavior. Blacklight found that Airbnb has canvas fingerprinting and logs the keystrokes you type in certain text fields. It also uses Facebook’s “advanced matching” feature, which can share data with Facebook even if you’ve blocked Facebook’s cookies. On M&Ms’ site, Blacklight found 31 trackers, 67 cookies, Facebook and Google trackers, a session recorder, and that it was logging keystrokes in the email and password fields.
There may be legitimate reasons for these scripts; canvas fingerprinting is sometimes used to detect fraud, so it makes sense that it would be on a site like Airbnb. And the keystroke logger could be used to auto-complete the email and password fields, making logging into your M&Ms account easier. But it also means the site may be recording what you type in submission fields before you click the “submit” button. Either way, now you know it’s there.
Blacklight says not to take its scan as the final word on the trackers a website does or doesn’t have — there may well be some that evade detection. It’s really more of a guide to help you make more informed decisions about your internet experience. So, now that you know how your favorite websites might be tracking you and which companies they might be sending your data to, what can you do to stop it? 
There are relatively simple ways to minimize the information websites can get about you, and they don’t require much technical know-how:


Turn off ad personalization wherever possible. You can do this on Facebook, Google, and Twitter, for instance. 

Use a more privacy-conscious browser. You should specifically look for a browser that rejects third-party cookies, which are often used to track you online. Safari and Firefox browsers block third-party cookies by default, and both feature “privacy report” functions that list what they’ve blocked for you; you can find those by clicking on the little shield icon to the left of the browser bar. Google’s Chrome has a setting that will allow you to block third-party cookies, and the company says it will be blocking third-party cookies entirely by 2022. 

Add tracker blocking extensions to your browser. Privacy Badger, Ghostery, and DuckDuckGo’s Privacy Essentials are three good examples. They’ll tell you how many trackers they blocked and what they are. Ad blockers like uBlock Origin, AdBlock, and AdBlock Plus will also block trackers. These extensions may compromise the functionality of some websites, and keep in mind that you are blocking the ads that many of them rely on for income. 

These are just a start, and there is no foolproof way to prevent all tracking on the internet. Again, some of these trackers will help you use the site you’re on; others will help pay for its existence. The best thing you can do is be as aware as possible of what websites can know about you and who else might be watching.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
ntribute today from as little as $3.
  
  
  
      




  Quibi was supposed to be revolutionary: A video service that was supposed to fill the gap between YouTube and HBO by bringing short, “premium” clips starring celebrities like Liam Hemsworth and Chrissy Teigen to your phone, for a price.
But that was in the spring. Now, Quibi might be headed to a fire sale: Just six months after launching — and after raising $1.8 billion — Quibi has started looking for a buyer. It’s a stunning admission that the high-profile service hasn’t found enough traction to continue on its own.
Quibi CEO Meg Whitman and Quibi founder Jeffrey Katzenberg have already pitched at least one potential acquirer in the last week, industry sources told Recode. The Wall Street Journal previously reported that Quibi was considering options including a sale, raising more money, or going public by backing into a shell company.
Katzenberg declined to answer questions via email. His company insists, via a statement from a PR rep and in its pitch to would-be acquirers, that the service has had a successful launch, despite well-documented failures to attract users. The fact that it’s trying to find a buyer undercuts those arguments.
But the more practical question about Quibi now is what exactly a buyer would get. There’s some cash — a source familiar with the pitch says the company has said it has more than $200 million on-hand — as well as the tech Quibi built to bring its video to your phone. It’s particularly proud of what it calls its “turnstyle” system, where viewers can rotate their phone 90 degrees to see a different version of the video they were watching. Quibi’s engineering team might also be valuable to a certain kind of buyer.
Okay. But what about all those shows Quibi has? Those should be worth ... something, right?  Even if people didn’t watch them on Quibi, it’s possible to imagine they might watch them on a different service that already has lots of viewers. 
Netflix, for instance, has demonstrated that once it takes a show and makes it available to its 200 million users, that show can become very popular, even if it wasn’t popular on its home network. Schitt’s Creek, the show that won seven Emmys, including for best comedy, last night, used to be on something called the Pop network, which you have likely never heard of — but Netflix made it a hit. Ditto for Cobra Kai, the Karate Kid spinoff which started out on YouTube but jumped to Netflix this summer and took off.
But the problem with that pitch when it comes to Quibi is that, in most cases, Quibi doesn’t actually own the shows it has paid for: It has licenses that last several years, at which point they go back to the companies that made them. (Disclosure: Vox Media produces shows for Quibi.) 
So if you’re buying Quibi because you want to get your hands on The Most Dangerous Game, you’re going to have to take advantage of it in the near future. Which means you’re unlikely to value Quibi has highly as its managers and investors would like.
All of which is an astonishing place for Quibi to be in. There was a lot of skepticism in advance of the service’s launch in April — mostly because there hasn’t been a successful example of a paid, mobile-only video-streaming service. And also because there’s no shortage of other video options these days. And also because, contrary to Quibi’s marketing pitch — which argued that Quibi would help people pass the time while they waited in line for a sandwich or for their coffee to brew — people have plenty of other things they can do on their phones.
Still, it would have been hard to find someone who thought Quibi would be looking for a way out just a few months after it started. Katzenberg, a Hollywood veteran, is a famously relentless networker and promoter who had spent years putting this together. If he’s eyeing the exit this soon, something has gone very wrong.


  
  
  
      






  A US judge has blocked the Trump administration’s plans to essentially disable WeChat, the mobile chat/payment service popular with millions of Chinese Americans.
Trump’s proposed ban of WeChat — which would have forced Apple and Google to remove the software from their app stores, and was meant to degrade the service so existing app users would find it unworkable — was supposed to go into effect at midnight Sunday.
But on Saturday night, Judge Laurel Beeler of the Northern California US District Court issued a temporary injunction against the Trump ban, citing free speech concerns, in a case brought by WeChat users. 
Beeler’s order means that, for now, moves the Trump administration announced last week to cut off both WeChat and TikTok — apps owned by Chinese companies that are popular with millions of American users — have been put on hold. 
Trump’s Commerce Department had also said it would disable TikTok, the mobile video app, on November 12 — after the US presidential election. But on Saturday, Trump said he had approved a deal that will give American companies Oracle and Walmart minority stakes in TikTok, and his Commerce Department delayed an order that was supposed to remove TikTok from app stores Sunday night.
“I have given the deal my blessing,” he told reporters at the White House Saturday. Trump had previously insisted TikTok would have to be owned completely by American companies in order to keep working in the US. Trump also said the new deal calls for TikTok to make “about a $5 billion contribution toward education,” without explaining what that meant; on Sunday, TikTok’s owner ByteDance said it was unaware of those plans.
The WeChat users’ suit against the Trump administration was originally filed last month, after the Commerce Department announced plans to act against the service, which is owned by Tencent, the Chinese tech conglomerate. 
WeChat users had argued the service is “a public square for the Chinese-American and Chinese-speaking community in the US that is effectively their only means of communication with their community,” Beeler wrote in her judgment. The judge ruled shutting down that public square could violate WeChat users’ rights:
The plaintiffs have shown serious questions going to the merits of their First Amendment claim that the Secretary’s prohibited transactions effectively eliminate the plaintiffs’ key platform for communication, slow or eliminate discourse, and are the equivalent of censorship of speech or a prior restraint on it.
Beeler said the government’s argument that WeChat could be a national security problem was plausible, given the tight ties between Chinese companies and the Chinese government. But she argued the Trump administration should consider other moves besides an outright ban:
Certainly the government’s overarching national-security interest is significant. But on this record — while the government has established that China’s activities raise significant national security concerns — it has put in scant little evidence that its effective ban of WeChat for all U.S. users addresses those concerns. And, as the plaintiffs point out, there are obvious alternatives to a complete ban, such as barring WeChat from government devices, as Australia has done, or taking other steps to address data security.
If the WeChat order had gone into effect Sunday night, the app was supposed to have disappeared from app stores, which means new users couldn’t use the service; the rules were also supposed to prevent WeChat users from transferring funds or making payments in the US. 
The ban was also supposed to generally weaken the service by preventing US tech infrastructure companies from supporting WeChat. “For all practical purposes, [WeChat] will be shut down in the US, but only in the US, as of midnight Monday,” Commerce Secretary Wilbur Ross announced on Friday. 
Now those plans have been delayed, at the very least.

  
  
  
      








  The images and reports out of California this week are overwhelming: concurrent colossal wildfires laying waste to property and landscapes, freaky orange skies, massive smoke clouds, worsening air quality, more than 64,000 people forced to evacuate, and all of it compounding the risks of Covid-19. 


The 2020 fire season has been record-breaking, in not only the total amount of acres burned at just over 3 million, but also 6 of the top 20 largest wildfires in California history have occurred this year. pic.twitter.com/CmmhH5wTVX— CAL FIRE (@CAL_FIRE) September 10, 2020



If this feels like déjà vu, here’s why: Wildfires are growing more common and more severe in California. The most recent season of horror was 2018, which had 10 large fires that each burned more than 500 acres. Most infamous was the Camp Fire, which left 86 people dead in Paradise and caused more than $16.5 billion in losses, according to the German insurance company Munich RE.
This August was California’s warmest on record (as it was for five other states as well), setting the stage for the extraordinary streak of extra-large fires burning now. Five of the current fires are in the 20 largest wildfires in the state’s history: the August Complex (the largest blaze in state history as of Thursday), the SCU Lightning Complex, the LNU Lightning Complex, the North Complex, and the Bear Complex. As their names hint, these are megafires that gained size and strength when smaller fires combined into unified blazes.
The heat wave that preceded this terrifying swarm was not a blip. The weeks of arid, hot air that crisped out the forests and shrubs now aflame are part of a familiar pattern of extreme weather events: the climate crisis accelerating right in our faces. 
As the climate heats up, many other states in the West, including Oregon and Colorado, are seeing larger, more devastating fires and more dangerous air quality from wildfire smoke. But California is at particular risk, both because its increasingly volatile weather may bring more droughts than other states and because it has more people and more buildings. Let’s walk through the details of how we got here.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        A Butte County firefighter douses flames at the Bear Fire in Oroville, California, on September 9, 2020.
      
      
        Josh Edelson/AFP/Getty Images
      
    
  


California’s forests have become tinderboxes
To understand why California is experiencing so many devastating fires year after year, let’s look at two basic forces at play.
The first is climate change. According to a 2019 paper in the journal Earth’s Future, California’s annual burned area has increased more than fivefold since 1972, which the authors attribute in part to a warming climate. The total annual area burned during summer fires is rising fastest, they note, though the climate fingerprint is getting clearer in the increase in areas burned in the fall as well. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Gov. Gavin Newsom’s Strike Force report on wildfires and climate change
      
    
  


California’s forests and shrublands have been subjected to wildfire pretty much forever; fire is a natural part of many of the state’s ecosystems and the Indigenous peoples of California set controlled burns to manage the landscape. What’s different now is that the season is getting longer, it’s gotten harder to manage the forest, and the fires are on average getting bigger and more destructive.
“Climate change is amplifying fire behavior and fire size,” Alan Ager, a researcher at the US Forest Service who studies how to manage wildfire risk on federally managed forests and other lands, told Vox in 2019. “Fire can travel larger distances” than in the past because there’s more fuel.
The basic recipe for a monster 21st century wildfire is this: Take hot air and no rain and moisture evaporating from trees, shrubs, and soil. After a series of these long, expansive, hot, dry spells, trees and shrubs will be transformed into ideal tinder to feed a fire. The bigger the area affected, the more available fuel. All you need then is a spark, which could come from a power line failure, a cigarette, or a firecracker.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Christina Animashaun/Vox
      
    
  


Climate models show that as temperatures continue to rise, the atmosphere and land in some regions, like California’s forests, will grow more arid. There will be more frequent and intense droughts, followed by intense periods of rain — a form of weather whiplash. This prompts the growth of thick underbrush, which then dries out in the subsequent droughts and becomes highly flammable kindling. 
A newer phenomenon scientists are seeing in 2020 is wildfires that grow dramatically overnight because temperatures aren’t dropping like they used to. “One of the things we see with human-caused climate change is that the overnight lows are getting warmer,” said Matthew Hurteau, associate professor of biology at the University of New Mexico. “In the past, the sun sets, the temperature drops, the relative humidity goes up, and fire behavior dies down, and that’s when a lot of progress gets made in terms of fire suppression, because the flame lengths are shorter.” 
But this year, the Bear Fire, one of the three fires that became the North Complex Fire, expanded by 100,000 acres overnight, destroying almost every structure in the 525-person community of Berry Creek, according to the Sacramento Bee. “That kind of fire growth, especially at night, that’s a climate signal for sure,” said Hurteau.
The second factor making the state more fire-prone is poor forest management.
 In 2019, journalist Mark Arax published an extraordinary feature story on the Paradise fire, California’s most destructive fire ever. In it, he tells the tale of how, in the 1990s, the state’s timber industry came to be dominated by rampant clear-cutting. Varied, diverse forests, with patches of scrub and trees alternating, served as natural fire breaks. Wildfires came to them periodically, as is natural and necessary for regeneration, but they did not spiral out of control.
After a clear cut, forests are replanted as monocrops. There are no natural breaks, no variation, which makes them extraordinarily vulnerable to rapidly spreading fire. 

  
    Related
  

  
    California’s wildfires are hardly “natural” — humans made them worse at every step
  

And in the early 2000s, park rangers practiced a certain form of forestry management — prescribed burns, clearing brush, remediating clear cuts. But it fell out of favor as an increasingly large, paramilitary fire brigade took over. “As rangers joined up with the ranks of better-paid firefighters,” Arax writes, “their numbers dwindled to maybe 250, even as the number of firefighters inside the [Department of Forestry and Fire Protection] jumped to 7,000.” 
Firefighters put out fires; they don’t do prescribed burns. But consistent fire suppression only increases the amount of dry, flammable material. 
As this LA Times story reveals, California’s clear-cutting and forest mismanagement continue to this day.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Paul and Ronne Falgren look through the rubble of their home burned by the LNU Lightning Complex Fire in Lake Berryessa, California, on August 31, 2020.
      
      
        Jane Tyska/Digital First Media/East Bay Times/Getty Images
      
    
  


More and more people are building (and rebuilding) in fire-prone areas
California also has a housing crisis, born largely of the fact that wealthier urban residents refuse to allow more housing to be built in urban areas, near jobs. Consequently, as more residents stream into the state, the price of existing urban housing stock rises and development sprawls outward. More and more of that development is being pushed into the “wildland-urban interface” (WUI), where wildfires are more frequent and more difficult to fight. 

  
    Related
  

  
    Two experts explain why solving the housing crisis helps the climate
  

Some 11.3 million people — more than any other state with regular wildfires — live in the WUI in California. That’s 30 percent of the state’s population living near a lot of potential wildfire fuel. And more than 2.7 million Californians currently live in “very high fire hazard severity zones,” areas where the population is expected to keep growing. (Cal Fire is currently updating its hazard zone maps and expects to roll out new ones by 2021.)
Again, California is not alone. A 2018 study in PNAS found that between 1990 and 2010, the WUI was “the fastest-growing land use type in the conterminous United States.” This is happening in lots of states.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Gov. Gavin Newsom’s Strike Force report on wildfires and climate change
      
    
  


But it’s particularly concentrated in California, where a million houses were built in the WUI during those same years. In Mother Jones, Jeffrey Ball has a feature story on the state’s terrible land use policies, which encourage sprawl, and specifically building (and rebuilding) in fire-prone areas, in a dozen different ways, including subsidized insurance. (See also this piece in MIT Technology Review by James Temple.)
A recent study by Ager and colleagues found that 1,812 communities in the western US could be significantly impacted by future wildfires. Of the top 20 most exposed communities on the list, 14 were in California.
Add all this together — increasing heat from global warming, several years of unusually high winds and low humidity, poor logging practices with fewer preventive burns, more people living on forested ridges and hills in remote, fire-prone areas — and the result is disaster. 
Why California can expect wildfire season to get worse
Parts of Northern California and the Sierra Nevada can expect to see the most fire activity directly linked to human-caused climate change in the coming decades, according to the Earth’s Future paper. 
But “you can throw a dart anywhere around Los Angeles and San Diego and you will hit an area with significant fire potential,” too, Chris Keithley, research manager for the Fire and Resource Assessment Program at Cal Fire, said. 
And there’s a huge mismatch, Ager’s study found, between the increased wildfire threat and how cities are planning future development.
The state’s population is also growing, leading to a significant overlap between the areas of high fire risk and areas with a growing population density, as you can see in these maps from a 2014 study of population trends in California projecting out to 2050:
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        A map showing population density growth projections (left) and a map showing fire hazards.
      
      
        Mann et al./Land Use Policy
      
    
  


The study estimated that by 2050, 645,000 new houses in California will be built in “very high” wildfire severity zones. 
Just as it’s time to consider retreating from the coasts because of sea level rise, it may be time to consider encouraging people to retreat from some of the riskiest fire-prone areas.
“I think planned retreat should be part of a suite of options,” said Paige Fischer, a social scientist who studies wildfires at the University of Michigan. So far, though, the state has done little to discourage new construction in high-risk areas or encourage people to move out of harm’s way. 
In response to the billions of dollars in losses from the California wildfires of 2017 and 2018, insurance companies are now beginning to refuse to renew fire and homeowner liability insurance and hike rates for homeowners in fire-prone areas, the New York Times has reported. 
But forcing people to move is an especially tough ask in California, given the housing crisis. Many Paradise residents who lost their homes in the Camp Fire had moved there to escape the unaffordable rents and home prices of the Bay Area. 
The federal and state governments have increased funding for fire suppression and managing fire fuel like the millions of dead trees on public and private lands, but much more is needed. 
“The thing that gets missed in all of this is that fires are a natural part of many of these systems,” said Matthew Hurteau, the University of New Mexico professor studying climate impacts on forests. “We have suppressed fires for decades actively. That’s caused larger fires.” 
More prescribed burning could help limit potential megafire fuels, but many communities oppose it because of the short-term smoke risk. “Fuel management efforts need to be substantially increased,” Ager agreed. 
That responsibility falls largely to federal and state agencies like the Forest Service that manage public lands. People who live in high-risk areas can also do more to manage the land and structures on their private property, for instance reducing flammable vegetation around homes and using fire-repelling building materials, Fischer said.
Climate change demands both immediate action to reduce emissions and immediate threats, and also long-term adaptation to a more hostile climate. California is a leader on the former — it has committed to 100 percent clean energy by 2045 and total, economy-wide carbon neutrality by 2045. And Gov. Gavin Newsom has also been trying to rally support for new funding from the state legislature to take on the threat of fire in a warming world.    
But he and other California leaders still have a long way to go in helping communities play better defense against, and prepare long-term for, wildfire. “We are overloaded with assessments and short on actions,” said Ager.


  
  
  
      




  Twitter is expanding its policies against voter suppression ahead of the 2020 election, saying it will take down or label more types of misleading election claims, such as falsely claiming a premature victory while results are still being tallied.
While social media companies make changes to their misinformation policies all the time, it’s notable that Twitter is making this change ahead of what could be a contested election, with a record number of people voting by mail due to the pandemic, and with President Trump essentially disputing the election’s results before people have even started voting. 
In the past few months, Trump has repeatedly tweeted false claims about voting, including the unsubstantiated assertion that mail-in ballots will lead to widespread voter fraud. He has also encouraged people to vote twice (which is illegal). While Twitter has labeled some of his posts as misleading in the past, it has done so sparingly — and in situations when Trump has made very specific incorrect claims, like the false allegation that everyone who lives in California will automatically receive a mail-in ballot (California plans to send mail-in ballots only to registered voters). 
Under Twitter’s new policies, the company could take down or label a lot more of Trump’s more general claims casting doubt on the voting process. Twitter won’t apply the rules retroactively, and it will start enforcing the policies on September 17.
“We will not permit our service to be abused around civic processes, most importantly elections,” Twitter wrote in a company blog post on Wednesday. “Any attempt to do so — both foreign and domestic — will be met with strict enforcement of our rules, which are applied equally and judiciously for everyone.”
The new rules strengthen a couple of points in Twitter’s existing rules against spreading false information about elections.
The rules now prohibit:

Spreading false information about the laws, officials, or institutions that carry out voting 
Disputed claims that could “undermine faith in the voting process itself,” such as “unverified information about election rigging, ballot tampering, vote tallying, or certification of election results”
Misleading claims about the results of the election, including “claiming victory before election results have been certified, inciting unlawful conduct to prevent a peaceful transfer of power or orderly succession”

These rules make it more clear what Twitter considers acceptable discourse on its platform. But as always, what matters is how Twitter actually enforces them.
One big thing that’s unclear is when Twitter will actually remove a tweet for violating these rules, or when it will take a lesser action by just labeling the tweet as misleading. 
Facebook similarly strengthened its rules against election misinformation last week, but many criticized the company for largely abstaining from fact-checking politicians like Trump. Twitter has been seen as more willing than Facebook to moderate Trump, particularly after it corrected the record on several of Trump’s false tweets, which Facebook chose to leave unchecked.



  
  
  
      






    
  
    
    
      
        
  


  






      
    
    
  
  



Fortnite has released its latest season, but millions of its players won’t be able to enjoy it: The game has been pulled from Apple’s App Store, leaving iOS and macOS users unable to update to the highly anticipated Season 4. But this struggle is bigger than one update, one game, or even one company. Fortnite’s Apple user base is the newest casualty in a years-long antitrust battle between developers and one of the world’s richest companies.
Epic Games sued Apple in mid-August, claiming that the company’s App Store practices violate the Sherman Act. Epic says that Apple’s requirement that all mobile apps come through its App Store (and the 30 percent commission Apple charges for app sales and in-app purchases) is a monopoly, and that Epic — as well as its fellow developers and their customers — should have alternatives. On September 8, Apple filed its response, and asked a federal judge to award it damages.
Apple, a $2 trillion company, has not only refused to consider changing its lucrative business model, but it also kicked Fortnite out of the App Store. Apple also wanted to cut off Epic’s access to its developer tools program, which would have affected any apps that use Epic’s Unreal Engine. While a court granted Epic’s request for a temporary restraining order that prevented Apple from doing so until a hearing next month, it would not force Apple to restore Fortnite to the App Store. So the game remains banned, leaving iOS and macOS players unable to update their apps to the just-released Season 4. It also means they can only play other Apple users who are also stuck in Season 3, as the rest of the Fortnite community update to the new season. For its part, Apple has said that Fortnite will be allowed back into the App Store if and when it follows the store’s terms and conditions.
As Recode’s Peter Kafka explained, apps on Apple mobile devices have to go through the company’s App Store, which charges a 30 percent commission for app purchases as well as any purchases made within the app itself. As a “freemium” game, Fortnite makes all of its money through in-app purchases of its virtual currency, and Apple gets a cut of that. When Epic attempted to get around this by offering customers the option of purchasing Fortnite currency directly from Epic at a discount, Apple kicked Fortnite out of the store for violating its terms of service. Epic responded with a lawsuit, joining an ever-louder chorus of developers and legislators who have accused the App Store of monopolistic practices, given its total control over the apps offered on its devices. And now Apple is firing back with a lawsuit of its own.
At a hearing scheduled for September 28, a judge will decide whether or not Epic will get a preliminary injunction that will force Apple to let Fortnite back into the App Store. And this is surely only the beginning of a protracted legal battle. The outcome of all this could significantly change the app ecosystem Apple helped create, possibly to developers’ and consumers’ benefits. Right now, however, everyone is losing.
The App Store can be mutually beneficial to Apple and app developers
When the App Store launched on iPhones in 2008, it was pitched as a win-win. Developers would have easy access to Apple users as well as tools to create and sell the apps they made — all of which was especially good for small developers that didn’t have the resources to do so otherwise. In return, Apple would get a rapidly expanding roster of apps to offer its consumers and a steady stream of cash from commissions it took off the purchase price of paid apps as well as in-app purchases. This concept has largely been a success. The App Store now offers millions of apps, and Apple says that in 2019 alone it generated more than $500 billion, most of which was not subject to the 30 percent commission Apple takes off in-app purchases and paid apps.
Because the App Store is the only way consumers can get apps on their iOS devices, Apple has been able to make whatever rules and set whatever prices it wants around all app purchases. App developers, therefore, have to agree to them or else lose their access to hundreds of millions of potential customers. For years, developers have complained about this, but they’ve had little recourse. As Epic recently found out, if you break the rules, you get banned. 
Developers have also accused Apple of using inside knowledge about which apps do well to inform its own decisions about which native apps to develop. Many of those Apple-made apps come preloaded on iOS devices and can’t be deleted, and they’re pushed to customers in a way that third-party versions are not. Apple also gives its native apps access to certain features, like Siri, that third-party apps don’t have, although the company has recently made efforts to give third-party apps that access.
These complaints, brewing for years, are now coming to the forefront as a handful of major developers protest against the perceived monopoly. 
Developers are fighting back
Epic is far from the only company to complain about the App Store. Spotify has been especially vocal. The Sweden-based music streaming platform filed an antitrust complaint with the European Commission in March 2019, saying it was forced to increase subscription rates for in-app subscriptions to make up for Apple’s fee. (Subscription services like Spotify are charged a 30 percent commission for their first year, after which it’s 15 percent.) Spotify has also claimed that Apple made its own rival streaming music service, Apple Music, after seeing Spotify’s success on its platform. 
Prompted by Spotify’s complaint, the European Commission announced in June 2020 that it was investigating Apple for antitrust violations. If the company is found to have violated the EC’s antitrust rules, consequences for Apple could be severe: a fine of up to 10 percent of the company’s annual revenue.
There have been legal issues in the United States, too. A few months after Spotify’s complaint in June 2019, American developers sued Apple, saying its rules and the lack of an app store alternative gave the company an unfair monopoly and drove up prices. This is after the Supreme Court ruling in May 2019 gave the green light to a class-action lawsuit from iPhone owners accusing Apple’s App Store of violating antitrust laws. If Apple loses the lawsuit — or decides to settle — the company could be on the hook for a massive payout to millions of App Store customers. Apple may also decide to change its App Store policies to avoid more lawsuits and payouts.
As for Epic, it has long been a critic of app store commissions in general. The company even refused to offer Fortnite in Google’s Play store until April 2020, but because Android doesn’t force users to get apps through that store, Android users could still get the game. This wasn’t possible for its Apple customers, and Fortnite has been in the App Store since 2018. According to the Wall Street Journal, the game has been downloaded on more than 130 million Apple devices and generated $1.2 billion in App Store spending — a tidy sum for Epic and Apple, but apparently not enough for Epic. 
On August 13, Epic basically dared Apple and Google to ban Fortnite from their stores by offering users the option to purchase virtual currency from Epic within the app, violating both companies’ rules. Both companies responded by banning Fortnite. Epic responded by suing both companies. It’s also mobilized its user base on social media, releasing a cheeky video and encouraging its users to share the hashtag #FreeFortnite. 

Following the Epic lawsuit, Facebook — which has its own antitrust issues — jumped on the bandwagon, claiming that the Apple tax will harm the pages and businesses using Facebook’s new paid events feature. Facebook is urging Apple to temporarily waive its commission, as Facebook is waiving its own commissions on the new feature for a year, giving any and all paid events proceeds to the pages that offer them. The social media giant framed this as a way to help businesses struggling during the Covid-19 pandemic, and as a result, Apple’s refusal to waive the fee could be seen as harmful to those businesses. 
The App Store advantage
Apple claims that the 30 percent App Store commission helps pay for the costs it incurs to host the apps. This includes peace of mind, as requiring consumers to download apps directly from the App Store helps the company ensure they meet certain standards and are secure — which is especially important when it comes to things like credit card information. 
The origins of the 30 percent cut are from the cut that Apple took from iTunes sales — between 30 and 40 percent per track. Back when the App Store was introduced, then-CEO Steve Jobs said the 30 percent commission was needed to “pay for running the store,” and that Apple didn’t “intend to make any money” from it. That may have been true in 2008 when the App Store launched with a few hundred apps, but Apple now makes billions from App Store commissions every year.
Apple also maintains that the vast majority of App Store apps are free, which means most of the apps benefit from the App Store ecosystem while giving Apple nothing in return. Free apps usually make their money from ads and by selling their data to third parties, which Apple doesn’t get a cut of. Apple has also pointed out in a recent study it commissioned that its commission is on par with the rest of the industry. But this ignores the fact that the industry was created by Apple in the first place and that other app stores followed its lead when setting their own commissions.
What this means for customers	
So, what does this all mean for you, the Apple App Store customer? Right now, it means you can’t download Fortnite or, if you already have it, you can’t download updates — which means you can’t play the new Season 4. You can still play Season 3, but your opponents will be limited to fellow Apple players. Everyone with a non-iOS device that has an updated app will move to Season 4, and inter-season battles are impossible. Players who use other platforms will also feel the effects if they can’t play their friends with Apple devices. Just how long this will last depends on which company blinks first and agrees to the other’s demands — or which company prevails if this drags on long enough to reach the courtroom.
Beyond Fortnite, Apple users may also be paying inflated prices for paid apps and in-app purchases that were set so developers could account for that 30 percent commission. As some developers have decided to forgo in-app purchases entirely to avoid paying Apple a commission, you might have taken the extra step of paying for app subscriptions on developers’ websites rather than through the apps themselves. Apple allows this for so-called “reader apps” like Spotify and Netflix, but Fortnite does not qualify. Apple’s rules also don’t make it easy for customers to figure out how to do this:


Decent chance we are imagining conflict. A little difficult to imagine Disney doing anything to irk Apple in any circumstance. Look how mute Spotify has to be. pic.twitter.com/Rz24WQOZwV— Peter Kafka (@pkafka) August 15, 2020



As evidenced by Fortnite’s removal from its store, Apple is now digging in its heels and defending these policies. But the company has given in on occasion — for instance, by allowing “premium” video providers like Amazon Prime Video to use its own payment systems for in-app purchases and avoid the 30 percent commission. (It remains to be seen if providers like Disney, which debuted its “Premier Access” for Disney+ subscribers on September 4 with a $29.99 Mulan rental, will qualify for this or if it will be forced to either pay the commission or go the Spotify route to circumvent in-app purchases.) 
In a slight relaxing of the status quo, the upcoming iOS 14 will allow users to set third-party apps as their default email and web browsing apps for the first time. But these concessions have also fueled developer complaints that Apple’s guidelines are arbitrarily enforced and preference is given to some companies over others.
None of this is a good look for Apple as antitrust investigations into it, both in the United States and abroad, heat up. Sen. Elizabeth Warren has helped lead this charge, saying in March 2019 that Apple should have to choose between selling apps and offering an app store. To do both gives the company an unfair advantage over its competitors.
“Apple and Google have bullied competitors who need their platform just to exist, all while favoring their own products and services,” Warren told Recode. “If we let companies act as both the umpire and one of the teams competing, they will continue to rig the game in their favor.”
In January 2020, CEOs from a few smaller companies testified before the House antitrust subcommittee about how tech giants, including Apple, have so much control and power that they usually own the market and offer their own products in it, making it impossible for other companies to compete.
“After years of living in fear, more companies and small businesses are beginning to speak out about the monopoly power of the tech giants,” Stacy Mitchell, the co-director of the Institute for Local Self-Reliance and well-known thorn in Big Tech’s side, told Recode. “The tech platforms are a major threat to innovation and entrepreneurship. Developers and small firms are starting to tell their stories — and more than just about anything else, that’s driving momentum in Congress, and it’s making it clear how much the antitrust agencies have to do to make up for lost time.” 
One way Apple could possibly avoid antitrust regulation — and satisfy Epic Games — is by allowing its mobile devices to obtain apps outside of the App Store, as Android does. This could give more money to developers and allow them to charge users less. But it also introduces new risks to the users if alternate app stores don’t have the same security standards as Apple or its in-app purchasing system. 
Consumers trust Apple, and it has a track record to back that up. Apple apps don’t have nearly as many security and malware issues that Android apps do. Part of the reason that Apple’s security reputation is much better is because the company strives to control every aspect of its devices, including the apps and security measures they’re forced to put in place. Consumers pay a premium for this, and so do developers.
There’s also the possibility that Apple will be able to insist on continuing to operate as normal. Developers will have to decide if the Apple tax is worth the access to Apple’s consumers. If enough of them remove their apps from the store, consumers may well decide to purchase a device with a different operating system that gives them access to the apps the App Store doesn’t. If they stay, Apple’s large cut may mean less money to the developer that would go toward improving existing apps or creating more, or that customers are paying more than they otherwise would if the App Store had some competition.
Apple, which once dealt primarily in goods, is increasingly becoming a services company — which means its income is also increasingly coming from those services rather than sales. Cutting off some of that income, either by giving developers an App Store alternative or lowering the commission, will likely force the company to make up that money some other way. And the customer always pays for that, one way or another.
Update, September 8, 3 pm ET: This story has been updated to include new details about legal actions between Apple and Epic Games.
Peter Kafka contributed reporting to this story.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  This fall’s election promises to be chaotic, nerve-wracking, and potentially calamitous. Lies will be flying everywhere, and Facebook will distribute many of them. 
And in response, Facebook plans to do … not that much.
You may be confused about this if you saw recent headlines about Facebook’s election actions, like the ones that came out this morning: “Facebook Moves to Limit Election Chaos in November,” said the New York Times. Earlier this week, Axios announced that Mark Zuckerberg is spending $300 million of his own money to help states fix their “election infrastructure.”
It’s not that these things aren’t true. It’s just that they won’t do anything to stop the misinformation and lies that are going to put the election at risk to begin with. And that’s because Facebook — like the other big social media platforms — is built in a way that lets bad actors spread lies easily, and makes it almost impossible to rein them in. The bug is the feature.
Let’s quickly talk about today’s news: Facebook says that it will ban new political ads in the week before the election. Facebook has been weighing this pre-election ad ban for nearly a year (which is why I asked Facebook ad boss Carolyn Everson about it at our Code Media conference last November). Why announce it just now? Facebook isn’t saying.
In any case, you can’t understand the ban unless you understand what the ban doesn’t do: 

The ad ban doesn’t prevent campaigns from running existing ads or ramping up the distribution of those ads. 
The ad ban also doesn’t prevent politicians from lying in those ads because Facebook has already said it won’t do that.
And Facebook isn’t doing anything about limiting the ability of politicians to target tiny groups of voters with ads built just for them — a key reform critics, including former Facebook security boss Alex Stamos, have been calling for.

So: Donald Trump and his campaign, for instance, can lie in Facebook ads, funnel the ads toward key voters, and flood their News Feeds with them all the way through the election. They just can’t run new versions of those ads, starting October 27.
Facebook also says it will take other steps to prevent people from being suckered by some of the lies they see on Facebook. The company says it will take down posts that claim you’ll get Covid-19 if you vote in person; and it will direct users to official election results if any “candidate or campaign” tries to declare victory before votes are counted.
Again, all fine things to do. But none of them counter the main problem about Facebook: It is built specifically to get its 2 billion users to share whatever they want, with anyone they want, instantly and without friction. And now that bad actors — everyone from spammers to Russian hackers to Trump and his enablers — have figured out how to use that system, they are doing so.
This morning, for instance, Trump doubled down on his calls for voters to try mail-in balloting, then to show up for in-person voting and see if they can vote again. Whether Trump is trying to actually get his voters to vote twice, which is illegal, or is simply trying to muck up the voting process and cast more doubt on the results doesn’t matter: It’s certainly an attempt to undermine the election.
And here it is, on Facebook:
  
  
    
    
      
        
  


  






      
    
    
  
  


By the way, that link, attached to Trump’s mischief post by a diligent Facebook employee doesn’t tell you that the president is trying to undermine the election. It just sends you to a well-intentioned page that provides links to local voting resources.
Twitter, of course, is Trump’s favorite place to lie directly to the public, and the same post is there, too. 
But Trump’s campaign and his surrogates love Facebook — both as a place to run ads and, crucially, as a place to run lies for free. The technical term for the latter is “organic marketing”: Instead of paying Facebook to spread your untruths, you get Facebook and its users to do it for free. 
As the Times’s Kevin Roose notes*, over and over again, right-wing publishers like Ben Shapiro’s The Daily Wire thrive on Facebook:
Pro-Trump political influencers have spent years building a well-oiled media machine that swarms around every major news story, creating a torrent of viral commentary that reliably drowns out both the mainstream media and the liberal opposition. The result is a kind of parallel media universe that left-of-center Facebook users may never encounter, but that has been stunningly effective in shaping its own version of reality. Inside the right-wing Facebook bubble, President Trump’s response to Covid-19 has been strong and effective, Joe Biden is barely capable of forming sentences, and Black Lives Matter is a dangerous group of violent looters.
And Facebook can’t do anything about this because Facebook is built to do this. Not specifically this, of course — it’s built to let you say anything to anyone about anything.
Facebook will occasionally crack down on the most egregious abuse. But it’s a global platform with 2 billion users, so it will perpetually be in a whack-a-mole scenario. And you have much better odds of getting noxious stuff onto Facebook than Facebook does at keeping it off the site or taking it down quickly. See, for instance, the successful effort to rally armed vigilantes to Kenosha, Wisconsin, last week.
Any attempts to fundamentally change that — changing the user-generated engine that fuels the site or the attention-based advertising structure that rewards users who make the most provocative stuff — would change Facebook. And since that’s not happening, you can’t count on Facebook to change the way people will use and abuse it this fall. 
Facebook can’t save us from Facebook. 
* Facebook is sensitive enough to these charges to have created a full rebuttal to Roose, which argues that conservative media isn’t quite as popular as he suggests. That said, Facebook often points to the success of right-wing media on its service when conservatives claim they are being censored on Facebook. Confusing!


  
  
  
      




  Sometime in the coming months, iPhone users will start seeing a new question when they use many of the apps on their devices: Do they want the app to follow them around the internet, tracking their behavior?
It’s a simple query, with potentially significant consequences. Apple is trying to single-handedly change the way internet advertising works.
That will affect everyone, from Apple’s giant tech rivals — most notably, Facebook, which announced today that it’s fighting back against Apple’s move — to any developer or publisher that uses ad technology to monitor what their app users are doing on the internet. 

Update, September 3, 2020, 1 pm ET: Apple has announced it will delay enforcement of its new rules until “early next year” to give developers and advertisers time to adapt; it remains unclear how this will affect Facebook’s response to Apple’s rules.

And it affects you, the person reading this story. At stake is your online privacy — and the advertising system that underwrites an endless supply of free content.
It’s a fascinating battle because there aren’t clear-cut winners and losers. And multiple players have more than one motivation.

Apple is in a position to attack intrusive advertising tech because it has consistently argued that it values privacy — and because the company doesn’t have much of an ad business of its own.
Facebook and other big ad players make billions of dollars tracing the detailed digital footprints their users leave — but they can also argue that ad money they generate allows people to consume news, entertainment, and the rest of the internet for free.
Meanwhile, ordinary internet users may say they value privacy — but it’s unlikely they have any idea how much personal data they give up when they skim a story or click on a link.

The battle may also generate collateral damage, according to one news publisher, who says Apple’s move will cut into its business at a time when it’s already struggling to hang onto any ad revenue it can find.
“It makes it much harder for us to monetize our Apple app users, who are an incredibly loyal readership. And quite frankly, it puts at risk our ability to provide an Apple app,” says Martin Clarke, publisher of DMG Media, which owns the UK’s Daily Mail and other publications; the company says its MailOnline iOS app attracts 1.2 million users per day. “There’s no point in providing an app for a platform that monetizes less well than other platforms.”
Apple announced its plan in June at its annual developer’s conference. But it hasn’t generated much attention outside of ad tech circles yet.
That will likely change in mid-September when the company is expected to roll out its new operating system, iOS 14. Without getting too far into the weeds, here’s what will happen:
For years, Apple has given each of its devices a unique identification code that makes it easier to track what you do on your phone. But under its new plan, any developer who wants to use that code — and also wants to use data collected by someone else, or who wants to send to outsiders data its users create — will have to ask you for permission. That will come via an opt-in, pop-up screen that looks like this:
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
      
        Screenshot via Apple
      
    
  



If a user says yes, then it’s business as usual: App companies and advertisers can combine information they know about a user’s behavior inside of an app, and their behavior on the rest of the web, and deliver highly targeted ads. And users will continue to see the results of their in-app behavior when they use other apps or travel around the web. Like when Zappos follows you around the internet, trying to get you to buy the shoes you looked at briefly.
But most publishers expect the vast majority of users not to sign off on having their behavior tracked. So advertisers will know much less about the people they’re trying to reach and what those people do when they encounter their ads. That’s almost certain, in the near term, to reduce the prices of the ads they sell because advertisers will find them less effective.

We don’t know exactly how that will play out for publishers who rely on that kind of targeted advertising for revenue. But we can take a guess: In 2017, Apple rolled out a similar restriction on behavior tracking for its Safari web browser, and publishers saw ad rates plummet for Safari users. 
Conventional industry wisdom is that Facebook, in particular, as well as Google, will have to rethink the way they sell their ads and are likely to see revenue get hit along the way.
Facebook has already put out research suggesting that its ad network, which runs ads on other publishers’ sites and apps, could see revenue drop by half if targeted advertising, which relies on tracking users’ behavior around the internet, goes away. And during Facebook’s most recent earnings call, chief financial officer Dave Wehner said Apple’s changes would create a “challenging headwind,” and that “it is something that people need to take very seriously. ... It’s an area of concern.”
It’s possible that most of the changes will be limited to the very biggest players in internet advertising because you need apps with very large user bases — likely 10 million monthly users or more —  to make money selling those kinds of automated, targeted ads. Which is one of the main reasons Facebook and Google are already swallowing up the majority of digital ad dollars. 
But DMG’s Clarke says the move will hit his company, even if it’s small compared to the giants. “Untargeted ads are basically worthless,” he says. Clarke says he thinks the revenue his company’s iOS app users generate could drop by 75 percent, which could prompt him to abandon the app altogether and ask his readers to read the Daily Mail on the web instead.
But other publishers are less worried about the effect on their app ad revenue and more concerned that Apple is attempting to change digital advertising on its own. 
“The instinct’s in the right place,” says Julia Beizer, who heads up digital for Bloomberg Media. “We all want to make a better, privacy-safe web. But rolling it out without consulting the industry means you’re asking publishers to bear the brunt of the sins of ad tech. Which isn’t fair.”
Other publishers say the move away from targeted ads could conceivably be a good thing for them because it would lessen the advantage Google and Facebook get from combining their massive pools of data about users’ behavior on their own app with the information they collect about what they do outside their apps. 
“If the playing field is level, and no one can track users and data, that’s bad for ad tech,” says Jason Kint, who heads up Digital Content Next, a publishers trade group. “But the world’s going this way.”
For example, the New York Times, which had already announced that it would be phasing out the use of data collected by other companies to target ads for its readers, says it simply won’t use third-party tracking data for its Apple app this fall once the changes kick in. 
Rebecca Grossman-Cohen, who heads up strategic partnerships for the Times, says the move creates “some loss” of advertising revenue, but it will be “minimal.” She says that Apple essentially sped up a decision the Times would have reached anyway: “We could fairly say we would have gotten to a similar place in any event.”
Which is more or less what Apple wants to convey. The company didn’t want to talk on the record, but executives there say they made the move to reduce tracking on its apps as an extension of other moves they have made to increase users’ privacy, like limiting targeting on Safari and requiring apps to get permission to track users’ location.
It is possible that Apple’s move to upend internet advertising will bring more scrutiny to the company, which is already fending off high-profile antitrust charges from Spotify and Epic Games. Last week, a group of publishers, including the New York Times, sent Apple a letter demanding better terms for the subscriptions it sells through its App Store. And the Daily Mail intends to complain to the US Department of Justice about Apple’s ad changes, according to a person familiar with the company’s plans. 
But the move also illustrates the fact that big tech companies aren’t just under pressure from lawmakers and government regulators — they’re also fighting each other. 
Facebook, for instance, has already complained publicly about Apple’s App Store policies twice this month. And it criticized Apple again in late August, while announcing that it simply won’t use Apple’s device identifier on its own apps.
That means Facebook won’t have to show users that pop-up screen asking for permission to track them across the web. But it’s unclear how much third-party data Facebook will use for its ad targeting, or what Apple’s reaction to Facebook’s move will be.
Facebook has already told Wall Street that it expects Apple’s new rules to affect its business. But today it is leaning into the idea that the change will mostly hurt publishers that use Facebook’s ad network to place ads on their own sites — and that it may ultimately have to shutter the network on Apple devices altogether. 
And just in case you didn’t get the political message Facebook is trying to deliver here, they spelled it out for you: Apple, not Facebook, is hurting small developers and publishers. “We understand that iOS 14 will hurt many of our developers and publishers at an already difficult time for businesses,” the company wrote on a blog post announcing its decision. “We work with more than 19,000 developers and publishers from around the globe and in 2019 we paid out billions of dollars. Many of these are small businesses that depend on ads to support their livelihood.”
Facebook is worth $800 billion. Apple is worth a staggering $2 trillion. Both companies are going to be fine no matter how this shakes out. And this is a fight about advertising technology, and even people who work in advertising technology for a living are bored by advertising technology. But watch this space: The way this brawl affects internet ads and the stuff that runs alongside them — the stuff you want to see — is worth watching. 

New goal: 25,000 
In the spring, we launched a program asking readers for financial contributions to help keep Vox free for everyone, and last week, we set a goal of reaching 20,000 contributors. Well, you helped us blow past that. Today, we are extending that goal to 25,000. Millions turn to Vox each month to understand an increasingly chaotic world — from what is happening with the USPS to the coronavirus crisis to what is, quite possibly, the most consequential presidential election of our lifetimes. Even when the economy and the news advertising market recovers, your support will be a critical part of sustaining our resource-intensive work — and helping everyone make sense of an increasingly chaotic world. Contribute today from as little as $3.



  
  
  
      




  Larry Ellison is abruptly shutting down the work of his charitable foundation, Recode has learned, yet another whiplash moment in the philanthropic history of one of the world’s richest people.
Ellison has decided to disband the team and program that worked for his sole philanthropy organization, the Larry Ellison Foundation, as part of an attempt to refocus his giving on combating the coronavirus, according to an email sent by the leader of one of Ellison’s grantees and obtained by Recode. Ellison will run that upcoming program through a “medical philanthropy” — one that does not currently exist. It is unclear what shape this new effort will take.
The sudden decision reverses at least two years of costly work by Ellison’s aides to plot a new path forward for the philanthropic efforts of the Oracle founder, who was seeking another chance to give away his $70 billion after decades of false starts and unmet promises.
Recode reported last week on Ellison’s blemished track record, including the fact that his foundation, which is based in the United Kingdom, was doing nothing publicly to deal with the Covid-19 pandemic. Now, Ellison has evidently decided to do over this do-over. 
An email written by Ratna Viswanathan, the CEO of an Ellison grantee called Reach to Teach, reads:
A couple of days ago I was informed by [foundation head] Matthew [Symonds] that Larry has decided to focus on his philanthropic efforts on fighting Covid 19 which is being handled out of the US from where he runs his medical philanthropy. He has decided to disband the LEF London office and the current team leading the Foundation from London ... The London LEF team is now working on winding down the London programme and office.
The announcement, marked “confidential,” was sent in early August to 10 subordinates of Viswanathan and obtained by Recode. Viswanathan, Symonds, and other Ellison aides did not return repeated requests for comment.
It makes sense that Ellison would feel compelled to invest his billions into dealing with the coronavirus pandemic, which has created a call-to-arms moment for the philanthropy sector. He obviously could not have predicted that crisis when he began to reboot his foundation in 2018. The demands today are different from what they were two years ago.
Yet it is still another remarkable change of heart. The closure of the London-based operation would be at least the third time that Ellison has, with little warning, shut down parts of his philanthropy program, leaving grantees in the lurch. In 2005, he decided one day to stop spending his money on researching infectious disease. And in 2013, after spending hundreds of millions of dollars on the cause, Ellison told aides out of nowhere to halt all new funding for anti-aging research, which had been his foremost philanthropic passion. So this fits a pattern.
After that decision in 2013, Ellison began embarking on a trek through the philanthropic wilderness, trying to determine where to spend his net worth — a figure that exploded in the ensuing years. In 2018, Ellison brought on a new executive director, tapped high-priced consultants, launched a six-month strategic review, renamed the foundation, moved its headquarters from the Bay Area to the United Kingdom, and hired as many as a dozen advisers. It was a complete and time-intensive overhaul and relaunch of what could be one of the world’s most important charities.
Now, that all appears to have been for naught.
The nonprofits that Ellison was backing, such as Reach to Teach, which educates rural students in India, may now have to scramble for new funding. Viswanathan said in her email to the nonprofit’s leadership that she was confident that Reach to Teach could survive until 2024, when the Ellison Foundation’s current grant expires, but that she will have to hire a fundraising director and consider a restructuring, even as she hopes that Ellison’s “soft spot” for the program, which he has funded since 2008, would give them the chance to receive Ellison funding again in the future.
Those in the foundation leadership “feel that he will return to these areas once there is a solution to address the pandemic and its disastrous fallout,” Viswanathan said.
Ellison has said nothing publicly about any efforts of his to deal with the coronavirus in a personal capacity. His most substantive work to date has been through his company, Oracle, which built and donated a database to the US Department of Health and Human Services to test the efficacy of various therapeutics for the infection. It is unclear, however, how widely used this database is, and epidemiologists have expressed concerns that it amounts to junk science because it does not use randomized-controlled experiments to measure the drugs’ impacts.
So it is not clear what Viswanathan is referring to when she speaks of Ellison’s “medical philanthropy” in the United States — he has not publicly funded new medical grants since 2013, when the philanthropy was called the Ellison Medical Foundation.
One possibility is that she was referring to the cancer research center that Ellison set up at the University of Southern California with $200 million in 2016. Ellison could be plotting to coordinate future Covid-19 work through that center, the Lawrence J. Ellison Institute for Transformative Medicine. But Ellison has no formal role at the institute, which opened last year.
No matter the structure, in some ways Ellison appears to be circling back into his former self: trying to use his billions to forge medical breakthroughs, even if not to live forever.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



The Apple-Google exposure notification tool is getting a major upgrade. The two companies just announced the debut of Exposure Notifications Express, which will enable their exposure notification tool to work without a public health agency needing to build or maintain an app around it. Now, states or public health authorities that don’t have the resources or desire to build an app, but still want to take advantage of the tool, will be able to do so.
Streamlining the process of getting people to use this tool is a big deal. The Apple-Google tool is one of the most promising Big Tech attempts to help stop or contain the Covid-19 pandemic, but it has struggled to win over a lot of users. Without widespread adoption, contact tracing apps, including the ones that incorporate the Apple-Google technology, are basically useless: Studies have shown that at least 60 percent adoption is needed for a contact tracing app to be effective. But some experts say far less than that is needed when combined with human contact tracers.
“We estimate that a well-staffed manual contact tracing workforce combined with 15 percent uptake [in a contact tracing app] could reduce infections by 15 percent and deaths by 11 percent,” Professor Cristophe Fraser in the department of health at Oxford University said in a statement.
So far, Maryland, Nevada, Virginia, and Washington, DC, have already signed on to use Exposure Notifications Express, while six states — Alabama, Arizona, North Dakota, Wyoming, Nevada, and Virginia — have apps that use the tool. While several other countries have apps that use the tool, the United States has left it to individual states to figure out their own contact tracing efforts, which have been less than enthusiastic. Although Apple and Google announced the exposure notification tool in April and launched it a month later, it wasn’t until August that Virginia became the first state to release a contact tracing app that used the tool. Virginia told Recode that it spent nearly $230,000 to develop the app and $1.5 million to market it — money that came from the federal CARES Act. Nearly 500,000 Virginians have downloaded the app so far, which is a small portion of the state population of about 8.5 million people.
One huge upside to the new Exposure Notifications Express tool, however, is that states no longer have to spend the time or money developing their own apps. They may not have to do much marketing, either. In states or regions that have enabled Exposure Notifications Express, a prompt will pop up on phones with the latest version of Apple’s or Android’s operating system and alert the user that the tool is available to them. The user just taps the screen to enable it. For Apple users, that’s all it takes to turn the tool on. Android users will then need to download an app that Google automatically generates for public health authorities. All public health authorities have to do is give Apple and Google some basic information and set up servers to host Bluetooth keys and exposure verification.
“As the next step in our work with public health authorities on Exposure Notifications, we are making it easier and faster for them to use the Exposure Notifications System without the need for them to build and maintain an app,” Apple and Google said in a statement. “Exposure Notifications Express provides another option for public health authorities to supplement their existing contact tracing operations with technology without compromising on the project’s core tenets of user privacy and security.”
Countries around the world have spent weeks trying various methods of contact tracing. The basic idea is that public health agencies can use contact tracing to notify people when they’ve potentially been exposed to the coronavirus so they can then quarantine and get tested accordingly. Contact tracing also helps track the virus’s spread, and that part is usually done manually, using human beings. Digital contact tracing tools, however, are designed to do this by using devices like smartphones to alert users when they’ve been close to a device tied to someone who’s potentially been exposed to the virus. The process of notifying people of potential exposure is where the Apple-Google exposure notification tool comes in handy. 
The tool works by sending out and receiving anonymous Bluetooth “keys” from nearby phones that also have the tool enabled. If someone tests positive for the coronavirus, they can notify their public health authority, which will then send out alerts to any phones that were in proximity to the infected person’s phone. The system is designed to keep as much information as possible on individual phones and preserve user privacy. Very little information goes back to the public health authority, and users always have the option to opt in or out of the tool. Apple and Google have said that user privacy was a major consideration in their development of the tool, to encourage as many people to use it as possible. The two companies worked together on the project to allow the tool to work across their iOS and Android operating systems.
Contact tracing apps haven’t lived up to their potential so far, but Exposure Notifications Express should make it as easy as possible for public health authorities to implement them and people to enable them. Now we might get a chance to see what digital contact tracing apps can do — if it isn’t too late, six months into the pandemic, for them to make a difference.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



The Apple-Google exposure notification tool is getting a major upgrade. The two companies just announced the debut of Exposure Notifications Express, which will enable their exposure notification tool to work without a public health agency needing to build or maintain an app around it. Now, states or public health authorities that don’t have the resources or desire to build an app, but still want to take advantage of the tool, will be able to do so.
Streamlining the process of getting people to use this tool is a big deal. The Apple-Google tool is one of the most promising Big Tech attempts to help stop or contain the Covid-19 pandemic, but it has struggled to win over a lot of users. Without widespread adoption, contact tracing apps, including the ones that incorporate the Apple-Google technology, are basically useless: Studies have shown that at least 60 percent adoption is needed for a contact tracing app to be effective. But some experts say far less than that is needed when combined with human contact tracers.
“We estimate that a well-staffed manual contact tracing workforce combined with 15 percent uptake [in a contact tracing app] could reduce infections by 15 percent and deaths by 11 percent,” Professor Cristophe Fraser in the department of health at Oxford University said in a statement.
So far, Maryland, Nevada, Virginia, and Washington, DC, have already signed on to use Exposure Notifications Express, while six states — Alabama, Arizona, North Dakota, Wyoming, Nevada, and Virginia — have apps that use the tool. While several other countries have apps that use the tool, the United States has left it to individual states to figure out their own contact tracing efforts, which have been less than enthusiastic. Although Apple and Google announced the exposure notification tool in April and launched it a month later, it wasn’t until August that Virginia became the first state to release a contact tracing app that used the tool. Virginia told Recode that it spent nearly $230,000 to develop the app and $1.5 million to market it — money that came from the federal CARES Act. Nearly 500,000 Virginians have downloaded the app so far, which is a small portion of the state population of about 8.5 million people.
One huge upside to the new Exposure Notifications Express tool, however, is that states no longer have to spend the time or money developing their own apps. They may not have to do much marketing, either. In states or regions that have enabled Exposure Notifications Express, a prompt will pop up on phones with the latest version of Apple’s or Android’s operating system and alert the user that the tool is available to them. The user just taps the screen to enable it. For Apple users, that’s all it takes to turn the tool on. Android users will then need to download an app that Google automatically generates for public health authorities. All public health authorities have to do is give Apple and Google some basic information and set up servers to host Bluetooth keys and exposure verification.
“As the next step in our work with public health authorities on Exposure Notifications, we are making it easier and faster for them to use the Exposure Notifications System without the need for them to build and maintain an app,” Apple and Google said in a statement. “Exposure Notifications Express provides another option for public health authorities to supplement their existing contact tracing operations with technology without compromising on the project’s core tenets of user privacy and security.”
Countries around the world have spent weeks trying various methods of contact tracing. The basic idea is that public health agencies can use contact tracing to notify people when they’ve potentially been exposed to the coronavirus so they can then quarantine and get tested accordingly. Contact tracing also helps track the virus’s spread, and that part is usually done manually, using human beings. Digital contact tracing tools, however, are designed to do this by using devices like smartphones to alert users when they’ve been close to a device tied to someone who’s potentially been exposed to the virus. The process of notifying people of potential exposure is where the Apple-Google exposure notification tool comes in handy. 
The tool works by sending out and receiving anonymous Bluetooth “keys” from nearby phones that also have the tool enabled. If someone tests positive for the coronavirus, they can notify their public health authority, which will then send out alerts to any phones that were in proximity to the infected person’s phone. The system is designed to keep as much information as possible on individual phones and preserve user privacy. Very little information goes back to the public health authority, and users always have the option to opt in or out of the tool. Apple and Google have said that user privacy was a major consideration in their development of the tool, to encourage as many people to use it as possible. The two companies worked together on the project to allow the tool to work across their iOS and Android operating systems.
Contact tracing apps haven’t lived up to their potential so far, but Exposure Notifications Express should make it as easy as possible for public health authorities to implement them and people to enable them. Now we might get a chance to see what digital contact tracing apps can do — if it isn’t too late, six months into the pandemic, for them to make a difference.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  Walmart’s much-anticipated membership program, Walmart+, will finally launch nationwide September 15, the company announced Tuesday, about six months after the Covid-19 pandemic pushed the retailer to delay its original timing. The brick-and-mortar retail giant needs the program to be successful in order to stop top-spending customers from fleeing to Amazon Prime.
Walmart+ will cost $98 a year, or $12.95 a month, and focus mainly on unlimited delivery of groceries and other general merchandise from Walmart stores that, for orders more than $35, will be delivered as soon as the same day. Members also get fuel discounts at Walmart gas stations and those of partners, as well as access to “Scan & Go” technology, which allows shoppers to use smartphones to scan goods at Walmart stores. The company says it will add more perks in the future. Recode previously reported these may include a branded credit card, early availability on product deals, and, potentially, access to a popular streaming video service.
Walmart wants the membership program to be “the ultimate life hack” for customers, Walmart Chief Customer Officer Janey Whiteside told Recode in an interview on Monday, arguing that its perks will save customers both time and money.
At the same time, Walmart+ will undoubtedly attract comparisons to Amazon’s Prime program, the ultra-popular delivery and entertainment membership program that boasts more than 150 million members worldwide and has developed into a retail industry wrecking ball since its launch in 2005. Amazon Prime includes express delivery of millions of products (including groceries), video streaming of a large library of TV shows and movies, music streaming, and other perks. It now costs $119 a year, and Prime customers spend more and shop more frequently than non-Prime members. 
And, most importantly for Walmart, more than half of Walmart’s top-spending families are now Prime members, as Recode previously reported. Which leads to the question: Will they really subscribe to both membership programs?
When asked about comparisons to Prime, Whiteside told Recode that “we didn’t necessarily launch Walmart+ to compete with anything else.” And that answer makes sense; the head-to-head comparison between the services does not look great for Walmart when considering online customers who value the widest selection of goods or the longest list of perks. 
In addition to the unlimited delivery perk — which is basically just a rebrand of Walmart’s existing Delivery Unlimited membership — Walmart+ only features two other benefits at launch. One is fuel discounts of up to 5 cents per gallon at Walmart, Murphy USA, and Murphy Express gas stations (Sam’s Club gas stations are slated to be included soon). The other perk is access to Walmart’s Scan & Go technology for in-store shopping, which allows shoppers to scan items with their phone, scan their phone at a self-checkout kiosk, and walk out of the store without stopping to pay. Walmart briefly tested but discontinued the tool two years ago. Walmart’s bet is that the mix of online, in-store, and on-the-go perks, like fuel discounts, will carry unique appeal. Whiteside said that “deepening a relationship further will mean we will get an even greater share of wallet from those customers.” Of course, some Walmart shoppers will also value the $21 difference between the annual fee of Walmart+ and Amazon Prime.
Amazon has made moves in recent years for Prime to appeal to households with less disposable income that historically have favored shopping at Walmart. Amazon added a monthly payment option for Prime fees in 2016, a 45 percent Prime fee discount for those on government assistance in 2017, and, most recently, ways for Prime customers to pay for orders with cash. By early 2017, Amazon Prime membership growth was strongest in the US for households making less than $50,000 a year, according to a study by Robert W. Baird & Co. 
The success of Walmart+ will likely hinge on how many customers are attracted to the core grocery delivery component of it. While Walmart’s overall grocery business is larger than Amazon’s and its prices are often cheaper, one fear is that top Walmart customers could eventually turn to Amazon for groceries as they get sucked further into the Prime suite of perks. Sources previously told Recode that some Walmart execs believe that top-spending Walmart families that subscribe to Amazon Prime will still be attracted to Walmart+ because its fresh grocery prices are often lower than those Amazon offers.
In the past, some Walmart executives have opposed a paid membership program, seeing Walmart’s competitive advantage as giving shoppers everyday low prices without the need to pay a membership fee. Whiteside promised that the low prices will remain even for those who don’t splurge for the bonus services.
“In no way does this membership program take anything away from customers who don’t choose to, or can’t afford to, engage with this,” she said.
On the company’s earnings call earlier this month, CEO Doug McMillon stressed the flexibility of Walmart’s customer offerings.
“We’re going to have multiple ways to serve them, and those families will decide in that moment how they want to shop,” McMillon said. “And sometimes they’ll be in the store, and sometimes they’ll do pickup, and sometimes they’ll do delivery, and many of them will buy a membership, and when they do they’ll get benefits from that.”






  
  
  
      




  Walmart’s much-anticipated membership program, Walmart+, will finally launch nationwide September 15, the company announced Tuesday, about six months after the Covid-19 pandemic pushed the retailer to delay its original timing. The brick-and-mortar retail giant needs the program to be successful in order to stop top-spending customers from fleeing to Amazon Prime.
Walmart+ will cost $98 a year, or $12.95 a month, and focus mainly on unlimited delivery of groceries and other general merchandise from Walmart stores that, for orders more than $35, will be delivered as soon as the same day. Members also get fuel discounts at Walmart gas stations and those of partners, as well as access to “Scan & Go” technology, which allows shoppers to use smartphones to scan goods at Walmart stores. The company says it will add more perks in the future. Recode previously reported these may include a branded credit card, early availability on product deals, and, potentially, access to a popular streaming video service.
Walmart wants the membership program to be “the ultimate life hack” for customers, Walmart Chief Customer Officer Janey Whiteside told Recode in an interview on Monday, arguing that its perks will save customers both time and money.
At the same time, Walmart+ will undoubtedly attract comparisons to Amazon’s Prime program, the ultra-popular delivery and entertainment membership program that boasts more than 150 million members worldwide and has developed into a retail industry wrecking ball since its launch in 2005. Amazon Prime includes express delivery of millions of products (including groceries), video streaming of a large library of TV shows and movies, music streaming, and other perks. It now costs $119 a year, and Prime customers spend more and shop more frequently than non-Prime members. 
And, most importantly for Walmart, more than half of Walmart’s top-spending families are now Prime members, as Recode previously reported. Which leads to the question: Will they really subscribe to both membership programs?
When asked about comparisons to Prime, Whiteside told Recode that “we didn’t necessarily launch Walmart+ to compete with anything else.” And that answer makes sense; the head-to-head comparison between the services does not look great for Walmart when considering online customers who value the widest selection of goods or the longest list of perks. 
In addition to the unlimited delivery perk — which is basically just a rebrand of Walmart’s existing Delivery Unlimited membership — Walmart+ only features two other benefits at launch. One is fuel discounts of up to 5 cents per gallon at Walmart, Murphy USA, and Murphy Express gas stations (Sam’s Club gas stations are slated to be included soon). The other perk is access to Walmart’s Scan & Go technology for in-store shopping, which allows shoppers to scan items with their phone, scan their phone at a self-checkout kiosk, and walk out of the store without stopping to pay. Walmart briefly tested but discontinued the tool two years ago. Walmart’s bet is that the mix of online, in-store, and on-the-go perks, like fuel discounts, will carry unique appeal. Whiteside said that “deepening a relationship further will mean we will get an even greater share of wallet from those customers.” Of course, some Walmart shoppers will also value the $21 difference between the annual fee of Walmart+ and Amazon Prime.
Amazon has made moves in recent years for Prime to appeal to households with less disposable income that historically have favored shopping at Walmart. Amazon added a monthly payment option for Prime fees in 2016, a 45 percent Prime fee discount for those on government assistance in 2017, and, most recently, ways for Prime customers to pay for orders with cash. By early 2017, Amazon Prime membership growth was strongest in the US for households making less than $50,000 a year, according to a study by Robert W. Baird & Co. 
The success of Walmart+ will likely hinge on how many customers are attracted to the core grocery delivery component of it. While Walmart’s overall grocery business is larger than Amazon’s and its prices are often cheaper, one fear is that top Walmart customers could eventually turn to Amazon for groceries as they get sucked further into the Prime suite of perks. Sources previously told Recode that some Walmart execs believe that top-spending Walmart families that subscribe to Amazon Prime will still be attracted to Walmart+ because its fresh grocery prices are often lower than those Amazon offers.
In the past, some Walmart executives have opposed a paid membership program, seeing Walmart’s competitive advantage as giving shoppers everyday low prices without the need to pay a membership fee. Whiteside promised that the low prices will remain even for those who don’t splurge for the bonus services.
“In no way does this membership program take anything away from customers who don’t choose to, or can’t afford to, engage with this,” she said.
On the company’s earnings call earlier this month, CEO Doug McMillon stressed the flexibility of Walmart’s customer offerings.
“We’re going to have multiple ways to serve them, and those families will decide in that moment how they want to shop,” McMillon said. “And sometimes they’ll be in the store, and sometimes they’ll do pickup, and sometimes they’ll do delivery, and many of them will buy a membership, and when they do they’ll get benefits from that.”






  
  
  
      




  Just last week, Facebook finally banned militia groups and pages that advocate for violence on its platform. But Recode’s quick Facebook search for “militia” groups and pages on Friday surfaced more than a dozen results for national and local militia groups, most of them private, with many of them openly calling for violence against protesters.
Two of these groups that Recode accessed had a combined 25,000 members and included posts where members encouraged and celebrated shooting people involved in recent Black Lives Matter protests. Some groups did not contain “militia” in the title but still encouraged members to take up arms. One page, called the “The III% Organization,” contained overtly racist and violent posts, such as a meme comparing BLM protesters to dogs and joking about running them over with a car. 
After Recode flagged seven of these groups and pages to Facebook, the company took down four of them for violating its policies, and said it independently took down another. 
Militia groups that organize on Facebook are under particular scrutiny this week after a 17-year-old self-identified militia member was arrested on suspicion of killing two people protesting the police shooting of Jacob Blake in Kenosha, Wisconsin. 
In the aftermath of that shooting, Facebook has faced sharp criticism, including from its own employees, for initially failing to remove a Kenosha militia page despite prior complaints from at least two Facebook users about the group inciting violence. The company eventually took the page and an associated event down, but only after the alleged shooter killed two protesters and injured another on Tuesday night. Facebook said the suspect was not a member of the Kenosha militia page in question. 
Many civil rights groups leaders, employees, and politicians have long accused the company of not doing enough to stop the spread of violent rhetoric on its platform.
The social media giant’s CEO Mark Zuckerberg said in a company meeting Thursday that the initial decision to not take down the Kenosha militia group’s page was a mistake, according to internal remarks first reported by BuzzFeed News, which the company later posted publicly. Zuckerberg said the company is working to take down any posts praising the alleged shooter, and that it’s all part of Facebook’s recently expanded policy against dangerous groups and threats. 


  Do you work at Facebook and want to talk? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.




While the militia groups Recode found on Friday represented a small fraction of Facebook’s some 2.7 billion users, their continued presence on the platform despite its new policies signals how big a challenge it is for Facebook to stop people from using its platform to organize violence and amplify hate speech. While Facebook, Twitter, and other platforms have adopted stricter guidelines over the years on violent speech, they’ve struggled to catch harmful content in real time, while balancing concerns about limiting free speech online with strict enforcement.
“The continued presence of these militia Facebook groups and the concerning content that they contain represent multiple layers of failure on the part of Facebook to adhere to its own policies that it repeatedly pushes in press releases and statements to media,” said Katie Paul, director of tech watchdog group Tech Transparency Project, which has been researching some of these militia groups. 
A spokesperson for Facebook issued the following statement to Recode, in part: 
The shootings in Kenosha have been painful for everyone, especially our black community. Mark addressed this at yesterday’s weekly company Q&A ... We launched [the dangerous individuals and organizations] policy last week and we’re still scaling up our enforcement of it by a team of specialists on our Dangerous Organizations team.
Under pressure, Facebook recently expanded its policies against violent individuals and organizations to restrict the influence of domestic militia groups and conspiracy groups like QAnon. While Facebook doesn’t have a blanket ban on militia groups that don’t overtly call for violence, it removed hundreds of them last week for advocating violence, and says it is continuing to take down groups and pages which do so. 
The militia Facebook groups and pages Recode reviewed on Friday advocate for US citizens to take up arms to counter what they describe as worsening lawlessness in the country, with many members aggrieved by property damage that’s occurred during protests for racial justice in cities across the US. 
While many of the protests across the United States in recent months have been peaceful, there has been significant damage to buildings in some areas, such as Minnesota, where insurance claims have been filed for tens of millions of dollars. The Kenosha protest shooting demonstrates how militia attempts to guard buildings from such damage can result in escalating conflict — and ultimately, lives lost. 
One private Facebook group called “United States Militia” had over 12,000 members and was active on Facebook until Recode flagged it to Facebook on Friday afternoon. Its description stated, in part, “Citizens are the militia” and that “we the people” “prepare for the worst and rejoice on the best ... with the blood of patriot’s and tyrants.” Recode reviewed comments from within the private group from screenshots provided by the Tech Transparency Project.
In response to one “United States Militia” member post about people setting fire to a car dealership during protests this week, one user responded that self-designated “patriots” should “shoot first and ask questions later.” Another posted in response, “Time for shoot to kill these asswipes!”
On another Facebook page called “Virginia Militia,” which had over 13,000 members until it disappeared from Facebook on Friday afternoon, more than 100 users commented in support of the alleged Kenosha shooter, arguing his violence was justified. This was a clear violation of Facebook’s stance against any praise of the suspected shooter. 
One commenter went so far as to advise other members to evade law enforcement if they are involved in a similar shooting. “I believe we should all take this as a sign,” wrote the user. “If you’re forced into a shootout, and you survive. Do not wait for police, do not turn yourself in. Grab your survival bag and go ghost. Get in contact with a trusted patriot and hide out.” Sixteen members of the group reacted with a thumbs-up. 
Recode also found several other groups and pages on Facebook that organize members for coordinated armed action, but that omit the word “militia” in their names or descriptions. 
One such page was called the “The III% Organization” — a reference to the far-right “3 percenters” movement, which advocates for armed militia to promote gun ownership rights and resistance to the US federal government in local affairs. This page also disappeared after Recode flagged it to Facebook.
A user in the group posted a meme on Wednesday, the morning after two protesters were killed in Kenosha, showing a man standing next to his car, with his hand over his chest and looking visibly relieved, with the caption, “When you think you ran over a dog but it was just a few BLM & Antifa Rioters.”
These groups are organizing and spreading their calls to violence in an increasingly polarized political atmosphere. In recent days, major right-wing media figures like Ann Coulter and Fox News host Tucker Carlson have attempted to justify vigilante violence at protests.
Twitter took down a tweet from Coulter saying she wanted the suspected Kenosha shooter to be president after individuals and groups like civil rights groups Color of Change raised concerns about its glorification of a suspected killer. 
At the same time, some conservative politicians have been sharply criticized for seemingly threatening state-sponsored violence against civil rights protesters since the George Floyd and Breonna Taylor protests began earlier this year.
Democratic presidential nominee Joe Biden has accused President Donald Trump of rooting for violence at recent protests around racial justice in the US. In May, Trump posted on social media that “when the looting starts, the shooting starts,” about protests in Minnesota against the police killing of George Floyd. In June, Sen. Tom Cotton (R-AR) published a deeply controversial column in the New York Times that the paper eventually said “should not have been published”; it was titled “Send in the Troops” and advocated for bringing military forces to protests.
The proliferation of militia groups and violent, partisan rhetoric isn’t just happening on Facebook, and it’s not even necessarily originating there — it’s a complex problem that involves elected officials and right-wing media figures, too. But even if militia groups aren’t contained to Facebook, the platform is making it possible for members to amplify their views. The groups and pages that Facebook only took down after Recode flagged them are a signal that the company has a major challenge ahead if it intends to effectively enforce its new policies prohibiting the propagation of violent views on its platform.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



At a Friday event, Elon Musk revealed more details about his mysterious neuroscience company Neuralink and its plans to connect computers to human brains. While the development of this futuristic-sounding tech is still in its early stages, the presentation was expected to demonstrate the second version of a small, robotic device that inserts tiny electrode threads through the skull and into the brain. Musk said ahead of the event he would “show neurons firing in real-time. The matrix in the matrix.” 
And he did just that. At the event, Musk showed off several pigs that had prototypes of the neural links implanted in their head, and machinery that was tracking those pigs’ brain activity in real time. The billionaire also announced the Food and Drug Administration had awarded the company a breakthrough device authorization, which can help expedite research on a medical device. 
Like building underground car tunnels and sending private rockets to Mars, this Musk-backed endeavor is incredibly ambitious, but Neuralink builds on years of research into brain-machine interfaces. A brain-machine interface is technology that allows for a device, like a computer, to interact and communicate with a brain. Neuralink, in particular, aims to build an incredibly powerful brain-machine interface, a device with the power to handle lots of data, that can be inserted in a relatively simple surgery. Its short-term goal is to build a device that can help people with specific health conditions. 
The actual status of Neuralink’s research has been somewhat murky, and Friday’s big announcement happened as ex-employees complain of internal chaos at the company. Musk has already said the project allowed a monkey to control a computer device with its mind, and as the New York Times reported in 2019, Neuralink had demonstrated a system with 1,500 electrodes connected to a lab rat. Since then, Musk has hinted at the company’s progress (at times on Twitter), though those involved have generally been close-lipped about the status of the research. 
Musk opened Friday’s event by emphasizing the wide variety of spinal and neurological conditions — including seizures, paralysis, brain damage, and depression — that Neuralink technology could help treat. 
“These can all be solved with an implantable neural link,” said Musk. “The neurons are like wiring, and you kind of need an electronic thing to solve an electronic problem.” 

But it’s worth highlighting that Musk wants Neuralink to do far more than treat specific health conditions. He sees the technology as an opportunity to build a widely available brain-computer interface for consumers, which he thinks could help humans keep pace with increasingly powerful artificial intelligence.
So while modest, Neuralink’s research already foreshadows how this technology could one day change life as we know it. At the same time, it’s a reminder that the potential, eventual merging of humans with computers is destined to introduce a wide range of ethical and social questions that we should probably start thinking about now. 
Neuralink wants to link your brain with computers, but that will take a while
Founded in 2016, Neuralink is a neuroscience technology company focused on building systems with super-thin threads that carry electrodes. When implanted into a brain, these threads would form a high-capacity channel for a computer to communicate with the brain, a system supposed to be much more powerful than the existing brain-machine interfaces being researched. 
One major barrier to inserting these incredibly tiny wires, which are thinner than a strand of human hair, is actually getting them past the skull and into the brain. That’s why Neuralink is also developing an incredibly small robot that connects the electrode to humans through surgery that’s about as intensive as a Lasik eye procedure. On Friday, Musk outlined how the company hopes to do the procedure without general anesthesia in a single-day hospital stay. That’s the goal at least, and would represent a huge leap forward from previous brain-machine interfaces, which have required more invasive surgeries.

“We’ve been connecting forms of computers to brains for 20 or 30 years already,” Nolan Williams, the director of Stanford’s Brain Stimulation Lab, told Recode, referencing deep stimulation used for patients with Parkinson’s as one example of connecting a brain and a computer.
“The brain itself uses certain frequencies and certain kinds of electrical thresholding to communicate with itself,” Williams explained. “Your brain is a series of circuits that kind of intercommunicate and communicate between themselves.” 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        A screenshot of the demo shows how a prototype can track the neural spikes of a pig that’s had the device implanted.
      
      
        Screenshot from YouTube
      
    
  


Essentially, a brain-machine interface can use the electricity the brain already uses to function along with a series of electrodes to connect the brain with a machine. Neuralink cites previous examples in which humans have used electrodes to control cursors and robotic limbs with their minds as the basis for its system. But what’s novel about Neuralink’s plan is making the process of connecting a device with the brain minimal, while also massively increasing the number of electrodes engaged. The company wants to make brain-machine interfaces not only easier to install but also more powerful. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        The Neuralink surgical robot, revealed on Friday, is supposed to manipulate and insert the tiny threads into the brain.
      
      
        Woke Studios
      
    
  


As the focus of Friday’s event, Musk showed what the second generation of that robot will look like: a large white structure with five degrees of freedom. 
“The robot is a super complicated, highly-precise machine which is able to both capture your brain and then with almost a sewing machine-like, micro-precise needle and thread, place the neural threads in the exact right location based on the surgeon decisions around what the safe locations are for the threads to be inserted,” Afshin Mehin, a designer and founder of the firm Woke, which worked on the robot’s outer device that holds the needle, told Recode.  
The machine operates at a very small scale, and Neuralink hopes to expand its capabilities. For instance, the current robot has a 150 micrometer gripper, and an even tinier needle — less than 40 micrometers — which can “grasp the implant’s threads then precisely insert each into the cortex while avoiding visible vasculature,” according to Neuralink robotics director Ian O’Hara. He added in an emailed statement that, while the robot currently handles only the insertion of the threads, Neuralink is working to expand the robot’s role in surgery to increase the number of patients it can help and make the procedure shorter. 
Musk said that, in the past year, Neuralink simplified its plans for a wearable device that connects to the threads implanted in the user’s brain. While the first generation of this device would have been installed behind a person’s ear, the newest version is a small, coin-size device that would sit under the top of their skull.
“It’s kind of like a Fitbit in your skull with tiny wires,” explained Musk, who compared the device to a smart watch. 
The research is still in early stages and, as it advances, will likely require focusing on how the technology can help people with specific, severe health conditions first, according to Stanford neurosurgery professor Maheen Adamson. While the medical applications of such technology could be wide-ranging, moving it from its current, nascent state will require the close oversight of the Food and Drug Administration, which would not comment specifically on Neuralink. 

  
    Related
  

  
    Why Elon Musk fears artificial intelligence
  

Again, Neuralink’s ultimate plans will go beyond treating specific conditions. The company has said it hopes to allow people to “preserve and enhance” their brains and to “create a well-aligned future.” While that might not sound like a particularly pressing need to the average person, the project fits into Musk’s long-standing concerns about artificial intelligence. Musk has previously said the technology could be more dangerous than nuclear weapons, and warned that AI could become too powerful, too quickly, preventing humans from keeping it in check. 
The ultimate goal for Neuralink, Musk explained at a 2019 launch event, is a “full brain-machine interface” that will achieve a “symbiosis with artificial intelligence.” But again, that is still far off. 
Brain-machine interfaces are nothing new, but they raise ethical concerns  
Neuralink and Musk are not the only ones interested in brain-machine interfaces. Facebook, for instance, is hard at work on its own brain-machine interface research with the University of California San Francisco. The company has its eye on creating a “hands-free” way of communicating with computers and has shared some very preliminary results. Last year, Facebook purchased CTRL-Labs, a startup that developed technology that measures neuron activity through a wearable worn on the arm, in order to control digital activity. 
Then there’s ongoing medical research, which is more common than you might think.
“This is something that is done today,” Steven Chase, of Carnegie Mellon’s Neuroscience Institute, told Recode. “There are clinical trials ongoing right now where quadriplegic patients have electrodes implanted in their brains, and they use those electrodes and the neural activity recorded on those electrodes to control external devices, such as cursors on computer screens or robotic arms.”  
In fact, some of the first medical research into such technology came in the second half of the 20th century, and to some extent, brain-machine interfaces currently exist with limited capabilities. The 1980s saw the invention of both deep brain stimulation and what’s called transcranial magnetic stimulation, which, according to the Mayo Clinic, uses “magnetic fields to stimulate nerve cells in the brain,” and can be used to treat patients with depression. 
In the early 2000s came BrainGate, an experimental device that uses an array of electrodes to essentially translate the desire to move limbs from the brain to a device, which is still being researched. The FDA in 2013 approved a system called the RNS Simulator that fires small electrical signals into the brain to stop seizures in some patients with epilepsy. 
Already, there are some rudimentary commercial devices that do things one could loosely compare to what a brain-machine interface does. There are headbands that claim to use EEGs to measure your brain activity and then use that data to do anything from enhancing meditation to piloting a drone. These applications are far from the technology Neuralink aims to provide, but they may hint at what our future could look like: Two years ago, DARPA used an experimental brain-computer interface, a surgical microchip, that allowed a paralyzed person to navigate simulated aircraft. 
“The idea of sort of sending complex thoughts wirelessly around the world is far, far, far away beyond our lifetime,” said Tim Marler, a senior research engineer at RAND. “This is definitely not science fiction. It will be mature and practical and commercial eventually, but there’s a lot of work to be done still.”
There is a huge technical divide between what’s currently possible in today’s research laboratories and the concept Musk envisions, which requires devices that can handle a significant amount of information going in and out of the brain. One broader hope for brain-machine-interface technology is that it could ultimately help people with paralysis to complete daily tasks on their own. As Chase, from Carnegie Mellon, explained, “The biggest thing these patients want is independence; this technology has the potential to offer them that.” 
“As we go deeper into this project, we’re asking ourselves: what is it going to be like to have a thought-based interface? Are you gonna have to think in a different way to send and receive ideas?” remarked Mehin, the designer. “Are you gonna have to train your brain to think a certain way? What’s it gonna be like to receive information?”
But in addition to the technological challenges, the development of brain-machine interfaces also ventures into uncharted ethical and legal territory. On Thursday, the government-funded think tank RAND issued a report on the need for policies around the use of brain-machine interfaces within the military context, where the technology could introduce new concerns like widespread hacking. Of course, with devices that could essentially strip data out of your own mind — including about peoples’ psychological and emotional states — the privacy implications of brain-machine interfaces are also enormous. 
“If brain-reading devices have the ability to read the content of thoughts, in the years to come governments will be interested in using this tech for interrogations and investigations,” neuroscience-focused researcher Marcello Ienca told Vox last year.
The list of challenges goes on. Chase raises another concerning scenario: a world in which this technology is only available to the wealthy, creating an extreme technological divide.  And then there are the unanticipated health risks of, you know, surgically inserting computer hardware into human brains. 
While there’s no reason to worry too much about that right now, Neuralink’s next big announcement is a sign that the idea of connecting human brains to computers on a more regular basis is quickly becoming a reality.
Update Tuesday, September 1, 6:15 ET: This piece has been updated with additional details from Neuralink. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

New goal: 25,000 
In the spring, we launched a program asking readers for financial contributions to help keep Vox free for everyone, and last week, we set a goal of reaching 20,000 contributors. Well, you helped us blow past that. Today, we are extending that goal to 25,000. Millions turn to Vox each month to understand an increasingly chaotic world — from what is happening with the USPS to the coronavirus crisis to what is, quite possibly, the most consequential presidential election of our lifetimes. Even when the economy and the news advertising market recovers, your support will be a critical part of sustaining our resource-intensive work — and helping everyone make sense of an increasingly chaotic world. Contribute today from as little as $3.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



This year’s back-to-school shopping list might have a new entry: a laptop, needed for the upcoming months (or more) of remote learning. Not only might it be the most expensive item on your list, it may also be the hardest one to find.
As students begin the new school year, some who will be attending virtually are scrambling to find the necessary equipment to do so. Shortages of laptops and tablets have caused delays that will stretch weeks — even months — into the school year. This could force children from households that can’t afford the devices to go without, widening the already sizable education gap between the rich and the poor.
Many schools have had plenty of time to plan for the fall and order supplies accordingly, and some of them did just that. The problem is there simply aren’t enough laptops and tablets to go around, especially when it comes to low-cost Chromebooks. Chromebooks are laptops that run Chrome, a simple operating system designed by Google, and can cost less than $300. They’re used by the majority of American school systems due to their comparatively low cost and Google’s push into the rapidly growing and increasingly lucrative education space, including apps like Google Classroom. School districts from Bozeman, Montana, to Austin, Texas, have reported backlogs in orders of computers needed for their students to participate in remote learning.
Similar pandemic-related supply chain issues have plagued other products, from meat to dumbbells — but the laptop shortage has been worsening. One industry analyst who spoke to Fast Company in May predicted that the situation would right itself by June, well in time for the new school year and its increased demand. Obviously, this has not happened.
One reason, according to the Associated Press, is the Trump administration’s sanctions, issued in July, on Chinese companies that are believed to use forced labor. While the ban only applies to American companies selling products to the sanctioned Chinese companies, the New York Times said it was likely that American businesses would stop doing business with those Chinese companies entirely. In letters to educators, Lenovo blamed the sanctions for its backlog of 3 million Chromebooks. HP, on the other hand, told school systems that its shortage of 1.7 million laptops is due to pandemic-related production shortages of components made in China. (HP and Lenovo did not respond to requests for comment from Recode.)
The United States is not the only country with a laptop shortage; schools around the world have also turned to remote learning and they also need affordable computers to do so. In January 2019, Google said 30 million Chromebooks were used in schools around the world. Those schools need Chromebooks for remote learning just as much as American schools do, which only increases demand.
The unpredictable nature of the Covid-19 pandemic hasn’t helped matters, either. Some school systems, the AP said, assumed in-person learning would return in the fall, or didn’t know what the reopening plan would be until well into the summer so they didn’t order as many devices as they would ultimately need. Even schools that planned ahead, however, have seen their orders backlogged by up to six months.
Though the shortage has been going on since March, it doesn’t show any sign of letting up soon. Acer America president Gregg Prendergast told Axios that the demand was “historic,” and hundreds of thousands of orders were still pouring in.
Lower-income students have fewer options, as usual
While some school systems have been able to meet their students’ needs, the ones that won’t get enough devices in time for the new school year have had to figure out how to make education available to those who don’t have access to a computer (or, for that matter, an internet connection). Austin Independent School District in Texas even pushed the school year’s start date back to September 8 in order to have more time to acquire devices or come up with alternate education plans for kids who still didn’t have access to them.
Families that can afford to have purchased their own devices from retailers, though supplies there are also dwindling because of the increased need from students as well as from adults who are working from home more, and from their companies that suddenly have to provide mass quantities of work-from-home equipment. Off-the-shelf solutions may not be ideal. Prendergast told the Wall Street Journal that those computers might not be optimized for educational use like the models purchased by schools. 
Some parents are sharing their own devices with their kids. That’s also not ideal, both from a cybersecurity perspective and a practical one: Laptops (and software) made for adults aren’t usually kid friendly and may not stand up to the rigors of frequent handling and usage by kids. 
Meanwhile, other families are improvising solutions to mimic some of the benefits of a normal school setting. Certain parents are paying to create “pandemic pods” that will give their children in-person instruction as well as some social interaction with their fellow podlings — benefits that the children of families that can’t afford to join a pandemic pod will not receive.
Other children will have to rely on learning centers that schools have set up to provide students with access to the online learning supplies they need. While school districts have promised to follow CDC guidelines to keep those centers safe, it’s still not ideal considering that remote learning is meant to keep kids away from large groups of people or crowded spaces in the first place. And some school districts are just going analog: Austin, for instance, will have “instructional packets” available for kids who are learning remotely but have no access to the internet or devices.
Consequences of inequality will last longer than a school year
Difficulty accessing necessary school supplies due to income inequality is not new in the US, and the education gap between the rich and the poor has grown over the last several decades. That’s not just because wealthy parents can afford to send their kids to expensive private schools — it’s true for public education, too. For months, experts have warned that the pandemic and the remote learning it has forced upon students will only exacerbate that gap, partially because of a lack of remote learning supplies.
A June report from McKinsey estimated that lower-income students would lose twice as many months of learning compared to the average student assuming that all in-class instruction resumes in January 2021 — and the effects of this would likely be permanent and have lasting effects on the entire country. In an August report, the consulting firm again stressed the need for equity in schools’ pandemic plans.
“Access to devices and internet connectivity is uneven even in affluent districts in developed systems,” the report said. “Addressing that is a critical first step to ensure equity.”
If nothing else, the historic demand for Chromebooks has been good for their manufacturers. HP’s most recent earnings report, issued Thursday, said about half of the company’s revenue came from notebook computer sales — a 30 percent revenue increase for notebooks year over year. Dell’s latest earnings report, also released on Thursday, said the company saw a double-digit growth in revenue from Chromebooks. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

New goal: 25,000 
In the spring, we launched a program asking readers for financial contributions to help keep Vox free for everyone, and last week, we set a goal of reaching 20,000 contributors. Well, you helped us blow past that. Today, we are extending that goal to 25,000. Millions turn to Vox each month to understand an increasingly chaotic world — from what is happening with the USPS to the coronavirus crisis to what is, quite possibly, the most consequential presidential election of our lifetimes. Even when the economy and the news advertising market recovers, your support will be a critical part of sustaining our resource-intensive work — and helping everyone make sense of an increasingly chaotic world. Contribute today from as little as $3.




  
  
  
      




  Facebook didn’t take heed when its users sounded the alarm about a militia group issuing a “call to arms” on its platform. That call to arms came before the violence in Kenosha, Wisconsin, on Tuesday night, which left two people dead and one injured, according to a new report. A man who has been arrested as a suspect in the shooting reportedly self-identified as a militia member, though he was not a follower of the Facebook page users had flagged.
Before the shooting, at least two Facebook users flagged a page called “Kenosha Guard” for inciting violence, according to The Verge. But the company told users that the page did not meet the company’s criteria for removal. On Wednesday morning, after violence at the protests already broke out — and armed militia groups took to the streets — Facebook ended up taking down the page for violating its policies on dangerous groups.
Just last week, Facebook expanded its Dangerous Individuals and Organizations policy to include domestic militia groups that encourage violence. But in spite of those recent efforts, it seems that some of the content and groups the company has deemed dangerous under this new policy are still slipping through the cracks. 
Facebook told Recode that it may have rejected users’ requests to take down the Kenosha Guard militia page because those requests weren’t initially routed to the right team. Once Facebook’s newly formed specialized team working on identifying dangerous militia groups looked at the group, the company said it took the page and a corresponding event down.
Facebook also said the shooting suspect, 17-year-old Kyle Rittenhouse, was not a member of the Kenosha Guard Facebook page or invited to the associated event. The company says it has removed Rittenhouse’s accounts on Facebook and Instagram. 


  Do you work at Facebook and want to talk? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.




“At this time, we have not found evidence on Facebook that suggests the shooter followed the Kenosha Guard Page or that he was invited on the Event Page they organized,” said a spokesperson for Facebook. “However, the Kenosha Guard Page and their Event Page violated our new policy addressing militia organizations and have been removed on that basis.”
Regardless of Facebook’s eventual removal of the militia group, some groups are criticizing the company for not taking action sooner — saying the situation is part of a larger pattern of not responding quickly enough to calls to violence on its platform.
“This crisis of hate-fueled violence requires immediate, drastic action from Facebook and all other platforms on which these groups gather,” said Rashad Robinson, president of the civil rights group Color of Change, in a statement to Recode. “Facebook’s superficial policy changes mean nothing when they aren’t enforced.”
This week, demonstrators in Kenosha have taken to the streets to protest the police shooting of Jacob Blake, a 29-year-old Black man who witnesses at the scene said was unarmed and was simply trying to break up a dispute. Tensions have increased at these protests as armed militia who say they support the police have showed up to counter protesters.
Facebook has struggled many times in recent years with how to deal with extremists and other groups associated with violence, who often use its platform to organize and build out their groups. 
Back in 2017, Facebook was criticized for letting the white supremacist Unite the Right Rally, which resulted in three deaths and dozens of injuries, keep its event page online for a month before it was taken down the day before the event. More recently, members of the far-right Boogaloo movement have attempted to organize violent insurrections against members of the US government on Facebook, such as plotting the murder of a federal agent in Oakland, California. 
In the past few months, Facebook has expanded its policies to restrict dangerous groups even if they are not overt terrorist organizations, including domestic militias, members of the Boogaloo movement, and supporters of the conspiracy theory QAnon. 
But Facebook’s delay in taking down the Kenosha Guard page shows that enforcing its new policies will be complicated and imperfect — after all, these groups have built themselves in part by using its platform. 

New goal: 25,000 
In the spring, we launched a program asking readers for financial contributions to help keep Vox free for everyone, and last week, we set a goal of reaching 20,000 contributors. Well, you helped us blow past that. Today, we are extending that goal to 25,000. Millions turn to Vox each month to understand an increasingly chaotic world — from what is happening with the USPS to the coronavirus crisis to what is, quite possibly, the most consequential presidential election of our lifetimes. Even when the economy and the news advertising market recovers, your support will be a critical part of sustaining our resource-intensive work — and helping everyone make sense of an increasingly chaotic world. Contribute today from as little as $3.

  
  
  
      




  
  
  
    
    
      
        
  


  






      
    
    
  
  



In the earlier days of the Covid-19 pandemic, many of the country’s public health departments, still reliant on fax machines, were woefully unprepared for the massive amounts of data they needed to process. Looking for a tidy private-sector solution to a messy government problem, the Department of Health and Human Services (HHS) paid a shadowy Silicon Valley company with ties to the Trump administration to build something new. That company is called Palantir Technologies, and if you don’t know much about it, that’s by design. 
A lot of that could change in the coming months, however: Palantir is about to go public.
Palantir specializes in data-gathering and analysis, most of which it does for government agencies. It has about $1.5 billion in federal government contracts alone, including, recently, with the Space Force and the Navy. In July, with new Covid-19 case numbers breaking records daily, the HHS announced an abrupt shift in how that data would be reported: hospitals would now report their data exclusively to HHS Protect, a new platform Palantir developed that would be run by another private company called TeleTracking. This would effectively replace the Centers for Disease Control and Prevention’s (CDC) National Healthcare Safety Network, per the Trump administration’s orders to hospitals to stop reporting their information to it. HHS Protect, which was not accessible to the general public, was now the only source for this information.
“Today, the CDC still has at least a week lag in reporting hospital data,” Michael Caputo, assistant secretary of the HHS for public affairs, told the New York Times then. “America requires it in real time. The new, faster, and complete data system is what our nation needs to defeat the coronavirus.”
A month later, however, the order to bypass the CDC was reversed as public outcry along with complaints from state attorneys general, members of Congress, and health workers mounted against it. HHS Protect also unveiled a public data hub for the information, though this has been plagued by reports that the data is incomplete, inaccurate, and more delayed than the CDC’s ever was.
Palantir, the architect of this complete data system, isn’t a household name like its Palo Alto peers, but the 17-year-old company founded by Peter Thiel is one of the most valuable private companies in Silicon Valley. That anonymity is a feature, not a bug: Palantir does most of its work for the government, including national security and intelligence operations. Its recently released financial documents say the company hopes to extend that reach even further, taking advantage of the Trump administration’s push to use commercial products rather than build its own to “become the default operating system for data across the US government.”
In recent years, headlines about the company have stressed its access to everything about all of us, which privacy advocates have long criticized. Palantir’s data-mining software has been credited with killing Osama bin Laden (a claim that has never been confirmed) and blamed for tearing unauthorized immigrant families apart. 
As the notoriously secretive surveillance startup that the White House is entrusting with the nation’s coronavirus data is about to go public, some details of how the company works and its Trump administration-inspired vision are, too.
The Lord of the Rings-based solution to 9/11
Palantir was founded in 2003 by venture capitalist and Paypal co-founder Peter Thiel along with Joe Lonsdale, Stephen Cohen, Nathan Gettings, and Alex Karp, its eccentric CEO who has a law degree and a PhD in neoclassical social theory and keeps 20 identical pairs of swimming goggles in his office. The company’s name comes from J.R.R. Tolkien’s “palantíri,” which are magical orbs that let their possessors see anything happening in the world at any time. The name fits, too, as Palantir’s vision has always been to create software that can mine and analyze large and disparate data sets, putting them all in one place and finding connections between them. 
The company came together not long after 9/11, when Palantir was pitched as a tool that could have identified and stopped the hijackers and would prevent similar attacks from happening in the future. Sure enough, by 2011, Bloomberg Businessweek was calling Palantir “an indispensable tool employed by the US intelligence community in the war on terrorism.” The magazine added, “Palantir technology essentially solves the Sept. 11 intelligence problem.”
Indeed, the CIA was one of Palantir’s earliest investors through its venture capital arm, In-Q-Tel (yes, the CIA has a venture capital arm). It was Palantir’s only customer for years as the company refined and improved its technology, according to Forbes. By 2010, Palantir’s customers were mostly government agencies, though there were some private companies in the mix. Having managed to quietly work its way toward a $1 billion valuation, it was then one of the most valuable startups in Silicon Valley. By 2015, Palantir was valued at $20 billion.
“I think it’s worth keeping in mind that Palantir sees itself not alongside Uber, Twitter, and Netflix, but alongside Raytheon, Lockheed Martin, and Booz Allen,” said the Intercept’s Sam Biddle, who has covered Palantir for years. “Palantir wants to be a defense contractor, not a Silicon Valley unicorn.”
Palantir has grown into a company with roughly 2,400 employees, most of them engineers who write the software that collects data, and embedded analysts who work on site with Palantir’s customers to make sense of it. Company culture has been described as cult-like, big on T-shirts and Care Bears, and “more Google than Lockheed.” Employees are called “Palantirians.” 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Team-building at Palantir is serious business.
      
      
        C Flanigan/Getty Images
      
    
  


One of Palantir’s product demonstrations, as described in Bloomberg Businessweek’s 2011 article, presents a fictional example of the software’s capabilities: A terrorist leaves a trail of data across Florida, including one-way plane tickets, condo rentals, bank withdrawals, phone calls to Syria, and security camera footage from Walt Disney World. Taken separately, these details don’t add up to much, but Palantir’s software ties together thousands of databases across various agencies and helps clients see connections across them. In this case, actions that are innocuous on their own are much more suspicious when combined, and the CIA could identify and stop a terrorist’s plan to attack a theme park.
Again, that was a hypothetical product demonstration, but Palantir’s technology has been credited with saving its financial institution customers hundreds of millions of dollars, being used to detect Chinese spyware on the Dalai Lama’s computer, thwarting Pakistani suicide bombers, and unraveling Bernie Madoff’s Ponzi scheme. Its customers have included the CDC, police departments in America and abroad, and large corporations like JPMorgan and Home Depot. Palantir even sued the US Army in 2016 to force it to consider using its intelligence software after the Army chose to go with its own. Palantir won the suit, and then it won an $800 million contract. 
Despite its high valuation and lucrative contracts, however, Palantir’s financial documents show the company has never made a profit, with a net loss of about $580 million in 2018 and 2019 and nearly $165 million in the first half of 2020. Those numbers are trending in the right direction, though the company admitted in the filing, “[W]e expect our operating expenses to increase, and we may not become profitable in the future” and “we may not be able to sustain our revenue growth rate in the future.”
“A monstrous government snoop”
Palantir’s work, the government agencies that contract it, and the relative lack of details about the company’s inner workings mean it’s often seen as secretive, all-knowing, and even malevolent. Seven years after touting Palantir’s terrorism-fighting abilities, Bloomberg Businessweek ran a feature on the company with the headline “Palantir Knows Everything About You.” In a book with the phrase “destroying democracy” in the title, Robert Scheer called Palantir a “monstrous government snoop, mining our most intimate data.” The company’s software has been criticized for its dragnet ways, pulling in records about millions of innocent people so it can catch a few possible criminals.
“Palantir’s data-mining software is used to analyze vast amounts of personal data held by the federal government to make determinations that affect people’s lives with little to no oversight,” said Jeramie D. Scott, senior counsel for the Electronic Privacy Information Center (EPIC), which successfully sued Immigration and Customs Enforcement (ICE) to get records on its work with Palantir. “Palantir analyzes databases containing telephone numbers, email addresses, financial data, call transaction records, and social media information. ... The documents EPIC obtained showed that ICE’s Palantir-based databases could analyze call records and GPS data as well as conduct social network analysis of the information linking different individuals.”
The company suffered one of its first rounds of bad press in 2011 when a hacker discovered it was part of a proposal to Bank of America to sabotage Wikileaks. In response, Palantir issued a public apology, created a “Council of Advisors on Privacy and Civil Liberties,” and suspended — but did not fire — the engineer responsible.
The post-9/11 world that Palantir was born into in 2003 then changed considerably in 2013 when leaks from Edward Snowden revealed that the National Security Agency used the directive of protecting the country at all costs in order to mass-collect the phone records of millions of Americans, leading to widespread outcry and some reforms. Palantir denied working with the NSA on that particular project but has worked with the agency on others, according to an internal video that was leaked to BuzzFeed News.
Palantir’s work with various police departments across the country has also brought renewed scrutiny to the company, especially in light of recent protests against police brutality. Palantir’s software powers the Los Angeles Police Department’s predictive crime program, called Operation LASER, which tries to identify and target potential criminals for increased surveillance. The program ended in 2019 amid doubts that predictive policing was an effective crime deterrent, as well as criticism from civil rights organizations that it unfairly targeted minority communities. It’s hard to get exact numbers on how many police departments Palantir has contracts with, but New Orleans’s and New York’s police departments are known customers, and Palantir boasts on its website of its work with the Salt Lake City Police Department.
Palantir declined Recode’s request for comment, but the company has said its technology is built with protections for privacy and civil liberties. While the company’s software obviously collects and works with data for its clients, the company says it doesn’t collect or use any of that data for itself.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Palantir CEO Alexander Karp.
      
      
        Scott Olson/Getty Images
      
    
  


Palantir’s less-than-great public image has come with some consequences. In the past few years, nonprofits have dropped Palantir as a corporate sponsor, and students regularly protest Palantir-related campus events and recruiting sessions. In an op-ed for the Washington Post, Karp noted that a “small group” of protesters regularly assembles outside Palantir’s offices, and he’s said that his own home is the site of near-daily protests. He has a personal security guard at all times. The Investor Alliance for Human Rights criticized Palantir’s work with the government and ICE, saying it was “failing to fulfill its human rights responsibilities” and noting that its use of personal data came with “legal risks” and could be in violation of state laws.
That reputation has followed Palantir even as its technology seems to be doing some good during the Covid-19 pandemic. The company is providing its services at almost no cost to the United Kingdom’s National Health Service (NHS), but headlines focused on how much patient data the company was getting access to in order to do that work and what it would do with it. The NHS has also provided patient data to other companies, including Microsoft and Amazon.
Stateside, there’s HHS Protect — another example of Palantir’s expansion into how the government collects and manages data and whom it trusts to do it (and, it seems, whom it does not). A spokesperson for HHS told NBC News in June that ICE would not have access to HHS Protect and that all information in it was de-identified anyway. But some politicians have still expressed their concerns about if and how patients’ personal health information will be protected, and that it won’t be shared with other federal agencies. They’ve specifically cited Palantir’s work on the project as one of their issues.
“Our concerns that HHS Protect could be misused in this way are compounded by the fact that Palantir has a history of contracting with ICE, including two active awards worth over $38 million in total,” they said in a June letter to HHS Secretary Alex Azar.  
Palantir admitted in its filing that negative coverage — which it said was often inaccurate and misleading — and the resulting public perception of its services could damage its relationships with current customers, dissuade potential new customers, and upset both investors and employees.
“Palantir is complicit in the surveillance, arrest and deportation of our communities through their work with ICE,” Jacinta Gonzalez, a senior campaign director at Mijente, an advocacy hub that has railed against Palantir and its ICE contracts for years, said in a statement to Recode. “Their S-1 recognizes that these are risky contracts to take on. We’re calling on investors everywhere not to invest when the IPO happens. An investment in Palantir only serves to profit off the continued surveillance and exploitation of communities by ICE.”
Palantir’s Peter Thiel problem
Palantir is also controversial because its co-founder and board chair, Peter Thiel, is controversial. Thiel, who was one of Facebook’s first outside investors and maintains a position on its board of directors, has seen his share of criticism over the years, but the libertarian billionaire really came into the public eye in 2016 when he revealed himself as the money behind Hulk Hogan’s privacy lawsuit against Gawker (which would ultimately kill the site) and an early Trump supporter. 
As most of liberal Silicon Valley’s big names publicly came out against Trump, Thiel was one of relatively few public figures who supported his candidacy. After speaking at the Republican National Convention, he gave the Trump campaign $1.25 million, and when Trump won the election, New York magazine said he was “poised to become a national villain.” Thiel has been rewarded for his support: He was chosen to be a member of the president’s transition team; in the early days of the Trump presidency, Politico dubbed Thiel “Donald Trump’s ‘shadow president’ in Silicon Valley”; and Thiel’s chief of staff and protégé, Michael Kratsios, served as the White House’s chief technology officer from 2017 until this month, when he was named acting undersecretary for research and engineering at the Department of Defense. Thiel’s Trump support is said to have changed going into the 2020 election, however, and he hasn’t donated to Trump’s campaign since October 2018.
Due, in part, to Thiel’s Trump links, the company has faced a new round of scrutiny. Its contract with ICE caused numerous civil rights organizations to blame Palantir for helping the agency find and deport unauthorized immigrants. While other companies were ending their relationships with certain government agencies over purported ethical concerns, Palantir renewed its ICE contract in 2019 despite reported opposition even from its own employees, some of whom left the company over it. Palantir’s CEO, on the other hand, has said it’s not his company’s place to decide how its software is used.
The company appears to have doubled down on this reputation, according to statements made by Karp in a letter included with the company’s public filing documents.
“Our company was founded in Silicon Valley. But we seem to share fewer and fewer of the technology sector’s values and commitments,” Karp wrote. “We have chosen sides, and we know that our partners value our commitment. We stand by them when it is convenient, and when it is not.”
Perhaps to that end, Palantir recently moved its headquarters from its longtime home of Palo Alto to Denver, Colorado.
“For a while, it suited Palantir to paint itself as this lean and mean ‘secretive” startup,’” said Biddle, who used to work at Gawker. “Now that they’re established and have clearly weathered popular outrage over their work with ICE and a lifetime association with Peter Thiel, it’s time to cash in.”
The company that would never go public is about to go public
Karp famously and repeatedly said that he would never take his company public, believing that staying private gave Palantir an edge its public company competitors didn’t have.
“The minute companies go public, they are less competitive,” Karp said in 2014. “You need a lot of creative, wacky people that maybe Wall Street won’t understand. They might say the wrong thing all the way through an interview. … You really want your people to be focused on solving the problem.”
But Karp has seemed more amenable to the idea in the last few years. When Palantir added its first female board member in June, a public filing seemed all but certain — according to California law, public companies must have at least one female board member. Palantir filed its initial paperwork with the SEC on July 6 in a confidential filing that allowed it to avoid revealing much about its inner workings to the public. Twitter, Uber, and Spotify, among other startup giants, have done the same thing. There’s no timeline for when the company might actually go public.
Despite Palantir’s enormous valuation, the company has never made a profit and “struggled to live up to” its “hot startup image,” as the Wall Street Journal said in 2018. Bloomberg reported last year that Palantir’s valuation had plummeted to half, maybe even a quarter, of its 2015 peak, as investors wrote down the value of their holdings and the company offered discounted shares to employees to boost morale. Big corporate clients such as Coca-Cola, American Express, Hershey, Nasdaq, Home Depot, and JPMorgan have dropped the service, as has the NSA, according to BuzzFeed News. 
But 2020 has been mostly good to Palantir, if to no one else. The company pulled in $480 million in revenue in the first half of the year, putting it on track to possibly hit $1 billion by the year’s end. It has that $800 million contract with the Army and is said to be increasing its corporate customer base with its “Foundry” product, which requires significantly less time, money, and employees to set up than the company’s custom-built solutions. Meanwhile, as evidenced by its recent work with HHS, the pandemic has increased worldwide demand for its software. Investors and employees alike have been itching for a return on their investment for years, and now might be the best time to make their wishes come true.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Palantir’s former headquarters in Palo Alto. The company recently moved to Denver, Colorado.
      
      
        Smith Collection/Gado/Getty Images
      
    
  


“The market right now is crazy,” Ashu Garg, a partner at venture capital firm Foundation Capital, told Recode. “There’s a junk rally for tech stocks in the public markets, and most tech stocks are very richly valued without a lot of discrimination around quality.”
Going public will mean Palantir’s opaque business practices will have to be more transparent, and the company may not be able to simply wave off public outcry over its work as it has in the past. But experts and advocates seem to doubt much will really change on either of those fronts.
“Going public might make some additional information public, but it does not guarantee oversight or accountability,” Scott said. 
Garg doesn’t think Palantir’s work with agencies like ICE and the resulting bad publicity will be too much of a detriment in the market, given how interwoven that work is and has always been with the company’s business model — not the case for, say, the Facebooks and Ubers and Zooms of the world. 
“Palantir’s core business, and probably its most profitable business, is its government business — especially work for three-letter agencies and the Department of Defense,” Garg said. “I don’t think that’s going to change.”
What remains to be seen, then, is if Palantir’s ability to marry 21st-century Silicon Valley disruption to 20th-century defense contracting will live up to its valuation when it hits the stock market. At a time when Big Tech companies are trying to make their data collection practices more transparent and say they’ll give consumers more control over them (and are facing increased pressure from lawmakers to do so), Palantir has been able to keep much of its work with our data secret. A successful IPO will only give it more reasons and opportunities to do so.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

New goal: 25,000 
In the spring, we launched a program asking readers for financial contributions to help keep Vox free for everyone, and last week, we set a goal of reaching 20,000 contributors. Well, you helped us blow past that. Today, we are extending that goal to 25,000. Millions turn to Vox each month to understand an increasingly chaotic world — from what is happening with the USPS to the coronavirus crisis to what is, quite possibly, the most consequential presidential election of our lifetimes. Even when the economy and the news advertising market recovers, your support will be a critical part of sustaining our resource-intensive work — and helping everyone make sense of an increasingly chaotic world. Contribute today from as little as $3.
  
  
  
      







  When stay-at-home orders due to the Covid-19 pandemic forced many US brick-and-mortar retailers to close up shop late winter and into this spring, some industry observers feared the government measures would widen the existing gap between the retail industry’s haves and have-nots.
A few months later, that fear has become reality.
Amazon, Walmart, and Target all recently reported record sales or profits metrics for their second quarters ending in June or July, as government-mandated store closures gave consumers fewer options for brick-and-mortar shopping and more consumers moved their spending online. Big grocery chains as well as home-improvement giants like Home Depot and Lowe’s that were deemed essential have also fared well.
Meanwhile, large specialty retailers that don’t sell groceries but that were doing fine pre-pandemic, like Kohl’s and the parent company of TJ Maxx and HomeGoods, have suffered steep declines, in part because governments determined they were “nonessential” and forced their doors closed in big markets for extended periods of time. Meanwhile, already-struggling department store chains like Macy’s announced large layoffs and store closures earlier this year, and their upcoming earnings reports could paint an even bleaker picture.
“This is what happens when 80 percent of your competition is forced out of business,” Sucharita Kodali, a retail analyst at Forrester Research, said of Amazon, Walmart, and Target’s blowout financial results. 
A drastic shift to online ordering has certainly buoyed the results for these companies. US e-commerce sales grew 44.5 percent year over year in the second quarter, according to estimates this week from the Census Bureau of the Department of Commerce. E-commerce sales had grown just 15.6 percent on average in the prior four quarters. Amazon was already the clear No. 1 choice for online shoppers in the US before the pandemic hit, and the crisis only cemented that fact. Strong and improving e-commerce operations at Walmart and Target have also played a role in their respective successes. Walmart said its online sales nearly doubled in its second quarter, while Target’s e-commerce sales nearly tripled, the retailer said this week. At the same time, the extent of suffering at chains like TJ Maxx highlights how these stores have failed to invest heavily in e-commerce.
The Big 3 of Amazon, Walmart, and Target were also well-positioned because each offers grocery delivery and pickup services in some form. Nearly 46 million US households placed an online order for grocery delivery or store pickup in June, according to the Brick Meets Click/Mercatus Grocery Survey. 
But Walmart and Target also benefited from the fact that many local governments allowed them to remain open because they sell food and other essential goods. Those rulings also permitted them to sell nonessential items in their stores like apparel, beauty items, and home furnishings while competitors focused exclusively on those categories couldn’t sell them because they were forced shut for being “nonessential.” Even with its strong e-commerce numbers, nearly 83 percent of Target’s sales in the quarter still took place at its stores. 
Target highlighted $5 billion in “market share gains,” or new business it took away from other retailers, in the first half of 2020. That total represents more than its total market share gains for the entirety of 2019, the retailer said. While Target saw strong, unsurprising growth in food and beverages, it recorded more than 30 percent growth in the nonessential home furnishings category and more than 10 percent in apparel.
The state of Vermont ordered Walmart, Target, and other big-box businesses to stop the sale of nonessential items like clothing in late March while nonessential retailers were ordered closed. But that was an exception to how most states and cities treated these giants.
“They were absolutely unfairly advantaged by being able to stay open,” Kodali said.

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.











  
  
  
      




  You’ve been online, so you’ve probably seen them. 
You’ve definitely seen them if you’re on Twitter.
They’re punchy. Provocative. They don’t look like political ads you’ve seen on TV before. And they’re aimed directly at Donald Trump.
Like this: 


Hey @realDonaldTrump, you can’t trust your campaign, your staff, or your family. They’re all talking about you — and we know it for sure. You might be the only one who doesn’t. pic.twitter.com/BcuBMCptUP— The Lincoln Project (@ProjectLincoln) July 7, 2020



And this:


Former U.S Navy Seal Dr. Dan Barkhuff wants to know if @realDonaldTrump is a coward who can't stand up to Putin or if he's complicit.Well, Donald, which is it? pic.twitter.com/rZUsgSpDv2— The Lincoln Project (@ProjectLincoln) June 30, 2020



And this:


“He is a f&*king moron.” - Rex Tillerson, Trump’s former Secretary of State pic.twitter.com/ZbsJ9300mu— The Lincoln Project (@ProjectLincoln) August 14, 2020



They’re all made by the Lincoln Project, a political action committee led by high-profile Republicans who want to topple Trump, using firepower they’ve previously trained on Democrats. 
And over the course of the last year, the group has moved from an insidery novelty to an online sensation to one that is raising real money. It might have a meaningful effect on this fall’s election.
Emphasis on “might.” The Lincoln Project’s strategy and tactics are a moving target, so it’s hard to get a grip on what it’s really doing and whether it will work. But the group most definitely has the attention of political insiders — some think it could be a useful ally to Joe Biden and actual Democrats; others suggest that it’s not much more than a publicity stunt. Either way, the Lincoln Project has your attention, whether you realize it or not.
What will it do with it?
An audience of one
The Lincoln Project is a high-concept pitch: What if Republican political operatives who used to spend their time fighting Barack Obama turned their sights on Trump? And, along the way, maybe most of the Republican Party they helped create?
It’s a catnip narrative, with echoes of great stories you love: Darth Vader, at the very last minute, switching sides to help Luke Skywalker defeat the Emperor.
It is also a narrative that drives some Democrats nuts. They argue that the Lincoln Project is at best a sideshow, doing things other campaigns have done and are doing that will have minimal impact on the 2020 election. (More recently, the group has been credibly accused of plagiarizing memes and videos, something that’s commonplace on social media but Not A Good Look for a group that prides itself on political and digital savvy and experience.)
At worst, they mutter, it may be a project that isn’t really meant to help Democrats but to do something else. Though they don’t know what that is.
Here’s what we know about the Lincoln Project right now: It is led by several “Never Trump” men who have many years of credentials at the top tier of Republican politics. Rick Wilson, for instance, worked on presidential campaigns for George H.W. Bush; Steve Schmidt worked for John McCain; John Weaver worked for both candidates. George Conway is a conservative lawyer who helped Paula Jones pursue a sexual harassment suit against Bill Clinton; now he is best-known as a high-profile Trump critic married to Trump adviser/surrogate Kellyanne Conway.
After launching in the New York Times late last year, the campaign has steadily attracted interest from political junkies and, increasingly, mainstream media (a representative for the group has not responded to requests for comment). 

  
    Related
  

  
    The Lincoln Project, the rogue former Republicans trying to take down Trump, explained
  

And in recent months, they have started to raise real money from donors — according to federal election filings, they raised $17 million in the second quarter of this year. But they have yet to spend much money running their ads. So far, they have spent less than $8 million on ad buys, according to political ad tracker Advertising Analytics. For context: Democratic PAC American Bridge has spent $30 million on media so far; Priorities USA, another Democratic PAC, is spending $2 million per week in battleground states.
We also know their ads — often made using news footage and turned around at internet speed — are consistently popular on Twitter, where they often rack up millions of views along with commentary from frustrated Democrats who want to know why their own party can’t do the same thing.
Which gets at a significant part of the Lincoln Project’s appeal, at least among the extremely online set. Finally, the argument goes, someone is using the same tactics Trump used in the 2016 race — against Trump. You can fill in the ellipsis … if Biden wins, and the Lincoln Project gets credit for some of that, then maybe the future of political messaging and elections looks a lot like what we saw Trump harness in four years ago — except now, everyone’s doing it.
You can see it most strikingly in the group’s ad mocking Trump’s halting walk after an appearance at West Point, a direct and intentional echo of Trump’s attacks on Hillary Clinton’s supposed frailty four years ago.


Is the President of the United States physically well? pic.twitter.com/6R4GExT0KL— The Lincoln Project (@ProjectLincoln) June 13, 2020




On the other hand: Trump’s ads and comments attacking Clinton’s health didn’t exist in a vacuum — they piled onto months of conservative media talking points, echoed and amplified by Fox News, about the topic, which bubbled up easily into social and mainstream media. 
In this election, there doesn’t appear to be a version of a 4chan-to-Breitbart-to-Fox News-to-the-New York Times cycle for the left — a way to move conversation, memes, and activation from the edges of the internet all the way to the center of mainstream media, which encourages the internet to keep at it. (Despite efforts to create it.) So you can feel the longing on Twitter that maybe the Lincoln Project will do the trick.


I don’t want anyone in the Lincoln Project involved in setting policy if Biden gets into office, given their history of supporting horrendous things, but they know how to make effective ads. Why are the Democrats not making ads like this? https://t.co/wfNXiS4nPz— Matt Novak (@paleofuture) August 14, 2020



But the Twitter-centric nature of the Lincoln Project’s work — so far — is also the main critique from Democratic campaigners. They argue that getting eyeballs on viral anti-Trump content — particularly on Twitter — means you’re reaching people who are already voting against Trump.
“I think [the ads] are helpful,” says Dan Pfeiffer, a former Obama aide who is now co-host of Pod Save America. “I think they are not as helpful as a lot of people think.”
Pfeiffer’s argument, echoed by other Democrats who are working on this year’s race, is that the Lincoln Project’s most barbed ads, which tend to generate the most attention and virality, are the ones least likely to convert an undecided voter or a wavering Trump voter to move over to Biden — if they see them at all.  
“Negative ads can still work on Trump,” he says. “But they have to introduce new information to people, and they have to reach people where they are.”
Democratic operatives I’ve talked to who think the Lincoln Project is overhyped often point to Republican Voters Against Trump, another PAC with — just like the name says — the same mission statement as the Lincoln Project. But their ads, which can also generate retweets and views on Twitter, focus specifically on first-person testimonials, which they think will be more effective in moving votes:


‼️ Jeffrey voted Trump in 2016, and it's safe to say he won't be doing so again. He takes you on a ride....You gotta watch the whole thing. (Warnings: 1. NSFW 2. Wicked "Good Will Hunting" Energy) pic.twitter.com/foIo4lmaDA— Republican Voters Against Trump (@RVAT2020) July 13, 2020



This leads to another one of the major critiques you hear about the Lincoln Project: To date, at least, the project doesn’t seem interested in actually tipping a swing state, which could actually move a close election. 
Instead, the biggest chunk of their ad spending — about $2 million, per ad tracking firm Medium Buying — has been in Washington, DC, where voters have nearly no impact on the election. Another $200,000 has been spent in New York City, where Trump received 18 percent of the vote in 2016.
The Lincoln Project says, for now, that they are mostly interested in one voter: Trump. And they are trying to reach him when he watches TV, either in the White House or at his estate in Bedminster, New Jersey, 40 minutes west of Manhattan. 
In theory, Trump will see the ads, which are supposedly purposely built to upset him, during “executive time” — “to use his mental infirmity and addiction to television to freeze him and manipulate him,” as Schmidt told the Washington Post. 
More plausibly, the Lincoln Project is hoping to needle Trump by getting the people who shape the discussion about politics, in Washington and New York, talking about and responding to the ads. It doesn’t matter how it gets to him or the people around him or the people supporting him, as long as it gets there.
The logic behind that strategy: If the Lincoln Project can distract Trump by focusing on them or their ads instead of doing ... something else, it’s worth it. And they say it’s working: They point to Trump tweets mocking the group, for instance. Or the fact that Trump spent an extended stretch of his failed rally in Tulsa, Oklahoma, talking about his West Point walk, following the Lincoln Project clips. 
And now they’re claiming credit for the demotion of former Trump campaign head Brad Parscale in mid-July, pointing to reporting from New York magazine’s Olivia Nuzzi. Trump saw this micro-targeted ad suggesting that Parscale was getting rich at Trump’s expense and asked him about it, according to Nuzzi.


This is just another example that @realDonaldTrump is the worst manager America has ever seen. Don, you got conned … by your IT guy. pic.twitter.com/ssO5CERqBu— The Lincoln Project (@ProjectLincoln) May 20, 2020



Maybe that ad, which popped up in May, really did worry Trump for a second. Or maybe it planted a seed of doubt in his mind about Parscale, who got pushed out two months later.
On the other hand: Nuzzi’s story has copious details on the many other reasons Trump would can Parscale, ranging from the poisonous, backbiting vibe of the entire Trump ecosystem to the Tulsa fiasco to the fact that polls show Trump is losing the race.
Beyond that, it’s hard to argue that you need a dedicated team and millions of dollars to throw Donald Trump off balance. We’ve been watching him closely for five years, and at this point we can say, with confidence, that he’s always distracted. This is a president who follows up his tweet suggesting that this year’s election should be postponed with another one promoting a Long Island pizza place.
Some Lincoln Project critics assign darker motivations to the group’s work. “It’s cynical self-promotion,” says a Democratic operative working on this year’s campaign, who spoke on the condition of anonymity. The operative argued that Schmidt and company are trying to reach political and media elite in Washington and New York — not to influence their votes, but to burnish their reputations. “This is not an accident that they’re not talking to voters,” says another Democratic campaigner.
Other Democrats simply worry that money the Lincoln Project rounds up to defeat Trump will eventually be used for something else — maybe even for actual Republicans at some point. They argue that well-meaning donors who think they’re helping an anti-Trump group don’t realize they’re helping the people who helped create Trump, by creating a political climate that made his election possible.
We’ll have a better idea of what the Lincoln Project really wants to do in the next few months. The group’s founders, acknowledging their role in building a Republican Party they no longer identify with, have said they’re not solely concerned with Trump. 
That’s why they’ve spent money targeting a handful of Republican senators in vulnerable races, including Susan Collins in Maine and Steve Daines in Montana (you can see J.L. Cauvin, who imitates Trump for a living, trying to tie them to the president in these Lincoln Project videos). 
They’ve also said they intend to ramp up their ad buys in the coming months and spend “tens of millions of dollars” on ads aimed at both Trump and other Republicans in swing states; this week the group announced a $1 million buy targeted in Ohio and three other swing states.
And if that’s going to happen, they’ll need to raise more money, which means that some of those viral videos may serve a purpose, after all.
“If they can raise enough money against the buzz they create, they can raise enough money to run a very targeted campaign,” says Mo Elleithee, a longtime Democratic strategist who runs Georgetown University’s Institute of Politics and Public Service. “In a race like this, you can be more effective with a scalpel rather than a bazooka.”

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.

  
  
  
      




  You’ve been online, so you’ve probably seen them. 
You’ve definitely seen them if you’re on Twitter.
They’re punchy. Provocative. They don’t look like political ads you’ve seen on TV before. And they’re aimed directly at Donald Trump.
Like this: 


Hey @realDonaldTrump, you can’t trust your campaign, your staff, or your family. They’re all talking about you — and we know it for sure. You might be the only one who doesn’t. pic.twitter.com/BcuBMCptUP— The Lincoln Project (@ProjectLincoln) July 7, 2020



And this:


Former U.S Navy Seal Dr. Dan Barkhuff wants to know if @realDonaldTrump is a coward who can't stand up to Putin or if he's complicit.Well, Donald, which is it? pic.twitter.com/rZUsgSpDv2— The Lincoln Project (@ProjectLincoln) June 30, 2020



And this:


“He is a f&*king moron.” - Rex Tillerson, Trump’s former Secretary of State pic.twitter.com/ZbsJ9300mu— The Lincoln Project (@ProjectLincoln) August 14, 2020



They’re all made by the Lincoln Project, a political action committee led by high-profile Republicans who want to topple Trump, using firepower they’ve previously trained on Democrats. 
And over the course of the last year, the group has moved from an insidery novelty to an online sensation to one that is raising real money. It might have a meaningful effect on this fall’s election.
Emphasis on “might.” The Lincoln Project’s strategy and tactics are a moving target, so it’s hard to get a grip on what it’s really doing and whether it will work. But the group most definitely has the attention of political insiders — some think it could be a useful ally to Joe Biden and actual Democrats; others suggest that it’s not much more than a publicity stunt. Either way, the Lincoln Project has your attention, whether you realize it or not.
What will it do with it?
An audience of one
The Lincoln Project is a high-concept pitch: What if Republican political operatives who used to spend their time fighting Barack Obama turned their sights on Trump? And, along the way, maybe most of the Republican Party they helped create?
It’s a catnip narrative, with echoes of great stories you love: Darth Vader, at the very last minute, switching sides to help Luke Skywalker defeat the Emperor.
It is also a narrative that drives some Democrats nuts. They argue that the Lincoln Project is at best a sideshow, doing things other campaigns have done and are doing that will have minimal impact on the 2020 election. (More recently, the group has been credibly accused of plagiarizing memes and videos, something that’s commonplace on social media but Not A Good Look for a group that prides itself on political and digital savvy and experience.)
At worst, they mutter, it may be a project that isn’t really meant to help Democrats but to do something else. Though they don’t know what that is.
Here’s what we know about the Lincoln Project right now: It is led by several “Never Trump” men who have many years of credentials at the top tier of Republican politics. Rick Wilson, for instance, worked on presidential campaigns for George H.W. Bush; Steve Schmidt worked for John McCain; John Weaver worked for both candidates. George Conway is a conservative lawyer who helped Paula Jones pursue a sexual harassment suit against Bill Clinton; now he is best-known as a high-profile Trump critic married to Trump adviser/surrogate Kellyanne Conway.
After launching in the New York Times late last year, the campaign has steadily attracted interest from political junkies and, increasingly, mainstream media (a representative for the group has not responded to requests for comment). 

  
    Related
  

  
    The Lincoln Project, the rogue former Republicans trying to take down Trump, explained
  

And in recent months, they have started to raise real money from donors — according to federal election filings, they raised $17 million in the second quarter of this year. But they have yet to spend much money running their ads. So far, they have spent less than $8 million on ad buys, according to political ad tracker Advertising Analytics. For context: Democratic PAC American Bridge has spent $30 million on media so far; Priorities USA, another Democratic PAC, is spending $2 million per week in battleground states.
We also know their ads — often made using news footage and turned around at internet speed — are consistently popular on Twitter, where they often rack up millions of views along with commentary from frustrated Democrats who want to know why their own party can’t do the same thing.
Which gets at a significant part of the Lincoln Project’s appeal, at least among the extremely online set. Finally, the argument goes, someone is using the same tactics Trump used in the 2016 race — against Trump. You can fill in the ellipsis … if Biden wins, and the Lincoln Project gets credit for some of that, then maybe the future of political messaging and elections looks a lot like what we saw Trump harness in four years ago — except now, everyone’s doing it.
You can see it most strikingly in the group’s ad mocking Trump’s halting walk after an appearance at West Point, a direct and intentional echo of Trump’s attacks on Hillary Clinton’s supposed frailty four years ago.


Is the President of the United States physically well? pic.twitter.com/6R4GExT0KL— The Lincoln Project (@ProjectLincoln) June 13, 2020




On the other hand: Trump’s ads and comments attacking Clinton’s health didn’t exist in a vacuum — they piled onto months of conservative media talking points, echoed and amplified by Fox News, about the topic, which bubbled up easily into social and mainstream media. 
In this election, there doesn’t appear to be a version of a 4chan-to-Breitbart-to-Fox News-to-the-New York Times cycle for the left — a way to move conversation, memes, and activation from the edges of the internet all the way to the center of mainstream media, which encourages the internet to keep at it. (Despite efforts to create it.) So you can feel the longing on Twitter that maybe the Lincoln Project will do the trick.


I don’t want anyone in the Lincoln Project involved in setting policy if Biden gets into office, given their history of supporting horrendous things, but they know how to make effective ads. Why are the Democrats not making ads like this? https://t.co/wfNXiS4nPz— Matt Novak (@paleofuture) August 14, 2020



But the Twitter-centric nature of the Lincoln Project’s work — so far — is also the main critique from Democratic campaigners. They argue that getting eyeballs on viral anti-Trump content — particularly on Twitter — means you’re reaching people who are already voting against Trump.
“I think [the ads] are helpful,” says Dan Pfeiffer, a former Obama aide who is now co-host of Pod Save America. “I think they are not as helpful as a lot of people think.”
Pfeiffer’s argument, echoed by other Democrats who are working on this year’s race, is that the Lincoln Project’s most barbed ads, which tend to generate the most attention and virality, are the ones least likely to convert an undecided voter or a wavering Trump voter to move over to Biden — if they see them at all.  
“Negative ads can still work on Trump,” he says. “But they have to introduce new information to people, and they have to reach people where they are.”
Democratic operatives I’ve talked to who think the Lincoln Project is overhyped often point to Republican Voters Against Trump, another PAC with — just like the name says — the same mission statement as the Lincoln Project. But their ads, which can also generate retweets and views on Twitter, focus specifically on first-person testimonials, which they think will be more effective in moving votes:


‼️ Jeffrey voted Trump in 2016, and it's safe to say he won't be doing so again. He takes you on a ride....You gotta watch the whole thing. (Warnings: 1. NSFW 2. Wicked "Good Will Hunting" Energy) pic.twitter.com/foIo4lmaDA— Republican Voters Against Trump (@RVAT2020) July 13, 2020



This leads to another one of the major critiques you hear about the Lincoln Project: To date, at least, the project doesn’t seem interested in actually tipping a swing state, which could actually move a close election. 
Instead, the biggest chunk of their ad spending — about $2 million, per ad tracking firm Medium Buying — has been in Washington, DC, where voters have nearly no impact on the election. Another $200,000 has been spent in New York City, where Trump received 18 percent of the vote in 2016.
The Lincoln Project says, for now, that they are mostly interested in one voter: Trump. And they are trying to reach him when he watches TV, either in the White House or at his estate in Bedminster, New Jersey, 40 minutes west of Manhattan. 
In theory, Trump will see the ads, which are supposedly purposely built to upset him, during “executive time” — “to use his mental infirmity and addiction to television to freeze him and manipulate him,” as Schmidt told the Washington Post. 
More plausibly, the Lincoln Project is hoping to needle Trump by getting the people who shape the discussion about politics, in Washington and New York, talking about and responding to the ads. It doesn’t matter how it gets to him or the people around him or the people supporting him, as long as it gets there.
The logic behind that strategy: If the Lincoln Project can distract Trump by focusing on them or their ads instead of doing ... something else, it’s worth it. And they say it’s working: They point to Trump tweets mocking the group, for instance. Or the fact that Trump spent an extended stretch of his failed rally in Tulsa, Oklahoma, talking about his West Point walk, following the Lincoln Project clips. 
And now they’re claiming credit for the demotion of former Trump campaign head Brad Parscale in mid-July, pointing to reporting from New York magazine’s Olivia Nuzzi. Trump saw this micro-targeted ad suggesting that Parscale was getting rich at Trump’s expense and asked him about it, according to Nuzzi.


This is just another example that @realDonaldTrump is the worst manager America has ever seen. Don, you got conned … by your IT guy. pic.twitter.com/ssO5CERqBu— The Lincoln Project (@ProjectLincoln) May 20, 2020



Maybe that ad, which popped up in May, really did worry Trump for a second. Or maybe it planted a seed of doubt in his mind about Parscale, who got pushed out two months later.
On the other hand: Nuzzi’s story has copious details on the many other reasons Trump would can Parscale, ranging from the poisonous, backbiting vibe of the entire Trump ecosystem to the Tulsa fiasco to the fact that polls show Trump is losing the race.
Beyond that, it’s hard to argue that you need a dedicated team and millions of dollars to throw Donald Trump off balance. We’ve been watching him closely for five years, and at this point we can say, with confidence, that he’s always distracted. This is a president who follows up his tweet suggesting that this year’s election should be postponed with another one promoting a Long Island pizza place.
Some Lincoln Project critics assign darker motivations to the group’s work. “It’s cynical self-promotion,” says a Democratic operative working on this year’s campaign, who spoke on the condition of anonymity. The operative argued that Schmidt and company are trying to reach political and media elite in Washington and New York — not to influence their votes, but to burnish their reputations. “This is not an accident that they’re not talking to voters,” says another Democratic campaigner.
Other Democrats simply worry that money the Lincoln Project rounds up to defeat Trump will eventually be used for something else — maybe even for actual Republicans at some point. They argue that well-meaning donors who think they’re helping an anti-Trump group don’t realize they’re helping the people who helped create Trump, by creating a political climate that made his election possible.
We’ll have a better idea of what the Lincoln Project really wants to do in the next few months. The group’s founders, acknowledging their role in building a Republican Party they no longer identify with, have said they’re not solely concerned with Trump. 
That’s why they’ve spent money targeting a handful of Republican senators in vulnerable races, including Susan Collins in Maine and Steve Daines in Montana (you can see J.L. Cauvin, who imitates Trump for a living, trying to tie them to the president in these Lincoln Project videos). 
They’ve also said they intend to ramp up their ad buys in the coming months and spend “tens of millions of dollars” on ads aimed at both Trump and other Republicans in swing states; this week the group announced a $1 million buy targeted in Ohio and three other swing states.
And if that’s going to happen, they’ll need to raise more money, which means that some of those viral videos may serve a purpose, after all.
“If they can raise enough money against the buzz they create, they can raise enough money to run a very targeted campaign,” says Mo Elleithee, a longtime Democratic strategist who runs Georgetown University’s Institute of Politics and Public Service. “In a race like this, you can be more effective with a scalpel rather than a bazooka.”

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.

  
  
  
      





    
  
    
    
      
        
  


  






      
    
    
  
  



Amid a slew of very concerning stories about the state of the US Postal Service, one particularly alarming photo recently went viral. It shows piles of blue USPS collection boxes stacked up behind a fence in what sort of looks like a dump. Amid mounting concerns about the prospect of the Postal Service processing a record number of mail-in ballots in the November election, to some the image might look like evidence of voter suppression. But it’s not — not quite, anyway.
As anxiety over President Trump potentially sabotaging the mail service mounts, this photo of collection boxes represents a particularly dangerous form of misinformation, one that social media companies are still figuring out how to moderate. One tweet, which has now been retweeted nearly 80,000 times, uses the image to illustrate a bold point:


Photo taken in Wisconsin. This is happening right before our eyes. They are sabotaging USPS to sabotage vote by mail. This is massive voter suppression and part of their plan to steal the election. pic.twitter.com/QXLWGIHTrz— Thomas Kennedy (@tomaskenn) August 15, 2020



  Facebook, YouTube, Twitter, and other social media companies are scrambling to take down and fact-check rampant misinformation about topics like Covid-19 and the 2020 election that spread on their platforms.
But complicating these companies’ efforts to moderate content is the fact that a majority of Americans — on both sides of the political aisle — believe that social media companies are censoring political viewpoints, according to a new poll by the Pew Research Center. 
About three in four Americans feel it is very likely or somewhat likely that social media sites “intentionally censor political viewpoints that they find objectionable,” according to the survey. It polled around 4,700 Americans across the political spectrum. While people from both parties thought that social media companies were likely censoring content for political reasons, Republicans were much more likely than Democrats — 90 percent of Republicans compared to 59 percent of Democrats — to hold this belief.
For several years, President Trump and leading Republican lawmakers have complained, without evidence, that social media platforms are systematically censoring conservative content. These politicians and their supporters have tried to prop up their allegations by citing anecdotal examples of conservative individuals’ accounts being removed and how many people who work in Silicon Valley tend to lean liberal.  
While media and technology experts have written off these complaints as lacking any serious empirical evidence, ultimately, the new survey suggests that, to some extent, evidence doesn’t matter. The belief that social media companies are using their power to further a political agenda is now mainstream. And the public’s negative perception about these companies’ intentions threatens to delegitimize their increasing efforts to fact-check and moderate content, particularly heading into the 2020 presidential election. 
How allegations of bias went from fringe to mainstream
Allegations of social media platforms being politically biased in the US go back to a much-discussed 2016 Gizmodo story about the Facebook content moderators who managed its now-defunct Trending news section. The article cited anonymous moderators who said they deprioritized news from right-wing outlets based on individual editorial judgment. Subsequently, Facebook eliminated human reviewers from its platform. And in 2018, Facebook removed the Trending news section from the platform entirely.
In the following years, accusations of alleged anti-conservative bias focused less on trending news stories and more on individual figures like far-right conspiracy theorist Alex Jones and his site InfoWars. For years, Jones used social media to espouse violent, sometimes racist, and harmful conspiracy theories (including the false claim that the Sandy Hook shooting was a hoax), amassing millions of followers along the way.
When Jones was finally booted off of YouTube, Apple, and Facebook in 2018 for consistently violating their rules around hate speech, he and his fans launched a crusade against social media platforms. They rallied against these companies and their supposed political biases, and warned of a coming purge against other conservatives. And they weren’t alone in their claims. High-profile Republican politicians like Sen. Ted Cruz backed Jones on this, even though Cruz said he was against some of Jones’s more extreme views. And in the ensuing years, Trump and leading Republican Party members have consistently repeated the same talking points, fueled by claims not just from Jones but several other conservative social media figures like Diamond and Silk.
“We’ve entered a world where politicians are parroting in a lot of ways the headlines of media manipulators and misinformers,” Joan Donovan, director of research at Harvard University’s Shorenstein Center on Media, Politics, and Public Policy, told Recode.
Ironically, many of these extremists like Jones grew their massive follower bases on social media, which is where they also successfully perpetuated the theory that these companies are biased against them. 
“This is a house of mirrors,” said Siva Vaidhyanathan, a professor of media studies at the University of Virginia. “A vast majority of Americans are getting their sense of what Facebook and Google are doing from completely unfounded claims that are circulating on Facebook and Google.”
Since major social media platforms like Facebook and YouTube don’t share much in the way of data or insights about what kind of content people see on the platform, or why they do or don’t see certain content, it’s hard to definitively prove there is not a political censorship problem on social media.
Responding to Republican lawmakers’ concerns, Facebook commissioned an external audit in 2019 to determine the existence of alleged anti-conservative bias. But the results were “little more than a formalized catalog” of Republicans’ grievances and didn’t “include any real quantitative assessment of bias” on the platform, as social media researcher Renee DiResta wrote in Slate at the time.
The data we do have shows conservative content is actually performing quite well on Facebook. The New York Times’s Kevin Roose has routinely gathered data from the Facebook-owned analytics company CrowdTangle, showing that the Facebook posts with the highest engagement (the ones people “liked,” clicked on, and shared the most) — are mostly from right-wing and conservative sources like Ben Shapiro and Fox News. Facebook has disputed this reading of its data, saying that other internal metrics provide a fuller picture of what posts are most viewed on the platform, but it hasn’t shared more than snippets of that data publicly.
In another study, the left-leaning media watchdog organization Media Matters did a study in 2019 of over 400 popular political pages on Facebook and found that conservative pages performed about equally as well as liberal ones.
And while it’s true that a majority of rank-and-file tech employees tend to vote liberal, recent reporting alleges that Facebook leadership relaxed rules on fact-checking in an attempt to avoid accusations that the platform has an anti-conservative political bias.
The wizard behind the curtain
While many people are concerned about what kind of content Facebook, YouTube, and Twitter take down or fact-check — particularly when it comes to posts from high-profile politicians like Trump — the truth is, only a very small percentage on Facebook and Twitter is ever fact-checked or taken down. 
The vast majority of what we see on these platforms is controlled by a set of algorithms, tailored to show us content that will keep us engaged and interested in sharing that content with other people.
There’s a lot we don’t know about exactly how social media algorithms work, since they are proprietary information. The public can’t see into the algorithmic black box that determines which articles, photos, and videos we most easily find online. So while it’s easy for people to get upset about a right-wing post that gets fact-checked or taken down, it’s harder to understand the kind of invisible algorithmic amplification that may be helping surface those kinds of right-wing posts in the first place.
“The truth of how Facebook works is beyond most people’s comprehension. People imagine that there are wizards behind the curtain working the levers,” Vaidhyanathan told Recode. “And that’s not what Facebook really is — which is this self-propelling, controlling machine that is amplifying all sorts of video, texts, and images, and sorting it according to commercial need.”
One potential solution, according to Donovan: “Platform companies should be more transparent about what kind of news is circulating on their platforms, and what kind of top stories are getting the most clicks, likes, and shares. Then we could start to put some of these allegations of bias to rest.”
While the executives at Facebook and YouTube may not be wizards behind the curtain working to manipulate people’s political views, they are the companies that control how hundreds of millions of Americans get their news every day.
And right now, the stakes in the US couldn’t be higher. Physicians say that medical misinformation spread on social media is literally killing people who trust what they read online more than they trust the advice of medical experts. In the middle of this, the US is holding a presidential election during a pandemic, when people are scared to or unable to vote in person. Already, mass confusion about mail-in voting has spread on social media — promoted by Trump — which concerns some civil rights leaders, who are urging Facebook and Twitter to set the record straight by fact-checking false claims, regardless of who posts them. 
While the Pew study showed that more than 70 percent of Democrats approved of social media companies labeling politicians’ posts as misleading, only 35 percent of Republicans shared that sentiment.
Across party lines, if Americans can’t trust social media companies to moderate content on these topics, the big question is: Who will they trust instead, and what information will they end up believing? 

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.

  
  
  
      





    
  
    
    
      
        
  


  






      
    
    
  
  



Within hours of Joe Biden announcing that he had selected Kamala Harris as his presidential running mate, conspiracy theories as well as racist and sexist misinformation about her proliferated online. Much of this harassing dialogue recycled or built on earlier false claims spread about Harris. And the actions social media has already taken — or avoided — are stoking anxieties about the role of misinformation in the campaign to come.
On QAnon and other right-wing Facebook pages, memes compared Harris to Rachel Dolezal and wrongly claimed she was not African American. On Twitter, a post with 20,000 “likes” pointed out that Harris’s sister takes hydroxychloroquine, without the context that she uses the medication for lupus, not Covid-19. And countless posts questioned her eligibility to run for vice president, a problem that was exacerbated by a controversial Newsweek op-ed pushing the same baseless question. On Thursday, President Trump, who infamously promoted the racist birther conspiracy theory about President Barack Obama, amplified the claims about Harris’s eligibility.

  
    Related
  

  
    Birtherism 2.0, explained 
  

“I heard today that she doesn’t meet the requirements,” Trump told reporters, referencing the article’s author as “very highly qualified, very talented lawyer.” In response to a follow-up question on the topic, the president concluded, “I don’t know about it. I just heard about it. I’ll take a look.” 
The recent spike in misinformation highlights how quickly old conspiracy theories get recycled on social media when a new context presents itself, and how powerless platforms seem to be to stop it. At the same time, the renewed onslaught of fake news about Kamala Harris yet again highlights how Trump and his campaign are eager to use misinformation to their advantage as they find ways to attack the Biden-Harris ticket. And as the Newsweek op-ed made clear, some media outlets are willing to participate. 
“We are working in tandem with the DNC to be vigilant around monitoring the lies, misinformation, and conspiracies Trump and his allies spread, which is informing our response to combat their recycled attacks, including many laced with sexism and racism, with the truth,” Matt Hill, the deputy press secretary for Biden’s campaign, told Recode. 
There has been a significant spike in misinformation about Harris in the few days since she was made the VP candidate. According to research from the media intelligence firm Zignal Labs, there have been more than 150,000 instances of people sharing, discussing, or promoting misinformation online related to Harris in the past week. Meanwhile, the progressive research group Media Matters found that right-leaning Facebook pages they analyzed posted about Harris twice as much as left-leaning groups in the past week, and the right-leaning posts saw 50 percent more engagement than those on more liberal pages. 
Social media platforms have each taken their own approaches to combating the surge in misinformation. In response to online discussion about the false claim that Harris isn’t legally qualified to be president, Twitter included in its Trending section a link to a fact-checked post saying that Harris is indeed eligible for the position. Similarly, a search for “Kamala Harris eligible” on YouTube pops up a link to a fact-checking page saying that she is eligible. Facebook has removed some content that violates certain policies and labeled some posts flagged by its fact-checkers as false. 
But the immediate reaction to her selection signals a long road ahead for Harris and the Biden campaign when it comes to combating misinformation and disinformation. Despite the actions taken by platforms, experts are still worried that the companies won’t be quick enough to stop the spread. And as we’ve learned from mixed responses to Trump posts in the past, the president’s willingness to amplify misinformation is bound to complicate things for social media companies going forward. 
Misinformation about Harris is nothing new
Much of the current assault of misinformation harks back to earlier rounds of fake news about Sen. Kamala Harris, as well as conspiracy theories about President Barack Obama. Upon the announcement of Harris as the vice presidential pick, the fact-checking outlet Snopes was quick to point out a slew of false claims about her that its researchers had already analyzed, including the false claim that Harris is the aunt of the actor Jussie Smollett and the equally false assertion that she lied about racial integration in the Berkeley public school system she attended. 
For example, false claims that Harris is not eligible to be president were circulating on social media as early as 2017 and have only grown in prominence since then. Last year, CNN host Chris Cuomo had to apologize after he amplified the theory on Twitter. Though YouTube has a rule banning birtherism, one video from 2019 with 100,000 views that claimed Harris wasn’t eligible to be president was still available on the platform earlier this week. After Recode asked YouTube about it, the video was taken down this week for violating the site’s rules about deceptive practices.
“It’s much like birtherism with Obama, and Trump asking for his long-form birth certificate back in 2008,” said Jacquelyn Mason, a senior investigative researcher at First Draft, a nonprofit that fights online misinformation. “By saying Kamala is not an American Black person, they’re essentially saying that she has no claim to be president, and then also negating her African American heritage by saying that she’s not African American.”  
Misinformation questioning Harris’s racial identity has continued to circulate at the same time. At one point last year, a tweet promoting that racist narrative was shared and then unshared by Donald Trump Jr. And as digital researcher Ben Decker outlined in Politico last March, a meme comparing Harris to Rachel Dolezal first appeared on the infamous, now-banned r/The_Donald Reddit page and then spread throughout the internet. He pointed out how that meme is now returning.
“In a lot of ways, if you look at all these nonsensically bogus attacks against Harris since she was a presidential candidate, there’s no reason that it wouldn’t just continue,” Decker told Recode. “Suddenly, all these pro-Trump conspiracy communities are constitutional law experts, trying to claim falsely that she’s ineligible to be vice president or president.”  
There is growing concern that disinformation and racist and sexist online harassment aimed at the Democratic vice presidential candidate could spiral out of control. After Biden announced that he would pick a woman as his running mate, there was “a set of coordinated and authentic attacks on the potential vice president lists,” said Arisha Hatch from the Color of Change PAC, who added that misinformation from those attacks often amplified sexist and racist tropes.
Accordingly, lines of defense to protect Harris and her campaign are forming. Earlier this week, several progressive and pro-abortion groups released a guide for the media for navigating gendered disinformation, and the same groups are building up a “war room” meant to respond to false and sexist attacks. About 100 women lawmakers also demanded in a letter to Mark Zuckerberg and Facebook COO Sheryl Sandberg this month that the company change its algorithms, which they say amplify misogyny and “gendered disinformation.” At the time, the lead organizer of the letter, Rep. Jackie Speier, told Recode that when “Biden announces his vice presidential pick, the onslaught on Facebook and other sites is going to be horrific.” 
So far, it seems as though Speier was right.
Social media companies have a history of allowing misinformation to spread
Historically, social media companies have struggled to moderate misinformation, including conspiracy theories and attacks on public figures. How they’re treating the present assault on Harris is no different. While companies like Facebook are sometimes willing to take down posts based on violations of specific policies they have written, these platforms often prefer to promote content from fact-checkers alongside posts that include misinformation rather than simply remove the posts themselves.
Facebook, so far, is dealing with some posts about Harris by enforcing some of its existing policies, such as its ban on bullying public figures and accounts that misrepresent their identities. However, Facebook is not actually removing content about Harris simply because it’s false. Instead, it’s relying on labels from fact-checkers, which are also supposed to reduce the distribution a flagged post receives. 
But it’s unclear how effective that method is. One post with the most engagement on the platform in the day following Harris’s announcement, according to data collected by Media Matters, was from right-wing personality Candace Owens who questioned the senator’s racial identity. And while Facebook fact-checked that content, it did not remove the post. A subsequent post from Owens that objected to the fact-checking was “liked” more than 100,000 times. 
A Twitter spokesperson says the company is focusing on removing content where there’s “a call to action that could potentially cause harm,” and that it’s committed to taking enforcement action when tweets violate Twitter rules, which do allow inaccurate statements about political candidates. Past precedent has led some to worry that this approach to misinformation won’t be enough to contain the coming surge of conspiracy theories.
“Do I think that [Trump] may give a primetime address about it? Probably not,” said Angelo Carusone, president of Media Matters, when asked about Trump spreading misinformation about Harris. “Do I think that there’s an extremely high likelihood that his account — either his own personal Twitter account or the campaign accounts — amplify and distribute memes that are repeating and touching upon these narratives? Absolutely.”
Those seeking to spread such disinformation don’t necessarily need to defend the theories, Carusone added, they just need to raise questions about the topic, which is powerful enough to inject a theory into the news cycle. That seems to be a big part of what happened this week. 
But despite the actions of the platforms, a major concern is that the narrative could ultimately be shared by the president himself, and his comments on Thursday demonstrate that he’s already willing to amplify the theory. Dealing with posts from Trump is especially problematic. Facebook has been historically reluctant to fact-check claims made by the president. 
After drawing criticism for not doing enough to moderate Trump, Facebook recently took down a Trump post in which he claimed that children are “almost immune” to Covid-19. That was the first time Facebook has ever taken down a Trump post. Twitter, on the other hand, has taken a slightly more aggressive approach to the president’s posts and, earlier this summer, started applying labels to some posts that Facebook chose not to act on. 
What remains to be seen — but is already very much a worry — is the extent to which the Trump campaign will use misinformation about the Biden-Harris ticket as a campaign strategy. After all, it remains unclear how social platforms could effectively respond should the president do so even more explicitly. But, somewhat similar to false claims about mail-in voting, spreading wrong and racist claims about Harris serves to weaken trust in the democratic processes. And, importantly, the consequences could well go beyond the current upcoming election. 
“My biggest hope,” said Hatch, from Color of Change PAC, “is that the level of disinformation, the level of racist and sexist commentary that will be allowed and amplify doesn’t dissuade that next generation of leaders from taking the step to put their hat in the ring.” 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.


  
  
  
      





    
  
    
    
      
        
  


  






      
    
    
  
  




Thousands of schools nationwide will not be reopening this fall. But in Las Vegas, the private K-12 Meadows School plans to use an artificial intelligence-powered thermal screening system to keep students safe as they return to classes. 
The system will scan for signs that students have elevated temperatures — an indication they might have Covid-19 — as they enter buildings for their classes. If they’re flagged, the students will be asked to wait separately for about 10 minutes, and then get their temperature taken again. If the result is within a normal range, they’re cleared to start their day. If not, they’ll be sent home. 
“Things are strange enough. Kids are going to be coming to school with masks. They’re going to be meeting friends with masks,” Jeremy Gregersen, the head of school at Meadows, told Recode. “They’re going to be meeting their teachers for the first time in person in strange new ways, and what we want is for kids to feel welcome and to normalize their arrival at school as early as possible.” 
Supplying the technology is an artificial intelligence company called Remark Holdings. The company, which also sells facial recognition systems, has been providing a thermal scanning system  — which also takes attendance — to more than 100 schools in China for over a year and is now repurposing its tech to assist semi-public places reopening amid the pandemic. 
Remark Holdings is not the only company doing so. A slew of firms, many of which already sold surveillance products, are adjusting their technology to the pandemic. The suite of products includes everything from computer programs that can identify whether or not a student is wearing a mask to artificial intelligence that measures how well people are social distancing. Sometimes, these capabilities are sold together as a package. 
Ultimately, these companies and their clients hope that the return to class can be made a little safer. Although many schools have delayed their plans to restart in-person classes, some experts discourage putting too much faith in these tools when students do go back to school. 
“It’s important that, even perhaps before talking about the privacy concerns around some of these technologies, it’s useful and saves time to take a step back and just ask whether it works,” Amelia Vance, the director of education privacy at the Future of Privacy Forum, told Recode. 
Using thermal imaging to screen for temperatures is a good example. While companies like Remark defend their tech, not everyone who has Covid-19 gets a fever, and not every high temperature is caused by an illness. Vance also says the science behind the technology itself is somewhat “rocky.”
Overall, while there might be some value in collecting different types of data amid the pandemic, these new technologies can also cause new problems and endanger children’s privacy. Here’s what we know so far about how schools are trying to strike a balance.
Computer vision and wearables can measure social distancing 
One new surveillance tool being retrofitted for the pandemic is artificial intelligence that can measure social distancing by using software to recognize human beings in images, and then measure how close together they are. The goal is to identify “hot spots” full of traffic, which can help school leaders adjust how they set up the flow of people. 
Some of that AI is being offered by companies that already have close relationships with schools. Among them is Avigilon, a company owned by Motorola Solutions that sells a wide variety of artificial intelligence-surveillance tools. The company already has relationships with thousands of schools, as the Wall Street Journal recently reported, and has now adjusted its video analytics to measure social distancing. 

Actuate is also getting into pandemic-era school surveillance. The company normally sells artificial intelligence that mines through school security video feeds for signs of a brandished weapon — an anti-gun violence measure — but is now offering AI that measures how well people are social distancing. Ben Ziomek, Actuate’s co-founder and CTO, told Recode that 900 schools are interested in the tech for when they reopen, and another 100 are interested in the tech it built specifically for Covid-19. 
“It’s pretty easy to have AI identify when a person is in video, and then, when we look at video footage, we extract the relative positions of the people,” he told Recode. “We use those relative positions of people to calculate: ‘Are they within 6 feet of each other?’ ‘How long were they there?’ ‘How many people are there?’ We can also count people; we can show how people move through space.”
Airports and other public spaces have started using similar tech. 
Meanwhile, some companies are integrating wearables that are meant to stop infractions of social distancing rules. One school in Ohio is planning to pilot an electronic beacon that could measure how well people are social distancing, as well as aid with contact tracing efforts. A potential privacy benefit to this approach is that the surveillance systems don’t always directly involve cameras. 
There are other wearables, however, that are already causing privacy debates. For instance, at Oakland University in Michigan, students are being offered a health monitoring device called a “BioButton” that would stick to their chest during in-person classes and track vital signs. It’s meant to alert university staff if someone seems like they might have Covid-19. Originally, the device was mandatory, but following pushback from students, the school made wearing the BioButton optional. 
AI hall monitors detect when people are wearing masks
A slightly different implementation of AI-powered software can quickly determine whether people are wearing personal protective equipment. This technology is being offered by a variety of companies, including Avigilon and Remark Holdings; it’s even being used in the Paris metro system. Now, some companies want to bring the mask-spotting tech to schools. 
PreciTaste, which usually sells artificial intelligence tools to the food industry, has designed a kiosk called “AI Welcome Center.” The sensor-laden display offers a voice-activated health quiz and performs temperature checks. The device also uses AI to tell if the person at the kiosk is wearing a face covering. 
“We trained it with images of our staff wearing different assorted masks, and the algorithm can just reliably detect whether or not a masked face is present or not,” Charlie Pynnonen, an engineer at PreciTaste, told Recode. “It takes no personally identifying information.”
While the system is being tested with some restaurant clients in New York and Munich, the company is hoping to install the kiosks in schools. The idea is that the system could be deployed at the entrance to dorms or other buildings. PreciTaste says it’s had interest from some schools in the United States, including a school district in Kentucky. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        PreciTaste, which sells artificial intelligence-based tools to the food industry, is now offering its mask detection systems to schools.
      
      
        PreciTaste
      
    
  


Actuate, the company using AI to measure social distancing, is also working on similar mask-spotting tech. The idea is to detect automatically whether people are wearing masks, and track mask compliance in a given location over time without the need for a human to constantly monitor several video feeds. This mask-detecting software is distinct from new facial recognition technology that’s designed to identify people while they’re wearing masks, which is an increasingly important pandemic-era feature for some security systems. 
Covid-19 apps are cheap and easy to deploy
Amid advanced hardware like AI-powered cameras and wearables, one of the most common tools we should expect to see in the weeks to come are apps — and lots of them. 
Some companies are launching contract tracing apps specifically built for schools, as universities, including Columbia and Duke, are encouraging some students to download apps that allow students to report symptoms. In New York, schools are being encouraged to take periodical surveys about Covid-19 testing and recent travel history, a task that can be completed with an app. 
Experts warn that these new apps come with privacy concerns, and that schools and app providers often aren’t as upfront about the way data is collected by the apps as they should be. 
“We don’t know how that information is being protected by the company. We don’t know how it’s being used by the school,” said Vance, from the Future of Privacy Forum. “There hasn’t been a lot of transparency.” 
Whether all this technology works as intended is still unclear. Many schools are being encouraged to delay their reopening while others have already committed to remote learning for the foreseeable future. So while companies selling this somewhat dystopian technology stand ready and waiting, reopening many schools seems to be infeasible at the moment, and it’s hard to know just how many schools that are reopening are actually installing such devices.
But that doesn’t mean some of this tech isn’t already at work. Even in empty schools, Acutate’s Ben Ziomek says his company’s artificial intelligence is measuring whether people are entering closed campuses and buildings. Usually, it’s just kids vandalizing school property. After all, they’re very, very bored, he says. They haven’t been to school in months. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.
  
  
  
      





  Facebook is launching a tool on Thursday to help register at least 4 million Americans to vote ahead of the 2020 election. 
The company’s new Voter Information Center is a hub on both Facebook and Instagram that will give you an easy way to check if you’re registered to vote. If you’re not, Facebook will prompt you to click a link where you can register in your state. 
The Voter Information Center will include information about how to vote — including how to vote by mail, if it’s an option where you live. Facebook is also using the initiative to ask users to sign up as poll workers, who are in short supply due to Covid-19.  Facebook’s efforts are in part a response to the pandemic, during which more people than ever are expected to vote using mail-in ballots, rather than voting in person. Lately, there’s been a lot of confusion and anxiety about that process, perpetuated in part by President Trump and other politicians making false statements about the accuracy of mail-in voting on Facebook and other social media platforms.
Facebook’s new voting center is an attempt to set the facts straight about voting, and the company said that it’s using politically neutral sources like the Bipartisan Policy Center to do so. But Facebook has stopped short of actually fact-checking or removing the misleading information that President Trump has shared on its platform about mail-in voting, something it has been sharply criticized for. Last month, Facebook started adding links to voter information underneath politicians’ posts about voting and says it will apply that label to all users’ posts about voting going forward. But these links don’t correct misinformation when politicians post it.
Here’s what the new Voting Information Center will look like:
  
  
    
    
      
        
  


  






      
    
    
  
  


In addition to the voting information hub, the company says it’s releasing a “Voting Alerts” feature to let state and local election authorities send notifications to users in their jurisdiction about voting updates, including “potential late-breaking changes” to the voting process. 
For months, Democratic politicians, civil rights advocates, and some of Facebook’s own employees have been criticizing Facebook CEO Mark Zuckerberg for not taking action on Trump’s misleading posts about voting-by-mail. Trump has falsely asserted on the platform that the state of California will send mail-in ballots to “anyone living in the state, no matter who they are or how they got there” (in fact, California only sends ballots to registered voters) and that this would lead to “a Rigged Election.” While Facebook left Trump’s post up as-is, Facebook’s competitor Twitter labeled an identical post on its platform as containing “potentially misleading information.”


  Do you work at Facebook and want to talk? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.




In a previous announcement in June about the Voter Information Center, Zuckerberg framed the company’s efforts as a better way of ensuring a healthy election than policing politicians’ speech. 
Zuckerberg wrote at the time, “Ultimately, I believe the best way to hold politicians accountable is through voting, and I believe we should trust voters to make judgments for themselves.”
Today’s launch of the Voter Information Center is Zuckerberg showing he’s serious about his commitment to encourage people to vote, but it remains unclear if that will be enough to counterbalance the politicians who use the platform to spread messages that encourage the very opposite to the 190 million Facebook users in the US.

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.



  
  
  
      




  In a rare public fallout for Netflix, the creators of the platform’s highly anticipated, live-action adaptation of Avatar: The Last Airbender, the acclaimed Nickelodeon cartoon, have walked away from the project. 
Avatar: The Last Airbender’s full run became available on Netflix this past June, attracting a huge audience and reigniting the 2000s cartoon’s popularity. But in separate posts published to their respective blogs and Instagrams, Avatar franchise creators Michael Dante DiMartino and Bryan Konietzko said they were no longer involved with the previously announced Netflix remake, due to prolonged creative differences. 

  
    Related
  

  
    Avatar: The Last Airbender is one of the greatest TV shows ever made. Now it’s on Netflix.
  


  
    Related
  

  
    Legend of Korra’s messy, complicated legacy 
  

“When Netflix brought me on board to run this series alongside Mike two years ago,” Konietzko wrote in his Instagram post, “they made a very public promise to support our vision. Unfortunately, there was no follow-through on that promise. ... [T]he general handling of the project created what I felt was a negative and unsupportive environment.”
“I realized I couldn’t control the creative direction of the series, but I could control how I responded,” DiMartino added on his own website. “So, I chose to leave the project.”
Netflix responded in an emailed statement, noting that the production would continue with Nickelodeon still attached. “We have complete respect and admiration for Michael and Bryan and the story that they created in the Avatar animated series.  Although they have chosen to depart the live action project, we are confident in the creative team and their adaptation.”
Both creators described the move as “a difficult decision” but stated they’d lost confidence that Netflix would honor their vision for the show. This comes almost two years after Netflix announced the live-action reboot of the series, eliciting a huge response. Despite its brief run, which ended more than a decade ago, Avatar fans remain loyal — and that Netflix had gotten the series’ creators involved seemed to give them confidence in the project. 
That may no longer be the case. It’s rare for Netflix to have such a public breakup with the creators of one of their high-profile productions, let alone creators whose names are synonymous with the beloved, well-established franchise the project is part of. More surprising is that Netflix has been enjoying tremendous success with the Avatar community this year. The recent release of Airbender on the platform — making it more accessible to viewers than it has been in years — saw the show trending in the Netflix Top 10 list for weeks. The Netflix debut has seemingly affirmed the show’s popularity while introducing it to wide swaths of new fans.
Airbender’s follow-up, Legend of Korra, also arrives on the platform later this week. That timing makes the announcement of DiMartino and Konietzko’s departure from the live-action series all the more curious — despite Netflix offering a home for the franchise, Airbender’s future with the streaming platform is now under new scrutiny. 
It also leaves the fate of the live-action series upended in a way that’s distressingly familiar to Avatar fans. It’s certainly not the first time the franchise has found itself in a precarious position when live-action is involved.
Netflix’s highly anticipated live-action Airbender reboot was supposed to wipe away the sour taste of a notorious flop
Over its beloved three-season run, Nickelodeon’s Avatar: The Last Airbender (2005–08) became a true game-changer. Set in a fantasy world mainly based on Asian cultures, Airbender was acclaimed for its sensitive multicultural storytelling, which revolved around a team of “benders” — people who can manipulate the four elements — using their powers to stop an aggressive, militarized nation from its violent conquest of neighboring countries. In addition to carefully avoiding harmful Orientalist tropes, the show took pains to depict its various cultural allegories as distinct. And with a pre-planned, three-season story arc, it got to deeply invest in both its world-building and characters over time, allowing it to evolve organically to a deeply satisfying conclusion.
People who love Airbender really, really love Airbender. But over the years, the Peabody award-winning show has had its setbacks. The most infamous of these is Paramount’s disastrous live-action film adaptation. Directed by M. Night Shyamalan, the film had one of the most toxic, embattled, and notorious productions in recent memory. Fans staged protests lasting well over a year due to the film’s muddying of the careful cultural origins of the show’s original world-building and its casting of white actors in ethnic roles — a practice that became known as racebending. 
When it was released in June of 2010, Shyamalan’s finished product turned out to be a giant artistic embarrassment, a critically panned and high-profile flop that significantly tarnished the director’s career. It also left a stain on Airbender’s legacy, turning off newcomers and longtime fans alike. 
The fate of the film might have decisively ended any attempts to turn the animated Airbender into a live-action anything. But when Netflix announced the live-action series adaptation in 2018, many fans were curious and excited to see what a faithful adaptation of the show could accomplish — especially with the creators’ involvement, unlike the hands-off approach DiMartino and Konietzko took with Shyamalan’s film. 


A reimagined, live-action “Avatar: The Last Airbender” series is coming to Netflix! (ᴄᴏɴᴄᴇᴘᴛ ᴀʀᴛ ʙʏ Jᴏʜɴ Sᴛᴀᴜʙ) pic.twitter.com/YsMoE4UguV— Netflix Queue (@netflixqueue) September 18, 2018



The news has dismayed fans, many of whom are now questioning whether they even want another attempt at a live-action interpretation of the show without the creators at the helm. After all, the movie already showed us that Konietzko and DiMartino’s original vision — an anime-influenced story with sophisticated themes and slow, satisfying character arcs — could be jettisoned completely if the project fell into the wrong hands. Capturing the spirit of Airbender appears to require a sensitive touch; from the sounds of the creators’ posts, it doesn’t sound like Netflix quite has that. 


Well, this is terrible news for all my fellow Avatar: The Last Airbender fans… https://t.co/Jsm8HxPn1G— Eric Goldman (@TheEricGoldman) August 12, 2020





I try not to have extreme fan-girl takes on things but I can tell you I am now ZERO percent interested in whatever live-action AVATAR nonsense it is Netflix is trying to make. https://t.co/uLFxAUC302— Joanna Robinson (@jowrotethis) August 12, 2020



Fans have been eager for the show to overcome the ignominy of the terrible movie, and 2020 has indeed felt like a major revival for the franchise. The Legend of Korra is a much more divisive follow-up, but it, too, is poised for a cultural resurgence once it debuts on Netflix on August 14. With Airbender back in the spotlight, the timing was right for a reappraisal of Korra — but now Korra’s Netflix debut may be overshadowed by its creators’ own exit from the live-action series.
Still, DiMartino and Konietzko stressed that this isn’t the end of the franchise or their involvement in it. And the Netflix live-action adaptation still seems to be happening. And as strange as it is for Netflix to be parting ways with such high-profile creators, it’s not like Netflix doesn’t have a proven track record when it comes to reimagining popular franchises from other media — just look at its wildly popular live-action adaptation of The Witcher, or the recent reboot of The Babysitter’s Club. Plus, whenever a deeply beloved series is adapted, remade, or rebooted, it’s bound to put off at least some hardcore fans, regardless of quality.
“And who knows? Netflix’s live-action adaptation of Avatar has the potential to be good. It might turn out to be a show many of you end up enjoying,” DiMartino wrote. “But what I can be certain about is that whatever version ends up on-screen, it will not be what Bryan and I had envisioned or intended to make.”

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.
  
  
  
      






  Facebook will start banning posts that contain blackface or that promote anti-Semitic conspiracy theories that Jewish people are running the world.
The social media giant announced the expansion of its hate speech policies in a press call on Tuesday morning. Under the new policy, Facebook will no longer allow visual or written posts that depict “caricatures of black people in the form of blackface” or “Jewish people running the world or controlling major institutions such as media networks, the economy or the government.”
These depictions represent examples of “implicit speech” that “has historically been used to disparage, intimidate, or exclude people based on protected characteristics like race or religion,” said Facebook VP of content policy Monika Bickert on the call. Bickert also said the company has been working on revising this policy for about nine months and consulted 60 outside organizations and experts on the matter. Facebook said it will start enforcing its ban on Jewish stereotypes immediately, and the ban on blackface will begin later this month.
While unlikely to eliminate long-standing concerns about the prevalence of hate speech on Facebook’s platform, the move is a notable expansion of the company’s policies restricting it. It is also the kind of restriction that not only covers overt racial slurs but also thinly veiled or disguised racism, which some civil rights groups have long called for Facebook to ban. When Anti-Defamation League CEO Jonathan Greenblatt spoke with Recode in July, he cited a popular Facebook group promoting baseless conspiracy theories connecting global political conspiracies to a prominent Jewish family as the kind of anti-Semitic content that thrives on the platform. 
ADL and other leading civil rights organizations like Color of Change and the NAACP have long been calling for Facebook to do more to stop the spread of hate speech on its platform. The organizations led a month-long boycott of Facebook in July, gaining the support of over 1,000 advertisers and increasing pressure on the company to expand its efforts against hate speech. 


  Do you work at Facebook and want to talk? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.




“This is a welcome yet overdue step from Facebook. It’s distressing that it took this long for the platform to crack down on these particular forms of hate, when it’s quite obvious they should not have been allowed to proliferate in the first place,” said a spokesperson for the ADL in a statement to Recode. “It’s equally as disturbing that Facebook still doesn’t view Holocaust denial as a violation of their terms of service.”
As Facebook moves to expand its hate speech policies, the platform is still rife with misinformation and sometimes racist content. Recent reports from NBC News and the Guardian show that Facebook groups promoting the QAnon conspiracy, a far-right conspiracy theory about an alleged “deep state” in the US government plotting against Donald Trump, have millions of members and are growing at a rapid pace. QAnon has been found to have anti-Semitic elements, and some of its supporters have committed real-world physical violence in an attempt to uncover the alleged secret plot. 
Facebook has long espoused an ethos of trying to moderate speech as little as possible. CEO Mark Zuckerberg has repeatedly said he doesn’t want to be an “arbiter of truth” on political issues. In 2018, Zuckerberg told Recode co-founder Kara Swisher that people should be allowed to deny the Holocaust on Facebook, even though, as someone who is Jewish, he finds those ideas “deeply offensive.”
But the line can be blurry between what’s a political opinion and what’s a perpetuation of a racist conspiracy theory. It will be up to Facebook to decide how it’s going to enforce the new stricter rules.

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.


  
  
  
      




  If you’re on Twitter — and especially if you’re a woman on Twitter — you’re probably well aware of the “reply guy” phenomenon. 
It goes something like this: You post something on Twitter — a joke, a fact, a personal story — and you get a bunch of unhelpful, distracting, and sometimes hateful replies alongside the thoughtful responses to your original tweet.
Twitter is rolling out a new feature today that it has been testing since May to help people shut out reply guys and other unwanted interactions on the platform. Now, when you tweet, you can limit who can reply to either the people you follow or the people you mention (by tagging their handle with the @ tag) in the tweet. People who can’t reply can still view, share, and “like” the tweets you limit replies on. 
Here’s what it looks like:
  
  
    
    
      
        
  


  






      
    
    
  
  


As for those who feel left out of the conversation — reply guy or otherwise — they can still quote and retweet these limited-reply tweets.
Limiting replies is an important feature because it’s designed to help some people who may otherwise feel bombarded with harassment on Twitter be more comfortable posting on the platform. According to Twitter’s internal research, people who submitted abuse reports are three times more likely to use these settings. 
The reply-limiting feature is also useful for people who are trying to have a focused conversation in a small group, like a virtual panel discussion or a 1:1 interview. Take, for example, the interview that Recode co-founder Kara Swisher had with Twitter CEO Jack Dorsey via Twitter back in February. The Q&A between Dorsey and Swisher was nearly impossible to follow because they were both bombarded with distracting replies that cluttered the discussion. (Dorsey joked about the event when Twitter first announced the reply-limiting feature in May.)
Since Twitter has been testing this feature, the company says it’s seen some solid progress, as detailed in a blog post by Twitter director of product management Suzanne Xie. Xie wrote that people who tested the feature are seeing positive results: They said they feel more protected from spam and abuse and are more comfortable sharing their thoughts freely as a result. 
“These settings help some people feel safer and could lead to more meaningful conversations, while still allowing people to see different points of view,” wrote Xie.
And, mercifully, Twitter says that “reply guys” aren’t blasting people’s DMs instead — for now. It’s too soon to tell how this feature will be received now that it’s being rolled out on a wider scale. But the hope is it will make the platform a little less awful for some people.

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.

  
  
  
      




  Last week, President Trump seriously escalated his threats toward the social media app TikTok, which he has accused of posing a threat to national security. If the Chinese-owned app doesn’t sell to an American company in 45 days, it will effectively be banned in the US.
You may be wondering how an app that’s best known as a place where teenagers post viral lip-syncing videos poses a national security threat. That largely comes down to the fact that TikTok is owned by a Chinese company, ByteDance. The US government worries the app could be used not only to surveil US users but to censor political speech and spread misinformation that could hurt democracy in the US.
Many of TikTok’s users and creators haven’t been deterred by government warnings. Take Laura Lee Watts, who posts skin care and makeup reviews on the app and has about 2 million followers. What she’s worried about is losing access to TikTok. 
“As a civilian, I’m not concerned about it all,” Watts told Recode. “Even if the Chinese government had my information, what are they going to do with it?”
While Watts’s data might not expose anything sensitive, she’s just one of the app’s 100 million US users. Several cybersecurity experts told Recode that the app could pose a risk — if indeed the Chinese government forced TikTok to share data. Beijing has been accused of employing hackers to uncover all kinds of intellectually sensitive information in the US and other countries, from Covid-19 vaccine research to defense secrets. So it’s not a complete stretch to consider how certain TikTok users could be exploited — say, a defense contractor who uses TikTok for fun but whose phone could have other hackable, sensitive data on it.
“There are reasonable concerns on the security side,” Adam Segal, a cybersecurity expert at the Council on Foreign Relations, told Recode. “But the issue is, how do you address them, and what precedent are you setting?” 
Some people have speculated that the president is targeting TikTok to retaliate against the app’s users that recently pranked Trump’s June campaign rally in Tulsa, Oklahoma, by reportedly registering thousands of tickets that they didn’t end up using. But TikTok isn’t the only Chinese-owned company to become a Trump target. In the recent past, he has halted Chinese development of 5G networks in the US, and he’s banned a Chinese company from buying the dating app Grindr. And last week, he issued an executive order that threatens to ban the popular messaging app WeChat, owned by the Chinese mega-company Tencent. Unlike TikTok, there’s no plan for WeChat to sell to a US bidder, making it a potentially more impactful part of Trump’s crackdown. 
Viewed together, Trump’s threats to ban TikTok and WeChat are part of his administration’s  broader strategy of being tougher on China. 
There are two related issues driving the conflict. The first is the US government’s concern that the Chinese government could force companies like TikTok’s ByteDance to surveil Americans. This is a worry shared by Republicans as well as some leading Democrats, like Sen. Richard Blumenthal. The second issue is the Trump administration’s perception that China is trying to take over the global technology industry, which has long been dominated by American powerhouses. For years, the Chinese government has banned major US tech companies like Facebook and Google from doing business in the country, and now the US is starting to reciprocate by banning Chinese apps.
“Tech is one of the most important battlegrounds for the China-US cold war because wrapped up in tech is the conversation of economic competitive strength and values,” said Segal.
There’s a lot that we don’t know about what risks Chinese-owned apps like TikTok pose to US citizens, since much of this information is considered classified American intelligence. But whether the risks are small or significant, the recent debates over what to do with TikTok and WeChat are part of what some are calling a new cold war between China and the US, with the US positioning itself as the moral leader upholding an internet that adheres to values of free speech, in contrast to the Chinese Communist Party, which regularly enforces strict censorship online.


  Do you work at TikTok and have thoughts about what’s going on? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.



What we know — and don’t know — about national security concerns
Trump has accused ByteDance and other Chinese tech companies like WeChat of posing serious threats to US national security.
The concern is that TikTok could funnel American users’ personal data to the Chinese Communist Party, “potentially allowing China to track the locations of Federal employees and contractors, build dossiers of personal information for blackmail, and conduct corporate espionage,” according to Trump’s recent executive order. The order makes it illegal for any person or company in the US to do business with TikTok after September 20. If TikTok sells to a US company before then, the ban will no longer apply.
So what’s actually going on? It’s true that TikTok automatically collects reams of user data, including location and internet address, searching history within the app, and type of device being used, according to its privacy policy. But many other popular social media apps do this, too. (TikTok has said that it collects less data than its competitors, like Facebook and Google, because it doesn’t track user activity across devices, which both companies do.)
Last month, a report found that TikTok was accessing users’ clipboard data and saving what people copy and paste. TikTok said this was an anti-spam measure and that it’s now stopped the practice. But TikTok wasn’t the only app found accessing clipboard data; several other major apps, from ABC News to HotelTonight, were found to be accessing people’s clipboard data as well. 
TikTok also sidestepped a privacy safeguard in Google’s Android operating system to secretly track users’ “MAC addresses,” which are unique identifiers tied to people’s phones, according to a recent Wall Street Journal report. TikTok seems to have stopped tracking these identifiers in November, the Journal reported.
But aside from the specifics of what TikTok does and doesn’t track, politicians like Trump are worried that, ultimately, TikTok is beholden to the Chinese government. And the Chinese government has broad authority, significantly more so than the US government does, to snoop on users’ data as it pleases.
TikTok has repeatedly denied that it has or ever would give up user data to the Chinese government. The company says it stores American user data on servers in the US and Singapore, which ostensibly would make it harder for the Chinese government to tap into. The company has also taken measures to separate its US business overall from its Chinese parent company. For example, TikTok doesn’t operate in China (the Chinese version of it, Douyin, does).
The CIA reportedly investigated TikTok’s security threat and found no proof that Chinese intelligence authorities have been snooping on Americans through TikTok, according to the New York Times. The agency’s assessment still found that Chinese authorities could potentially tap into Americans’ data through the app, according to the Times’s summary of the classified report. That’s why last December, the Department of Defense cautioned military personnel to delete TikTok from their smartphones over security concerns. And the Senate voted unanimously to ban federal employees from using TikTok on government devices last week. 
“There’s no publicly available evidence that TikTok has ever done anything wrong,” said Segal, “but the concern is that because the Chinese National Intelligence Law of 2017 says any Chinese company can be drafted into espionage, a company could be forced to hand over the data.”
TikTok’s efforts to separate its US business from its parent company’s Chinese operations are not enough to placate the growing intensity of anti-China hawks in Trump’s administration. And there doesn’t seem to be much TikTok can do — other than sell to a US company like Microsoft, which is the frontrunner out of a few major US companies that are reportedly in talks to buy TikTok’s US operations.
A second area of concern is that apps like TikTok and WeChat censor content that the Chinese Communist Party disapproves of. On this front, there are more documented concerns, especially about WeChat. 
WeChat has been found to intercept and censor political messages sent by Chinese users to US users. A report in May by Canadian researchers CitizenLab found that the app was blocking certain messages, including a political cartoon depicting the late Nobel laureate Liu Xiaobo, who was critical of the Chinese government. The report also found that WeChat was analyzing messages sent by international users, including those in the US, to scan for and block politically sensitive content before it could circulate among Chinese users.
With TikTok, there have been accusations — without definitive proof — of censorship at the behest of the Chinese government. Last year, internal company documents showed TikTok was instructing its staff to moderate content in line with the Chinese government’s censorship of topics like the Tiananmen Square massacre and Free Tibet, according to leaked guidelines published by the Guardian. But these guidelines were part of broad rules against controversial discussions on international politics across countries, so there’s no explicit proof that this was a directive from the Chinese government to TikTok. Another oft-cited concern about potential political censorship on TikTok is that during last year’s Hong Kong independence protests, there weren’t a lot of results for popular hashtags of the protest movement. But there’s no proof that the company was actively censoring content or whether people just weren’t posting about it.
The US’s escalating trade war with China over tech
It’s important to put all of this in context. TikTok and WeChat’s political troubles in the US don’t exist in a vacuum, but rather inside a larger web of complex China-US politics. Since 2018, Trump has waged a trade war with China over free trade policies that he feels disadvantage US manufacturing. And increasingly, tech has become tangled up in this war, involving Chinese-owned dating apps, drone companies, and telecom hardware makers.
“There is no bottom to the US-China relationship right now; it keeps getting worse and worse,” Segal told Recode. “The administration is looking for more and more ways to contain, hurt, and damage China.” 
And technology, which has helped dramatically strengthen the Chinese economy in the past few decades, is seen as one of the most important areas of competition. 
Last August, as China and the US were escalating tit-for-tat tariff increases on imported goods from each country, Trump issued an executive order aimed at the Chinese telecommunications giant Huawei over concerns that the company was a cybersecurity threat. Trump gave Huawei a partial death penalty in the US by putting it on an “entity list” barred from doing business with US companies.
Huawei is a big deal outside of the US. It sold 250 million phones last year — that’s more than Apple. So the Trump administration’s effective ban had ripple effects. Google had to stop running its Android operating system on Huawei phones and killed its plans to build a smart speaker with the company. The US government’s restrictions also rolled back Huawei’s plan to manufacture equipment to build out a massive 5G internet network in the US, which the Trump administration worried the company could use to intercept data on behalf of the Chinese government. The US has since offered a reprieve to US companies, allowing them to work with Huawei through temporary licenses on setting 5G standards. 
Even Chinese-owned dating apps have attracted the US government’s attention. Last April, the US government undid a deal that had sold the popular dating app Grindr to Chinese owners, citing national security concerns. The decision came from a little-known government agency, the Committee on Foreign Investments in the US (CFIUS), which reviews the national security risks of major transactions involving foreign corporations. CFIUS has similarly been reviewing the 2017 merger that led to TikTok’s creation, when ByteDance acquired the user base of the lip-syncing app Musical.ly and rebranded it as TikTok.
Some of the Trump administration’s targets seem to pose a more obvious security threat: roaming drones that could be tapping video feeds and surveilling US turf. The Trump administration is reportedly considering issuing an executive order banning the Chinese drone manufacturer DJI, the most popular drone maker in the world, whose equipment is commonly used for military and rescue purposes. The US Department of the Interior has already grounded at least 800 DJI drones out of fear that the Chinese government could exploit them to spy on Americans. Last month, researchers found major flaws in DJI’s security features, which collected “large amounts of personal information that could be exploited by the Beijing government,” according to the New York Times.
The consequences of these mounting tensions over Chinese-owned tech could have a number of side effects. An obvious possibility is that China could retaliate. The US’s actions could also give other countries precedent to start cutting off their app markets from US companies — for example, a European country could, citing privacy concerns, bar its citizens from accessing Facebook. Either would be bad for the US economy in the long run, said Bobby Chesney, a professor at the University of Texas who specializes in national security law.
But, Chesney stressed, the US isn’t making the first move here. American companies have long been banned in China, where companies that started off by building copycats of major US tech apps — Baidu is China’s answer to Google, Didi its Uber, Weibo its Twitter — have grown into tech powerhouses. US social media companies have tried, unsuccessfully, to enter the Chinese market. 
“Good luck running Twitter in China,” said Chesney. “The playing field is very much not level in the other direction.” 
What’s next
Trump has given both TikTok and WeChat a September 20 deadline before his executive orders will be enforced. If TikTok and WeChat don’t follow these orders by then, their business operations could be fined $300,000 per violation, and “willful offenders” could face criminal charges. TikTok is reportedly planning to sue the administration over the legality of the order.
If TikTok sells its US operations to an American company in a manner that’s approved by the Trump administration, it would steer clear of further regulation. The process of unwinding TikTok from its Chinese owner could be a messy process and take up to a year, according to a report from Reuters, but it would leave TikTok’s valuable US user base intact.
For WeChat, there’s no such known escape (for now) from the regulatory crackdown because there are no publicly known potential US buyers. That could mean that some 19 million Americans who use the app will be cut off from it by the end of next month. Many US WeChat users use the app to communicate with family overseas in China, where many other communication apps like Skype and WhatsApp are blocked.
But regardless of what happens with WeChat and TikTok, the Trump-China tech war will likely continue. According to policy analysts, it’s hard to see a world in which Trump backs down from these escalating restrictions on the Chinese tech sector in the US. And even if Joe Biden wins the presidency, the Democratic candidate has still taken a notably tougher stance on China than in his earlier days in the Obama administration.
The Trump administration is picking new targets beyond TikTok and WeChat — and the videoconferencing app Zoom, which has become nearly ubiquitous during the coronavirus pandemic, could be next. Though Zoom is an American company, it has faced criticism for routing some of its US calls through Chinese servers (Zoom said this was a mistake and is no longer routing free video calls in China). Politicians including House Speaker Nancy Pelosi, who referred to the company as a “Chinese Entity,” have warned of its security risks. 
Several analysts told Recode that some of the concern about TikTok and other Chinese technology companies is valid. But the way the TikTok order in particular has been executed — with Trump going back and forth on whether he’d approve a TikTok-Microsoft sale, and at one point demanding a cut of the deal — has been haphazard and has given the global business community a sense of distrust toward the US government.
The uncertainty has also impacted TikTok creators like Watts.
“For these kids, it’s their whole life,” said Watts of creators on TikTok. In recent weeks, she has put out videos attempting to calm her fans and fellow creators, who worry the app could be shut down overnight. She’s hopeful that Microsoft will reach a deal to buy TikTok. She said she understands the Trump administration’s concern that TikTok could be used as a Chinese spy app — but she isn’t convinced.
“It’s not that I disagree, but I think there’s a presumption of guilt without any proof,” said Watts. 

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.


  
  
  
      




  The ultimate fate of TikTok could be shaped in part by the efforts of one billionaire tech investor who has earned the distinction of being a particularly rare type of Silicon Valley unicorn: a major donor to Donald Trump. 
ByteDance, the Chinese parent company of the hit video sharing app, is in a boxing match with the Trump administration, which has threatened to ban it from the United States next month. But TikTok has an investor who looms large in the drama because of his personal ties and access to the White House.
Doug Leone, one of the leaders of one of Silicon Valley’s most celebrated firms, Sequoia Capital, and his wife together have given about $400,000 over the past two years to Trump’s campaign and affiliated groups such as the Republican National Committee and a pro-Trump super PAC. That makes the Leones two of the very biggest donors to Trump in Silicon Valley, where business leaders have almost entirely run away from publicly backing the president.
Now the links between Leone and Trump may shape the ending of one of the most complex business and geopolitical stories of 2020. Leone has told “people he could use his influence with Trump to help the company,” the Washington Post reported this weekend. Leone planned to reach out to Steven Mnuchin and Jared Kushner, two top Trump aides who have both been integral to Trump fundraising efforts, “to see what it would take to save TikTok,” the Wall Street Journal added.
The stories show how cultivating ties with the White House can pay dividends, even if the donations were not born from a premeditated lobbying push.
Leone declined to comment. Trump’s campaign didn’t return a request for comment.
TikTok and Microsoft — the company that is seeking to purchase it and circumvent a threatened ban of the app — both have built-out Washington operations with professionals that twist arms for a living. After all, Microsoft CEO Satya Nadella spoke with Trump directly about the potential deal. But one hallmark of Trump’s Washington has been that convincing the administration to take a certain action can be a highly irregular process, driven by personal relationships and access to Trump advisers, both formal and informal. Plus, Trump has displayed a well-documented obsession with whether someone has shown loyalty to his “team” or not.
Leone, a longtime Republican, reportedly has that access. And while Trump might not know Leone well enough to pick him out of a lineup, suffice it to say that not every Silicon Valley venture capitalist can jawbone the secretary of the Treasury or the president’s son-in-law to help a portfolio company. 
It’s still unclear whether Leone’s outreach has actually accomplished anything. And there’s no evidence that Leone only gets an audience because of a few hundred thousand bucks in donations. To be sure, Sequoia’s global managing partner is a prominent business leader in his own right. Precisely why someone gets a call returned — Is it the donations? His stature? A little bit of both? — is impossible to know.
Leone is an exception in tech. The liberal tech industry has been fighting with Trump since his first day on issues like immigration. In fact, Leone’s co-leader at Sequoia for a long time, Michael Moritz, this cycle has given millions to boost Democrats. Silicon Valley has made supporting Trump something of a scarlet letter, so much so that even the few leaders who do back him now feel pressure to do so quietly. That’s what makes Leone’s public donations unusual, especially in the image-conscious world of venture capital.
When Recode wrote last year about the Leones’ gifts to Trump — then measuring only about $100,000 — some leading voices in tech spoke to the liability that the donations could pose to Sequoia, even though it is generally regarded as the highest-performing investing firm in Silicon Valley.
Swati Mylavarapu, a former venture capitalist who is now a top Silicon Valley Democratic fundraiser, put it this way on Sunday:


   Hi every entrepreneur who wants to raise money from Sequoia. You get to make a choice. Do your investors reflect your and your company's values? https://t.co/0POQLyx6oV— Swati Mylavarapu (@Swatipedia) August 10, 2020



That liability remains very real. But what the news over the weekend makes clear is that there is an upside — less obvious, to be sure — that complements it. One of Sequoia’s most lucrative investments is in ByteDance, with Sequoia’s stake said to be worth as much as $15 billion. And while the gifts from Leone have been made in a personal capacity and officially have nothing to do with Sequoia, his firm could benefit from the personal donations if they help maintain the value of TikTok.
The few other Silicon Valley marquee names who have supported Trump have reaped some rewards. Peter Thiel, who braved significant blowback to give over $1 million to boost Trump during his 2016 run, has stocked the administration with his allies and former aides. Larry Ellison, who hosted a fundraiser that brought $7 million to Trump’s coffers earlier this year, has pressed Trump to publicly push hydroxychloroquine, an experimental drug treatment for the coronavirus that Ellison favors.
Leone is a much lower-profile billionaire than those two titans. Some Trump fundraisers outside of Silicon Valley say they haven’t even heard of him. 
But with this much money on the line — and this much international intrigue at play — having him on TikTok’s side can only help.

Will you become our 20,000th supporter? When the economy took a downturn in the spring and we started asking readers for financial contributions, we weren’t sure how it would go. Today, we’re humbled to say that nearly 20,000 people have chipped in. The reason is both lovely and surprising: Readers told us that they contribute both because they value explanation and because they value that other people can access it, too. We have always believed that explanatory journalism is vital for a functioning democracy. That’s never been more important than today, during a public health crisis, racial justice protests, a recession, and a presidential election. But our distinctive explanatory journalism is expensive, and advertising alone won’t let us keep creating it at the quality and volume this moment requires. Your financial contribution will not constitute a donation, but it will help keep Vox free for all. Contribute today from as little as $3.


  
  
  
      




  A lot of people are paying attention to every utterance from Bill Gates, America’s billionaire-cum-conscience in the coronavirus era, about the United States’s response to the pandemic. But much less attention is paid to what Gates is actually pouring his fortune into: steeling the rest of the world for the coronavirus vaccine they will need next year.
Gates on Friday said he and his foundation would spend $150 million to distribute vaccines, if they are found, to some of the world’s poorest people. It’s one of the largest financial commitments for coronavirus response to date from Gates, the world’s second-richest person. The Gates Foundation is handing the money to the Serum Institute, the largest manufacturer of vaccines globally by volume, to produce 100 million doses that would cost at most just $3 each.
Gates has been among the foremost leaders on vaccine production over the last two decades, spending $4 billion on the global vaccine development effort known as Gavi. And for months, the billionaire has expressed profound worries that while rich countries may fare okay at surviving the coronavirus, the pandemic will devastate poor countries that can’t afford to administer the treatment, whenever it arrives.
“We’re trying to make sure we can end it not just in the rich countries,” Gates said in an interview with Bloomberg this week, stressing that he is focusing on the vaccines in development that would be affordable in the developing world, such as those being pursued by the pharmaceutical companies AstraZeneca and Novavax. “Those are the ones most scalable and low-cost.”
What Gates is essentially doing is deploying his bank account to set a “ceiling price” for the vaccines being developed by those two companies. Serum is preparing to manufacture those vaccines for India, where Serum is based, and up to 91 other poor or middle-income countries.
The Gates Foundation has now pledged about $500 million in total to respond to the pandemic, though the $150 million announced on Friday is technically an interest-free forgivable loan. Most of that $500 million is focused on this nuts-and-bolts work of vaccine expansion.
Part of the foundation’s plan to inoculate the world does depend on which vaccine ends up proving to be the most successful at protecting us from the disease. Twenty-eight different possible vaccines have progressed to human trials, each of which has different manufacturing costs and requires different materials and precision. Some of the leading vaccine candidates, such as the ones being pursued by Moderna and Pfizer, are likely to be more expensive to produce because they are RNA vaccines that are fundamentally costlier.
“Because of the way you manufacture them, and the difficulty of scaling up, they are more likely — if they are helpful — to help in the rich countries. They won’t be the low-cost, scalable solution for the world at large,” Gates said in an interview with WIRED.
Then there is the profit margin. The vaccine industry has pledged to keep its own profits from Covid-19 vaccines low, although not necessarily to sell the doses at cost.
So Gates’s impact depends on what happens in the research labs. If the cheaper vaccines — like the ones being manufactured by Serum — are the ones that prove strongest, then it will be easier and cheaper to protect people in poor countries. Gates is already working with other vaccine researchers — like Johnson & Johnson, which is also pursuing a low-cost vaccine — to secure doses for the developing world. 
Working in Gates’s favor is that his foundation has a lot of experience in distributing cheap vaccines around the globe. Gates has put more than $4 billion over the last two decades into Gavi, which is estimated by the Gates Foundation to have immunized 750 million children and saved 13 million lives.
Prior to Friday’s announcement, Gates had committed $100 million to Gavi specifically to buy and deliver a Covid-19 vaccine. In June, Gates had promised to send $1.6 billion to Gavi over the next five years for its broader vaccine work.
So while Gates can’t control how much the vaccine costs, he does bring enormous credibility and a track record of putting his billions to work to distribute it as cheaply as possible, and via a tested supply chain.
The other thing that Gates can control is his voice. And while he has been a heavy presence on the interview circuit since the very beginning of the pandemic in the US in March, he has renewed his efforts over the last few days, popping up seemingly everywhere.
His message? The American government needs to not just think about Americans. So Gates is pressuring lawmakers in the upcoming relief bill to commit more money to Gavi and not succumb to “vaccine nationalism.”
“I’ve talked to Pence, I’ve talked to Mnuchin, Pompeo — particularly on the issue of, ‘Is the US showing up in terms of providing money to procure the vaccine for the developing countries?’” he told Wired. “There have been lots of meetings, but we haven’t been able to get the US to show up.”
  
  
  
    Will you help keep Vox free for all?
  
  
    Millions rely on Vox’s journalism to understand the coronavirus crisis. We believe it pays off for all of us, as a society and a democracy, when our neighbors and fellow citizens can access clear, concise information on the pandemic. But our distinctive explanatory journalism is expensive. Support from our readers helps us keep it free for everyone. If you have already made a financial contribution to Vox, thank you. If not, please consider making a contribution today from as little as $3.
  




    
  
    
    
      
        
  


  






      
    
    
  
  



About 100 female lawmakers from across the world have sent a letter to Facebook CEO Mark Zuckerberg and COO Sheryl Sandberg demanding that the company do a better job combating misogyny on its platform, especially hateful content directed at female public figures. 
The letter, which was signed by prominent US politicians including House Speaker Nancy Pelosi, Sen. Tammy Baldwin, and Reps. Jackie Speier, Alexandria Ocasio-Cortez, and Ilhan Omar, comes as Facebook faces widespread criticism from civil rights groups and politicians over its handling of hateful content and misinformation. 
“[M]uch of the most hateful content directed at women on Facebook is amplified by your algorithms which reward extreme and dangerous points of view with greater reach and visibility creating a fertile breeding ground for bias to grow,” the lawmakers write in the letter, which was announced at a forum about misogyny and social media at George Washington University on August 6. 
The letter comes just days after Facebook opted not to take down a fake, viral video of Pelosi being shared on its platform that makes her seem drunk. This is the second time in just over a year that Facebook has left up one of these so-called “shallow-fakes” about Pelosi. While some other platforms removed both Pelosi videos, Facebook only added a fact-checkers’ label to the video and reduced its distribution.  
Rep. Jackie Speier from California’s 14th District is a co-chair of the Democratic Women’s Caucus and the primary organizer of the letter. She told Recode in an interview on Thursday that the recent Pelosi fake proves that Facebook isn’t doing enough to tackle sexism that spreads on its network. In particular, she emphasizes that Facebook’s systems magnify the hateful content, harassment, and misinformation targeted at female public figures. That’s why she wants Facebook to conduct an internal audit of misogyny on its platform. 
The following has been lightly edited for clarity and length.
The Nancy Pelosi fake shows how Facebook’s algorithms can amplify misogyny 
Rebecca Heilweil 
Earlier this week, we saw Facebook fact-checkers label — but choose to not take down — a fake video of Speaker Nancy Pelosi that made her seem intoxicated. Meanwhile, several of Facebook’s peer social media platforms did take that video down. I’m curious what you think of Facebook’s approach here, especially given that this is the second video like this that we’ve seen made about Speaker Pelosi.
Rep. Jackie Speier
So it’s deeply troubling on a number of levels. ... 2.6 million people saw it on Facebook and Facebook’s response is that “it is partially false.” It’s not partially false — it’s completely false. 
And Facebook is in my district. It’s a company that has added great value and great jobs for so many of my constituents. One of the statements they make in their mission is that they promote diversity and they promote equality. There is no question: Many studies have already established that women on these platforms are maligned in much greater instances than men, that they actually give license to persons that engage in misogynistic and hate speech, and do so with impunity because they are anonymous.
And the algorithms that Facebook uses enhance and amplify issues and statements that create controversy. So — of course — it ends up amplifying misogynistic statements, [and] doctored videotape of the speaker to make her look like she was drunk. ... We can take the slings and arrows. All of us have pretty tough skin. You gotta have tough skin to be in this business, but you do not have to be the subject of constant, persistent misogyny. 
There have been so many threats on my life over the length of my service. Two in the last two years actually were taken up by the local district attorney, and individuals were convicted. So there has been an increase of this kind of vile behavior, and it’s got to stop. And we’re putting Facebook on notice that they’ve got to be part of bringing some normalcy back to this process and to espouse their mission about diversity and inclusion and empowerment.
Rebecca Heilweil
In the letter, you use the term “gendered disinformation.” I’m curious what you mean by that. Could describe what’s going on?
Rep. Jackie Speier
It’s not just run-of-the-mill harassment online. It seeks to spread deceptive and inaccurate information and images about women, and particularly to follow storylines that draw on misogyny and distrust of women’s leadership.
“Self-policing isn’t working,” and Congress has a role in regulation
Rebecca Heilweil 
This comes after a civil rights audit that Facebook essentially failed and an advertiser boycott of Facebook that civil rights groups organized. A lot of people are very upset with how Facebook is handling content on its platform. As a member of Congress, when or if do you think it’s Congress’s role to step in and legislate in this area?
Rep. Jackie Speier
For the longest time, I think Congress has allowed the internet to be the wild, wild West, and we wanted to take this embryonic platform and allow it to grow — and e-commerce as well. I think we recognize now that it needs to be regulated. And in many respects, these platforms want to be regulated. The self-policing isn’t working, and it’s being enforced in a disparate manner. 
Rebecca Heilweil
At the recent antitrust hearing, some of your conservative colleagues seemed to take an opposite approach, saying that Facebook is censoring them too much and that Facebook is being too aggressive in moderating content. And I’m curious what you think of that argument. 
Rep. Speier
I don’t think that argument holds any water. In some respects, I think they do that in order to encourage Facebook to be more conservative. ... Hate speech is wrong anywhere and everywhere. So I don’t think there’s a question as to whether or not you should tolerate that. We don’t tolerate hateful graffiti to be placed on buildings, on churches ... you know, it’s a form of verbal graffiti. 
Rebecca Heilweil 
You call out algorithms specifically, but there’s horrifying sexist content online because there are real people who are willing to post this content. What does it mean for Facebook to change the algorithm, when often it’s real people posting this in the first place? 
Rep. Jackie Speier 
Part of what we’re asking in the letter is that if they post this kind of content, that they should take it down. If they repeatedly post this kind of content, you should take down their actual site. I think that this attitude that we don’t have responsibility doesn’t fly anymore.
Rebecca Heilweil 
As someone who is a woman lawmaker in office, could you talk a little bit about what it’s like to be someone who is online?
Rep. Jackie Speier
Let me tell you how bad it is. I don’t read my Facebook posts, I do not read my Twitter comments, in part because it distracts me from doing my job. I mean, when someone writes on Facebook, and they don’t like something I’ve posted, and they say, “they should have finished you off in Jonestown.” Or when, recently, they called me a “post-menopausal cunt.”
That is venomous. ... If they were doing that in person, they wouldn’t get away with it. 
Sexist attacks on social media discourage women politicians
Rebecca Heilweil
Let me share with you the comment that Facebook issued in response to your letter:
We appreciate Representative Speier and the other policymakers who have shared their personal experiences with us and we will continue working with them to surface new solutions. Abuse of women on the internet is a serious problem, one we tackle in a variety of ways — through technology that identifies and removes potentially abusive content before it happens, by enforcing strict policies, and by talking with experts to ensure we stay ahead of new tactics. Cindy Southworth, Facebook’s Head of Women’s Safety
Rep. Jackie Speier
I think they need to do much more. I think they need to do an internal audit of misogynistic comments on Facebook pages. And I hope that they will join in meeting with the Democratic Women’s Caucus and explore this further. ... But, you know, it’s very consistent with their attitude, which is, “we’re just a platform, we’re just here to give people the public forum.” They’re much more than a platform.
Rebecca Heilweil 
It seems like this question of misogyny and sexism on Facebook touches on a lot of issues that Facebook is dealing with right now, like misinformation and content moderation. I want to close by asking whether you think this is a platform that’s prepared for the upcoming November election.
Rep. Jackie Speier
No, I don’t think it is. As soon as Vice President Biden announces his vice presidential pick, the onslaught on Facebook and other sites is going to be horrific. We can’t tolerate misogyny, just like we can’t tolerate racism. ... What I’m most concerned about is the effect that this kind of content will have on women who contemplate running for office and who are dissuaded from running because they don’t want their names just besmirched like this [or] their families impacted. 
And we do have to recognize that there’s a kind of a format that’s used. Most of these comments about women in politics are that we are either dumb or we are too emotional. They’re the same kinds of assessments of women when we wanted to get the vote, for goodness’ sake. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  Amazon founder Jeff Bezos quietly created a new company to help execute his $10 billion pledge to combat climate change, Recode has learned, offering a clue into the plan known as the Bezos Earth Fund, which has been shrouded in secrecy since it was announced half a year ago.
Bezos’s team has started a new limited liability company, Fellowship Ventures LLC, that appears to be involved in the historic philanthropic commitment, according to public records reviewed by Recode. That LLC applied for the trademark — with Bezos’s hand-signed authorization — for the “Bezos Earth Fund” in July, a move that suggests the LLC will be key to his plans, or perhaps even run the charitable program outright.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        A screenshot from the trademark application filed by Fellowship Ventures LLC.
      
      
        United States Patent and Trademark Office
      
    
  


The creation of the company is the first glimpse into the most serious philanthropic play yet by the world’s richest man, even as other details remain hidden. Bezos aides have consistently declined to share any information about his climate change giving — including basic questions about how it will be structured — since it was first announced in February.
The details are essential because the $10 billion pledge, one of the largest individual charitable commitments ever, is expected to remake the world of climate change philanthropy. Questions abound: Will Bezos use any of the $10 billion to make donations to pro-climate-science political candidates or advocacy groups? Over what time period will Bezos give the money away? And what type of disclosures will Bezos share with academics, researchers, and reporters about where the money goes?
If Bezos does plan to use this LLC to make the donations, it would limit transparency into the Earth Fund as LLCs are not required to file publicly available tax documents. Trademark experts tell Recode that it’s also possible, however, for the LLC to merely end up owning the trademark to the “Bezos Earth Fund” name and then lend that trademark to another to-be-created Bezos entity that may be structured in a more transparent way, such as a traditional foundation.
Bezos’s team isn’t saying. Amazon declined to comment on Fellowship Ventures, and Bezos’s personal lawyers who signed the documents didn’t return Recode’s requests for comment. Bezos previously said he would start making grants to climate change organizations this summer.
The Amazon founder isn’t done clinging to secrecy. The trademark application by Fellowship Ventures was filed first in Jamaica, a trick sometimes used by companies to shield information about their plans, trademark experts say, because Jamaica makes it impossible to access applications online.
(A side note: Bezos’s ex-wife, MacKenzie Scott, made headlines last week by announcing her own $1.7 billion in charitable donations. Scott is one of the world’s wealthiest people. And she, too, is using a less-transparent vehicle — a donor-advised fund — to make at least some of those gifts, two grantees tell Recode.)
It’s likely that Fellowship Ventures is working on other projects on Bezos’s behalf, too, although the full scope of its work isn’t clear. Billionaires — and especially billionaires like Bezos, who is nearing a net worth of $200 billion — oversee vast empires to manage their personal affairs and family offices. They’ll create new LLCs to execute a particular real-estate deal, for instance, or to manage the work of a new contractor.
Bezos’s empire includes his space-exploration company Blue Origin, his ownership of the Washington Post, and a clock in a hollowed-out Texas mountain that Bezos is building to last 10,000 years.
That’s what makes the creation of yet another LLC all the more intriguing. Bezos already controls LLCs that help oversee his existing charitable work, including Zefram LLC, which owns the trademark to the “Bezos Day One Fund,” his philanthropy to combat homelessness and support education unveiled in 2018. One possibility is that a new vehicle was needed after Bezos’s costly — and no doubt financially complicated — divorce last summer. Fellowship Ventures was incorporated in Delaware last summer, too, according to records obtained by Recode.
Zefram, for what it’s worth, is named after a fictional spaceship designer on Star Trek, a favorite of the Amazon founder. And the words “fellowship” and “venture,” too, have long held special meaning for Bezos — so much so that they’re part of his customary toast: “To adventure and fellowship!”
“The word ‘fellowship’ conjures a vision of traveling down the road together. It has more ‘journey’ in it than friendship,” Bezos shared when interviewed by his brother in 2017. “Friendship is great too, but fellowship captures friendship and traveling down that path together.”
Details like the name and the structure are some of the few scraps of insight into how the world’s wealthiest person is going to spend his billions. And that’s one of the big criticisms of billionaire philanthropy, that the mega-rich can release as much or as little information about their charitable gifts as they choose.

  
  
  
      




  President Trump issued an executive order on Thursday night that will effectively ban any US company or individual from making transactions with ByteDance, the Chinese parent company of the TikTok app, after 45 days. 
While this conditional ban on TikTok’s US business operations will likely face serious legal challenges, and it’s unclear how immediately enforceable it is, the order creates a serious challenge for TikTok, a wildly popular video streaming app with some 100 million US users. 
For months, Trump and other bipartisan politicians have periodically raised concerns about TikTok as a potential national security threat, worrying that the app’s Chinese parent company could censor content in the US or access American users’ sensitive data at the behest of the Chinese Communist Party. 
The executive order states that TikTok “automatically captures vast swaths of information from its users,” and that this “threatens to allow the Chinese Communist Party access to Americans’ personal and proprietary information — potentially allowing China to track the locations of Federal employees and contractors, build dossiers of personal information for blackmail, and conduct corporate espionage.”
The company has vehemently denied these accusations, saying repeatedly that it does not share user data with the Chinese government. But reports last year showed a lack of TikTok content about subjects controversial with the Chinese government — such as videos of the Hong Kong protests. These reports have fueled US government suspicions that the company is influenced by the Chinese government, particularly as China has been expanding its surveillance state in recent years and US-China diplomatic relations have become more strained.
Trump has gone back and forth about how he planned to take action on TikTok. As recently as last Friday, he warned of an eminent executive ban on TikTok. On Monday though, Trump seemed to reverse his stance and said in a White House press briefing that instead of banning it, he would allow a US-based company to purchase the app. 
“I don’t mind if — whether it is Microsoft or someone else — a big company, a secure company, a very American company, buys it,” said Trump about TikTok. Trump also warned that TikTok will be “out of business in the United States” by September 15 if the company doesn’t reach a deal to sell by then. Now, the executive order has established a timeline of 45 days from Thursday (which would be September 20) for the sale to happen before TikTok will no longer be able to conduct business in the US.

  
    Related
  

  
    The case for and against banning TikTok
  

Though TikTok, which is owned by the Chinese company ByteDance, is best known as a place where teens share short, often lighthearted musical videos, it has become the center of geopolitical controversy between the US and China over technological power.
But getting rid of an app used by some 100 million Americans isn’t easy, even if you’re the president. According to a New York Times report on Sunday, after Trump’s advisers convinced him that an executive action to ban TikTok would face severe legal and political consequences, Trump decided he would allow the tech giant Microsoft to continue its previous talks to buy the app, which had reportedly been in the works for weeks. Since Microsoft is a US-based company, the idea is that if Microsoft took control, it would ensure all of TikTok’s user data is stored in the US, secure from the potentially prying eyes of the Chinese government. Microsoft CEO Satya Nadella talked to Trump about it over the weekend, according to a Microsoft blog post published on Saturday evening, and has agreed to work out a deal — or not — by September 15.


  Do you work at TikTok and have thoughts about what’s going on? Please email Shirin Ghaffary at shirin.ghaffary@protonmail.com to reach her confidentially. Signal number available upon request.



 
Here’s a rundown of the recent controversy surrounding TikTok and what’s expected to happen next:
TikTok’s political troubles
TikTok has faced intense political scrutiny for months leading up to Trump’s executive order.
Republicans escalated their attacks on TikTok this summer, with some bipartisan support from Democrats as well. Last Thursday, Sens. Richard Blumenthal (D-CT) and Josh Hawley (R-MO) sent a letter to the Justice Department demanding that the agency open an investigation into TikTok and Zoom over reported violations of “Americans’ civil liberties” and national security concerns about relationships between these companies and China. This followed statements in July from Trump and Secretary of State Mike Pompeo, who both said the Trump administration was considering banning TikTok altogether. 
For the past year, it’s been thought that the app has been under government review for national security reasons. Treasury Secretary Steven Mnuchin confirmed this last week, and said he’s expecting the review to conclude soon. The government committee in charge of this review, the Committee on Foreign Investment in the United States (CFIUS), has the power to recommend the president force TikTok to sell to a US company.
Even if Trump can’t enforce a full-on ban, a government decision that forces TikTok’s parent company to sell it off would be a game changer for the social media industry, and would threaten to disrupt the app’s extraordinary popularity. And for established social media giants Facebook and Google, the decision could significantly weaken their fiercest new competitor.
A forced sale of TikTok could have negative consequences beyond the people running TikTok, too. The move threatens to jeopardize the success of an app that’s had a meteoric rise from a relative underdog to one of the most downloaded apps in the world. And since TikTok is one of the only recent social media startups to compete with tech giants like Facebook, weakening TikTok could further concentrate power among a few tech giants in the US.
“While we do not comment on rumors or speculation, we are confident in the long-term success of TikTok,” a spokesperson for TikTok told Recode last Friday, adding the company is “committed to protecting their privacy and safety as we continue working to bring joy to families and meaningful careers to those who create on our platform.”
How a sale would work
You may be asking how Trump can force a company as popular as TikTok to sell itself, or go so far as to try to ban it. The answer is complicated and bureaucratic. 
To force a sale, Trump could issue an order for ByteDance to divest from TikTok through CFIUS, an interagency committee that reviews foreign acquisitions and investments in US businesses that can threaten national security. The committee, chaired by Mnuchin, has the power to block or reverse mergers and acquisitions involving US and foreign companies. 
Increasingly, the agency has been exercising its authority over foreign-owned tech companies operating in the US. Last year, CFIUS helped block one of the biggest deals in tech history, after Trump followed its recommendations to stop Singapore-based Broadcom from acquiring the US semiconductor company Qualcomm. The committee also forced Chinese owners to divest from the dating app Grindr and the health startup PatientsLikeMe. 
But as Brookings Institution fellow Geoffrey Gertz has written, tech companies weren’t always the target of CFIUS. In the past, the committee “tended to focus on companies with military or intelligence connections,” but more recently, personal data and high-tech intellectual property have become a bigger focus for the committee. 
Last year, CFIUS started investigating ByteDance, which had purchased the Chinese-owned lip-sync video platform Musical.ly in 2017 and then rebranded and launched a similar app in the US under the name TikTok. When that investigation comes to a close, the committee’s recommendations will reportedly lead to Trump’s order for ByteDance to sell TikTok or divest its US operations.
It’s unclear how CFIUS would enforce a potential unwinding of ByteDance and TikTok, but last year, the committee issued a $1 million fine to an undisclosed company for not following through on a mitigation agreement, its first penalty of that kind. It could also fine TikTok — or Trump could just continue to dangle the threat of banning TikTok altogether, no matter how legally or politically contentious that would be. 
In a press briefing on Monday, Trump said that whoever ends up owning TikTok should pay the Treasury department of the US government a “substantial amount of money” as part of the deal. As some have pointed out, including Axios’s Dan Primack, Trump’s comments could be “skating very close to announcing extortion.” It’s not immediately clear how Trump would try to ensure the US government gets a cut of the sale or whether it’s even legal to do that. 
What comes next
If Microsoft or another major US company does purchase TikTok, it’s likely that TikTok as we know it would remain largely unchanged.
TikTok is a valuable brand in a lucrative industry with a massive, devoted user base — so for Microsoft, buying TikTok would be an opportunity to seriously compete with other major tech companies like Facebook and Google in the social media space.
Microsoft also has experience when it comes to purchasing already successful companies and allowing them to retain their independence — as it did when it acquired the platform for software developers, GitHub, in 2018, and the video game Minecraft in 2014.
Depending on how Microsoft chooses to run TikTok — if it acquires it — the app could continue to grow, and with the backing of a major US tech company, it might more seriously take on other social media companies, including Facebook. Microsoft isn’t the only potential buyer — other firms could try to buy TikTok or share ownership. Reportedly, Microsoft may invite outside investors to join them in the deal, according to the Wall Street Journal.
It’s too soon to say what impact a sale would have on the app’s popularity and growth. But in the meantime, there are plenty of Clippy jokes to make.
On Monday, a spokesperson for TikTok told Recode in a statement that the company is “committed to continuing to bring joy to families and meaningful careers to those who create on our platform as we build TikTok for the long term. TikTok will be here for many years to come.”
Update, August 6, 9:54 pm ET: This article has been updated to include news of Trump’s executive order against TikTok.
Update, August 3, 3:05 pm ET: This article has been updated to include new comments from President Trump and new reporting about ongoing negotiations between Microsoft and TikTok.


  
  
  
      




  Mark Zuckerberg told Facebook employees that the company would be extending its policy for corporate employees to work from home until July 2021. The social media company has around 48,000 employees worldwide. 
The Facebook CEO announced the move in a weekly Q&A virtual meeting with employees. Previously, Facebook said employees would start returning to its offices in January 2021. But as coronavirus infection rates continue to spike across the US, Facebook is following Google and Uber to become the latest tech company to extend its timeline for office closures. The move reflects a broader sense among tech companies that office life will not be back to normal until much later than initially anticipated at the onset of the pandemic.
“Based on guidance from health and government experts, as well as decisions drawn from our internal discussions about these matters, we are allowing employees to continue voluntarily working from home until July 2021,” said Pamela Austin, a spokesperson for Facebook in a statement to Recode. The company will also be giving employees an additional $1,000 for home office needs. 
On Facebook’s earnings call last week, Zuckerberg said he saw “no end in sight” for when his staff would be able to return to the company’s offices. But his latest comments represent the first time he’s given a hard date for how long Facebook’s offices will remain largely closed. Facebook had also previously stated it was canceling all large corporate events until June 2021.
The extended timeline may prompt many Facebook employees to move. In a recent company poll, about 75 percent of Facebook’s corporate employees said they were highly confident they would move to a different city if they could work remotely. However, Facebook has said it will adjust employees’ salaries if they leave the regions in which they were hired, like the San Francisco Bay Area, for comparatively cheaper areas. The poll was seemingly taken before employees were aware their salaries may be docked if they did choose to move. 
In May, Zuckerberg said Facebook would start allowing some high-performing, senior-level employees to request to permanently work from home. It was positioned as part of Zuckerberg’s long-term goal of having some 50 percent of the company’s workforce be working entirely remotely in the next five to 10 years.
While many of Facebook’s corporate engineers, designers, and business managers can mostly perform their jobs from home, it’s unclear what will happen to the thousands of service workers who normally keep Facebook’s physical offices running. Facebook uses third-party contractors to staff positions such as janitors, cafeteria workers, and shuttle bus drivers who take corporate employees to work and home. In March, Facebook, Google, Apple, and other major tech companies promised to continue to pay their support staff their pre-pandemic wages even during temporary office closures. But it’s unclear if these companies will maintain that promise as offices are staying closed for longer than expected.
“We have continued to work closely with our vendor partners and pay our CWs [contract workers] while working from home even if they are not able to perform their roles,” a Facebook spokesperson told Recode. “We’ll continue to assess what we can do as COVID-19 evolves.”
The spokesperson did not directly respond to Recode’s question about whether the company will continue to pay contracted service employees until July 2021 at pre-pandemic levels, or if the company is planning cuts to these positions. 

  
  
  
    Will you help keep Vox free for all?
  
  
    Millions rely on Vox’s journalism to understand the coronavirus crisis. We believe it pays off for all of us, as a society and a democracy, when our neighbors and fellow citizens can access clear, concise information on the pandemic. But our distinctive explanatory journalism is expensive. Support from our readers helps us keep it free for everyone. If you have already made a financial contribution to Vox, thank you. If not, please consider making a contribution today from as little as $3.
  






  Facebook’s TikTok knockoff, Instagram Reels, is making its big United States debut today. The feature is Instagram’s answer to TikTok, the wildly popular short-form video streaming app. It would be a big deal if this copycat product is as successful as Instagram Stories, but that’s hardly guaranteed.
Reels lets users make 15-second edited videos overlaid with music directly in their Stories, Feed, or a new Reels section in the Instagram Explore tab, as my Vox colleague Rebecca Jennings explains here. It’s a near clone of TikTok, which similarly lets people upload lip-sync videos. The timing of the US release of Reels, which Facebook has been testing in other countries since late last year, has also been surprisingly fortuitous. After threatening to ban TikTok in the US, Trump now appears to be forcing a sale of TikTok to an American company like Microsoft. 
Though the Reels launch is drawing attention for all kinds of reasons, let’s not forget that Facebook has a long history of creating knockoffs. To put the latest effort into perspective, here are some of the more notable times the social media giant has tried to copy its competitors — with varying levels of success.
Lasso, the TikTok clone that didn’t work
  
  
    
    
      
        
  


  






      
    
    
  
  


Reels actually isn’t the first time Facebook has tried to clone TikTok. The last time did not end well.
Back in November 2018, not long after TikTok first bought China-based lip-syncing app Musical.ly and launched in the US, Facebook quietly came out with a standalone app called Lasso that, just like Reels, allowed users to take 15-second videos and overlay them with music. At the time of the launch, a Facebook spokesperson called it a “standalone app for short-form, entertaining videos — from comedy to beauty to fitness and more,” which sounds very similar to TikTok and a lot like how the company is now describing Reels. 
But as a standalone app without a built-in userbase, Lasso struggled to win users. By June 1, 2019, the app reportedly had fewer than 80,000 daily active Android users in Mexico, its biggest market, according to metrics from App Annie that TechCrunch reported. Facebook shut down Lasso in July of this year ahead of its expected Instagram Reels launch. 
On a call with reporters on Wednesday, Instagram’s head of product Vishal Shah said that Instagram has learned lessons from Lasso. “A new standalone app, while it gives you a lot of flexibility, also requires to kind of build an audience,” said Shah on the call.

Stories, the most successful clone to date 
  
  
    
    
      
        
  


  






      
    
    
  
  


Instagram’s ephemeral Stories feature is now one of the most-used parts of the app. But back when the feature launched in 2016, Instagram was all about the Feed, the chronological collection of your photos that lives more permanently in a user’s profile. The arrival of Stories was immediately controversial as it was seen as a blatant copy of a Snapchat feature with the same exact name. Like Instagram Stories, Snapchat Stories let users share with their friends content that expired within 24 hours. 
That came after Facebook tried, unsuccessfully, to acquire Snapchat back in 2013; when Snap CEO Evan Spiegel refused to sell, Mark Zuckerberg launched a copycat app called Poke (more on that later). Zuckerberg reportedly approached Spiegel again in 2016 before launching Stories. At that time, Facebook defended itself from criticism that it was copying its rising competitor using very similar language to how they’re talking about the similarity between Reels and TikTok today.
“They deserve all the credit,” Instagram co-founder Kevin Systrom told TechCrunch about Snapchat. “This isn’t about who invented something. This is about a format, and how you take it to a network and put your own spin on it.”
Compare that to what Instagram’s Shah said about TikTok on a press call before the Reels launch: “We’re inspired by other companies, too. We were clear on that with Stories with Snap, we’re clear on that with TikTok short-form video,” Shah said. “At the same time, you know, these things are not invented in any one place.”
It wasn’t at all clear how Stories would fare when it launched. In addition to Poke, Facebook had already launched another failed ephemeral messaging app called Slingshot. But Instagram Stories turned out to be a huge success. Instagrammers were suddenly relieved of the pressure to have a perfectly curated lifestyle to permanently document in their “grid” of forever-photos, and instead could share content more freely in their disappearing Stories section. About half of Instagram’s 1 billion users in 2018 used the Stories feature every day. 
The number of Instagram Stories users also quickly surpassed Snapchat users, effectively weakening one of its biggest competitors and making it one of the most successful of Facebook’s competitor clones to date. 

Hobbi, the Pinterest clone that never took off
  
  
    
    
      
        
  


  






      
    
    
  
  


Pinterest has long cornered a market of users that any company running ads finds attractive. These include people planning weddings, redecorating their homes, or exploring a new hobby — all activities that generally involve buying new things. So it’s no surprise that Facebook launched an app that looked awfully similar to Pinterest, called Hobbi, early in 2020. 
Facebook marketed the app as one that helps you “capture and organize your creative process” for activities like cooking, home decor, and gardening, according to a Verge report around its release in February. The app was meant for people to document their crafts, and then the app would help publish a highlights reel when the project was done.
But only four months after launch, Facebook shut down Hobbi. One problem with it: The app was “fairly bare bones,” TechCrunch wrote at the time, only allowing users to view their own videos and snippets of other top users instead of a feed of peers’ content. In the end, Hobbi became an example of Facebook seeming to half-commit to copying an important competitor — and failing.

Rooms, the Zoom clone 
  
  
    
    
      
        
  


  






      
    
    
  
  


Videoconference chat app Zoom has been one of the breakout stars of the tech startup world in the past few years, its success cemented by the pandemic forcing people to take more virtual meetings than ever. So of course, Facebook launched a competing video chat service in April called Rooms. The feature is built into the Facebook Messenger app and lets users hold group chats with up to 50 people at a time. Facebook says it will later expand to Whatsapp, Instagram, and Portal as well.
As my colleague Sara Morrison previously explained, Rooms seems to positions itself as more secure than its competition, with features like making Rooms links intentionally difficult for hackers to guess. This claim may sound a bit rich considering that Facebook has repeatedly publicly stumbled on privacy. (Just hours before the Rooms launch, Facebook announced it settled a $5 billion case with the FTC over the company’s role in the Cambridge Analytica scandal.)
Also, Rooms stands to win big in the video chat space because Facebook already has a broad built-in user base. Facebook already owns wildly popular messaging app Whatsapp, and Facebook says 700 million users of Whatsapp and Messenger participate in calls every day. Zoom, for comparison, had about 300 million daily participants in meetings back in April. 
Zoom has also faced criticism for routing some calls through China-based computer servers, which it says was a mistake. This comes at a time when Trump has increased his crackdown of major US companies that have ties to China. If Zoom ends up following TikTok in the regulatory hot seat, Facebook’s Rooms could, like Instagram Reels, be positioned to take advantage of the competition’s political baggage. 
But with Rooms and all of the examples above, there’s concern that Facebook is running into antitrust problems when it blatantly copies its competition. Just last week, Congress pressed Facebook CEO Mark Zuckerberg on concerns that his company has engaged in anti-competitive behavior, particularly around its Instagram acquisition. Instagram co-founder Kevin Systrom worried that if he didn’t sell his company to Facebook back in 2012 that Zuckerberg would go into “destroy mode” to crush the competition, according to recently released text messages between Systrom and an Instagram investor. Some critics argue that Facebook is going into “destroy mode” yet again, with TikTok as its latest target. 
It’s true that Facebook seems to be unbothered by potential regulatory concerns about the similarities between Reels and TikTok. But while there are serious anti-competitive questions about Facebook yet again releasing a copycat app of one of its biggest competitors, Reels is only a viable threat to TikTok if it actually succeeds.
For now, it’s too soon to say if Reels will become another now-forgotten Lasso or Hobbi, or whether it will be the next Instagram Stories.

  
  
  
      




  Facebook’s TikTok knockoff, Instagram Reels, is making its big United States debut today. The feature is Instagram’s answer to TikTok, the wildly popular short-form video streaming app. It would be a big deal if this copycat product is as successful as Instagram Stories, but that’s hardly guaranteed.
Reels lets users make 15-second edited videos overlaid with music directly in their Stories, Feed, or a new Reels section in the Instagram Explore tab, as my Vox colleague Rebecca Jennings explains here. It’s a near clone of TikTok, which similarly lets people upload lip-sync videos. The timing of the US release of Reels, which Facebook has been testing in other countries since late last year, has also been surprisingly fortuitous. After threatening to ban TikTok in the US, Trump now appears to be forcing a sale of TikTok to an American company like Microsoft. 
Though the Reels launch is drawing attention for all kinds of reasons, let’s not forget that Facebook has a long history of creating knockoffs. To put the latest effort into perspective, here are some of the more notable times the social media giant has tried to copy its competitors — with varying levels of success.
Lasso, the TikTok clone that didn’t work
  
  
    
    
      
        
  


  






      
    
    
  
  


Reels actually isn’t the first time Facebook has tried to clone TikTok. The last time did not end well.
Back in November 2018, not long after TikTok first bought China-based lip-syncing app Musical.ly and launched in the US, Facebook quietly came out with a standalone app called Lasso that, just like Reels, allowed users to take 15-second videos and overlay them with music. At the time of the launch, a Facebook spokesperson called it a “standalone app for short-form, entertaining videos — from comedy to beauty to fitness and more,” which sounds very similar to TikTok and a lot like how the company is now describing Reels. 
But as a standalone app without a built-in userbase, Lasso struggled to win users. By June 1, 2019, the app reportedly had fewer than 80,000 daily active Android users in Mexico, its biggest market, according to metrics from App Annie that TechCrunch reported. Facebook shut down Lasso in July of this year ahead of its expected Instagram Reels launch. 
On a call with reporters on Wednesday, Instagram’s head of product Vishal Shah said that Instagram has learned lessons from Lasso. “A new standalone app, while it gives you a lot of flexibility, also requires to kind of build an audience,” said Shah on the call.

Stories, the most successful clone to date 
  
  
    
    
      
        
  


  






      
    
    
  
  


Instagram’s ephemeral Stories feature is now one of the most-used parts of the app. But back when the feature launched in 2016, Instagram was all about the Feed, the chronological collection of your photos that lives more permanently in a user’s profile. The arrival of Stories was immediately controversial as it was seen as a blatant copy of a Snapchat feature with the same exact name. Like Instagram Stories, Snapchat Stories let users share with their friends content that expired within 24 hours. 
That came after Facebook tried, unsuccessfully, to acquire Snapchat back in 2013; when Snap CEO Evan Spiegel refused to sell, Mark Zuckerberg launched a copycat app called Poke (more on that later). Zuckerberg reportedly approached Spiegel again in 2016 before launching Stories. At that time, Facebook defended itself from criticism that it was copying its rising competitor using very similar language to how they’re talking about the similarity between Reels and TikTok today.
“They deserve all the credit,” Instagram co-founder Kevin Systrom told TechCrunch about Snapchat. “This isn’t about who invented something. This is about a format, and how you take it to a network and put your own spin on it.”
Compare that to what Instagram’s Shah said about TikTok on a press call before the Reels launch: “We’re inspired by other companies, too. We were clear on that with Stories with Snap, we’re clear on that with TikTok short-form video,” Shah said. “At the same time, you know, these things are not invented in any one place.”
It wasn’t at all clear how Stories would fare when it launched. In addition to Poke, Facebook had already launched another failed ephemeral messaging app called Slingshot. But Instagram Stories turned out to be a huge success. Instagrammers were suddenly relieved of the pressure to have a perfectly curated lifestyle to permanently document in their “grid” of forever-photos, and instead could share content more freely in their disappearing Stories section. About half of Instagram’s 1 billion users in 2018 used the Stories feature every day. 
The number of Instagram Stories users also quickly surpassed Snapchat users, effectively weakening one of its biggest competitors and making it one of the most successful of Facebook’s competitor clones to date. 

Hobbi, the Pinterest clone that never took off
  
  
    
    
      
        
  


  






      
    
    
  
  


Pinterest has long cornered a market of users that any company running ads finds attractive. These include people planning weddings, redecorating their homes, or exploring a new hobby — all activities that generally involve buying new things. So it’s no surprise that Facebook launched an app that looked awfully similar to Pinterest, called Hobbi, early in 2020. 
Facebook marketed the app as one that helps you “capture and organize your creative process” for activities like cooking, home decor, and gardening, according to a Verge report around its release in February. The app was meant for people to document their crafts, and then the app would help publish a highlights reel when the project was done.
But only four months after launch, Facebook shut down Hobbi. One problem with it: The app was “fairly bare bones,” TechCrunch wrote at the time, only allowing users to view their own videos and snippets of other top users instead of a feed of peers’ content. In the end, Hobbi became an example of Facebook seeming to half-commit to copying an important competitor — and failing.

Rooms, the Zoom clone 
  
  
    
    
      
        
  


  






      
    
    
  
  


Videoconference chat app Zoom has been one of the breakout stars of the tech startup world in the past few years, its success cemented by the pandemic forcing people to take more virtual meetings than ever. So of course, Facebook launched a competing video chat service in April called Rooms. The feature is built into the Facebook Messenger app and lets users hold group chats with up to 50 people at a time. Facebook says it will later expand to Whatsapp, Instagram, and Portal as well.
As my colleague Sara Morrison previously explained, Rooms seems to positions itself as more secure than its competition, with features like making Rooms links intentionally difficult for hackers to guess. This claim may sound a bit rich considering that Facebook has repeatedly publicly stumbled on privacy. (Just hours before the Rooms launch, Facebook announced it settled a $5 billion case with the FTC over the company’s role in the Cambridge Analytica scandal.)
Also, Rooms stands to win big in the video chat space because Facebook already has a broad built-in user base. Facebook already owns wildly popular messaging app Whatsapp, and Facebook says 700 million users of Whatsapp and Messenger participate in calls every day. Zoom, for comparison, had about 300 million daily participants in meetings back in April. 
Zoom has also faced criticism for routing some calls through China-based computer servers, which it says was a mistake. This comes at a time when Trump has increased his crackdown of major US companies that have ties to China. If Zoom ends up following TikTok in the regulatory hot seat, Facebook’s Rooms could, like Instagram Reels, be positioned to take advantage of the competition’s political baggage. 
But with Rooms and all of the examples above, there’s concern that Facebook is running into antitrust problems when it blatantly copies its competition. Just last week, Congress pressed Facebook CEO Mark Zuckerberg on concerns that his company has engaged in anti-competitive behavior, particularly around its Instagram acquisition. Instagram co-founder Kevin Systrom worried that if he didn’t sell his company to Facebook back in 2012 that Zuckerberg would go into “destroy mode” to crush the competition, according to recently released text messages between Systrom and an Instagram investor. Some critics argue that Facebook is going into “destroy mode” yet again, with TikTok as its latest target. 
It’s true that Facebook seems to be unbothered by potential regulatory concerns about the similarities between Reels and TikTok. But while there are serious anti-competitive questions about Facebook yet again releasing a copycat app of one of its biggest competitors, Reels is only a viable threat to TikTok if it actually succeeds.
For now, it’s too soon to say if Reels will become another now-forgotten Lasso or Hobbi, or whether it will be the next Instagram Stories.

  
  
  
      




  Facebook’s TikTok knockoff, Instagram Reels, is making its big United States debut today. The feature is Instagram’s answer to TikTok, the wildly popular short-form video streaming app. It would be a big deal if this copycat product is as successful as Instagram Stories, but that’s hardly guaranteed.
Reels lets users make 15-second edited videos overlaid with music directly in their Stories, Feed, or a new Reels section in the Instagram Explore tab, as my Vox colleague Rebecca Jennings explains here. It’s a near clone of TikTok, which similarly lets people upload lip-sync videos. The timing of the US release of Reels, which Facebook has been testing in other countries since late last year, has also been surprisingly fortuitous. After threatening to ban TikTok in the US, Trump now appears to be forcing a sale of TikTok to an American company like Microsoft. 
Though the Reels launch is drawing attention for all kinds of reasons, let’s not forget that Facebook has a long history of creating knockoffs. To put the latest effort into perspective, here are some of the more notable times the social media giant has tried to copy its competitors — with varying levels of success.
Lasso, the TikTok clone that didn’t work
  
  
    
    
      
        
  


  






      
    
    
  
  


Reels actually isn’t the first time Facebook has tried to clone TikTok. The last time did not end well.
Back in November 2018, not long after TikTok first bought China-based lip-syncing app Musical.ly and launched in the US, Facebook quietly came out with a standalone app called Lasso that, just like Reels, allowed users to take 15-second videos and overlay them with music. At the time of the launch, a Facebook spokesperson called it a “standalone app for short-form, entertaining videos — from comedy to beauty to fitness and more,” which sounds very similar to TikTok and a lot like how the company is now describing Reels. 
But as a standalone app without a built-in userbase, Lasso struggled to win users. By June 1, 2019, the app reportedly had fewer than 80,000 daily active Android users in Mexico, its biggest market, according to metrics from App Annie that TechCrunch reported. Facebook shut down Lasso in July of this year ahead of its expected Instagram Reels launch. 
On a call with reporters on Wednesday, Instagram’s head of product Vishal Shah said that Instagram has learned lessons from Lasso. “A new standalone app, while it gives you a lot of flexibility, also requires to kind of build an audience,” said Shah on the call.

Stories, the most successful clone to date 
  
  
    
    
      
        
  


  






      
    
    
  
  


Instagram’s ephemeral Stories feature is now one of the most-used parts of the app. But back when the feature launched in 2016, Instagram was all about the Feed, the chronological collection of your photos that lives more permanently in a user’s profile. The arrival of Stories was immediately controversial as it was seen as a blatant copy of a Snapchat feature with the same exact name. Like Instagram Stories, Snapchat Stories let users share with their friends content that expired within 24 hours. 
That came after Facebook tried, unsuccessfully, to acquire Snapchat back in 2013; when Snap CEO Evan Spiegel refused to sell, Mark Zuckerberg launched a copycat app called Poke (more on that later). Zuckerberg reportedly approached Spiegel again in 2016 before launching Stories. At that time, Facebook defended itself from criticism that it was copying its rising competitor using very similar language to how they’re talking about the similarity between Reels and TikTok today.
“They deserve all the credit,” Instagram co-founder Kevin Systrom told TechCrunch about Snapchat. “This isn’t about who invented something. This is about a format, and how you take it to a network and put your own spin on it.”
Compare that to what Instagram’s Shah said about TikTok on a press call before the Reels launch: “We’re inspired by other companies, too. We were clear on that with Stories with Snap, we’re clear on that with TikTok short-form video,” Shah said. “At the same time, you know, these things are not invented in any one place.”
It wasn’t at all clear how Stories would fare when it launched. In addition to Poke, Facebook had already launched another failed ephemeral messaging app called Slingshot. But Instagram Stories turned out to be a huge success. Instagrammers were suddenly relieved of the pressure to have a perfectly curated lifestyle to permanently document in their “grid” of forever-photos, and instead could share content more freely in their disappearing Stories section. About half of Instagram’s 1 billion users in 2018 used the Stories feature every day. 
The number of Instagram Stories users also quickly surpassed Snapchat users, effectively weakening one of its biggest competitors and making it one of the most successful of Facebook’s competitor clones to date. 

Hobbi, the Pinterest clone that never took off
  
  
    
    
      
        
  


  






      
    
    
  
  


Pinterest has long cornered a market of users that any company running ads finds attractive. These include people planning weddings, redecorating their homes, or exploring a new hobby — all activities that generally involve buying new things. So it’s no surprise that Facebook launched an app that looked awfully similar to Pinterest, called Hobbi, early in 2020. 
Facebook marketed the app as one that helps you “capture and organize your creative process” for activities like cooking, home decor, and gardening, according to a Verge report around its release in February. The app was meant for people to document their crafts, and then the app would help publish a highlights reel when the project was done.
But only four months after launch, Facebook shut down Hobbi. One problem with it: The app was “fairly bare bones,” TechCrunch wrote at the time, only allowing users to view their own videos and snippets of other top users instead of a feed of peers’ content. In the end, Hobbi became an example of Facebook seeming to half-commit to copying an important competitor — and failing.

Rooms, the Zoom clone 
  
  
    
    
      
        
  


  






      
    
    
  
  


Videoconference chat app Zoom has been one of the breakout stars of the tech startup world in the past few years, its success cemented by the pandemic forcing people to take more virtual meetings than ever. So of course, Facebook launched a competing video chat service in April called Rooms. The feature is built into the Facebook Messenger app and lets users hold group chats with up to 50 people at a time. Facebook says it will later expand to Whatsapp, Instagram, and Portal as well.
As my colleague Sara Morrison previously explained, Rooms seems to positions itself as more secure than its competition, with features like making Rooms links intentionally difficult for hackers to guess. This claim may sound a bit rich considering that Facebook has repeatedly publicly stumbled on privacy. (Just hours before the Rooms launch, Facebook announced it settled a $5 billion case with the FTC over the company’s role in the Cambridge Analytica scandal.)
Also, Rooms stands to win big in the video chat space because Facebook already has a broad built-in user base. Facebook already owns wildly popular messaging app Whatsapp, and Facebook says 700 million users of Whatsapp and Messenger participate in calls every day. Zoom, for comparison, had about 300 million daily participants in meetings back in April. 
Zoom has also faced criticism for routing some calls through China-based computer servers, which it says was a mistake. This comes at a time when Trump has increased his crackdown of major US companies that have ties to China. If Zoom ends up following TikTok in the regulatory hot seat, Facebook’s Rooms could, like Instagram Reels, be positioned to take advantage of the competition’s political baggage. 
But with Rooms and all of the examples above, there’s concern that Facebook is running into antitrust problems when it blatantly copies its competition. Just last week, Congress pressed Facebook CEO Mark Zuckerberg on concerns that his company has engaged in anti-competitive behavior, particularly around its Instagram acquisition. Instagram co-founder Kevin Systrom worried that if he didn’t sell his company to Facebook back in 2012 that Zuckerberg would go into “destroy mode” to crush the competition, according to recently released text messages between Systrom and an Instagram investor. Some critics argue that Facebook is going into “destroy mode” yet again, with TikTok as its latest target. 
It’s true that Facebook seems to be unbothered by potential regulatory concerns about the similarities between Reels and TikTok. But while there are serious anti-competitive questions about Facebook yet again releasing a copycat app of one of its biggest competitors, Reels is only a viable threat to TikTok if it actually succeeds.
For now, it’s too soon to say if Reels will become another now-forgotten Lasso or Hobbi, or whether it will be the next Instagram Stories.

  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Sens. Jeff Merkley and Bernie Sanders are proposing more federal regulation for facial recognition technology. Among other limits, their newly announced National Biometric Information Privacy Act would require private companies and corporations to get written consent from people in order to collect their biometric data. Should companies violate the consumer protection in the law, regular citizens and state attorneys general could sue them. 
Over the past few months, there’s been a national reckoning over facial recognition that’s centered on whether law enforcement should have access to this powerful technology. Major tech companies including IBM, Amazon, and Microsoft have now paused or scaled back facial recognition systems that are sold to police departments. Senators have even proposed legislation that would put a moratorium on its use by the federal government and discourage its use by law enforcement, which followed the news of the first known false arrest caused by police use of facial recognition. 
The new bill is a sign that officials aren’t just concerned about government or law enforcement having access to this controversial, biometrics-based technology. It signals that the growing movement to regulate the technology will also take into account how private companies and corporations make use of facial recognition and the specific issues their use of the technology can spur. 
Worry over private companies’ use of facial recognition has been growing steadily this year. In July, Reuters reported that the pharmacy chain Rite Aid had deployed facial recognition in 200 stores, often at locations in low-income and minority neighborhoods. There’s also concern over facial recognition that could be integrated into home security systems, like the ones sold by Amazon’s camera-doorbell company Ring, as well as lingering anxiety over the power of private companies like Clearview AI, which collected billions of photos without the consent of those pictured, and at one point had designs to provide this technology to private chains like Macy’s and Best Buy.
“We can’t let companies scoop up or profit from people’s faces and fingerprints without their consent,” said Sen. Jeff Merkley in a statement. “We have to fight against a ‘big brother’ surveillance state that eradicates our privacy and our control of our own information, be it a threat from the government or from private companies.”
A staffer familiar with the legislation said one purpose of the law is to increase consumer understanding of the pervasiveness of facial recognition, but they would not comment on how this law would impact specific companies. They explained that the law is modeled after Illinois’s Biometric Information Privacy Act, a powerful piece of state legislation that has cost Facebook $650 million in fines over its facial recognition-enabled tagging, and likely keeps Google from making available a facial recognition feature available on Nest home security cameras in the state. The statute has also kept Clearview from operating in Illinois following a lawsuit filed under that state’s law. 
That the Illinois law has already caused significant legal hurdles for existing providers of biometric technology is a sign that a national version of the same legislation likely won’t be well-received. After all, companies affected will have to secure their consent from consumers before collecting biometric information as well as let them know what information they already have on file. Companies will also be financially liable in any lawsuits linked to the new law. Taken together, these measures could drastically transform how facial recognition is sold and operated in the American private sector. 

  
    Related
  

  
    How can we ban facial recognition when it’s already everywhere?
  

It’s not yet clear how the written consent requirement would play out for companies as varied as Clearview AI, Facebook, and Amazon, which use and deploy the technology in different ways. Importantly, the law notes that the written consent can’t be combined with an employment contract or “any other consent or permission-seeking instrument or function.”  
The law isn’t about outlawing surveillance cameras or devices that take pictures of people’s faces. It’s specifically designed to target the software that’s used to extract biometric information, which is a specific, algorithmic process. Evan Greer, the deputy director of the digital rights group Fight for the Future, which supports the legislation, emphasizes that the stakes are higher when it comes to biometrics, which is data collected about your body. 
“If your Social Security number leaks, you can get a new Social Security number. If your credit card number leaks, you can get a new credit card number,” Greer told Recode. “If a biometric scan of your face leaks, you can’t get a new face.”
Greer added that regulation like the new bill by Sanders and Merkley would discourage the rapid adoption of this technology.
Merkley has been concerned with the private sector’s use of biometric technology since at least February when he discussed the topic in an interview with Recode. The Oregon senator also expressed concern over the data collected by companies like Clear, which aims to use biometrics to bring people back to public spaces amid the Covid-19 pandemic. Notably, Sanders has also been a critic of facial recognition; he was the first 2020 candidate to call for a complete ban on police use of facial recognition. He told Vox in December that “this technology is dangerous and under-regulated, and it represents an intrusion into the privacy rights of Americans.” 
Previous Senate legislation proposed to regulate commercial facial recognition include the Commercial Facial Recognition Privacy Act, a bipartisan proposal that was put forward by Sen. Brian Schatz and Sen. Roy Blunt, and the Algorithmic Accountability Act, a law that would empower the Federal Trade Commission to study their algorithms for bias, neither of which has passed. But the movement to clamp down on facial recognition seems to be growing, and there’s increasing demand from activists for a full ban on the technology for any application. 
“There’s a lot of businesses across the world that are harvesting and monetizing the biometrics, without us — the general public — really having a lot of knowledge about exactly what’s going on or consenting to the practice,” said India McKinney, the director of federal affairs of the Electronic Frontier Foundation, which also supports the legislation.
It’s unclear how much traction this newest proposal will receive, but the bill certainly sets a higher bar for companies selling facial recognition, a technology that remains broadly unregulated at the federal level. Still, the proposal is a clear indication that, as the move to rein in facial recognition continues, officials won’t just be worried about cops, but all the entities that can access this technology. While privacy advocates say that’s great news, companies selling facial recognition probably won’t feel the same. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Sens. Jeff Merkley and Bernie Sanders are proposing more federal regulation for facial recognition technology. Among other limits, their newly announced National Biometric Information Privacy Act would require private companies and corporations to get written consent from people in order to collect their biometric data. Should companies violate the consumer protection in the law, regular citizens and state attorneys general could sue them. 
Over the past few months, there’s been a national reckoning over facial recognition that’s centered on whether law enforcement should have access to this powerful technology. Major tech companies including IBM, Amazon, and Microsoft have now paused or scaled back facial recognition systems that are sold to police departments. Senators have even proposed legislation that would put a moratorium on its use by the federal government and discourage its use by law enforcement, which followed the news of the first known false arrest caused by police use of facial recognition. 
The new bill is a sign that officials aren’t just concerned about government or law enforcement having access to this controversial, biometrics-based technology. It signals that the growing movement to regulate the technology will also take into account how private companies and corporations make use of facial recognition and the specific issues their use of the technology can spur. 
Worry over private companies’ use of facial recognition has been growing steadily this year. In July, Reuters reported that the pharmacy chain Rite Aid had deployed facial recognition in 200 stores, often at locations in low-income and minority neighborhoods. There’s also concern over facial recognition that could be integrated into home security systems, like the ones sold by Amazon’s camera-doorbell company Ring, as well as lingering anxiety over the power of private companies like Clearview AI, which collected billions of photos without the consent of those pictured, and at one point had designs to provide this technology to private chains like Macy’s and Best Buy.
“We can’t let companies scoop up or profit from people’s faces and fingerprints without their consent,” said Sen. Jeff Merkley in a statement. “We have to fight against a ‘big brother’ surveillance state that eradicates our privacy and our control of our own information, be it a threat from the government or from private companies.”
A staffer familiar with the legislation said one purpose of the law is to increase consumer understanding of the pervasiveness of facial recognition, but they would not comment on how this law would impact specific companies. They explained that the law is modeled after Illinois’s Biometric Information Privacy Act, a powerful piece of state legislation that has cost Facebook $650 million in fines over its facial recognition-enabled tagging, and likely keeps Google from making available a facial recognition feature available on Nest home security cameras in the state. The statute has also kept Clearview from operating in Illinois following a lawsuit filed under that state’s law. 
That the Illinois law has already caused significant legal hurdles for existing providers of biometric technology is a sign that a national version of the same legislation likely won’t be well-received. After all, companies affected will have to secure their consent from consumers before collecting biometric information as well as let them know what information they already have on file. Companies will also be financially liable in any lawsuits linked to the new law. Taken together, these measures could drastically transform how facial recognition is sold and operated in the American private sector. 

  
    Related
  

  
    How can we ban facial recognition when it’s already everywhere?
  

It’s not yet clear how the written consent requirement would play out for companies as varied as Clearview AI, Facebook, and Amazon, which use and deploy the technology in different ways. Importantly, the law notes that the written consent can’t be combined with an employment contract or “any other consent or permission-seeking instrument or function.”  
The law isn’t about outlawing surveillance cameras or devices that take pictures of people’s faces. It’s specifically designed to target the software that’s used to extract biometric information, which is a specific, algorithmic process. Evan Greer, the deputy director of the digital rights group Fight for the Future, which supports the legislation, emphasizes that the stakes are higher when it comes to biometrics, which is data collected about your body. 
“If your Social Security number leaks, you can get a new Social Security number. If your credit card number leaks, you can get a new credit card number,” Greer told Recode. “If a biometric scan of your face leaks, you can’t get a new face.”
Greer added that regulation like the new bill by Sanders and Merkley would discourage the rapid adoption of this technology.
Merkley has been concerned with the private sector’s use of biometric technology since at least February when he discussed the topic in an interview with Recode. The Oregon senator also expressed concern over the data collected by companies like Clear, which aims to use biometrics to bring people back to public spaces amid the Covid-19 pandemic. Notably, Sanders has also been a critic of facial recognition; he was the first 2020 candidate to call for a complete ban on police use of facial recognition. He told Vox in December that “this technology is dangerous and under-regulated, and it represents an intrusion into the privacy rights of Americans.” 
Previous Senate legislation proposed to regulate commercial facial recognition include the Commercial Facial Recognition Privacy Act, a bipartisan proposal that was put forward by Sen. Brian Schatz and Sen. Roy Blunt, and the Algorithmic Accountability Act, a law that would empower the Federal Trade Commission to study their algorithms for bias, neither of which has passed. But the movement to clamp down on facial recognition seems to be growing, and there’s increasing demand from activists for a full ban on the technology for any application. 
“There’s a lot of businesses across the world that are harvesting and monetizing the biometrics, without us — the general public — really having a lot of knowledge about exactly what’s going on or consenting to the practice,” said India McKinney, the director of federal affairs of the Electronic Frontier Foundation, which also supports the legislation.
It’s unclear how much traction this newest proposal will receive, but the bill certainly sets a higher bar for companies selling facial recognition, a technology that remains broadly unregulated at the federal level. Still, the proposal is a clear indication that, as the move to rein in facial recognition continues, officials won’t just be worried about cops, but all the entities that can access this technology. While privacy advocates say that’s great news, companies selling facial recognition probably won’t feel the same. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  A year ago, Jeff and MacKenzie Bezos set records with the world’s biggest divorce settlement. On July 28, MacKenzie Bezos (now MacKenzie Scott) announced that she’s spent the time since aiming to set a much more inspiring record — for how fast she can give the money away. 
When the couple divorced in 2019, they were splitting the largest personal fortune in history, estimated at the time at about $145 billion. The couple announced a settlement in April 2019 that left Jeff Bezos 75 percent of his Amazon fortune, while Scott departed the marriage with $35 billion, making her at the time of the announcement the third-richest woman in the world (a recent Forbes ranking now has her at fourth).
Right away, Scott indicated that her approach to philanthropy would be profoundly different from the approach she and Bezos had used as a couple. Jeff Bezos’s forays into philanthropy have been limited. While the wealthiest man in the world, he has not signed the Giving Pledge to eventually donate a significant share of his wealth, and he’s donated a far smaller percentage of it than other ultra-wealthy figures like Bill Gates, Mark Zuckerberg, Warren Buffett, or Mike Bloomberg. When he has given, I’ve criticized his approach for a lack of rigor and clarity.
One month after the divorce, Scott signed the Giving Pledge that Bezos never did. “I have a disproportionate amount of money to share,” she wrote in her pledge letter. “I won’t wait. And I will keep at it until the safe is empty.” And it seems like she’s been acting on that declaration.
MacKenzie Scott’s deeply unusual approach to philanthropy 
A year later, she has published an update, and it’s an astonishing one. In the past year, she has donated $1.7 billion to 116 organizations working in areas of interest to her, from racial justice and LGBTQ equality to climate change and global health. 
All the organizations listed are established nonprofits, selected, Scott says, for their leadership’s “track record of effective management and significant impact in their fields.” The largest area of grants — $586.7 million — went to organizations working on racial equity, an issue where awareness has grown quickly over the past few months amid protests sparked by George Floyd’s killing by Minneapolis police. Other top priorities included economic mobility ($399.5 million), gender equity, public health, and global development (more on these below). 
The total amount — $1.7 billion — is obviously just a fraction of her fortune, but it is deeply unusual for billionaires to give away that much money this quickly, especially without a preexisting organization to do grant research and vetting. 
Her methods, too, are unusual. “It was a gift that just fell from the sky,” Jorge Valencia, executive director and CEO of one of those 116 organizations, the Point Foundation, told the Chronicle of Philanthropy. The organization, which offers scholarships to LGBTQ+ students, did not apply for a grant and had no connection to Scott. 
And while it’s common for philanthropists to give grants that are restricted for a specific purpose, paid out over the course of several years or conditional on various benchmarks for grant success, Scott says she did none of that. “I gave each a contribution and encouraged them to spend it on whatever they believe best serves their efforts. Unless organization leadership requested otherwise, all commitments were paid up front and left unrestricted to provide them with maximum flexibility,” she wrote in her announcement.
“It’s an interesting contrast to the more technocratic giving of the tech billionaires,” Rob Reich, a Stanford philosopher who writes about the role of philanthropy in society, told me. 
Another interesting contrast is the way Scott approached publicizing her giving. The announcement two years ago that Jeff Bezos planned to give $2 billion to education and homelessness charities attracted, Reich says, “fanfare with zero follow-up.” Almost two years later, the website for Bezos’s Day One Fund lists just under $200 million in grants, about 10 percent of the amount initially pledged. Half of the initial pledge was for education, and no progress in this area has been officially announced yet (though Bezos has posted updates on Instagram).
In 2020, Bezos announced on Instagram a planned $10 billion in grants to fight climate change through what he called the Bezos Earth Fund. The Bezos Earth Fund has no website. Bezos’s original Instagram post says that grants will start this summer, though they appear to have not yet started. 
All this is not unusual (and it doesn’t suggest that Bezos won’t eventually meet his commitments; he has paid out other grants he’s made, including $100 million to Feeding America for coronavirus relief earlier this year). Typically, philanthropic announcements get widespread coverage even if they are substantially in advance of the actual disbursement of money. And in some cases, money is disbursed to donor-advised funds or other instruments, which means they may take even longer to reach recipients. There is nothing wrong with taking your time to make grants if that means the grants do more good — but it’s easy for delays to mean that givers enjoy all the positive publicity of a major grant long before anyone’s life is improved by it.
Scott, by announcing her gifts only after she’d already disbursed all the money, avoids that pitfall — and could offer a glimpse of a new model of how to give, one that is focused on moving money quickly, not attaching any requirements or conditions, and shifting the power dynamics of the philanthropy world. 
Does this model of giving work?
Scott’s fast, massive disbursements and other recent experiments in quickly moving large sums of money to where they are needed, with much less review and fewer application steps than in traditional grantmaking, “weakens the case that giving away $1.7 billion is difficult,” Reich said. “There remains a question about whether it’s difficult to do well.” 
Giving away money very quickly with a minimal process does have some disadvantages. 
Many charitable interventions don’t work, and the differences between the best organizations and the average organizations can be quite large. It’s reasonable that many funders don’t want to take that chance. 
But there’s a good argument that at least some funders should be happy to make lots of grants, many of which may disappoint them anyway. Vetting often adds lots of overhead, delays, and communication problems for charities; a faster process that gets money where it’s needed sooner can make a big difference. In some specific fields (say, scientific research), studies have shown that all the effort-intensive work to find the “best” grants is fairly arbitrary; researchers don’t agree with each other’s rankings at all. In a case like that, you might as well just get money out the door, with minimal vetting — as Scott has done.
And in some areas, like coronavirus relief, getting money to people quickly is really important. If it takes months to make a grant and more months for the money to arrive, it may be too late to help. Scott donated to GiveDirectly, a nonprofit that gives people cash, no strings attached, and which has dramatically expanded its operations this year in order to help people around the world deal with the coronavirus crisis. 
Scott’s team reached out to GiveDirectly after having already done their research, GiveDirectly’s managing director Joe Huston told me. Very little staff time was tied up in making the donation happen. (The nonprofit tracks how many resources are expended per dollar raised and said that Scott’s gift was one of the lowest-scoring, on that metric, they could remember.) The money arrived in early June, and 95 percent of it was sent out to recipients within 10 days.
“The pandemic is giving donors experience in handing over the reins in philanthropy,” Huston told me, so that help can reach people as fast as it’s needed. “My hope is that when people are just looking to help, they’ll start with that in general.”
There are other pitfalls to trying to give away money quickly, though Scott avoided many of the biggest ones. Lots of donors making large gifts gravitate toward targets like Stanford, Harvard, or MIT — big research universities with well-staffed donor relations departments that can absorb enormous gifts. (“For the love of God, rich people, stop giving Ivy League colleges money,” my colleague Dylan Matthews wrote after one such mega-gift, and I agree.) Scott donated to several historically Black colleges and universities; in each case, her donation of $20 million to $40 million was the largest single donation in the school’s history. 
The money will help ”lift the financial burden off of deserving students and help make ends meet so they can focus on graduating on time,” Howard University said in a statement. “This pure act of benevolence is clearly a game-changer and it could not have come at a better time,” Hampton president William R. Harvey told the HBCU Digest.
In general, picking organizations run by people affected firsthand by the injustices Scott targeted was a priority. “On this list, 91 percent of the racial-equity organizations are run by leaders of color, 100 percent of the LGBTQ+ equity organizations are run by LGBTQ+ leaders, and 83 percent of the gender-equity organizations are run by women, bringing lived experience to solutions for imbalanced social systems,” she wrote in her note announcing the gifts. 
That fact might provide a useful lens for evaluating her donations. MacKenzie Scott does not know how to solve racial justice, women’s rights, or LGBTQ+ equality. She just happens to, unlike most of us, be in possession of $35 billion, and so she decided that if she gave much of that money to Black activists and LGBTQ+ activists and women’s activists, probably they would be better suited than she is to figure out how the money could be spent to solve those problems. 
The same theme recurs in Scott’s letter and in nonprofits’ descriptions of her process. There wasn’t very much vetting because Scott does not particularly expect that she’s better at vetting than these organizations are. There weren’t restrictions on the grants because Scott does not particularly believe she’s more suited than the recipients to guess what restrictions would be useful. She is “trusting the leaders of the organizations chosen,” Reich told me, “with a very deliberate eye toward leaders with the lived experience of the work they’re doing.”
There’s something deeply inspiring about that. I am in favor of philanthropists putting in the work to identify the most effective approaches to social problems and direct their money with precision where it will do the most good, when they have the resources to do that. I think that work is often well worth the effort. 
“There’s room for the bigger foundations, the Bill and Melinda Gates Foundation, that kind of heavyweight model,” Huston told me when I asked whether more philanthropists should be imitating Scott. “But I’m glad there’s more examples, like [MacKenzie] Bezos, like Twitter’s Jack Dorsey,” where philanthropists make donation decisions quickly and trust the decision-making to others.
If you have $35 billion, that fact does not in itself make you qualified to figure out how to fix the world — and if you think that other people are more qualified, you might decide the best plan is to just shovel the money out the door so they can run with it. That seems to be MacKenzie Scott’s approach to philanthropy so far — and a society grappling with the role of billionaires in our world and in our giving should be watching.
Sign up for the Future Perfect newsletter and we’ll send you a roundup of ideas and solutions for tackling the world’s biggest challenges — and how to get better at doing good.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



A teenager in Florida allegedly played a major role in the massive Twitter hack earlier this month that commandeered some of the platform’s highest profile accounts, including Elon Musk’s and former President Barack Obama’s, to scam people out of about $120,000 in bitcoin.
Graham Ivan Clark, 17, was charged with 30 felonies related to the hack, according to a local news station in Tampa, Florida, where he lives. Though federal authorities led the investigation, Clark was charged by the state’s attorney because, state attorney Andrew H. Warren said, Florida law makes it easier for Clark to be tried as an adult.
Two adults — Mason John Sheppard, 19, of the United Kingdom, and Nima Fazeli, 22, of Orlando, Florida — were also charged by the Department of Justice with felonies related to the hack. Sheppard was charged with three felonies, and Fazeli was charged with one. There may be more arrests to come; the charging documents say an as-yet-unidentified hacker named “Kirk” “played a central role.” This is consistent with TechCrunch’s earlier reporting that said a hacker named “Kirk” was behind the attack.
“We appreciate the swift actions of law enforcement in this investigation and will continue to cooperate as the case progresses,” Twitter said in a statement. 
Though initial reports said the hack might be an inside job, given how much access the perpetrator had to the company’s internal controls, Twitter now says its employees were targeted by a “phone spear phishing attack”:
Not all of the employees that were initially targeted had permissions to use account management tools, but the attackers used their credentials to access our internal systems and gain information about our processes. This knowledge then enabled them to target additional employees who did have access to our account support tools. Using the credentials of employees with access to these tools, the attackers targeted 130 Twitter accounts, ultimately Tweeting from 45, accessing the DM inbox of 36, and downloading the Twitter Data of 7. 
Assuming this is true, it should serve as a cautionary tale. Spear phishing via mobile devices has become more common, especially since people don’t check links on their mobile devices the way they might in a message received on their computers.
“People often overlook their phone because they think of it more as a personal device, not a work device,” Mark Ostrowski, security evangelist at cybersecurity company Check Point, told me back in May when I wrote about how to improve cybersecurity hygiene while working from home.
The details of the hack suggest that Twitter employees should have practiced better cyber hygiene, and there was nothing the account holders themselves could have done to prevent what happened.
“We will continue to organize ongoing company-wide phishing exercises throughout the year,” Twitter said in a statement shortly after the hack.
Details from the charging documents appear to show that finding the alleged hackers wasn’t a heavy lift for investigators. Fazeli and Sheppard’s Discord handles, where they allegedly discussed purchasing access to hacked accounts with “Kirk,” were the same as their handles on a forum for people interested in acquiring “OG” Twitter accounts, which are typically very short (one letter or number each) and among the first profiles created for the service. Using that forum’s records, investigators were able to link those accounts to email addresses, Coinbase accounts, and IP addresses that made identifying them fairly simple. Fazeli, for example, used his real name in his email address, which he verified with  his driver’s license. 
Lawmakers blame Twitter for lax security
Politicians on both sides of the aisle had scathing words and warnings for Twitter in the wake of the mid-July attack, which caused 45 accounts to request bitcoin from their followers, promising they would receive double their donation in return. The hacker also, as stated above, was able to access 36 accounts’ direct messages and seven accounts’ Twitter data. But, politicians stressed, the breach — and its consequences — could have been much worse, and they demanded that Twitter do better to stop something like this from ever happening again.
Sen. Ron Wyden, a Democrat from Oregon, expressed concern over the security of direct messages in the attack and said Twitter hadn’t done enough to protect them, despite previous assurances that it would. In a statement, the senator told Recode that he felt let down by Twitter and its executives, especially as they promised him they would improve their security:
In September of 2018, shortly before he testified before the Senate Intelligence Committee, I met privately with Twitter’s CEO Jack Dorsey. During that conversation, Mr. Dorsey told me the company was working on end-to-end encrypted direct messages. It has been nearly two years since our meeting, and Twitter DMs are still not encrypted, leaving them vulnerable to employees who abuse their internal access to the company’s systems, and hackers who gain unauthorized access. While it still isn’t clear if the hackers behind yesterday’s incident gained access to Twitter direct messages, this is a vulnerability that has lasted for far too long, and one that is not present in other, competing platforms. If hackers gained access to users’ DMs, this breach could have a breathtaking impact, for years to come.
Meanwhile, others drew direct lines between the threats exposed by the breach and the upcoming presidential election. Sen. Richard Blumenthal blamed Twitter for its “repeated security lapses” and “failure to safeguard accounts” that could have caused the incident.
“Count this incident as a near miss or shot across the bow,” Blumenthal, a Connecticut Democrat, said in a tweet. “It could have been much worse with different targets.”
Sen. Josh Hawley, a Republican from Missouri who has been a frequent Big Tech critic in his short DC tenure, tweeted a letter that he said he sent to Twitter CEO Jack Dorsey even as the attack was happening.
“Millions of your users rely on your service not just to tweet publicly but also to communicate privately through your direct message service,” Hawley wrote. “A successful attack on your system’s servers represents a threat to all of your users’ privacy and data security.”
Hawley then asked how accounts protected by two-factor authentication could possibly be hacked, if user data was stolen, and what measures Twitter takes to prevent system-level hacks.
As Massachusetts Democratic Sen. Edward Markey said, both the service and its users mostly dodged a considerable bullet.
“While this scheme appears financially motivated and, as a result, presents a threat to Twitter users, imagine if these bad actors had a different intent to use powerful voices to spread disinformation to potentially interfere with our elections, disrupt the stock market, or upset our international relations,” he said in a statement to Recode. “That is why Twitter must fully disclose what happened and what it is doing to ensure this never happens again.”
As for why arguably the most high-profile and influential Twitter account of all, President Trump, wasn’t affected by the hack, it’s possible that his account has special safeguards that the other accounts didn’t. Trump’s Twitter account was famously deleted by an employee in 2017, so it would make sense that Twitter put things in place to prevent that from happening again. Now we’ll see what the social media platform does to protect the rest of its users.
Update, July 31, 2020, 5:15 pm: Updated to include information about the arrests and details about how the hack occurred.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




    
    
  

    
  
    
    
      
        
  


  






      
    
    
  
  



NASA’s Perseverance rover launched at 7:50 am ET on July 30, the first day of a flight that will take the fifth NASA rover to Mars. During its mission, the boxy, car-sized vehicle and its extendable arm will be charged with looking for signs of ancient life and gathering data about Mars’s geology and climate. It will even lay the groundwork for eventual human exploration of the planet. 
To make all that possible, the rover carries a stunning display of technology designed especially for Perseverance’s historic mission, from pieces of a new spacesuit to an autonomous helicopter, the first aircraft ever sent to another planet. Those tools will help the rover gather data about the planet’s atmosphere, which it can then send back to NASA. There’s also an excavation system that can collect high-quality samples of Martian soil to be stashed and later analyzed by a future mission to Mars. 
In the years the new rover is expected to operate, these machines will battle challenges that terrestrial technology never has to deal with, including the Red Planet’s super-thin atmosphere, limited resources, incredibly cold temperatures, and delayed communication with human overlords on Earth. 
To give you an idea of how all this will happen, we’ve outlined some of the coolest features that will be on display when Perseverance finally arrives on Mars in February. 
Perseverance is armed with advanced self-driving tech
Key to its mission’s success is the ability for Perseverance to self-drive. The vehicle has a computer devoted to its autonomous capabilities, and as Wired explains, it was designed and built specifically for this mission. The autonomous driving feature is essential because Mars is simply too far away for humans to give the vehicle constant, real-time instructions. So the rover needs to fend for itself. 
“One of the fundamental constraints of any kind of space exploration — whether you’re going to Mars or Europa or the moon — is that you have limited bandwidth, which means a limit on the amount of information you can send back and forth,” David Wettergreen, research professor at Carnegie Mellon’s Robotics Institute, told Recode. “During the periods of time when the robot can’t communicate, autonomy is important for it to enable it to keep doing tasks, to explore on its own, to make progress, rather than just sitting there waiting for the next time it hears from us.”

But building an autonomous vehicle for Mars is not necessarily as easy as building a self-driving car here on Earth (and that’s not easy, either). For one thing, the vehicle needs to be primarily concerned with safety, not with speed or the comfort of its passengers. After receiving basic instructions from humans about where it needs to go, Perseverance has to figure out the least-dangerous route on its own. If it crashes, the rover might render itself useless.
“Mars is not a fixed, flat, nice, paved road. Mars is really challenging terrain. There is dirt, rocks, sand, slopes, cliffs — all these things that the rover is going to have to avoid,” explained Philip Twu, robotics system engineer at NASA. “In addition to cameras, the rover is also going to need computers, algorithms, and software to be able to process all that imagery data into essentially a 3D picture that it’s then going to go ahead and use to plan.”
Fortunately for Perseverance, Mars is not a place where a self-driving rover needs to worry about crashing into another car or hitting a pedestrian. 
“On Mars, there’s nothing moving around,” said Wettergreen. “They’re moving slowly, so they can take the time to build a detailed model, do a lot of analysis on that model, and then decide what to do next.”
A robotic arm will take samples of Mars that will be studied back on Earth
The vehicle is also armed with a 7-foot-long arm equipped with a drill that’s designed to collect rock and soil samples from beneath Mars’s surface. Those samples will then be stored in as many as 43 containers that the rover carries around on the planet. Once those samples are collected, they’ll be left in tubes that will sit on Mars’s surface for a future mission to pick up. 
The arm alone isn’t all that impressive as a piece of space technology. Instead, its virtue is all the stuff that it comes, well, armed with. 
“It’s like a Swiss Army knife of scientific instruments,” said Wettergreen. “What’s so amazing about it is all of these different functionalities and capabilities that they’ve been able to pack into such a small package.” 
For instance, on the arm is a robotic claw equipped with a laser and other tools, including a camera called Watson that NASA compares to “a geologist’s hand-lens, magnifying and recording textures of rock and soil targets,” which is part of a tool — fittingly named Sherloc — that comes with special spectrometers and a laser. There’s also a tool called PIXL that can analyze incredibly tiny chemical elements and, in NASA’s words, take “super close-up pictures of rock and soil textures” to help scientists figure out whether Mars could have been home to microbial life in the past. 
High-tech cameras and microphones will give the rover “senses”
Integrated into the rover are a slew of extremely high-quality cameras — 23 in total — that will help the vehicle survey the planet. The cameras won’t just help Perseverance get around Mars, but they’ll also take images of samples collected on the planet and record the vehicle’s arrival on the surface in full color. Meanwhile, NASA says that so-called “engineering” cameras will take on tasks like helping the vehicle avoid potentially treacherous areas, like sand dunes and trenches, while others will help the system navigate without human intervention. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        There are 23 cameras aboard Perseverance.
      
      
        NASA
      
    
  


At the same time, the rover will pick up sound data through its two microphones. Those devices will listen to the rover as it arrives and travels on the planet. There’s a special microphone that works in conjunction with a laser to study the chemistry of the planet’s geology by zapping it and recording the sound of the zapping. As NASA explains, the microphone hears the intensity of the “pop” made by the laser turning the rock into plasma, which “reveals the relative hardness of the rocks, which can tell us more about their geological context.” 
A self-driving helicopter will fly on another planet. That’s a first.
Also aboard the rover is Ingenuity, which will — if all goes as planned — be the first helicopter to fly on Mars as well as the “first aircraft to attempt controlled flight on another planet,” according to NASA. That makes Ingenuity an experiment on its own, one that has undergone extensive testing on Earth. Its mission is to demonstrate that flight on Mars, where it will conduct up to five test flights, is possible, and that flights can be conducted autonomously on the planet. 
While the device is essentially a drone, it’s specially crafted for Mars, which has less gravity than Earth. This makes ascent easier, but due to the planet’s comparatively thin atmosphere, flight itself is more challenging. As The Verge reports, the blades of the helicopter can make more than 2,000 revolutions a minute, several times the speed of helicopter blades whipping around in Earth’s atmosphere. Ingenuity is incredibly light, weighing in at around 4 pounds.

But the tiny vehicle’s autonomy is not just designed to help with navigation; it’s also build to keep Ingenuity alive. 
“Mars is very, very cold. It gets to about negative 130 degrees Fahrenheit at night. That’s pretty cold,” explained Twu. “So the autonomy onboard the helicopter is also involved with finding a way to keep the helicopter warm enough to survive all the Martian nights.” 
If the helicopter is ultimately successful, it will help NASA make decisions about where flight could lend assistance during future missions to the planet. Similar drones could serve as scouts that survey the terrain of Mars — especially places that rovers can’t easily get to  — or, as NASA says, become “full standalone science craft carrying instrument payloads.” 
Will we be seeing any of this tech on Earth one day? It’s hard to say right now, but Twu notes that NASA is famous for its spinoffs. 
“Time and time again, we’ve seen that technology developed for NASA missions — a lot of them for space missions — end up having terrestrial applications here on Earth,” he said. “All technology development can cross-pollinate and advances in one area inevitably result in advances in other areas.” 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  Amazon registered nearly $89 billion in sales and $5 billion in profit over the last three months, setting company records on both figures and blowing away Wall Street expectations as Covid-19 pandemic shutdowns pushed more shoppers online and into Amazon’s arms.
CEO Jeff Bezos had said last quarter that the company planned to spend around $4 billion on pandemic-related health and safety efforts for its workers throughout April, May, and June, and that those expenses might wipe out all of the company’s profit over the quarter. Instead, Amazon customers surprised company executives by expanding their pandemic-driven purchases beyond low-profit goods like groceries — sales of which still tripled year over year in the quarter — into more profitable “hardline” categories like electronics and “softline” goods like clothing. That’s what helped the company set a record profit, eclipsing the previous record of $3.6 billion in profit in the first quarter of 2019. 
“Prime members ... were shopping more often and with larger baskets,” Amazon Chief Financial Officer Brian Olsavsky told reporters on a media call Thursday afternoon. Recode previously reported that Prime membership sales have risen significantly since the crisis began.
For months, the pandemic has transformed Amazon from a popular convenience shopping destination into an essential service as stay-at-home orders pushed more shopping online and as non-grocery brick-and-mortar chains have suffered from temporary store closures. But it also posed challenges for the tech giant. The company suffered from long delivery times as it reworked its warehouse policies to keep up with demand; along the way, it hired 175,000 warehouse and delivery workers to help with the sales boom and to fill in for workers who got sick or chose to stay home.
But the company also faced a labor crisis as some workers said the company’s warehouse safety measures were not adequate or consistent enough, and decried the company’s move to end bonus hourly and overtime pay at the end of May. Amazon ended up announcing another one-time bonus in June, declining to label the bonus as hazard pay, describing it instead as an appreciation for workers dealing with increased demand during an unprecedented time. An Amazon spokesperson on Thursday refused to say whether the company might reinstate the extra hourly or bonus pay even though increased orders and sales show no signs of abating for the company and its workers.
The record quarterly profit and sales figures, which eclipsed Wall Street analyst expectations by a staggering $8 billion, highlight Amazon’s growing power in the US economy. And it comes a day after a committee of US lawmakers grilled Bezos, the world’s richest man, on the business practices that Amazon has used to ascend to its increasingly dominant position. 
Much of the lawmakers’ questioning on Wednesday focused on how the company treats and competes with the third-party sellers that account for more than half of the items Amazon sells in its online stores. Amazon revenue from these sellers grew 53 percent in the last quarter, up from growth of just 25 percent in the same quarter last year. Amazon CFO Olsavsky told Recode on the media call that growth in Amazon’s Fulfillment by Amazon  (FBA) service — which lets sellers qualify their goods for Prime shipping by paying Amazon to handle warehousing and shipment of goods — was a main driver in seller fees. 
Some critics have argued that Amazon coerces sellers into using FBA by giving better placement to goods that ship with Prime and by penalizing them for late deliveries if they ship on their own. One seller told federal regulators in a letter last year that the practice forces him to charge consumers higher prices than he otherwise would. Amazon, including Bezos in his congressional testimony, has defended FBA as a service that saves sellers considerable hassle. 
While Amazon’s business continues to benefit from the pandemic, some other mass retailers are also seeing a boost. Both Walmart and Target have posted standout earnings reports in recent months, and their market share in US online commerce has increased, according to shopping data research firm Rakuten Intelligence, though Amazon’s share is still around seven times larger than Walmart’s.
  
  
  
      




  Twitter and Facebook are not fact-checking factually unsupported claims about mail-in voting in the 2020 election posted by President Trump on both platforms Wednesday. The inaction has angered some critics, who say these companies allow the spread of dangerous misinformation to go unchecked online, harming the integrity of the election as a whole.
Trump’s posts made the completely unsubstantiated assertion that universal vote-by-mail, a practice in which states automatically mail a ballot to all registered voters, will lead to the most “inaccurate” and “fraudulent” election in history, and then went on to suggest delaying the election “until people can properly, securely, and safely vote.” 


With Universal Mail-In Voting (not Absentee Voting, which is good), 2020 will be the most INACCURATE & FRAUDULENT Election in history. It will be a great embarrassment to the USA. Delay the Election until people can properly, securely and safely vote???— Donald J. Trump (@realDonaldTrump) July 30, 2020



Top Republican and Democrat legislators quickly shut down Trump’s suggestion that the US should delay the election, something the president would need the Democrat-controlled House of Representatives’ approval for. It would be illegal if he tried to delay the election using his own authority. 
Meanwhile, as my Vox colleague Ian Millhiser has explained, there is no substantial evidence that voting by mail will lead to inaccurate or fraudulent results. In fact, Oregon has only seen about a dozen cases of fraud out of 100 million mail-in ballots in the past two decades. Trump has nevertheless made a habit of casting doubt on the legitimacy of mail-in voting. The president’s latest comments are part of a larger pattern of him attacking the integrity of the 2020 election by making untrue statements on the matter.
So why aren’t Facebook and Twitter doing anything about the misinformation in Trump’s latest post?
A spokesperson for Facebook told Recode that the company won’t take action because of its long-standing policy of not fact-checking politicians. (The company did place a link to voter registration information underneath Trump’s post, something it does for all posts about voting in the election.) The hands-off policy toward moderating politicians is in line with CEO Mark Zuckerberg’s philosophy of making Facebook an open platform for discussion and not being an “arbiter of truth” on political matters. This is something civil rights organizations, advertisers, and even some of Zuckerberg’s own employees have increasingly decried.
Twitter, however, does reserve the right to fact-check politicians under its misinformation policies. In May, the company placed a warning label over a pair of Trump posts falsely claiming that mail-in voting in California would lead to fraud. When Recode asked Twitter why the company fact-checked Trump on his earlier post containing false claims about mail-in voting but failed to do so this time around, a spokesperson for the company told Recode that, under the company’s policies, Twitter does not take down “broad, non-specific statements” about the integrity of elections or civic processes. 
Social media companies’ inaction on the posts was swiftly met by resistance from civil liberties groups such as Color of Change and the NAACP Legal Defense Fund.


.@Twitter: Trump’s latest tweet should be removed. He needs to be removed from your platform. Elections in many states will rely on the very mail-in ballots he’s falsely tweeting about & his ability to spread mass voter misinformation here only aids his plans of being re-elected— ColorOfChange (@ColorOfChange) July 30, 2020



Similarly, Jesse Lehrich, the co-founder of the nonprofit Accountable Tech, which is pushing Facebook to tighten its rules on harmful speech, warned that by doing nothing social media companies could exacerbate the problem of misinformation online. “Platforms should immediately remove — or clearly label and limit the reach of — this morning’s post and implement election misinformation policies that are responsive to the threats we face,” Lehrich said in a statement.
It’s understandable why tech companies like Facebook and Twitter don’t want to be seen as overly meddling in political issues, as this could invite complaints that they’re favoring one side over the other. In a congressional hearing on antitrust on Wednesday, Republicans hammered Zuckerberg as well as Google CEO Sundar Pichai for alleged “anti-conservative bias.” To support these accusations, the lawmakers offered shaky evidence, including the removal of harmful conspiracy theories promoted by Trump and his family related to the coronavirus. 
Trump’s recent statements also come at a time when his administration is facing accusations of hindering the United States Postal Service. While the USPS has long been struggling, particularly during the pandemic, it’s faced new reports of financial strain under a new, Trump-appointed postmaster general, Louis DeJoy. Under DeJoy’s leadership, the USPS has reportedly been planning to delay mail delivery and slash post office hours to save money.
One thing that his latest posts made clear is that Trump is escalating his attacks on the legitimacy of the US election process via social media — which, if it translates to real-world action, could send the country into what many have argued is dangerous and potentially constitutional-crisis territory. The question for Facebook and Twitter is to what extent they will continue to be hands-off.
  
  
  
      




  On Wednesday, four of the most powerful people on the planet testified before Congress. The CEOs of Amazon, Apple, Facebook, and Google — who together control companies worth nearly $5 trillion — called in individually via videoconference and fielded questions for nearly six hours. Some questions were tough. Some were just all over the place.
It was a scattered and, at times, awkward hearing. Members of the House Judiciary Subcommittee on Antitrust received only five minutes at a time to ask all four executives questions. The event was also full of all the technical glitches you can expect in a standard video chat, as well as some anticipated but nevertheless distracting political drama.
Democrats focused on whether the companies are engaged in anti-competitive practices that crush competitors and harm consumers. Much to Democrats’ frustration, many Republicans diverted from antitrust issues to focus on unfounded claims of anti-conservative bias on social media platforms and Google.
While the marathon hearing won’t result in any immediate political or regulatory action, it could apply pressure to antitrust regulators at the Federal Trade Commission and Department of Justice and set the stage for broader government probes into these companies’ business practices and the laws that have failed to rein them in. The event also stands to influence the public’s opinion of tech at a time when nearly half the country thinks the industry needs government regulation. Naturally, some emerged from the debacle looking like winners. There were also some clear losers.
Winners
Rep. Pramila Jayapal
Pramila Jayapal, a Democratic Congress member from Washington, came prepared with sharp lines of questioning for the witnesses, particularly for Amazon CEO Jeff Bezos and Facebook CEO Mark Zuckerberg, who she grilled about the 2012 acquisition of his company’s then-competitor Instagram.
Jayapal directly quoted internal documents to press Zuckerberg when he repeatedly denied strong-arming Instagram founder Kevin Systrom into a deal. Instead of moving on to another question, Jayapal read back old emails and strategically reminded Zuckerberg that he was under oath to tell the truth. After a tense back-and-forth, the Facebook CEO told Jayapal he disagreed with her characterization of his messages to Systrom. 
“I think it was clear that this was a space that we were going to compete in one way or another,” Zuckerberg said. “I don’t view those conversations as a threat in any way.” 


.@RepJayapal asks Facebook CEOs Mark Zuckerberg about copying and threatening smaller competitors like Instagram and Snapchat in order to pressure them into selling to the company.Full video here: https://t.co/CNTNmIDxiY pic.twitter.com/Itwu0lRf77— CSPAN (@cspan) July 29, 2020



But Jayapal, who had clearly done her homework, had more proof, citing messages in which Systrom shared his worries with an investor that Zuckerberg would go into “destroy mode,” copying and crushing his app if he rejected a deal.
“Facebook is a case study in monopoly power, because your company harvests and monetizes our data and then your company uses that data to spy on competitors and to copy, acquire, and kill rivals,” said Jayapal at the end of her round of questioning on the Instagram deal.
Jayapal’s questions produced some of the most damning evidence presented at the antitrust hearing. It demonstrated how Facebook’s market dominance can strike fear in its competitors, even if it’s still undecided whether that fits the legal threshold for anti-competitive behavior.
Tim Cook
Even though his company has faced intensifying allegations of antitrust violations in recent weeks, Apple’s CEO Tim Cook only received about half of number of questions as his tech CEO rivals, according to an analysis by VentureBeat.
Cook did face some tough questions about whether Apple takes too big of a cut from the revenue of third-party developers that sell apps in its App Store. Spotify, for example, has very publicly complained about the issue, leading to European regulators launching a pair of antitrust investigations in June. Yet, on Wednesday, members of Congress didn’t focus their ire on Apple’s issues as they did on those of the other tech CEOs.
Overall, Cook won by laying low. To date, he has been masterful at staying quiet when he needs to in order to win at the hyperpartisan politics of the Trump era. And Wednesday’s hearing proved to be no exception. 
Losers
Mark Zuckerberg 
The Facebook CEO, who’s testified at congressional hearings before, faced some of the toughest questions from lawmakers on both sides. Some argue that the young executive held up well to questioning, but the fact that Zuckerberg became the focus of so much ire undoubtedly directed negative attention to Facebook.
Democrats pressed Facebook about its inability to stop hate speech and misinformation from spreading on the platform. But where Zuckerberg struggled was in the line of questioning from members like Jayapal, who made strong cases for how Facebook has engaged in anti-competitive business practices over the years in what Jayapal called a “copy, acquire, and kill” strategy toward pushing competitors out of business.  
Zuckerberg defended Facebook on this point in several ways. First, he argued that if Facebook hadn’t acquired smaller rivals like Instagram, those startups may never have succeeded at the scale they’re at today because they needed Facebook’s support and talent to grow. Second, Zuckerberg said that Facebook isn’t really a monopoly because it competes with companies like Google on ads, Amazon on commerce, and TikTok on social. Third, he claimed that if United States regulators break up tech giants like Facebook, other Chinese competitors (hint: TikTok) who are “less committed” to American values could take over.
But Zuckerberg’s grilling from Congress didn’t stop with acquisitions. Zuckerberg faced a slew of angry questions from Republicans who accused, with scant evidence, that Facebook is inherently biased against conservatives. Even if these claims are unfounded, they left Zuckerberg defending his company from all kinds of angles.
Sundar Pichai
The famously soft-spoken, even-keeled Sundar Pichai, CEO of Google, was forced into controversy at Wednesday’s hearing, where he faced a hammering over Google’s privacy practices and dominance in the ads market.
Like Zuckerberg, Pichai was also on the receiving end of a lot of hell about largely unfounded accusations of political bias on Google. But the partisan Republican attacks didn’t end there. Several Republicans, like Reps. Ken Buck and Matt Gaetz, also took issue with Google dropping a major contract with the Pentagon to develop AI to be used in drone warfare. The project, nicknamed Maven, was abandoned in part due to thousands of Google employees’ open dissent, arguing that they didn’t sign up to be in the business of war. 
In his questioning, Pichai acknowledged that employees’ anger about Maven factored into the decision to drop the project, which is an obvious and understandable conclusion that Gaetz framed as some kind of bombshell revelation. Pichai also said the company is actively working with US government agencies. 


“We are not working with the Chinese military.”Rep. Gaetz asks Google why an American company would aid the Chinese military pic.twitter.com/yG1r9oR8w7— Bloomberg QuickTake (@QuickTake) July 29, 2020



This all puts Pichai in a tough spot, though. On the one hand, he has to keep his engineers — a narrow pool of talent who are notoriously difficult to recruit — happy. And many of those engineers are vehemently against developing technology that can cause physical harm. At the same time, as evidenced in the hearing, Pichai has to answer to angry lawmakers who see any refusal not to engage in military or police contracts as unpatriotic. 
Google’s previous work to build a search engine in compliance with the Chinese Communist Party’s strict censorship was also a subject of questioning by Republicans, who accused Pichai of cozying up to the Chinese government.
For Google, all of this matters because it’s the company most likely to face potential antitrust action in the near future. The Republican-led DOJ is expected to file a lawsuit against Google this summer. So you might say there was no chance for Pichai to win on Wednesday.
Both winners and losers
Jeff Bezos
The Amazon CEO and world’s richest man was calm, polite, and mostly direct in his answers during his first time testifying before Congress. He also survived more than an hour at the beginning of the hearing without being asked a single question thanks to a reported tech issue and didn’t face much in the way of “smoking gun” internal documents that would have put him in a tougher spot when the queries finally began.
But Bezos’s testimony did little to quash a variety of concerns about the power Amazon wields over its 1.7 million small and midsize sellers. These include suspicions over how the company uses data to develop competing products as well as its penchant for suspending or banning sellers with little notice or explanation. Bezos was also specifically called out for increasing the cut of sales that Amazon charges smaller sellers, a sum they must pay in order to have a shot at success on the giant shopping marketplace. 
When confronted with anecdotes or accusations of Amazon abusing its power as the ruler of its marketplace, Bezos would express his disapproval, vow to look into it, and try to assure lawmakers that the problems weren’t systemic. Even if he’s right and the abuses are a series of one-offs, the pattern shows that Amazon has grown so powerful that it can massively disrupt or upend its own partner businesses — even it does so by neglect or by accident. 
Rep. Jim Jordan
The Ohio Congress member was the most outspoken of his Republican colleagues in attacking the tech giants, claiming that they censor or otherwise suppress conservative viewpoints in various ways. “I will just cut to the chase. Big Tech is out to get conservatives,” he said in his opening. So if success for Republicans was based on airing as many unproven theories of bias as possible, Jordan was the star. And if you watched the hearing and didn’t know who he was before, you certainly do now.


"Put your mask on!"Shouting breaks out among members of the House subcommittee during tech hearing, after Rep. Mary Gay Scanlon suggests Rep. Jim Jordan is pushing "fringe conspiracy theories" https://t.co/83sKht0bRx pic.twitter.com/E6fEZKT6tO— CBS News (@CBSNews) July 29, 2020



But Jordan wasted precious opportunities to ask hard questions of the tech titans on the actual task at hand: to document whether or how they abuse their power over partners and competitors. To a certain extent, these hearings are a form of theater and most politicians are looking for a catchy sound bite showing them speaking truth to power. If you give an amazing performance for a different show than the one the audience paid for, though, you deserve to be booed off the stage.





  
  
  
      




  You could be forgiven for missing it, given the surplus of news, but the past few years have seen a profusion of climate change commitments from big tech companies. Facebook, Google, Amazon, and Apple have all promised to shrink their climate footprints, each attempting to outdo the others.
Climate advocates are naturally leery of these commitments. Those who lived through the faddish interest in climate in the mid-2000s, around the release of Al Gore’s An Inconvenient Truth, will recall the endless torrent of breathless corporate announcements. NBC had a “green week,” big corporations bought cheap offsets to become “carbon neutral,” automakers sold SUVs with vegan leather seats, and dozens of companies sold “sustainable” coffee cups, T-shirts, and tchotchkes. It was a greenwashing parade.
But times really have changed. The steps tech companies are taking these days represent a sea change in engagement. Climate change has moved out of the public relations department, into the C-suite, and down to the shop floor. 
To explore the strength of recent corporate climate commitments (and their limits), I want to focus in on Microsoft, a widely acknowledged leader in the field. Earlier this year, it committed not just to reducing its emissions but to going carbon negative, wiping out all the carbon the company and its suppliers have emitted since its founding in 1975. In recent weeks, Microsoft has released a flurry of announcements updating its progress, so now seems like a propitious time to take a close look.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Microsoft president Brad Smith, CFO Amy Hood, and CEO Satya Nadella preparing to announce Microsoft’s plan to be carbon negative by 2030.
      
      
        Brian Smale/MSFT
      
    
  


Over the past week, I’ve been talking to corporate sustainability experts and people who have worked with, and at, Microsoft. I tried to piece together how big a deal its work on climate is — how seriously to take it, what influence it may have, and where it might fall short.
To spoil the ending: It is a big deal. The company is setting new standards, especially in the rigor and transparency it is applying to the effort, and it is deliberately attempting to bring other companies, both suppliers and competitors, along with it into a world of shared metrics and data. There is more it could do, but it is earning its good climate reputation.
I’ll dig into what Microsoft is doing and what makes it unusual. But first, some background.
A quick note on kinds of emissions
In the carbon world, the emissions of a company (or person, city, or country) can be divided into three buckets:


Scope 1 emissions come directly from resources the business owns or controls, like furnaces or delivery vehicles.

Scope 2 emissions come from the power plants that generate the electricity the business uses.

Scope 3 emissions are indirect, “embedded” in the materials and services the business uses, representing the emissions of the full supply chain. (Business travel is a common example — there are carbon emissions embedded in every plane ticket.)

In the early days of corporate climate engagement, companies typically measured and reduced only their direct energy emissions (scope 1 and 2). But in the past several years, in part thanks to the example set by companies like Dow, Unilever, Apple, and Microsoft, measuring and taking responsibility for scope 3 emissions has become the new norm. 
This is significant, because for most companies, including Microsoft, scope 3 emissions are substantially larger than scope 1 and 2 combined. 
“At Microsoft, we expect to emit 16 million metric tons of carbon this year,” president Brad Smith wrote in a January blog post. “Of this total, about 100,000 are scope 1 emissions and about 4 million are scope 2 emissions. The remaining 12 million tons all fall into scope 3. Given the wide range of scope 3 activities, this higher percentage of the total is probably typical for most organizations.”
Microsoft has a recent history as a sustainability leader
On Monday, Microsoft announced it has completed the largest-ever test running data center servers on hydrogen fuel cells, which can be powered by zero-carbon hydrogen generated from renewable energy. Currently, even if they run entirely on renewables, data centers have diesel generators on site for long-term backup in case of an outage. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Power Innovations built a 250-kilowatt fuel cell system to help Microsoft explore the potential of using a hydrogen fuel cells for backup power generation at data centers. In a proof of concept, the system powered a row of servers for 48 consecutive hours.
      
      
        Power Innovations
      
    
  


With 160 data centers worldwide and multiple generators per data center, that adds up to a lot of diesel generators. The company has pledged to phase them all out by 2030. That’s why it is testing fuel cells as backup power. 
It is the latest in a string of climate initiatives that go back almost a decade. The company has been 100 percent carbon neutral, through the purchase of carbon offsets, since 2012. In 2013, it implemented an internal carbon tax on the scope 1 and 2 emissions of all divisions, with the revenue going toward sustainability improvements. It created a business unit focused on climate solutions, which produces things like AI for Earth. It recently succeeded in buying enough renewable energy to account for all US domestic operations.
Its latest sustainability report recounts all these efforts and more, including substantial efficiency upgrades at its campuses. In 2016, it won a climate leadership award from EPA. 
“We’ve seen them as a leader since 2013,” says Nicolette Bartlett, climate change director at the Carbon Disclosure Project (CDP), a global clearinghouse of corporate sustainability data. The CDP has a scorecard, which takes into account hundreds of sustainability and transparency metrics, and Microsoft has consistently gotten an A. “It really matters to them,” Bartlett says.

  
    Related
  

  
    Why the US bears the most responsibility for climate change, in one chart
  

In recent years, thanks to the IPCC report and pressure from investors and employees, concern over climate change has risen to the highest levels of the company. Josh Henretig, who spent 12 years on the company’s global sustainability team, rising to senior director before leaving in February, says he witnessed the shift from his team pushing to his team being pulled. “We started to almost stumble under the full weight and examination that the executive team imposed on us around the question: What’s really required?” he says.
“At this stage,” says Verena Radulovic, director of corporate engagement at the Center for Climate and Energy Solutions, “Microsoft has enough experience with reducing its own emissions, and support from its leadership to keep doing so, that it is able to take its climate commitment to a more ambitious level.”
And that’s what it did in January.
Microsoft will go carbon negative and wipe out all the carbon it has ever emitted
In January, Microsoft made a startling announcement: Not only will it reduce its scope 1, 2, and 3 emissions by 55 percent, it will continue beyond that and go carbon negative, drawing down more carbon than it emits, by 2030. By 2050, it will draw down enough carbon to account for all the company’s emissions since its founding in 1975.
“It set a new bar for what is considered climate leadership,” says Radulovic.
As you can see on the graph below, the target represents a radical acceleration of Microsoft’s carbon reduction efforts. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Microsoft’s net emissions reached a peak in recent years, and would need to continue a steady decline to reach zero by 2030.
      
      
        Microsoft
      
    
  


The January announcement, which came from Smith, the company’s president, backed by CFO Amy Hood and CEO Satya Nadella, laid out a set of principles that would guide the company’s approach:

Grounding in science and math
Taking responsibility for our carbon footprint 
Investing for new carbon reduction and removal technology
Empowering customers around the world 
Ensuring effective transparency 
Using our voice on carbon-related public policy issues 
Enlisting our employees

The post goes into detail on each. I’ll just hit some highlights. 
Nos. 1 and 2 are about proper measurement, scope 1-3 emissions, and historical emissions. “While we at Microsoft have worked hard to be ‘carbon neutral’ since 2012,” Smith writes, “our recent work has led us to conclude that this is an area where we’re far better served by humility than pride.” 
“We had some very heartwarming, but also uncomfortable, conversations,” says Henretig. 
Through these discussions, the company concluded that voluntary offsets are insufficient. It is now moving to a model where it directly contracts with renewable projects through power purchase agreements, (PPAs) — it is aiming to hit net zero for its scope 1 and 2 emissions by 2025 — and will compensate for what it can’t directly reduce with negative emissions. 
In this area, especially, Microsoft is showing real leadership. 

  
    Related
  

  
    Can you really negate your carbon emissions? Carbon offsets, explained.
  

As for No. 3, the company announced it will establish an investment fund that will target early-stage clean energy technologies, aiming to spend $1 billion over the next four years.
Some critics have argued that the venture capital model, built around big bets with potentially big returns, is a narrow way to approach the needs of the energy sector. Just recently, for instance, the International Energy Agency argued that crucial early-stage technologies need enabling infrastructure to continue developing.
“I think it’s a missed opportunity,” says consultant and former corporate social responsibility (CSR) executive Lindsay Baker. “There are opportunities to invest in infrastructure and other types of projects that have a market rate of return, more in line with just getting your money back — I would really like to see corporations making more of those kinds of investments.”  
Baker also notes that there are “plenty of opportunities for charitable giving that will help move the needle on climate,” including in lab-stage research or companies still in product development. A company like Microsoft, with well over $100 billion in the bank, could put some money toward these other areas as well, or at least divert a portion of its $1 billion to them. 
Nonetheless, a billion dollars in VC money is nothing to sneeze at. Nor is the signal Microsoft has sent to other companies by committing to a goal it admits it does not yet have the technology to achieve. It says going carbon negative will require “negative emission technologies (NET) potentially including afforestation and reforestation, soil carbon sequestration, bioenergy with carbon capture and storage (BECCS), and direct air capture (DAC).”

  
    Related
  

  
    Sucking CO2 out of the atmosphere, explained
  

Some of those technologies don’t exist at meaningful scale yet, and Microsoft is making a concerted effort to accelerate them. Especially if it can inspire other companies to make similar investments — Amazon announced a $2 billion climate fund in June — the spillover effects will help boost the entire sector.
“While much of Microsoft’s focus is on technologies that will help it reduce its own footprint,” says Radulovic, “the hope and vision is that these technologies will scale and others can use them.”
No. 4 is about products and services Microsoft will design that will enable its clients to reduce their own emissions. We will return to No. 4 in a bit, since some of the biggest controversies reside here.
No. 5, transparency, is another area where the company is showing leadership. Every year, Microsoft will publish a sustainability report, breaking down its emissions and progress against its goals. It has had its targets verified by the Science Based Targets Initiative as being in line with a pathway to limiting temperature rise to 1.5°C. In reporting its emissions, it is following the World Resources Institute’s Greenhouse Gas Protocol. And it is sharing its data with the CDP. In short, it is modeling best practices in transparency.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Microsoft’s 2018 greenhouse gas emissions, by sector.
      
      
        MSFT
      
    
  


No. 6 is also interesting, but we’ll come back to that later as well.
The company just announced its first concrete steps toward its target
This month, Microsoft chief environmental officer Lucas Joppa published an update on Microsoft’s progress, with several new announcements. 
First, Microsoft is joining with nine other large companies — A.P. Moller-Maersk, Danone, Mercedes-Benz, AG,  Natura & Co, Nike, Starbucks, Unilever, and Wipro, along with the Environmental Defense Fund — in Transform to Net Zero, “a cross-sector initiative to accelerate the transition to a net zero global economy.” It will run on much the same principles that Microsoft laid out for itself, including science-based measurement and transparency, with a commitment to knowledge sharing and norm-setting.
“When you look at the reach of these initial eight companies, as well as the supply and value chains of those companies, you start to get a pretty big market share,” says Jenn Crider, senior director of communications at Microsoft. It will exert a pull on other companies to use “a common and standardized approach to the math, the language, and the accounting,” she says.
Second, Microsoft debuted a sustainability calculator that will help its cloud clients calculate and reduce their carbon footprint. Third, it pledged to be completely free of diesel fuel and diesel generators by 2030. Fourth, it raised its internal carbon tax and broadened it to encompass scope 3 emissions. Fifth, it updated its Supplier Code of Conduct to require suppliers to calculate and report their full emissions. 
Sixth and perhaps most intriguingly, it has issued a request for proposals (RFP) seeking, for this fiscal year, a million metric tons of “carbon removal from a range of nature- and technology-based solutions that are net negative and verified to a high degree of scientific integrity.” It recognizes that these technologies are not fully developed, acknowledges that it will make mistakes, and says it is explicitly “using this RFP to harvest and share best available science and market intelligence on carbon removal,” to make things easier for other companies that want to follow suit.
“Someday, CO2 removal will be fully commoditized,” says Julio Friedmann, a carbon researcher at the Center for Global Energy Policy at Columbia University, who has helped advise Microsoft on its RFP. “These actions help put us on that course.”
It will be extremely interesting to see which and what type of carbon removal projects Microsoft ends up choosing through its RFP.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        A mockup of a direct air capture (DAC) machine from Carbon Engineering. 
      
      
        Carbon Engineering
      
    
  


Seventh, Microsoft announced the first investment from its $1 billion Climate Innovation Fund: $50 million will go to Energy Impact Partners, “a leading venture capital firm focused on decarbonized, decentralized energy industry transition that shares learnings among partners and facilitates collaboration.”
Eighth and finally, the company is taking action on environmental justice, partnering with renewables developer Sol Systems on 500 megawatts of distributed solar energy projects “in under-resourced communities, working with local leaders and prioritizing minority and women-owned businesses.” Given that the average residential rooftop solar system is a bit over 5 kW and commercial solar rooftop systems around 100 kW, that’s a lot of solar projects, representing the “single largest renewable energy portfolio investment Microsoft has ever made.”
Alongside those projects, the company will provide $50 million in “community-led grants and investments that support educational programs, job and career training, habitat restoration and programs that support access to clean energy and energy efficiency.” 
So that’s one big target, seven principles, and eight initiatives. What should we make of it?
Microsoft is earning kudos for its climate efforts 
I’ve talked with numerous experts in corporate sustainability to wrap my head around how to judge Microsoft’s efforts. Without exception, they praised Microsoft as a leader on climate change. Its commitment to good science, shared metrics, transparent reporting, and full carbon responsibility (not relying on offsets) is already setting a good example. 
“In Microsoft being among the first large companies to set such an ambitious target,” says Radulovic, “it allowed others, especially in non-tech sectors with more risk-averse or less innovative cultures, a safe space to do the same.”
It is difficult to trace direct causal lines between Microsoft’s announcements and those of other companies. Major corporate initiatives take years to develop. Their true effects will be measured by how many companies they pull into their wake in years to come. This was a common theme from experts in the field: Microsoft will have its biggest impact through the partnerships and collaborations it forms to spread its tools and ambitions.
Another notable feature of Microsoft’s efforts is the clear support from the top of the company. “All the big environmental announcements come from the CEO himself, which means there’s C-suite buy-in for everything they are doing,” says Jen Boynton, who works in corporate social responsibility at Cisco. “He’s making the commitment, he’s accountable, and there is financial and investor skin in the game.”

You could think of this as the evolution of corporate climate engagement, both within individual companies and across sectors: It begins in public relations, moves to the “environmental department,” and then gets taken up by top leadership, who look to their engineers to figure it out.
“The sustainability guys tend to think inside of a box,” says Bartlett, “but as soon as the shop floor gets hold of it, it becomes part of the DNA of the organization.”
Brian Janous, general manager of energy and sustainability at Microsoft, recalls the effect at the company when carbon reporting was expanded from scope 1 and 2 (energy) to scope 3 (supply chain, materials, and everything else): “Suddenly everyone is coming out of the woodwork. ‘Oh, we have to solve this, we have to solve that. We have to think about the amount of electricity being used to manufacture Xboxes. We have to think about the electricity being consumed by the people that use Xboxes.’”
It brought designers and engineers from every division to the task, people whose lives revolve around solving problems within resource parameters. Microsoft has made carbon a parameter for every team of engineers in the company now, and they are going to work on it. 
And there’s one other feature worth celebrating. “The thing about Microsoft’s work that I love, love, love is the investment in climate equity and environmental justice,” says Alison Murphy, who has directed sustainability and social impact work at companies like Lime and Lululemon. “This has been missing from the corporate dialogue. More companies should take this kind of intersectional lens.”
As much as Microsoft is doing, though, this is climate change, which means it’s never enough. Climate advocates and activists are not going to stop pushing for more. What would more look like? 
As I’ve asked around, the areas where Microsoft’s efforts could be critiqued fall into roughly four buckets. 
Microsoft could go even further by requiring suppliers to reduce emissions
The same day Microsoft published its updates on progress, Apple announced that it would aim to be “carbon neutral across its entire business, manufacturing supply chain, and product life cycle by 2030,” an astonishing goal for a company that manufactures, ships, and disposes of so many devices. 
“Apple has said their suppliers will all run on renewable energy,” says Bartlett. “It set targets for them.”
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Since 2014, Apple has purchased enough renewable energy to offset the usage of all its data centers.
      
      
        Apple
      
    
  


So far, Microsoft — which deals more in software and thus has a smaller scope 3 footprint — has only said that its suppliers must measure and report their full emissions. “Right now I read it to say, ‘we’re working with suppliers to find efficiencies’,” says Elizabeth Jardim, a corporate campaigner at Greenpeace USA. “And efficiency is important. But it only gets you so far.”
Apple will not simply cut off suppliers, Bartlett says, but will work with them to build their capacity to reduce emissions. “It’s not going to be every company in your supply chain” that needs special attention, she says. “It’s the 80/20 rule — go for the big ones first.”
There are signs Microsoft is heading in the same direction. In its commitments thus far, “you see a forecasting of where we’re going,” says Crider. “The first step is reporting requirements; the next steps will be reduction. You can make the assumption that there will be requirements on that reduction over time.”
For now, Apple is setting the bar on supply chain reductions, but it’s a close race.
It could stop selling products to companies that use them to dig up fossil fuels
Microsoft says it will develop products and services that will help its clients reduce their emissions, which is laudable. But there remains the question of how its other products are used. 
In particular, attention has recently focused on contracts for cloud and AI services between big tech companies like Amazon, Google, and Microsoft and some of the world’s largest oil and gas companies. Journalist Brian Merchant had a great exposé on this at Gizmodo last year. The services in question “are explicitly aimed at streamlining, improving, and rendering oil and gas extraction operations more profitable,” he wrote.
In May, Greenpeace issued a report looking closer at “how tech companies are helping big oil profit from climate destruction.” It found, among other things, that “Microsoft’s contract with ExxonMobil alone could lead to emissions greater than 20% of Microsoft’s annual carbon footprint.”
“Right now, the emissions from those contracts are not included in [Microsoft’s] carbon footprint,” says Jardim. “They’re not even tracking it.”


Microsoft has made a pledge to be carbon negative by 2030. Its approach has won praise from climate scientists, but the company also counts some of the worst emitters — oil and gas giants such as Chevron and Exxon Mobil — among its customers. https://t.co/j9x0DQIuLs— The Seattle Times (@seattletimes) June 7, 2020



In response to the Greenpeace report (which followed on the heels of years of criticism from tech workers, investors, and politicians), Google announced that it will no longer “build custom [artificial intelligence or machine learning] algorithms to facilitate upstream extraction in the oil and gas industry.” 
In Microsoft’s January announcement, Smith writes that the company is “committed to continuing to work with all our customers, including those in the oil and gas business.” Because a prosperous future will require more energy, he says, “it’s imperative that we enable energy companies to transition.” (The company issued a response to the Greenpeace report, which says much the same thing.)
“Another acceptable path forward would be to show us how Microsoft’s machine learning technology is actually scaling up renewables or scaling down fossil fuel production,” says Jardim. “Right now their contracts are not doing that.” Improving fossil fuel extraction projects doesn’t do much to help fossil fuel companies transition away from fossil fuel extraction. 
The oil company contracts are “a revolving debate within the company right now,” Henretig says. “It’s one of the areas a lot of employees are feeling conflicted about.”
If they want to stay ahead of the pack, Microsoft and Amazon should listen to their employees and follow Google’s lead. 
It could throw some elbows on public policy
Microsoft says it will use its voice to advocate for public policy in four areas: more public research, “the removal of regulatory barriers” to clean energy, market-based mechanisms, and universal standards for measuring the carbon content of consumer goods. 
That is, relative to the breadth and specificity of its other commitments, fairly weak tea. It sounds like a devotion to incremental, bipartisan policy, which is not only inadequate but has proven nearly impossible to achieve in practice. 
In its defense, the company has spoken up on some important issues. It pushed for more renewables in Virginia, supported the carbon-tax initiative in Washington, and opposed the rollback of Obama’s Clean Power Plan.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Support for Washington’s carbon-pricing initiative, 1631, was deep.
      
      
        Hannah Letinich, Yes On 1631
      
    
  


“It’s great to see Microsoft and others stepping up in ways that clearly acknowledge the urgency of the climate crisis,” says Bill Wiehl, founder of ClimateVoice, a nonprofit working to organize tech workers behind climate ambition. “Now we need them to step up their lobbying for a broad range of public policies to address climate change, everywhere they operate.”  
Microsoft could speak up for clean energy money in the next stimulus bill, call out denialist politicians, push back on state-level conservative efforts to block electric vehicles or prop up coal plants, or help push a national clean electricity standard or tightened fuel economy standards. There’s a whole lot of policy needed to get where Microsoft says the world needs to go.
Perhaps most importantly, Microsoft is still a part of the US Chamber of Commerce, a conservative trade group that relentlessly lobbies against clean energy. Will Microsoft leave the chamber (as Apple did in 2009) or at least step off its boards and lobby within it for a new direction (as Nike did in 2009)? Microsoft said it won’t participate in Chamber climate initiatives, but that’s it so far. (Read my story on a trio of senators going after the chamber on climate.)
Microsoft isn’t fully throwing around its weight. “We do have a PAC, the PAC does make investments,” says Crider, “but not at a level that sways an election in one direction or the other.”
A more vigorous form of power politics is called for in an age of climate crisis. 
It could clearly pledge to eliminate its own emissions
Microsoft aims to reduce its full emissions by 55 percent by 2030, with negative emissions technology soaking up the rest. While it has said it will draw down enough carbon to account for all its historic emissions, it has not said how fast, or even whether, its own emissions will reach zero after 2030.
While carbon negative is an admirable and standard-setting target, it is, in the end, a way of buying time. Every sector and business that possibly can hit true zero — run on 100 percent carbon-free energy — must ultimately do so. Pushing for negative emissions is not a license to ease up on the broader goal.
Microsoft should make clear that true zero emissions, as fast as possible, is still its long-term target. “Its voice saying that we need to get to zero is really powerful,” Bartlett says. “Ultimately, you need a business model that will flourish in a zero world, right?”
True zero emissions is a bit of a moonshot for Microsoft, but if Apple can do it, Microsoft can too. And there are reasons to think it will try.
“Obviously, the first thing we want to do is reduce emissions,” says Janous. “The goal is, get our scope 3 emissions down to as close as possible to zero. The commitment we made, 55 percent reduction — I think we’re going to do better than that.”
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Can this be done sustainably? (Above, a Microsoft Xbox exhibit at a July event in Shanghai, China.)
      
      
        Zhou You/VCG via Getty Images
      
    
  


Microsoft is doing what it can within the bounds of capitalism
Most of Microsoft’s emissions are from energy and will ultimately be eliminated by a cleaner, more robust electricity grid. Janous says the company is experimenting with using its data centers to provide backup and other ancillary services to grids, in pursuit of a “holistic solution” to grid issues, but to get there, “markets need to evolve to create more opportunities for flexibility.”
While Microsoft is working on a better energy grid, its peers will be approaching the problem from other angles. “It’s not like we’re all going to solve electricity, right?” says Janous. “Amazon’s going to work on transportation; Apple is going to work on materials and inputs. I’m excited about the breadth of impact we’re going to have as an industry, because we are all going to attack this thing a little bit differently.”
It is difficult to predict anything in today’s world, but there’s every reason to expect that large, well-established companies like Microsoft, Dow, Apple, Unilever, and Amazon committing to net zero will reverberate. 
It’s not just that the target could become the expected norm in the business world (though that appears to be happening faster than anyone expected). It’s that all the people working in those companies, and all the people who interact with those companies, will see that reducing emissions produces a torrent of innovation. They will see that the process draws top talent to these companies and gives their young, diverse workforces focus and motivation. 
They will see that common purpose brings out the best in people and that decarbonization is not a hair shirt or a sacrifice, but a chance to design and build a better world. They will take what they’ve seen to the voting booth.
It is the nature of climate change that virtually nothing that is possible today amounts to enough, and that’s true of Microsoft’s climate efforts. Within the conventional boundaries of US consumer capitalism, the company is unquestionably a leader, but if climate is a crisis, it may call for pushing at those boundaries: throwing some political elbows, cutting off some clients, perhaps even questioning the imperative for continuous growth. 
Microsoft has shown what can happen when engineers get ahold of the carbon problem. Now its leaders should trust its engineers and move farther, faster.

  
  
  
      




  The world’s richest man, Jeff Bezos, testified before US Congress members for the first time on Wednesday, but he said little to assuage one of their biggest concerns: that Amazon’s grip on online retail gives it the power to make or break small merchants on a whim.
Bezos, along with the CEOs of Apple, Google, and Facebook, appeared via videoconference before a bipartisan group of 15 US House members who have been investigating the four tech giants over the last year. The stated goal of this antitrust subcommittee’s investigation has been to document whether these corporate titans abuse their power in industries ranging from retail to social networking, and to evaluate whether the country’s antitrust laws are modern enough to guard against such abuses.
“Their ability to dictate terms, call the shots, upend entire sectors, and inspire fear represent the powers of a private government,” Rep. David Cicilline (D-RI), the chair of the subcommittee, said in his opening remarks at what was a five-hour-plus hearing. 
For Bezos, much of the questioning from lawmakers focused on how Amazon competes against, and profits from, the 1.7 million small and mid-sized merchants who help stock Amazon’s digital shelves. Amazon boasts that 60 percent of its retail sales now come courtesy of these sellers rather than from Amazon stocking and reselling goods itself. 
But some Amazon sellers have complained over the years that as Amazon’s market share in US online commerce has increased — to about 40 percent today, which is about seven times more than the next competitor — the company has squeezed and otherwise harmed them in new and different ways because they have no viable online alternatives. 
According to Cicilline, Amazon sellers have told the subcommittee that “[Amazon has] never been a great partner, but you have to work with them.”
One concern has been the data Amazon uses from its own merchants to help inform what products to develop under its own private-label brands, such as Amazon Basics. In April, the Wall Street Journal published a report stating that Amazon employees have used data from individual sellers to help Amazon decide which private-label products to pursue. That contradicts what a top Amazon lawyer, Nate Sutton, told Congress earlier this year when he said that Amazon’s policy is to only use data on a product when there are at least two sellers selling it. 
On Wednesday, Bezos told Rep. Pramila Jayapal (D-WA), who represents Amazon’s hometown of Seattle, that the company’s investigation into violations of the policy outlined in the Journal report was ongoing. “I’m not satisfied that we have gotten to the bottom of it, and we’ll keep looking at it,” he said. 
And Jayapal made her point clear: “So you might allow third-party sellers onto your platform. But if you’re monitoring the data to make sure that they’re never going to get big enough that they can compete with you, that is the concern that the committee has.”
Bezos argued that other retailers don’t even have such a policy, which is completely beside the point — no other US retailer operates a marketplace even close to the size of Amazon’s. But worse, Bezos not providing an update on the investigation just means that the concern over these potentially anti-competitive practices remains unsettled.
The lawmakers also questioned Bezos on what some view as an increasing cut of sales that Amazon takes from small merchants. According to a recent study by the Institute for Local Self-Reliance, a nonprofit that advocates for a strong economy built on independent businesses versus giant corporations, Amazon collected 30 percent in fees on average in 2019 from a given sale that a seller made. That number was up from 19 percent five years earlier, according to the ILSR estimates. Some sellers have said Amazon’s cut is even higher than that. In an episode of the Land of the Giants: The Rise of Amazon podcast last summer, one Amazon toy seller told Recode that Amazon now collects fees that equate to nearly half of each of his company’s sales, when including the cost to advertise his products on the site.
Bezos’s defense of these increases centered on the value he says Amazon is providing in exchange for these fees. The CEO talked up Amazon’s advertising platform as a way for businesses to get discovered — but some sellers and brands see the cost as a tax that has to be paid in order to do business on the platform. The CEO did little to put to rest the open question of whether small businesses on Amazon can be successful without giving his company a bigger and bigger cut of their earnings.
Bezos also mentioned how Amazon’s warehousing program Fulfillment by Amazon (FBA) allows merchants to store, ship, and have customer service taken care of through Amazon. In order for most sellers to qualify their goods for Amazon Prime delivery, they have to pay for FBA storage. And Bezos admitted that Amazon’s algorithm that determines in real time which seller wins a given sale indirectly factors in whether a seller is a customer of FBA. This admission could offer additional ammunition to critics who argue that Amazon is using its control over the largest US e-commerce marketplace to essentially force its merchants into paying for more and more services, such as FBA.
Then there’s the frequency with which Amazon changes its policies and the algorithms that power its platform in ways that can make or break its merchants’ businesses, essentially overnight. One Congress member told Bezos the story of a textbook seller on Amazon who says her business was kicked off of the platform without notice or explanation after her business had grown large. Seemingly arbitrary suspensions by Amazon are not a new complaint.
Bezos said he was “surprised” to hear about such a story and that he would like to speak to the seller. But he also countered with a defense that he believes such treatment is not “systemic” at Amazon.
For Bezos, this was his first time testifying on Capitol Hill, at least in part because Amazon has, for the most part, been a good thing for millions of online shoppers. As I’ve written before, Amazon offers buyers incredible convenience, good prices, fast delivery, and a vast selection. And US antitrust enforcers tend to favor companies that treat consumers well and keep prices low, while typically targeting business practices or mergers that they believe will harm consumers, such as by raising prices for a product or service. 
But Amazon is now worth $1.5 trillion, and Bezos is the world’s richest man. Along the way, media and regulatory scrutiny has intensified. The Federal Trade Commission has been probing various Amazon business practices over the last year to see if Amazon has violated existing antitrust laws. And the House antitrust subcommittee will next issue its own report concluding its investigation that could argue for new or modified antitrust legislation that can account for the harm to innovation and competition that some legislators say is done by tech giants like Amazon, even while they seemingly treat consumers well. 
Even if Bezos didn’t shut down lawmakers’ concerns about potentially anti-competitive practices, his first congressional testimony at times came across as the most authentic of the CEOs at the hearing. At the same time, on several occasions he politely dismissed anecdotal seller complaints presented during the hearing as one-offs rather than being core to Amazon’s DNA.
And therein lies one of his problems. Even if Bezos is right and Amazon only rarely abuses its position over its own sellers, the complaints shared during the hearing show that the company has grown so big and powerful that even abuses of neglect have the power to crush the small businesses that power Amazon’s success — but also are so dependent on the platform that it can crush them without even noticing.
  
  
  
      




  Wednesday’s congressional antitrust hearing was a historic occasion, offering Congress a chance to grill four of the most powerful men in the world, who control four companies — Facebook, Apple, Amazon, and Google — each so massive that they rival nation-states in their power. Observers have grown increasingly concerned about the unprecedented and outsized impact of these companies on the economy, the millions of American citizens who use their products, and the thousands of smaller businesses that try, often unsuccessfully, to compete with them. 
But the Republican members of the hearing instead primarily focused on one specific thing: unfounded claims that tech companies are biased against conservatives.
“I will just cut to the chase. Big tech is out to get conservatives. That’s not a suspicion, that’s not a hunch, that’s a fact,” said Rep. Jim Jordan (R-OH), who from the onset of the hearing led Republicans on the committee in questions about anti-conservative bias. At one point, he repeatedly yelled at a Democratic colleague, Rep. Mary Scanlon, interrupting her allotted questioning time because he took offense to her implication that his focus on anti-conservative bias was promoting “fringe conspiracy theories.”
It’s not surprising that many Republican committee members chose to focus on supposed tech bias. But it is a significant distraction from what really matters: whether tech companies have used their power to crush their competition and exploit users’ online behavior and data in a manner that hurts Americans of all political persuasions.
Allegations that social media platforms have an anti-conservative bias has for years been a rallying cry of President Trump and the Republican party. And leading up to Wednesday, Republicans attacked the focus of the Democrat-run House Judiciary subcommittee hearing — calling on it to focus more on anti-conservative bias and for Twitter CEO Jack Dorsey to appear. Twitter is a small company compared to, say, Facebook, but it has recently taken measures to moderate President Trump’s posts for violating policies around misinformation and hate speech, enraging Republicans.
Democrats, meanwhile, tried to steer the conversation back to issues more directly relevant to antitrust, like if and how these companies intimidate their competition, such as when Facebook acquired its then-rival Instagram in 2012; or whether these companies exploit their users’ privacy, like how Google tracks individuals’ online browsing across the web with cookies; or if Apple is shutting out its competitors by taking an unreasonable cut of profits coming in from independent app developers in its App Store. 
What really matters here is whether these companies’ business practices are ultimately harming consumers, most of whom have no choice but to use Big Tech in one way or another if they want to do basic things online like search the web, order goods, or stay in touch with their friends.
In an earlier era, Republicans and Democrats on the committee might have come together to try to focus on what’s been seen as an area of relative bipartisan agreement: protecting the free market. That didn’t happen at today’s hearing. Instead, it was a display of partisan divides.
While it’s true that many rank-and-file corporate employees at Facebook, Google, and Apple — who tend to be college-educated individuals living in major metropolitan areas — identify as politically liberal, like many others in their demographic, there’s no definitive proof that Facebook, YouTube, Google, or any other major tech platform discriminates against conservative content. 
In their testimony, Republicans at Wednesday’s hearing cited investigations from right-wing news outlets and groups, like Project Veritas and Breitbart News, but at most, these sources seem to indicate that many Big Tech employees hold liberal political beliefs — a phenomenon that is neither illegal nor inherently conspiratorial. 
Historically, the types of content and pages that consistently perform well on Facebook are often right-leaning news and pundit pages, like Breitbart News and Ben Shapiro. And as my colleague Peter Kafka wrote this spring, these same tech companies often face criticism from Democrats over how their platforms incentivize users to post polarizing and politically extreme content because their algorithms prioritize engagement — and polarizing content is good at getting users to engage with it. 
After Jordan used his initial allotted round of time for questioning Google CEO Sundar Pichai about alleged anti-conservative bias (based on leaked emails from a former Google marketing executive which said the company used its products to reach Latinos with voting information in the 2016 presidential election), he twice interrupted Democratic colleague Rep. Mary Scanlon (D-PA) to yell at her across the floor — initiating a screaming match with Democratic subcommittee chairman David Cicilline, who tried to maintain order. 
Jordan reacted this way after Scanlon said she would like to focus her questioning back on antitrust issues instead of what she called “fringe conspiracy theories.” After Cicilline’s repeated calls for order — and after another person on the congressional floor, unidentifiable from the livestream, yelled at Jordan to “put your mask on!” — Jordan let up and let Scanlon continue questioning Amazon CEO Jeff Bezos with her allotted time.
The entire debacle was another reminder that today’s hearing is mostly a political spectacle, a moment primed for soundbites, and many Republicans at the hearing chose to use their time with these powerful company leaders to promote their own political agendas. Congressional hearings like this one aren’t expected to directly lead to antitrust action, but they can help set the stage for that when politicians strategically use their time to extract answers from leaders with pointed — and unified — lines of questioning.
Ranking Republican James Sensenbrenner did offer a narrow window of measured optimism for bipartisan cooperation toward the end of the hearing. He said that antitrust probes hold a meaningful and historic place in American government but that, in the case of tech, regulators need to revisit old decisions and step up enforcement of existing laws rather than write a whole new set of rules.
But largely, Sensenbrenner’s Republican colleagues were uninterested in discussing antitrust issues, under new or existing laws alike. As one Democratic congressional staffer told Recode, Republicans seemed more interested in creating explosive confrontations by decrying alleged “liberal bias” that are made-for-replay on conservative cable TV (indeed, Jordan’s clips have already been a subject of discussion on Fox News). Today’s hearing was a sign that in the current hyper-polarized political climate, it’s unlikely Congress will lead any real, meaningful, bipartisan legislative effort to rein in Big Tech anytime soon. 
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Hours before the House antitrust subcommittee hearing featuring testimony from the CEOs of Facebook, Google, Amazon, and Apple, a blog post from TikTok chief executive Kevin Mayer proclaimed that all platforms should “disclose their algorithms, moderation policies, and data flows to regulators” and challenged the app’s competitors to follow suit. This is quite a call to arms — and one that was obviously carefully timed. 
In the post, Mayer makes a broad call for competition between social media companies and argued that TikTok could be a positive force for the United States, one that would protect its user data, with or without new regulation. And along those lines, Mayer also promised that TikTok would be more upfront about its algorithms and content moderation. Ultimately, he said, TikTok would be a model for how other social media companies could be more transparent, a commitment that echoed recent calls for TikTok to become this very example. 
The post comes as TikTok faces concerns over potential security risks related to its parent company, the Beijing-based company ByteDance. Earlier this month, Secretary of State Mike Pompeo even threatened to “ban” TikTok, though it’s unlikely the Trump administration could actually do this on its own; Joe Biden’s presidential campaign also recently instructed its staff to delete the TikTok app from their phones. Meanwhile, assessing the true risk of the app remains difficult. As Shira Ovide wrote at the New York Times earlier this month, “politicians, like American tech bosses, engage in fear-mongering about Chinese tech so often that it’s hard to know when to believe them.”
TikTok, for its part, says that no foreign government plays a role in its moderation. 
“Our content and moderation policies are led by our US-based team in California and aren’t influenced by any foreign government,” a TikTok spokesperson told Recode. “At our virtual Transparency and Accountability Center, guests can see firsthand how we moderate and recommend content.” 
After all, other US social media companies stand to benefit from action against TikTok. Facebook is currently preparing to fully launch a music-based product called Reels with Instagram, and is even reportedly recruiting TikTok stars to promote the competing service. In his recent blog post, Mayer accused Facebook of making attacks “disguised as patriotism and designed to put an end to our very presence in the US.” Meanwhile, influencers who’ve gained massive audiences on the app are being wooed away to other rivals, like the Los Angeles-based music app Triller.
But now, in an apparent effort to allay concerns over its platform, TikTok is on a quest to prove that it is transparent about how it handles content. With Mayer’s request for all social media companies to disclose their algorithms, it’s obvious that TikTok wants to appear more transparent than Facebook and others. However, it’s not entirely clear that these efforts will address the many other concerns about TikTok. 
“TikTok has become the latest target, but we are not the enemy,” Mayer wrote in the post. “The bigger move is to use this moment to drive deeper conversations around algorithms, transparency, and content moderation, and to develop stricter rules of the road.”  
As evidence, Meyer pointed to the TikTok Transparency Center, which was announced back in March. The center in Los Angeles will purportedly provide some experts “the actual code that drives our algorithms,” Mayer said, and also let them observe content moderation in real time. Mayer argues that this new initiative puts TikTok “a step ahead of the industry.” That announcement was followed by a blog post in June that explained some of the basics of the company’s For You algorithm, which powers one of the popular parts of the TikTok app. The company is also opening another transparency center in Washington, DC, and hiring for positions meant to interface with the federal government.
It’s unclear if TikTok’s recent efforts will be enough to quell apprehensions about the platform. Several experts told Recode that they questioned whether TikTok’s pledge to disclose how its algorithms work will actually reveal much meaningful information, such as what type of content the company’s system chooses to amplify. 
“Revealing the code is helpful and certainly more than other platforms have shared in the past,” said Kelley Cotter, a postdoctoral scholar at Arizona State University who studies public understanding of algorithms. “Revealing the code will not, in itself, tell us if the algorithm has an influence.”
Others wondered if TikTok can reveal details about its algorithms without exposing the personal data of its users. According to Nicolas Kayser-Bril, a journalist at AlgorithmWatch,  the machine learning algorithms used by social media platforms are dependent not only on the code that operates them but also on training data that can influence how they operate. “In the case of TikTok’s algorithms, the training data probably contains highly personal information from users, which should not be revealed as such, even to researchers,” said Kayser-Bril. 
Alongside its push for greater transparency around its algorithms and content moderation, TikTok is also working hard to distance itself from ByteDance and the Chinese government. TikTok made this case in a recent statement to Vox:
Protecting the privacy of our users’ data is of the utmost importance to TikTok. There’s a lot of misinformation about TikTok right now. The reality is that the TikTok app isn’t even available in China. TikTok is led by an American CEO, with hundreds of employees and key leaders across safety, security, product, and public policy in the U.S. TikTok stores U.S. user data in Virginia, with backup in Singapore, and we work to minimize access across regions. We welcome conversations with lawmakers who want to understand our company. 
However, not everyone is convinced that this new push toward transparency is enough to assuage worries that TikTok might be used as a tool for foreign influence. After all, regardless of how much we know about how TikTok recommends content, the company is also collecting massive amounts of data about millions of users in an effort, some say, to make its AI even more powerful for a variety of purposes.
“From a national security perspective, there’s concern around using that data for espionage purposes, blackmail,” Kiersten Todt, a scholar at the University of Pittsburgh Institute for Cyber Law, told Recode. “Artificial intelligence is only as good as the data that goes into it. So if the Chinese government has the most data of any other country in the world, then what it can produce from an AI perspective could potentially give it a tremendous advantage.”
But regardless of the true security risks of TikTok, fear over the app may have prompted a new standard for what it means for a social media company to be transparent with its users. If TikTok lives up to its transparency promises, other social media companies may very well feel pressure to follow suit. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  On Wednesday, four of the most powerful men in the world will answer pointed questions from US lawmakers who have real concerns about the massive tech companies these men lead. The big, overriding theme of the virtual antitrust hearing: Are Facebook, Google, Amazon, and Apple too powerful? So powerful that they should be broken up?
The hearing, which will be livestreamed, is a historic event and you should watch it, or at least follow coverage.
But you should also go into this with your expectations set: It’s quite likely that the House Judiciary Antitrust Subcommittee’s hearing on Wednesday, featuring Alphabet CEO Sundar Pichai, Amazon CEO Jeff Bezos, Apple CEO Tim Cook, and Facebook CEO Mark Zuckerberg, will generate a viral soundbite or two. It’s even possible that something one of the CEOs says will become ammunition used against them in a legal fight down the road.
But what won’t happen on Wednesday is anything that has a direct impact on how, or if, the biggest tech companies in the world are regulated.
Some of you already know that congressional hearings are just that — a chance to hear from citizens and government officials. At a minimum, they are a place to get public answers on the record, like the grilling US Attorney General William Barr received from the House Judiciary Committee on Tuesday over everything from his handling of the Mueller investigation to his use of federal troops to quell protests in Washington, DC, and Portland, Oregon. 
And sometimes, like in Wednesday’s case, they can also be high-profile bits of theater. 
But they don’t directly lead to any rule-changing or law-making. That stuff will or won’t happen, for the most part, off-camera. Some of it will be in closed-door meetings between regulators and lobbyists from the tech companies; some may be in legal proceedings, like an expected antitrust suit in the works against Alphabet, led by state attorneys general. The Department of Justice and the Federal Trade Commission are also pursuing investigations into all four of the companies whose leaders are testifying on Wednesday. 
In theory, any antitrust lawsuit could eventually play out with an order to break the companies up, like Standard Oil at the beginning of the 20th century and the Bell Telephone system in the 1980s. (Twenty years ago, a court order was also supposed to break up Microsoft, though that never went through.) But that’s a very long game.
As a reminder: With the exception of Amazon’s Bezos, the other three tech leaders have all sat before Congress before. If you are deep into this stuff, you may recall the one significant takeaway from Zuckerberg’s testimony in the spring of 2018, which was focused on Facebook’s privacy practices: In four words, plus a smirk, he explained Facebook’s business model to Sen. Orrin Hatch.

A year after that hearing, the US government did actually do something about Facebook — it got the company to agree to a record-breaking $5 billion settlement and layered on oversight that’s supposed to (in theory) make the company less likely to trample its users’ privacy rights. But that action came via the Federal Trade Commission; when it comes to Facebook and its privacy failures, Congress hasn’t done anything meaningful. 
It is always possible, theoretically, that Congress will pass meaningful legislation regarding the internet and the companies that dominate it. But things would have to change radically for that to happen, because right now different factions of Congress speak completely different languages when it comes to internet regulation. Democrats worry about the power of big tech companies to reshape the economy and to abuse Americans’ privacy; Republicans harp on unsupported claims that big tech companies are censoring conservatives. 
And that dynamic extends beyond Congress. Barr, who has taken a very public interest in pursuing antitrust claims against Google, among other big tech companies, has said, out loud, that tech companies are using their power to “censor different viewpoints,” and that “one way this can be addressed is through the antitrust laws and challenging companies that engage in monopolistic practices.” 
So expect to hear questions directed at Zuckerberg and Pichai, at least, along those lines at the hearing.
On the other hand, this won’t be a replay of earlier Silicon Valley-comes-to-Washington hearings, during which a series of politicians made unwitting proclamations about their tech ignorance. US Rep. David Cicilline (D-RI), who is running the antitrust subcommittee hosting Wednesday’s hearing, is someone with a relish for this work, and he will undoubtedly produce documents and transcripts that make the four CEOs squirm at different times. And unlike other Big Tech hearings, questions here will be limited to the subcommittee, which means less opportunity for pointless digressions.
And as we saw on Tuesday when he questioned Barr, Cicilline is also quite adept with an array of barbed questions and commentary. 


Rep. Cicilline: Is it ever appropriate for the President to solicit or accept foreign assistance in an election?Barr: Depends what kind of assistance pic.twitter.com/jMwv3c2zG2— Acyn Torabi (@Acyn) July 28, 2020



So go ahead and watch, because this stuff really is important: The men who rule technology are, increasingly, rulers of their own mammoth kingdoms, and they are long overdue for a public accounting. And if we can get that out of the hearings, it will be well worth it. But if you want actual change, you’re going to have to wait.
  
  
  
      




  As the US continues to struggle to contain the Covid-19 pandemic and social distancing recommendations remain in place, millions of US children and adolescents aren’t expected to attend school in-person in the fall — meaning they’ll often be stuck inside their homes and using the internet as a primary means of human connection. The situation has resurfaced a long-standing, difficult-to-answer question: Is technology going to ruin my teenager’s brain?
For years, some have blamed the growing rate of teenagers suffering from mental health issues in the US on the drastic increase in how much they’re engaging with digital devices compared to previous generations — but there isn’t much hard evidence to back up those claims.
It’s true that we’re seeing unprecedented levels of teenagers using digital devices — some 95 percent of US teens have access to a smartphone, and 45 percent say they are online “almost constantly,” according to a 2018 Pew poll. And during the same period of time that internet and smartphone use has increased for a generation of young people, the suicide rate in the US has also steadily increased (across all ages overall), with a disproportionate increase in the rate of tween girls aged 10 to 14 in particular. 
But the rise of teenagers’ use of social media at the same time that depression rates have been going up shows correlation, not causation. Meaning, there’s no evidence to prove that teenagers’ use of social media is why we’re seeing this increase in depression instead of any number of other confounding factors — like their family life, economic conditions, or anything else. To know if that’s the case or not, we need a lot more comprehensive research that isolates these other factors. 
Considering how high the stakes are, answering this question is important — and pressing.
A new report from the nonprofit Common Sense Media, a national advocacy group focused on digital access and safety for children and families, reflects the urgency and state of the conversation. The report, written by UC Irvine child psychology professor Candice Odgers and Common Sense research director Michael Robb, attempted to review the existing research about the effects of tech use on teens, draw conclusions about the overall risks and benefits, and suggest recommendations for parents, educators, and the public. But one of the biggest conclusions from Common Sense’s report is that the existing research doesn’t tell us enough, and we need more refined scientific evidence to know anything conclusive about the effects of social media on teenagers’ mental health.  
In the meantime, it’s important to understand what the concerns around the impact of social media on teenage mental health are — and what to consider when it comes to figuring out how to help teens have a balanced relationship with technology, especially while living through a pandemic. 
The concern: “Our kids are walking around with slot machines in their pockets”
Jennifer Siebel Newsom, first partner to California’s Governor Gavin Newsom, who contributed an essay to the report, wrote that she understands digital devices like tablets and laptops are necessary tools for getting an education during Covid-19 school shutdowns. But she says she’s also worried about the effects of these devices on children and teens’ mental health. 
“[A]s a mom, I can’t ignore the reality in my home. Distance learning for my four kids this spring opened the floodgates to media and its adverse effects. What started with using Zoom and Gmail for homework assignments became internet searches bringing up age-inappropriate information — and misinformation,” wrote Siebel Newsom. “All of a sudden my eldest were sneaking off to their rooms, or hiding devices under their beds at night.”
Both Siebel Newsom and others who contributed to the report, like former Democratic presidential candidate and entrepreneur Andrew Yang, want to see technology and media companies accept more responsibility for their impact on children, even if we don’t yet have research showing exactly what that impact is. Yang, in particular, called for the government to drastically fund more research and step in, if needed, to incentivize tech companies to educate children, rather than entertain them, to collect ad dollars. (The children’s digital advertising market is expected to be worth $1.7 billion by 2021, according to a report from PwC.)
“Right now, the interests of parents are directly at odds with the interests of the technology companies,” wrote Yang. “They’re monetizing our attention and profiting off of our time. As they say, the addictive nature of smartphones is a feature, not a bug. We parents are outgunned and at a total loss.”
Newsom’s and Yang’s comments draw on much larger anxieties in the American public about what kids are doing with their time online and how this will impact their development.
The evidence is inconclusive and more research is needed
So, while there’s no shortage of concern about how much time adolescents are spending on their phones, what does the research actually say?
Unfortunately, not enough for us to draw any quantitative, evidence-based conclusions — and that’s why we should start doing more research in earnest now. The report’s meta-analysis of the most up-to-date research on social media and depression revealed a mix of “small positive, negative, and mostly neutral” links between adolescents’ use of technology like social media and their mental health. 
Authors of the report looked at two large-scale reviews of existing research on the topic published earlier this year and found results associating adolescents’ mental health and use of digital technology “inconsistent” — even when an association was present, it accounted for less than 1 percent of the variation.
This failure to find a stronger link between teen depression and technology use “is not surprising,” Odgers and Robb write, “given that mental health disorders emerge from a complex set of social, genetic, and experiential factors, which have varying influence across development and situations.” Still, “small effects can be meaningful,” the report states, “but with existing evidence we have no way to separate cause from effect in social media research with adolescents.”
If researchers wanted to actually separate cause from effect, we would need research that asks more specific questions and is backed by harder data, as my Vox colleague Brian Resnick has previously explained. Self-reported surveys on teens’ well-being can be biased — so another option would be for scientists to use brain scans showing neurological development over time to track the tangible effects of social media on children’s well-being.
While there is at least one large study like this underway, funded by the National Institutes of Health, it will be several years before we see its results. Until then, researchers are asking for more granular data from companies like Apple and Google to help them understand exactly how kids are using their devices. Are they bingeing on Fortnite or watching educational YouTube videos? So far, tech companies like Apple largely haven’t given researchers the option to see people’s screen-time data that shows how much they use different apps on their phones — even with their consent.
With all this in mind, the report calls for the government and other groups to fund more research on this topic. 
Recommendations: Quality over quantity 
While the jury’s still out on exactly how social media impacts teens, the report does offer some child psychologist-backed recommendations for teen tech use.
An important one: It’s not how much teens use apps that matters but how they’re using those apps. In essence, it’s quality over quantity. Examples of quality use include a teen using multiplayer video games to socialize with their peers and build stronger friendships, the report argues. Another example of positive tech use is when first-year college students use their smartphones to maintain close contact with their parents. One study cited in the report showed these students were better at bouncing back from outside stress than their peers who stayed in touch with their families less often. 
The report stressed that while many families have instituted rules about how much their kids use technology, the fights parents are having with their children over this are actually making things worse. In fact, “conflict over screens is likely to be more harmful to adolescents’ mental health than screen time itself,” Common Sense’s report states.
While there’s a lot we still don’t know about technology and its impact on teens’ mental health, it’s an important issue that’s only become more serious during the pandemic. That’s why we need better research before jumping to any conclusions. 
You can read the report in full here.

  
  
  
      




  
  
  
    
    
      
        
  


  






      
    
    
  
  



Social media platforms are struggling to contain a new round of coronavirus conspiracy theories due, in part, to Donald Trump. 
On Monday night, the president retweeted accounts that posted a video falsely claiming that hydroxychloroquine cures Covid-19, including one tweet from his son Donald Trump Jr. Many of those tweets were later removed, and Twitter suspended several of the users behind them, including Trump’s son, for 12 hours. But the video itself has continued to spread across social media platforms, raising fresh questions about how companies like Facebook and Twitter handle misinformation.
The video in question, which Trump Jr. called a “must watch,” features Houston doctor Stella Immanuel, who claimed that a combination of hydroxychloroquine, zinc, and the antibiotic Zithromax was a “cure” for the coronavirus and that “you don’t need to wear a mask.” The Food and Drug Administration (FDA) has said that hydroxychloroquine is “unlikely to produce an antiviral effect,” and the Centers for Disease Control and Prevention (CDC) recommends wearing masks to stop the spread of the virus. The video of Immanuel quickly went viral, drawing millions of views on Facebook, Twitter, and YouTube in a matter of hours.
The event itself also had some political support. Immanuel was speaking at a gathering called the “White Coat Summit,” held on Monday by a group called America’s Frontline Doctors. The press conference, which was held on the steps of the Supreme Court, was organized by the right-wing group Tea Party Patriots and also featured Rep. Ralph Norman (R-SC). A spokesperson for Norman told Recode that he didn’t know ahead of time what Immanuel was going to say. 
“While the Congressman does not agree with her statement on the use of masks, and certainly has no expertise in medications, he strongly believes that she has a right to say what she came to say without being censored by big tech,” Rep. Norman’s spokesperson said.
What got Don Jr. suspended from Twitter
According to the Daily Beast, Immanuel has a history of strange medical claims, including that fibroids and cysts are caused by having sex with demons in dreams and that alien DNA is being used in medical treatments. Records from the Texas Medical Board show that Immanuel, who was trained as a pediatrician, is a licensed physician with a practice in Houston that has the same address as her church, Fire Power Ministries. Immanuel’s sermons, several of which are hosted on her YouTube channel, include messages of support for President Trump. Most of those videos have only a few thousand views.
Why this specific video about hydroxychloroquine went viral so quickly is likely a combination of the high profile of the right-wing personalities and publications that shared it and the controversial subject matter. Although Twitter attracted attention by suspending Don Jr.’s account for posting the video of Immanuel’s speech, the source of the virality appears to be the conservative publication Breitbart, which shared a livestream of Immanuel’s speech to its 4.7 million Facebook followers. Facebook later removed the post, but not before it got millions of views on the platform. 
“We’ve removed this video for violating our policies that prohibit false claims about cures for Covid-19, since no such cure currently exists,” a Facebook spokesperson told Recode in a statement. “Under this policy, we remove posts that make claims like this video did that hydroxychloroquine is an absolute cure for Covid-19. From April to June we removed more than 7 million pieces of content on Facebook and Instagram for violating this policy and have shown messages to people who have reacted to, commented on or shared this kind of content.”
YouTube also says it has taken steps to remove the video. “We have removed the video for violating our COVID-19 misinformation policies,” a spokesperson for Google, which owns YouTube, told Recode.
The Breitbart detail is especially problematic for Facebook. The social network drew criticism after it named Breitbart as a partner in its Facebook News initiative, which involved collecting trusted news sources in a dedicated tab. Once described by co-founder Steve Bannon as “a platform for the alt-right,” Breitbart is also known to spread misinformation. A Facebook spokesperson confirmed to Recode that Breitbart is still eligible to appear in the News tab — meaning it could appear in people’s individual News tabs — but its content has never appeared in the top stories feature, which is what’s curated by Facebook employees.
Meanwhile, hydroxychloroquine continues to be the source of many conspiracy theories, so one might expect social media companies to be prepared to halt the spread of dubious content related to the topic. Some proponents of these theories simply deny the results of studies showing that hydroxychloroquine is not an effective cure, while others cite anecdotes they hear from doctors, quote evidence from less-than-ideal trials, and cherry-pick their facts. According to John Gregory, a senior analyst for health at NewsGuard, an app that rates news site trustworthiness, there’s also “a larger, vaguer narrative that hydroxychloroquine is being held back because Trump spoke about it, and because it’s politically beneficial for Democrats in the United States if hydroxychloroquine doesn’t work.”   
But it’s not entirely clear where any of these social media platforms draw the line between actionable misinformation and allowable claims about hydroxychloroquine. After all, this isn’t the first time that the major social media platforms have been forced to chase down viral videos involving doctors making unproven medical claims. 
In early May, a conspiracy theory video called Plandemic gained millions of views after being shared in conspiracy theory Facebook groups, including those associated with the group QAnon. The 26-minute video featured a discredited doctor making a slew of false claims, like the idea that the novel coronavirus was manipulated in a lab, and pushed the idea that hydroxychloroquine could be an effective treatment for Covid-19. The Plandemic video was removed from the major platforms, but not before it “had been viewed more than eight million times on YouTube, Facebook, Twitter and Instagram, and had generated countless other posts,” according to the New York Times. 
The video featuring Immanuel and shared by the Trumps appears to have gone viral much more quickly and garnered millions more views. According to preliminary research from Zignal Labs, a media intelligence platform, the original video livestream received about 18 million engagements on Facebook. There were more than half a million mentions of the video on Twitter and other non-Facebook platforms as of Tuesday morning. Other estimates put the number of views of the video at over 20 million.
How Facebook and Twitter keep struggling with Covid-19 misinformation
Facebook has generally tried to take a mostly hands-off stance when it comes to the president’s posts. The company did remove campaign ads that promoted a “census” that was not the official census and that featured imagery associated with Nazis, but it declined to act on a Trump post that said “when the looting starts, the shooting starts” in reference to nationwide anti-police brutality protests. To date, Facebook has not deleted any of Trump’s posts related to hydroxychloroquine.
Twitter has been more aggressive in its dealings with the president. The company first fact-checked Trump in May, applying a warning label to two tweets that included misinformation about voting-by-mail. A Trump tweet that included the same “shooting ... looting” language that was in his Facebook post also received a warning label. In this week’s incident involving the Immanuel video, several tweets that were retweeted by President Trump have been deleted, although others claiming that hydroxychloroquine can treat the coronavirus remain up as of publication. (Twitter did not respond to a request for comment.)
Still, what Trump tweets about hydroxychloroquine can influence online conversations. According to research from Timothy Mackey, a public health professor at UC San Diego, the president’s posts seemed to contribute to an increase in discussion of both hydroxychloroquine and other Covid-19-related medications on social media in the past day. 
“Hydroxychloroquine social media conversations spike when there are statements from the Trump administration and officials, which create an echo chamber of related news coverage,” Mackey explained in an email. “That then leads to aggregators [and] supporters of Trump who amplify that message [and] misinformation, and then that gets exposed to other users, who react with both positive and negative sentiment.”
Some might argue that any posts claiming that hydroxychloroquine is a cure for Covid-19 should get flagged for misinformation, if only because most scientific studies tell us that the drug doesn’t make a difference. Much like mask-wearing, however, the use of hydroxychloroquine has become politicized.
Typically used to prevent malaria and treat certain autoimmune disorders, hydroxychloroquine was identified as a potential treatment for the coronavirus after a February report from a French doctor claimed that a combination of the drug and Zithromax cured 100 percent of his small sample size of coronavirus patients. Despite later, more in-depth studies that have shown the drug combination has no apparent impact on the virus, conservative pundits and then the president seized on the idea that the drug might be some sort of cure. That has not proven true after more research. The National Institutes of Health halted its clinical trials of the drug on June 20, and the FDA revoked its emergency use authorization of hydroxychloroquine on July 1. Yet claims that hydroxychloroquine is an effective treatment for Covid-19 persist, with Trump claiming he took a two-week course of the drug in May.
Again, Twitter has taken an assertive approach when it comes to misinformation about Covid-19 treatments. Following the virality of the Immanuel video, the company made “Hydroxychloroquine is not an effective treatment for Covid-19, according to the FDA” one of its trending topics. This happened just hours after Trump castigated Twitter for never featuring “good” trends about him, calling the omission “illegal.” Twitter did not append a fact-check to that tweet, but there is no US law that says social media platforms have to feature good trending topics about the president.
As is often the case with social media companies moderating users who violate their policies, some are accusing Facebook and Twitter of censorship. The very fact that tech companies take down certain content can motivate other users to post it again. In fact, this process can be part of a strategy that involves posting controversial content on multiple platforms and domains so that the moderators end up in a game of whack-a-mole. And in many cases, a post or video might be too far on its way to going viral before Facebook or Twitter realizes that misinformation is being spread.
“It’s extremely difficult for them to detect these things before they’re uploaded,” explained James Grimmelmann, a law professor at Cornell who studies content moderation. “You’re not going to be able to use AI to distinguish true claims about Covid from false claims about Covid from false claims being presented to criticize.”
To avoid the problem of misinformation being shared too widely, Grimmelmann suggests that Facebook, Twitter, and other social media companies should set a threshold for looking at content that appears to be going viral and then have human reviewers look at it. “You should be able to have somebody look at it and make the decision before it has been [seen by] 17 million,” Grimmelmann said.
At this point, it might be too late for this new Covid-19 misinformation. On Monday evening, Immanuel complained that her Facebook profile page and videos had been removed, threatening that the entire platform would “be down in Jesus name” if they were not restored. She then went silent for about 13 hours. By Tuesday afternoon, Immanuel was back, and tweeted her video again. It gained 104,000 views and 11,200 retweets in 90 minutes and has yet to be removed.
  
  
    
    
      
        
  


  






      
    
    
  
  


Update 7:55 pm July 28, 2020: This article has been updated to include a statement from Facebook and additional commentary from experts.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  
  
  
    
    
      
        
  


  






      
    
    
  
  


We know that face masks help protect others from Covid-19, and it looks like they also provide some protection against facial recognition technology — for now. A preliminary study from the National Institute of Standards and Technology (NIST) analyzed how well the technology fared when identifying people wearing face masks. Broadly speaking, the facial recognition algorithms designed before the pandemic struggled to recognize faces behind the masks.
The new government study reveals less about how poorly facial recognition algorithms deal with face masks than they do about how companies are already hard at work building algorithms that can adapt to new situations. The pandemic is showing how face mask adoption might end up making facial recognition technology even more powerful than it was before. 
“The good news here is very short-lived,” Albert Fox Cahn, the executive director of the Surveillance Technology Oversight Project, told Recode. “This just highlights that there’s a global arms race right now to develop facial recognition software that can track people, even when we are wearing masks.” 
The error caused by mask-wearing isn’t too surprising. Anyone who’s tried to unlock their iPhone with Face ID while wearing a mask knows that the technology fails in the new scenario. Facial recognition algorithms are generally trained to identify you based on aspects of your facial geometry, and a face mask hides a huge portion of what the algorithm is trying to analyze, namely your nose and mouth, the NIST researchers explain. 
The extent to which face masks can trip up algorithms has been serious enough that, amid the George Floyd protests, the Department of Homeland Security sent out a notice in May warning that “violent adversaries” of law enforcement could take advantage of mask-wearing to avoid being spotted by facial recognition. Of course, protesters themselves were concerned about the exact same surveillance technologies being used to threaten their civil liberties. 
Now, the NIST research serves as evidence that masks are a real stumbling block for some facial recognition systems. The non-regulatory agency’s research looked at 89 facial recognition algorithms, including those from Panasonic and Samsung, and analyzed their performance on images of 1 million people. The study used photographs of people that were collected when crossing the United States border as well as images that had been included in applications for immigration benefits. The first group of photos was then “digitally masked,” meaning that artificial shapes in various colors that mimicked masks were superimposed on the images of faces, obscuring the subject’s nose, mouth, and part of their cheeks. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        This screenshot from the NIST study shows what various people looked like when a “digital mask” was applied.
      
      
        NIST
      
    
  


The NIST study found that wearing masks can reduce the accuracy of facial recognition algorithms, and according to the agency’s press release, “the best of the 89 commercial facial recognition algorithms tested had error rates between 5% and 50% in matching digitally applied face masks with photos of the same person without a mask.” Some vendors’ algorithms performed better than others, and performance varied based on the shape and color of the mask. Generally, facial recognition is more accurate when applied to people wearing round masks, while algorithms could be less accurate when the subjects “wore” black masks, compared to a light blue mask. 
Generally, this would seem like good news for those who are worried about their privacy and interested in finding ways to spoof facial recognition technology. But again, these types of errors are likely temporary, as companies that produce facial recognition technology are racing to update their algorithms to better adapt to face coverings. As Recode previously reported, firms were already touting their algorithms’ ability to account for masks as early as February, and Panasonic indicated it had cracked the mask problem even earlier. Since the pandemic started, a slew of facial recognition companies, including UK-based Facewatch, California-based Sensory, and the China-based firms Hanwang and SenseTime, have all begun to tout their ability to recognize people wearing masks. 
“I do think that this is a solvable problem, and that it will require continued research and development efforts to close the accuracy gap,” Shaun Moore, the CEO of TrueFace, whose technology was evaluated in the NIST study, said in an email. “The more (mask) data that we are able to train our algorithms on the better the performance will be.”
Fox Cahn, from the Surveillance Technology Oversight Project, offered a more dystopian interpretation of what’s to come. He dismissed the idea that concepts like anti-facial recognition shirts and make-up would be able to fool facial recognition technology in the future. “We’ll get to the point where the cameras are so prolific — and the technology is so powerful,” he said, “that anything short of a full bodysuit is going to be trackable.”




NIST also hinted that the struggles of the technology it reviewed are short-lived. One of the authors of the NIST report, computer scientist Mei Ngan, said the researchers “expect the technology to continue to improve” in identifying mask-wearing subjects. Accordingly, NIST plans to consider more algorithms that have been updated in order to recognize people wearing masks in its next round of research. Meanwhile, independent researchers are using photos of people wearing masks posted online to build databases of images intended to help improve their facial recognition algorithms, as CNET reported in May. 
Masks aren’t the first time facial recognition has been noted for inaccuracies. For years, facial recognition systems have been flagged for being disproportionately inaccurate on women, people of color, and especially women with darker skin. Lauren Sarkesian, a senior policy counsel at the think tank New America’s Open Technology Institute, told Recode that the issue of masks and facial recognition serves as a reminder that the technology remains broadly unregulated in the United States, and we often don’t even know when it’s in use. While some localities have passed laws regulating or banning government use of the technology, there is still no national law regulating facial recognition, though there are several proposals. 
“This technology is dangerous — both when it works and when it doesn’t,” Sarkesian said, “because as these accuracy issues are resolved in the algorithms, the surveillance power of the facial recognition technology grows.”
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  When Jeff Bezos appears before a committee of US Congress members on Wednesday for an antitrust hearing, it’ll be a first on several fronts: The first time the Amazon founder and CEO will testify before Congress. The first time Bezos will have to respond publicly to a damaging report alleging that Amazon uses data it collects from its own merchants to compete against them, which is something the company has told Congress it doesn’t do. And the first time in a very long time that the world’s richest man will be subjected to an extended and critical line of questioning in a public setting. 
It’s long overdue.
Bezos is slated to appear — via videoconference because of Covid-19 — alongside three other kingmakers of the digital economy: Apple CEO Tim Cook, Facebook CEO Mark Zuckerberg, and Sundar Pichai, the CEO of Google and its parent company, Alphabet. 
Over the last year, the House antitrust subcommittee, a bipartisan group of 15 lawmakers, has been investigating the dominance of these tech giants to determine if they’re abusing their power. And, if they are, whether regulators are simply not effectively enforcing current antitrust laws or whether Congress needs to create new laws on competition. After all, the core US federal antitrust law was created more than a century ago — many decades before the internet set the groundwork for a few industry titans to amass unprecedented power.
The leader of the subcommittee, Rep. David Cicilline of Rhode Island, has said from the start that he wanted to hear from these CEOs before he and his colleagues close the probe with a final report summarizing their findings and making recommendations. 
The committee cannot charge the companies with antitrust violations nor break them up in any way. But the lawmakers can recommend additions to existing antitrust law or suggest that new laws be crafted. The committee’s investigation, and the ensuing final report, could also apply pressure to the antitrust regulators at the Federal Trade Commission and Department of Justice to more aggressively enforce current laws to rein in the companies. 
Because the hearing will be virtual, it likely won’t carry the drama of other packed Capitol Hill hearings involving tech leaders, like Zuckerberg’s 2018 Facebook data privacy hearings in front of almost the entire US Senate. But for Bezos, and Amazon’s critics, this is still a big moment. The Amazon chief executive is the only one of the four CEOs who hasn’t had to face congressional questioning before. 
Cook testified at a US Senate hearing in 2013 to defend Apple’s strategies to minimize tax liability. Zuckerberg has testified in front of Congress on several occasions, about issues ranging from data privacy to political advertising. And Pichai appeared in front of the House Judiciary Committee in 2018, as lawmakers grilled him on topics ranging from allegations of the company having an anti-conservative bias to its potential plans to launch a search engine in China that would have allowed its government to censor search results. 
One likely reason Bezos hasn’t showed up before: Amazon has, for the most part, been a good thing for millions of online shoppers. It offers buyers incredible convenience, good prices, fast delivery, and a vast selection. How many of these Congress members’ constituents have a problem with that, especially in the US, where antitrust enforcers typically favor companies that treat consumers well? Instead, these enforcers typically target business practices or mergers that they believe will harm consumers, such as by raising prices for a product or service.  
But the problem for Amazon and Bezos is that the company has grown so enormous and expanded into so many different industries — from online retail to cloud computing to brick-and-mortar grocery and more — that its business dealings impact its customers, and millions of other Americans, in many other ways. Some of these people make their living by running small businesses as third-party sellers on Amazon’s marketplace. They’re facing increasing competition from Amazon, which sets all the rules and frequently changes its policies and the algorithms that power its platform in ways that can make or break their businesses. 
Other people impacted by Amazon policies work in Amazon warehouses and delivery centers, earning pay rates and benefits that are often superior to that offered by retail competitors — but some of these workers say they are asked to hit performance marks that are more suited to robots than humans. And then there are Amazon’s competitors — big and small across a growing number of industries — that have been flattened in the tech giant’s wake. When it comes to online retail, the pandemic has only accelerated the trend to online shopping and given Amazon a new leg up over many of its traditional retail competitors.
Increased media scrutiny of these issues is one reason this moment has finally come. Amazon’s market cap ($1.5 trillion) and Bezos’s staggering personal net worth ($184 billion) have probably played a role, too. And then there’s Lina Khan, an antitrust scholar who wrote “Amazon’s Antitrust Paradox,” a widely read legal paper that argued current antitrust doctrine is ill-suited to rein in internet giants like Amazon. Khan is now a lawyer on the congressional antitrust subcommittee.
Despite this growing scrutiny from politicians and the public, Bezos’s company has openly flaunted its power over the last few years. In its hometown of Seattle, Amazon successfully quashed a proposed payroll tax meant to combat homelessness by threatening to pause construction on its massive new Seattle headquarters, which would have boosted commercial activity in the city’s downtown neighborhood. 
Later, the company plowed funds into a local Seattle election to try to defeat those politicians who supported the payroll tax. It has also sparred with its hometown US House representative, Pramila Jayapal, in a baffling move that has cemented her status as a company critic while she also serves on the very antitrust subcommittee investigating the company.
In 2017, Amazon used its plan to build a second company headquarters, dubbed HQ2, to invite hundreds of municipalities across the US to compete with each other in offering tax cuts to incentivize it to move to their cities. After choosing New York City as one of two chosen HQ2 locations and garnering $3 billion in incentives, Amazon abruptly canceled its plans rather than negotiate with locals who criticized the move. 
On the labor front, the company, which is the country’s second-largest private-sector employer after Walmart, has alarmed some elected officials, including New York State Attorney General Letitia James and US Democratic senators like Elizabeth Warren and Cory Booker, who have investigated or inquired about the company’s practices. 
During the pandemic, Amazon has fired at least six of its own employees who either protested the warehouse conditions during the health crisis or spoke out about the company’s treatment of workers. (Amazon has said all were fired for violating company policies and not because of their dissent.) 
And in its core retail business, the company has continued to promote its own branded products on Amazon in questionable ways, even though this type of competition with its own sellers is at the very center of the antitrust subcommittee’s concerns. It didn’t help when the Wall Street Journal published a report this April citing former Amazon employees who say the company has used data to compete against its own sellers in ways that a top company lawyer previously said in Congressional testimony didn’t happen inside the company.
For these reasons, among others, Bezos’s time to answer to Congress is finally here. The company touches tens of millions of people across the US in myriad ways. And it has garnered even more business and labor power during the pandemic as brick-and-mortar shops either temporarily or permanently close their doors, and more shoppers take their shopping dollars online.
Bezos has said in the past that all large institutions deserve scrutiny and that Amazon welcomes it. 
“I say, ‘Look, we are a large corporation. We deserve to be inspected. It’s going to happen. Don’t take it personally,” Bezos said in 2018. “Because when you take it personally, you start to do things that are counterproductive.’”
“There’s only one way to handle it,” he added, “and that is that we have to conduct ourselves in such a way that when we are scrutinized, we pass with flying colors.”
On Wednesday, Bezos will need to do just that.
  
  
  
      




  At his Park Avenue penthouse — 62 floors high and with a sparkling nighttime view of the Manhattan skyline — billionaire Peter Thiel last fall introduced to his friends an immigration hardliner who he would back with over $1 million to try and transform the Republican Party.
The guest of honor was not President Donald Trump. It was Kris Kobach.
Thiel, a California venture capitalist, and Kobach, who is running in Kansas for the Republican nomination for the US Senate, were quite far from Kansas that evening. But the alliance between the two iconoclasts has reshaped the race in that state, giving Kobach a puncher’s chance to beat the GOP establishment — possibly upsetting Republicans’ hopes to hold their Senate majority.
In the months since that fundraiser, Thiel has pumped $850,000 into a super PAC behind Kobach — and that could be just the start if he wins. The investment in Kobach is Thiel’s most significant political bet since he risked his standing in Silicon Valley to support Donald Trump in 2016. And the wisdom of that bet will be tested just next week, when Kobach faces the voters of Kansas in a race that polls show to be tight.
If Kobach is elected, it could give Thiel a loyal ally in the Senate because of how powerful he is proving to be in the race. That alliance becomes all the more key if Trump loses this fall, which would weaken the Washington influence of Thiel, a board member at Facebook and a founder of Palantir, which has deep ties to the government.
Patrick Miller, a political scientist at the University of Kansas who is closely following the primary, told Recode Thiel’s money was “absolutely critical to Kobach being a viable candidate.”
“I don’t want to overstate it and say that Kobach wouldn’t have a campaign without him. But I think the money that that super PAC is putting into the race — primarily through this one rich guy — is absolutely the lifeblood of the pro-Kobach campaign at this moment,” Miller said. “You take that money away and Kobach doesn’t have a lot of campaign left.”
Thiel and Kobach, neither of whom returned requests for comment, are not personally close. Friends of Thiel’s say his support of Kobach hasn’t come up in recent conversations. But Thiel likes to “collect” politicians who he finds well-credentialed and intellectual, particularly those who fight with the elites or the media.
“He has a really strong preference for people who stick their middle finger up to the status quo and conventional wisdom,” said a person familiar with Thiel’s political thinking. “There is nobody who I think was more obviously sticking his middle finger up at conventional wisdom quite like Kris Kobach.”
Both with a zest for the controversial, the litigious, and the troll, Kobach and Thiel have also each collected enemies in their political crusades. They both inveigh against identity politics, the Washington establishment, and a globalism that doesn’t put America first. 
Kobach has said Thiel first reached out to him in 2005, which is when Kobach, then a lawyer, began challenging a California state law that gave in-state tuition to undocumented immigrants. A self-professed libertarian, Thiel, who was born overseas and is himself an immigrant, has long had sharply restrictionist views on immigration; back in 2008, he reportedly donated $1 million to NumbersUSA, a hardline immigration group which today is a prominent backer of Kobach’s.
Kobach, as Kansas’s secretary of state from 2011 to 2019, emerged during the ensuing decade as one of the most polarizing figures in national politics, urging the GOP to more seriously investigate voter fraud (of which there is limited evidence) and to crack down on illegal immigration. He had, however, always been seen as a fringe player. That was until Trump arrived. 
Kobach has since advised Trump’s administration on implementing a “Muslim registry” that would have specifically tracked immigrants to the US from Muslim-majority countries. He’s also weighed in for the administration on voting issues, was considered for Cabinet posts by a transition committee on which Thiel served, and earned Trump’s endorsement during his unsuccessful bid for governor of Kansas in 2018.
That race was when the Thiel-Kobach bond was tied. Thiel secretly put somewhere between $250,000 and $500,000, delivered in two tranches, into an outside group backing Kobach’s gubernatorial bid, according to a person familiar with the gifts. Some details of Thiel’s stealth donations in that race were previously reported by the Kansas City Star.
For Thiel, the Harvard graduate in Kansas fits in with a new crop of Ivy League-bred, pugilistic Republicans Thiel has cultivated as friends. Despite his proud support for Trump in 2016, Thiel’s influence in Trump’s orbit has faded and he has reportedly soured on the president, not even yet making a donation to his campaign in the 2020 cycle, compared to the over $1 million he spent to back Trump four years ago. Nowadays, Thiel has instead grown closer to rising stars such as Josh Hawley, the Missouri senator who has made a name for himself by fiercely going after Google, which the tech billionaire wants to investigate.
Thiel’s only checks to candidates this cycle have gone to Tom Cotton, another ambitious, Harvard-educated GOP senator, and Kobach. Unlike Hawley, though, Kobach has barely, if ever, weighed in on tech issues.
But part of their alliance can be explained by more personal reasons. A key lubricant binding Thiel to his new candidate is Ann Coulter, the conservative provocateur who last week called Democrats the “antifa party.” She is close with Thiel, and is a Kobach super-fan.
Asked by Recode why she thought Thiel and Kobach felt such a kinship, Coulter took a dig at the GOP establishment and offered that “it might have something to do with Kobach having twice the I.Q. of Mitch McConnell.”
Coulter and Thiel have been fellow rabble-rousers in the conservative movement since at least 2010, when Thiel introduced her to speak to “Homocon,” a conference for gay conservatives held at his apartment. Coulter would even dedicate one of her books to him the following year. The two remain close, attending Hollywood parties together.
It was Coulter who co-hosted that Kobach fundraiser at Thiel’s apartment last fall, an event that brought Kobach before wealthy people like GOP power broker Rebekah Mercer and Erik Prince, the founder of the military contracting firm Blackwater.
“You couldn’t turn around last night without stepping on a billionaire,” Coulter said of the Thiel event in one radio interview.
All that billionaire money has drawbacks, though. While there is a history of rich out-of-staters spending big money to influence Kansas politics, Thiel’s involvement poses a vulnerability.
“Kris’s support comes from those who don’t care about the people of Kansas. He is relying solely on help from Democrats, Hollywood, and New York City,” said Eric Pahls, the campaign manager for Rep. Roger Marshall. Marshall, currently a US representative for Kansas, is the main Republican rival Kobach faces in the primary for the Senate seat.
Thiel made the decision to donate to the super PAC, called Free Forever, after meeting with its leadership in New York last year and hearing a pitch on Kobach’s chances and why the Senate race wouldn’t turn out like the governor’s race, according to a source familiar with the matter. The tech billionaire has no direct decision-making power at the super PAC — he is merely briefed on the ads and other content it produces after it’s made. But Thiel is pleased enough that he’s cut at least three successive checks to the group, the most recent for a half-million dollars last month.
The expectation that Thiel will sink in even more money — rumors swirl in Kansas politics about the precise size of Thiel’s budget — has kept rival campaigns on their toes. 
The super PAC Thiel is backing has spent more than four times what Kobach’s campaign itself has spent on television and radio ads, according to data shared with Recode by the media tracking firm Medium Buying. The heavy amount of mailers sent by the PAC have run the gamut of attacking Marshall as “anti-American” for being insufficiently tough on immigration, alleging that he voted to fund “Rosie O’Donnell summer camp,” “global warming musicals,” and “transgender plays,” and promising that Kobach will “stop the next Ruth Bader Ginsburg.” Thiel may not be making decisions, but the messaging feels Thiel-esque.
If Kobach wins next week, he’ll have a tough general election on his hands, too. Even more Thiel money would almost certainly be needed.
And if Thiel does donate more in the fall, he will likely end up spending more money on Kobach’s behalf than he had on behalf of his previous bet: Trump.



  
  
  
      





    
  
    
    
      
        
  


  






      
    
    
  
  



Now that airports are emptier than they’ve ever been, the air travel security platform Clear is adapting its technology for new pandemic-centric purposes. On Thursday, the NHL announced that it would be using the tech to screen players and staff during the playoffs, and earlier this month, the employees of the 9/11 Memorial & Museum started using the screening platform to allow staff back onsite. The new moves are a sign that Clear wants to use its tech to get people back to work — and back to big events.
The basic premise is that regular screenings using a new Clear service called Health Pass can keep people who might have Covid-19 out of high-traffic spaces. Basically, its approach involves using new systems that can quickly identify whether someone might have Covid-19. Right now, that process could include a daily health quiz, temperature scans, and linking users to their lab results — while also confirming their identity through biometrics, which can include iris and fingerprint scans as well as facial recognition. The concept has already drawn skepticism from public health and privacy experts.
This new line of business represents a pivot for Clear, which is best known as the company that offers paid memberships to bypass airport security lines. While everyone else is stuck in the slow line waiting to speak to a TSA agent to check their travel documents and confirm their identity, Clear members can bypass the line and speed straight to the baggage screening. That’s because in addition to paying Clear’s membership fee, customers have volunteered more of their personal information, mainly their biometric data, to help airport security quickly confirm their identity at the airport. Importantly, while Clear’s technology has been certified by the Department of Homeland Security, it is not the same as the Transportation Security Administration’s (TSA) Pre-Check. However, as Clear is quick to point out on its website, fliers can purchase both.
Clear isn’t the only company pushing futuristic, sometimes questionable technology meant to help us return to some sense of normal. Universities have urged students to promise that they’ll download health screening apps. Facial recognition technology companies have pushed versions of their tech that, for instance, incorporate “fever-detection.” Some have even floated artificial intelligence-enabled cameras that can measure how well people are social distancing.
But Clear stands out as an established brand that’s become a familiar sight at airport security checkpoints and already has quite a few customers. The company claims that it has confirmed identities more than 50 million times since it relaunched in 2010. At that time, Clear had been acquired out of bankruptcy but was already operating in at least 16 airports. Now, Clear technology is used in more than 50 airports, as well as other large venues including sports stadiums like the AT&T Center in San Antonio and the American Airlines Arena in Miami. 
While the venue expansion started a few years ago, Clear announced its Health Pass program in May and now has its sights set on a number of new partners, including restaurants. Clear Health Pass is being used to screen food service employees — keeping potentially sick people from returning to shared work areas — through partnerships with the restaurant group that runs the salad chain Chopt and the taco restaurant Dos Toros, through which Clear is currently running pilots at two locations. Clear’s health screening tech is also being used by the 450 staff members of the 9/11 Memorial & Museum, which is currently using the platform’s health quiz.
The company’s new partnership with the NHL will mean its Health Pass system will be used to coordinate this season’s playoffs in Edmonton and Toronto, Canada. According to the company, the system will help track about 3,000 people who are involved, including players, coaches, and support staff. In total, 24 teams, including the Pittsburgh Penguins and the Chicago Blackhawks, are expected to play. Their announcement comes as other organizations, like the NBA, and large venues, like Disney World, turn to other health safety initiatives in an effort to restart their activities. 
In the case of the NHL, the idea isn’t just to do screenings but to create closed mini-ecosystems of secured areas. In each city where there are games, there will be at least 30 physical access points that are managed with kiosks that will take temperatures. After registering their identity within Clear’s biometric system, those involved in the playoffs will be expected to fill out a regular health survey after leaving hotel rooms, take a selfie with their phone to confirm their identity, and then use a QR code to register their information at the kiosk and have their temperature taken. 
While the company’s current offering seems more limited to health quizzes and temperature checks, Clear appears to be setting the groundwork for including more detailed medical information in its screening process down the line. On its website, the company advertises that the “possibilities are endless,” and hints that “vaccine status” could one day be incorporated, seemingly echoing contentious proposals for immunity passports that the World Health Organization has warned against. 
Unsurprisingly, privacy advocates are skeptical of the new Clear Health Pass, which they argue doesn't really solve the problem of keeping an area free of infectious people. If anything, they say, the service introduces a host of privacy concerns. They worry that deploying this Health Pass in a wide range of venues could facilitate unwanted surveillance and become a ubiquitous part of life. 
“Forcing someone to upload extensive medical information to a privately run centralized database that’s then going to decide whether or not they can enter public accommodations or potentially go to work or find a plane, that’s really a nightmare scenario,” John Davisson, an attorney at the Electronic Privacy Information Center, told Recode. 
Kenneth Goodman, the director of the University of Miami’s Institute for Bioethics and Health Policy warns that a program like Clear’s Health Pass could become a form of “security theater.” 
“What exactly is any company going to do to ensure that people who get through security, come to the party, get on the plane, go to the restaurant, are actually free of the virus?” Goodman said. “The idea that fever detection or a quiz — which of course you could lie to — is going to improve our safety on the streets or at airports or stadiums or anywhere else is so far not particularly based on solid, peer-reviewed evidence.” 
Albert Fox Cahn, the executive director of the Surveillance Technology Oversight Project, called Health Pass an “absolutely horrifying product.” He points out that Clear’s ultimate goal seems to be based on the assumption that people have access to regular testing, which is a questionable premise in the United States. Also, public health experts still don’t have a full understanding of how Covid-19 immunity works or how antibody tests should be factored into the equation. 
“After 9/11, we passed emergency surveillance powers, and we were told, ‘don’t worry, they’re going to go away in a few years,’” Fox Cahn told Recode. “As of this year, we’re still debating whether to renew them. We have to assume that any surveillance infrastructure we build to respond to this crisis will become a permanent facet of American life.” 
Lawmakers are also worried about what Clear’s doing. In May, Sens. Jeff Merkley and Cory Booker, who have called for regulating facial recognition, sent a letter to Clear demanding more information on its pandemic-related business plans, pointing to the Health Pass in particular. The senators specifically warned that “facial recognition technology risks a state of undetectable, constant government surveillance that can track one’s movements and associations with organizations such as schools and places of worship.” They also expressed concern about algorithmic bias, which could make Clear inaccessible to some users. 
In a June reply to the letter from Sens. Merkley and Booker, Clear chief executive Caryn Seidman-Becker wrote, “We do not conduct passive monitoring and we do not collect images of non-members.” She also emphasized the opt-in nature of Clear’s system as well as the company’s commitment to security. With regard to Health Pass, she said “biometric data is not shared with employers or venues, and our biometric tools are not themselves used as diagnostic tools.” Seidman-Becker also said that her company considers “racial and gender equitability” when choosing their facial recognition algorithms, which Clear purchases from third-party providers. 
A Clear spokesperson told Recode that police and governmental agencies can’t access the data it oversees and that Clear would not share personal information about its members.
At the same time, Clear seems to be eager to become the technology platform that allows places to reopen, both for the short and long haul. Despite the concerns of privacy advocates who doubt the service’s utility and privacy protections, the new Health Pass system is winning big clients, like the NHL. And as the pressure to return to normal life continues, who knows where you might run into Clear technology next. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Facebook has agreed to pay $650 million to settle a years-long class action lawsuit over its use of facial recognition in what continues to be a less-than-great summer for the social media company.
The lawsuit is over Facebook’s photo-tagging feature, which used facial recognition software to identify faces in users’ photos. The state of Illinois, however, has a law against businesses collecting biometric data — which includes facial recognition — without first obtaining consent. Illinois argued that Facebook didn’t get that consent before enabling the new feature by default to its millions of Illinois users, and sued the company in 2015. 
Facebook has denied it did anything wrong, but it tried to settle the suit for $550 million in January. That offer was rejected by a judge for not being enough; as NPR reported, the $550 million figure would result in payouts of just $150 to $300 per person. Facebook could have been on the hook for $47 billion if it lost the suit and affected users got the full payout of $5,000 per person allowed by the law. 
“It’s $550 million,” the judge said in a June 4 hearing, according to court documents obtained by Recode. “That’s a lot. But the question is, is it really a lot?”
He continued, “That is a significant reduction from the $1,000 that the Illinois legislature set as the baseline. ... The Illinois legislature set this privacy expectation and privacy right as a serious one, and they put a high price tag on it for that reason.”
So Facebook has now added an additional $100 million to its proposal. According to court documents, that will net each affected user $200 to $400 each. The settlement was agreed to by both sides of the suit but has yet to be approved by a judge.
As it did back in January, Facebook told Recode, “We are focused on settling as it is in the best interest of our community and our shareholders to move past this matter.”
The new settlement comes as facial recognition software is more controversial than ever. IBM, Microsoft, and Amazon are all pausing or limiting law enforcement’s access to their technology in response to anti-police protests, and the New York Times recently reported on the first known false arrest based on facial recognition. At the same time, private companies are increasingly turning to facial recognition software, using it in photo-tagging services (like the aforementioned Facebook) or identity verification to unlock devices (like Apple’s iPhone). There’s also proposed bicameral legislation from Democrats that would ban the use of facial recognition technology by federal law enforcement and withhold federal funding from state and local agencies that don’t enact similar bans.
It also comes as Facebook weathers a summer that probably didn’t go as well as it once hoped. The company faces an antitrust investigation from Congress (along with Amazon, Apple, and Google), and CEO Mark Zuckerberg will be testifying before the House Judiciary Antitrust Subcommittee on July 27. 
In June, Facebook’s own employees protested the company’s decision not to take action on posts from President Trump that threatened to respond to protests with gunfire, saying “when the looting starts, the shooting starts.” That incident led to civil rights groups calling for an ad boycott from many major companies over its perceived inability (or lack of desire) to stop hateful content that proliferates on its platform. And, a few weeks ago, Facebook released the results of a civil rights audit — which it largely failed.
Facebook has tried with varying success to respond to these controversies. The company cracked down on some Trump content, for instance, by removing campaign ads that featured possible Nazi imagery. Facebook also recently announced that it would be studying racial bias in Facebook’s and Instagram’s algorithms. And, at the end of June, Zuckerberg announced that the company would be doing more to remove or label posts that incite violence, contain hate speech, or suppress voting. But many still feel that these measures are too little and too late.
One possible bright spot in all of this for Facebook, actually, is that $650 million settlement. While it’s obviously a lot of money, it’s far less than the $47 billion it could have lost if the case went to trial and, if accepted by the judge this time, means the end of a costly lawsuit. Facebook’s most recent quarterly earnings report showed revenue of nearly $18 billion, almost all of it from advertising. A $650 million loss will barely register.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      




  TikTok was never supposed to be political. When it launched in the US in 2018, the video app was marketed as a fun place to discover goofy content and experiment with its sophisticated editing software and vast music library. Yet nearly two years and 165 million nationwide downloads later, TikTok has been a platform for teachers strikes, QAnon conspiracy theories, Black Lives Matter protests, and a teen-led campaign to sabotage a Trump rally in Tulsa, Oklahoma. The TikTok algorithm is perfectly suited to spread political content faster and to a wider audience than any social media app in history, whether the company wants to admit it or not. 
Now TikTok is proving itself to be political in a much broader way, one that challenges the very existence of the app. White House officials are talking seriously about attempting to ban it (how the government would choose to do so is less clear) in the wake of rising tensions with China, where TikTok’s parent company ByteDance is based. 
There are two major factors at play when we talk about the risks TikTok’s ownership could potentially pose: data privacy and censorship. While the former is potentially easier to understand (the Equifax hack, where members of the Chinese military were charged with stealing the personal information of 145 million Americans, is perhaps the most famous example), the latter, which includes how TikTok instructs its moderators and changes its algorithm, could have more existential — and more difficult-to-predict — consequences for the US at large. 
Will a ban actually happen? President Trump’s chief of staff, Mark Meadows, said in July that a decision could come in “weeks, not months.” But the conversation is a lot more complicated than “Is China stealing our data?” although that’s likely how the Trump White House would prefer to frame it. TikTok has become a straw man for fears over a serious competitor to Silicon Valley: If a generation of kids is synonymous with an app owned by China, what does that mean for America’s role in global technology? 
Experts in cybersecurity and Chinese tech make it clear that the issue is not black and white, and that serious concerns about national security are likely rooted not in xenophobia but in the fact that the Communist Party of China (CCP) under President Xi Jinping has a track record of surveillance, censorship, and data theft. There are also those who warn that the US banning TikTok and other Chinese-owned apps could set a dangerous precedent for a less free and open internet — ironically, the sort of internet modeled after that of China. 
A short history of the US government’s TikTok anxieties
The government’s interest in TikTok’s ties to China and its communist leadership stems from last fall, when Sens. Marco Rubio (R-FL), Chuck Schumer (D-NY), and Tom Cotton (R-AR) called for an investigation into the company. Their statements came after reports from the Guardian and the Washington Post revealed that TikTok had at one point instructed its moderators to censor videos considered sensitive by the Chinese government.
By November, the Committee on Foreign Investment in the United States (CFIUS), which investigates the potential national security implications of foreign acquisitions of US companies, announced that it would be reviewing ByteDance’s acquisition of Musical.ly, the app that would become TikTok. Meanwhile, TikTok has been steadfast in its claim that it does not send US user data to China and does not remove content sensitive to its government and would not if it were asked. Two Chinese intelligence laws from 2014 and 2017, however, require companies to assist with any government investigation and hand over all relevant data without refusal. 
In a statement to Vox, a TikTok spokesperson wrote: 
Protecting the privacy of our users’ data is of the utmost importance to TikTok. There’s a lot of misinformation about TikTok right now. The reality is that the TikTok app isn’t even available in China. TikTok is led by an American CEO, with hundreds of employees and key leaders across safety, security, product, and public policy in the U.S. TikTok stores U.S. user data in Virginia, with backup in Singapore, and we work to minimize access across regions. We welcome conversations with lawmakers who want to understand our company. We’re building a team here in Washington, D.C. so lawmakers and experts can come to us with questions or concerns. We know that actions speak louder than words, which is why we’re opening Transparency Centers in LA and DC so that lawmakers and invited experts can see for themselves how we moderate content and keep our users’ data secure.
In early July, Secretary of State Mike Pompeo told Fox News that the US was considering a TikTok ban after months of rising tensions with China and a ban of more than 50 Chinese apps including TikTok in India the week prior. Since then, TikTok users have been panicking over the potential loss of the internet’s greatest time waster; the Senate just advanced a bill to ban TikTok from all government devices. Facebook, too, is closing in: The company announced it will launch its copycat product, Instagram Reels, in the US in August. 
“Banning” TikTok isn’t as straightforward as it may sound in a country built upon the First Amendment, but there are several ways it could take place. The first is that CFIUS could force ByteDance to sell off TikTok to a US-owned company by determining it a national security risk (that’s what happened to Grindr after it was sold to a Chinese company). Another is that it could put TikTok on what’s called the “entity list” so that US companies like Apple and Google would be forced to remove it from their app stores. Adi Robertson at The Verge has a thorough examination of all these possibilities, but let’s get to the real issue at play. 
The case for banning TikTok: Some experts say China cannot be trusted to run a global tech company
The case for banning TikTok, for many cybersecurity professionals, is relatively simple: The risk is simply too great, no matter how wonderful the content on the app may be. Kiersten Todt, managing director of the Cyber Readiness Institute, says that despite what TikTok claims, “If the Chinese government wanted that data, they would be able to get that data.” 
While that may not scare the app’s large user base of teenagers who are pretty sure the Chinese government doesn’t care about their scrolling habits, Todt says it’s possible China could be building dossiers on high-profile individuals, including information like passwords, bank accounts, internet addresses, or geolocation, all of which could then be cross-referenced with even more personal data on other apps. 
“I’ve been in the national security space for a couple of decades, and there is decades’ worth of evidence and data around Chinese interest, intent, and capability to hack the US, whether that’s through intellectual property or through data theft,” Todt says. “The Chinese government hacked the broadest database of personnel in the US government. They’re the only ones who have done that.”
Todt’s other concern relates to China’s role in the global tech wars at large. “Artificial intelligence is only as good as the data that goes into it, and so if China continues to collect all of this data from populations around the world, its artificial intelligence has a lot more data input into it. How might it aggregate that data for the purposes of innovation, research and development and science?” she asks. “That can sound xenophobic, but it is a national security statement, just as we are cautious about Russia and Iran and North Korea for different reasons.”
There are other arguments for banning TikTok, ones that relate to moderation and censorship. “I find the data privacy issue to be a bit of a red herring,” says Jordan Schneider, host of the ChinaTalk podcast and newsletter. “The Chinese government has many likely more impactful ways of getting blackmail or corporate secrets or just general information about individual US nationals.”
Instead, Schneider argues that the problem is the Chinese Communist Party’s potential ability to influence conversation about politics on the app. “People today are very concerned about the amount of power [Facebook’s] Mark Zuckerberg has to value one type of speech over another or impacting elections by tweaking the algorithms and end up changing people’s opinions on certain things. So imagine if someone with the equivalent of Mark Zuckerberg’s level of power over the US has no choice but to do what the CCP wants it to do? My sense is that is the case with ByteDance.” He uses recent examples of Chinese disinformation campaigns on Twitter, Facebook, and YouTube around topics like the Hong Kong protests and Taiwanese independence. 
“I think they’ve probably learned the lesson of 2016, which is that Russia can interfere in elections and basically get away with it,” he says. What might that look like? For the average TikTok user, it won’t really look like anything. “You can just push certain videos more than others, and there’s no open API to double-check these things,” Schneider says. “At the end of the day, the Chinese government clearly has the leverage to push ByteDance to do this sort of thing, and would honestly be dumb not to, because the prize is enormous, which is the ability to influence who the next president of the United States is.” 
The case for keeping TikTok: Chinese tech companies are not synonymous with the Chinese government 
It would be easy to leave it there, but Samm Sacks, a senior cybersecurity policy fellow at Yale Law School’s Paul Tsai China Center and New America who has testified before the Senate Judiciary Committee, warns against conflating Chinese tech companies with the CCP. “There is much more of a push and pull in that relationship there, particularly around the security services’ access to private data,” she says. 
Plus, she argues that the incentive — to censor content and steal user data — is worth less than owning one of the world’s most important global tech companies. “TikTok was intended to thrive and fly on its own overseas, and so it’s not necessarily in the Chinese government or ByteDance’s interest to set up the company to be secretly beholden to Beijing. There’s a commercial incentive at play that I think we have to take into account.” 
TikTok has, for many people in American politics and tech, become an existential threat that no amount of distancing itself from China — building headquarters in the US and London, hiring a former Disney executive as its CEO — will undermine. TikTok’s terms of use and black box algorithm are virtually identical to Facebook’s policies, but its success has foreshadowed a potential end to Silicon Valley’s dominance. Unspoken in many tech executives’ dismissal of TikTok is protectionism and, arguably, xenophobia. 
Should the US government ban TikTok, Sacks says, it would be “an important step toward the US government controlling the way that Americans use the internet, which is ironically a step toward Beijing’s own cyber-sovereignty, the very thing we’ve been railing against for years.” 
It also would likely be against the US’s commercial interests. “It offers a blueprint for others around the world to think, ‘Maybe we don’t trust the way that Silicon Valley companies are handling our data, so let’s just ban them, too,’” she says. “We’re already starting to see the rise of digital sovereignty in Europe and in India in these really important markets, and when we think about the so-called tech competition with China, particularly with artificial intelligence and machine learning, what is it that’s going to give US companies an edge? It’s access to large international data sets. If we are increasingly closed out of markets around the world and access to that data because we’ve helped create a blueprint for how to do it with China, I could see those same tools turned around on us.”
Instead, Sacks has called for a comprehensive federal data privacy law that would be applied to all platforms, not just Chinese-owned ones, that would create standards for better data security, algorithmic transparency, and better management of online content. “All of the things that I think we’re using is China as a foil and saying, ‘That company is a threat, let’s stamp them out,’ [could be dealt with by] developing our own vision for how we want to govern the internet in a more democratic, secure way,” she says.
China aside, a TikTok ban would have serious effects on American youth culture, where hundreds of teenagers have now built massive followings and spread important political messaging on an app that allowed them to reach huge audiences. It’s changed not only the experience of being online but the experience of being a young person. 
TikTok has serious flaws — conspiracy theories in particular, some related to QAnon, Pizzagate, and the coronavirus, have thrived unchecked on the app — but there’s still no evidence that the Chinese government has anything to do with any of those. Would setting a precedent against any one Chinese-owned tech company solve the immediate issues that affect American social media users, namely misinformation, content moderation, and transparency? Or would it allow Silicon Valley companies like Facebook to continue to mimic competitors’ software and grow ever larger and more powerful? It’s now in the hands of the government to decide. 

  
    
      
        

    
    vox-mark
    
    
    
        
    

      
    
    
      
        Sign up for the
        
          newsletter
        
      
      The Goods
    
    
      Get our newsletter in your inbox twice a week.
    
    
  
    
  
  
  
    
      
        
          
            Email (required)
          
          
        
        
          
        
      
    
  
    
      
        By signing up, you agree to our Privacy Notice and European users agree to the data transfer policy.
        For more newsletters, check out our newsletters page.
      
    
    Subscribe
  


    
  


  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Facebook has announced it’s building teams that will study racial bias baked into the algorithms used on its platform and on Instagram, which it owns. The move is a significant acknowledgment that the algorithms driving two of the most influential social media platforms can be discriminatory.
Instagram will create an “equity team” charged with tasks like analyzing the enforcement of its harassment policies and studying its algorithms for racial bias, the Wall Street Journal reports. Facebook spokesperson Stephanie Otway told Recode that the team will continue to work with Facebook’s Responsible AI team to study bias, and added that Facebook will also create a similar equity team. 
“The racial justice movement is a moment of real significance for our company,” Vishal Shah, a vice president of product at Instagram, said in a statement. “Any bias in our systems and policies runs counter to providing a platform for everyone to express themselves.” 
Algorithmic bias can be pervasive and impact how a platform treats users by affecting the content and ads they see as well as the way their own posts get filtered. Users can have trouble spotting algorithmic bias on their own since, for example, most can’t necessarily compare their News Feed with those of other users. Researchers, civil rights groups, and politicians have sounded alarm bells about algorithmic bias on its platforms, and now Facebook is devoting more resources to addressing the problem. 
Notably, the Facebook news comes amid an advertising boycott of the platform organized by major civil rights groups, including the NAACP and the Anti-Defamation League, and just two weeks after the company shared the results of its civil rights audit, which panned Facebook for failing to address racism and misinformation on its site. 
Ahead of the boycott, Instagram acknowledged and pledged to deal with algorithmic bias on its platforms more directly. As demonstrations against police brutality and racism swept across the United States in mid-June, Instagram head Adam Mosseri announced that the company would look into racial bias on Instagram, including in its account verification policies and approach to content filtering and distribution. 
“While we do a lot of work to help prevent subconscious bias in our products, we need to take a harder look at the underlying systems we’ve built, and where we need to do more to keep bias out of these decisions,” Mosseri wrote at the time.

  
    Related
  

  
    Why algorithms can be racist and sexist
  

We don’t know much about the new efforts yet. Facebook’s Otway emphasized that these initiatives are still in the early stages and said the new team will be charged with reviewing a wide variety of issues that marginalized groups may encounter on the Instagram platform. As an example, she suggested that the company will get behind tools that focus on supporting minority-owned businesses. 
The company seems especially willing to invest in efforts analyzing the role of bias in its systems after its recently concluded civil rights audit highlighted two pilot programs at the company: a Facebook-built tool called Fairness Flow and a fairness consultation process launched in December. The auditors also called for Facebook to establish “processes and guidance designed to prompt issue-spotting and help resolve fairness concerns” that employees company-wide must follow. 
“The company certainly has the resources to be more proactive and aggressive in its actions, to be a leader,” Kelley Cotter, a graduate student who studies public understanding of algorithms at Michigan State University, told Recode at the time of the audit. “That Facebook still appears to be in an ‘exploratory’ phase after years and years of civil rights complaints evidences its reluctance to prioritize public values like equity and justice over its private interests.” 
Automated tools can discriminate in myriad ways. Bias can be inherent in algorithms and artificial intelligence based on who builds these technologies, which assumptions are programmed into them, how they’re trained, and how they’re ultimately deployed. One notable source of this bias can come from data: If an algorithm is trained using a database that isn’t representative of a particular demographic group, it’s very possible the algorithm will be inaccurate when applied to people who are part of that group. 
Algorithmic bias can have life-changing and dangerous impacts on people’s lives. Résumé-screening algorithms can learn to discriminate against women, for example. Facial recognition systems used by police can also have racial and gender biases, and they often perform worst when used to identify women with darker skin. In June, we learned of the first known false arrest of a Black man living in Michigan caused by a facial recognition system. 
On social media platforms built by Facebook, there’s concern that bias could show up anywhere an automated system makes decisions, including in how Instagram filters content  and whose posts get flagged by Facebook’s content moderation bots. 
There’s also concern that the lack of racial diversity among Facebook’s employees could hinder its efforts to make its product more equitable. Just under 4 percent of roles at Facebook are held by Black employees, and just over 6 percent are held by Hispanic employees, according to the company’s diversity report. Facebook would not share statistics on the racial diversity of the teams that work on its algorithms and artificial intelligence. According to Nicol Turner Lee, the director of the Brookings Institution’s Center for Technology Innovation, “Without representative input, the company may find itself generating trade-offs that further the differential treatment or disparate impacts for communities of color.”
Meanwhile, the capacity these systems have to be discriminatory is why some say the algorithms themselves need to be externally audited, which Facebook thus far has not opted to do. 
Facebook “seems to plan to keep the results of its research in-house,” Nicolas Kayser-Bril of Algorithm Watch told Recode after the announcement of the new teams. “It is unlikely that, were the new ‘equity and inclusion team’ to make claims regarding discrimination or the remediation thereof, independent researchers will be able to verify them.”
After all, Facebook can say it’s improving its algorithms again and again, but it’s not clear how those outside the company, including Facebook and Instagram users, would ever know if the changes were actually making an overall difference.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.


  
  
  
      




  On Tuesday, Facebook finally labeled President Trump’s misleading posts about voting — but it’s not the fact-check that people have been asking for. 
Instead, the social media platform is enforcing a new policy to add a link to official voting information any time a Facebook user — whether an everyday user or a prominent politician — posts anything about voting in the US. Democratic party leaders, including a Biden campaign spokesperson, have sharply criticized the move, arguing that it’s too little, too late. 
That’s because Facebook is adding the label to posts about voting regardless of whether they share accurate or inaccurate information. It added a similar link to voting information to some of Democratic presidential candidate Joe Biden’s recent posts, including one that said, “We have to vote Donald Trump out this November.”
The Trump post Facebook labeled on Tuesday makes an unsubstantiated claim that mail-in voting will lead to a “corrupt” and “rigged” election. The label underneath Trump’s post doesn’t comment on the content at all and instead links out to instructions on how voters can register to vote by mail. 
Facebook’s new voting post policy is a part of its broader effort, first announced in June, to inform US voters about the upcoming presidential election. The new policies came amid widespread criticism that Facebook isn’t doing enough to stop the dissemination of voter misinformation by President Trump — particularly after Twitter for the first time labeled a previous Trump tweet about mail-in voting as containing “potentially misleading” information in May.
Biden campaign spokesperson Bill Russo called Facebook’s policy an “absolute, abject failure” in a tweet on Tuesday morning, in one of two tweets criticizing Facebook’s implementation of the new labeling policy.
Russo did not return Recode’s request for further comment on the matter; another spokesperson for the Biden campaign, Mike Gwin, declined Recode’s request for comment. 
Facebook spokesperson Andy Stone declined to comment to Recode about Democratic party leaders’ concerns about the labeling but noted that the recent labeling was part of Facebook’s voter information plans announced last month. 
Several other Democratic party leaders also weighed in on Twitter, arguing it was unfair for Facebook to label Biden in the same manner as Trump. They said Biden’s post simply shared his opinion about whom to vote for while Trump’s post was denying the legitimacy of the US election process. 


Trump campaign taking full advantage of the false equivalency set up by @Facebook's spinelessness on voting misinformation.https://t.co/oFEM6Pi9Tp https://t.co/NY37aOcQyh pic.twitter.com/I507XcU7cT— Timothy Durigan (@timothydurigan) July 21, 2020



One Democratic operative who spoke with Recode on the condition of anonymity called Facebook’s labeling of Trump’s posts “laughably inadequate.” 
“They [Facebook] are essentially just putting a link in the post for Trump and not in any way saying the content of the information might be wrong or not widely trusted information,” said the Democratize operative, who spoke on the condition of anonymity because they don’t have clearance to speak with the press. “You’re letting the president of the US go on your platform and say something that’s not correct about mail-in voting.”
In response to Facebook’s labeling of Trump’s posts, a spokesperson for the Trump campaign, Samantha Zager, shared the following statement with Recode:
“The President was absolutely correct. Universal vote by mail is ripe for fraud and would lead to a corrupt election. The same label has been applied to posts on Joe Biden’s page.”
Trump’s claims that voting by mail in the US will lead to fraud are unsubstantiated and widely disputed by leading political science and election integrity experts. 
Aside from the labeling, Facebook has been pushing to register 4 million of its US users to vote — something Zuckerberg has previously framed as a better way for his company to support democracy than by policing what politicians can say on the platform. 
In recent days, Zuckerberg has also publicly denied being overly sympathetic to Trump and unwilling to moderate the president’s Facebook posts. The tech CEO has also more publicly criticized Trump than he has in the past, speaking against his immigration policies and his handling of the Covid-19 pandemic.
But Facebook’s refusal to take a stand of any kind regarding the accuracy of Trump’s posts, and its continued insistence that it’s a neutral platform, suggest it hasn’t learned much since 2016. (Back then, Zuckerberg said shortly after the election that it was “crazy” to suggest that Facebook had influenced its outcome — a statement he later apologized for.) What has changed since the last US presidential election is the expectations that the public and politicians have for the platform — and for some, its new policies aren’t cutting it.
  
  
  
      




  
  
  
    
    
      
        
  


  






      
    
    
  
  



Social media and livestream feeds are being monitored and used against protesters in Portland, Oregon, as federal agents continue to patrol the city despite requests from local and state officials that they leave. As President Trump threatens to send feds to protests across the country, what’s happening in Portland is a possible future for other American cities.
The Trump administration deployed law enforcement officers from multiple federal agencies under the Department of Homeland Security (DHS) to Portland in early July, and they set up shop in the Mark O. Hatfield Federal Courthouse, which has been the site of demonstrations and the target of vandalism since May. From there, the federal agents have been accused of firing pepper spray and less-lethal munitions at protesters, severely injuring at least one of them. They’ve also started patrolling the city in unmarked cars, pulling people off the street and throwing them into vans while refusing to identify themselves. And a recent DHS directive has authorized agents to conduct surveillance of Americans to protect federal buildings as well as statues and monuments. Federal officials have said that they are not targeting peaceful demonstrations but that they are trying to quell civil unrest and violence.

  
    Related
  

  
    The unmarked federal agents arresting people in Portland, explained
  

State and local officials as well as civil rights groups have decried their presence and methods. The state’s chapter of the American Civil Liberties Union (ACLU) called it a “constitutional crisis,” and governor Kate Brown said they were “adding gasoline to the fire.” Portland protests are now attracting significantly larger crowds than they were before. President Trump, on the other hand, has threatened to expand this use of federal forces to other cities, including Chicago and New York. Many believe that Trump is using the unrest to push his “law and order” campaign strategy as election day approaches, positioning himself as a president who cracks down on crime when Democrat-led cities don’t.
“Donald Trump did not send in this paramilitary force to keep people safe,” Oregon Sen. Ron Wyden said the Senate floor on Tuesday. “Trump is doing this to create images of chaos, to air them on far-right television and in campaign ads, and scare the country into reelecting him.”
Court records reveal some of their surveillance methods, including undercover agents stationed within the protesters, visual surveillance from the upper floors of the courthouse, and monitoring journalists’ livestreams for illegal activity in lieu of security cameras, which the court documents state have been damaged or stolen.  
In the early hours of July 13, agents of the Federal Protective Service (FPS), a division of the DHS charged with guarding federal courthouses, surveilled protesters through citizen journalists’ livestreams, whose footage is being used both to amplify protesters’ messages and as evidence against them. 
According to an affidavit from an FPS agent, law enforcement officers were watching a YouTube livestream when they saw a protester take a flaming wooden board and place it against the exterior wall of the courthouse. The footage appears to show the protester wedge the board between the stone courthouse wall and the wooden boards installed over a courthouse window to protect it from protester damage. A second protester appears to pick up the board and lean it against the wooden boards instead. Shortly afterward, a third person removes the board and extinguishes the fire. The first protester’s face is almost entirely covered and can’t be identified. But the second protester, whose face is unobscured, turns toward the camera as he walks away from the courthouse. 

According to court documents, a DHS Intelligence Operations Specialist “analyzed” the livestream, took screenshots of the second protester’s face, and sent them back to agents inside the courthouse. The federal agents then “maintained surveillance” of the protester from inside and outside of the courthouse for several hours before arresting Kevin Benjamin Weier, who they say is the second protester. According to the affidavit, Weier told investigators that he was on the scene when the flaming board was placed against the courthouse but denied that he placed the board himself or even touched it. Weier did not respond to request for comment, nor did the DHS.
Weier has been charged with attempted arson of a federal building. If convicted, he faces between five and 20 years in prison. 
The footage cited in the court documents for Weier’s case came from a YouTube channel that combines several live feeds into one stream. The videos, branded with a “LIVE: From The End Of The World” banner, are also hosted on Twitch. One of the feeds came from Tre Stewart, who was streaming from his Facebook page, and it is his footage, seen via the YouTube channel, that was used as evidence to charge Weier. Stewart did not respond to request for comment, but Anteros Oberon, who runs the YouTube channel, did.
“Though we recognize we have no legal authority to demand that the federal government cease to use ours or anyone else’s feeds,” Oberon told Recode, “we do denounce the practice by law enforcement and call upon them to adjust their standards and practices in such a way as to not endanger journalists and citizens exercising their constitutionally protected rights around the nation.”
Oberon added that people can opt out of having their feeds included in the LIVE: From The End Of The World livestream.
Weier’s case is not the only example of federal agents using social media and livestreams to collect evidence against Hatfield courthouse protesters. Jacob Michael Gaines was arrested and charged with assaulting a federal officer, and part of the evidence against him was screenshots from a video embedded in a tweet, which sourced that footage from an account called “Portland Independent Documentarians.” But those screenshots were not the basis of the charges against him; Gaines was arrested as he allegedly hit an officer with a hammer. In Weier’s case, the livestream footage was the only evidence cited as probable cause to arrest him.
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        A protester waves an American flag in a fog of tear gas fired by federal officers outside the Mark O. Hatfield courthouse in Portland, Oregon, on July 21, 2020.
      
      
        Nathan Howard/Getty Images
      
    
  


Both cases illustrate how law enforcement is using social media and livestreams to monitor protests and collect evidence of alleged crimes. As protests against police brutality spread across the country this summer, organizers have repeatedly warned protesters not to post images or videos that show people’s faces and requested that the media do the same. This has occasionally caused tension between protesters, who do not want law enforcement to use their images to identify and further target them, and the media, which is charged with covering the protests as they happen and typically doesn’t make such concessions.
In a statement, Oberon acknowledged that concerns that the media is putting protesters at risk are “valid” but thought it was safer to have documentation of the protests than allow law enforcement to work free of accountability.
“In my own view, to be quite honest, I fear the moments when we are offline, when streamers are not showing what is happening,” Oberon said. “We have seen time and time again that, as soon as our feeds go down, the protestors face fresh assaults from law enforcement.”
The cases in Portland show a possible result of the DHS’s recent authorization for its officers to use public information sources, including social media, to gather intelligence on people or groups who may be planning to damage monuments and statues. According to Lawfare reporters Steve Vladeck and Benjamin Wittes, who first reported the authorization document, DHS officials are instructed to use the “least intrusive collection techniques feasible” when surveilling Americans but “are not permitted to engage in electronic surveillance or unconsented physical searches.”
“Protestors can reasonably expect that there will be ongoing DHS collection and analysis of public source information about — and likely the social media postings of — people involved in protests,” Vladeck and Wittes wrote.
From the beginning of his presidency, Trump has repeatedly threatened to deploy federal agencies to cities that he believes are not sufficiently enforcing the law, but the protests have finally given him enough of an excuse to do it. That the mayors of these cities have said that federal forces are neither desired nor needed doesn’t seem to matter.
Shirin Ghaffary contributed reporting to this story.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.




  
  
  
      




  When Netflix started streaming TV shows and movies back in 2007, it had the market more or less to itself. At the time, video on the internet meant YouTube, where you could see dogs on skateboards or short SNL clips. 
Now, everyone is racing to catch up to Netflix and launching their own streaming services. But the big media companies are far behind. 
That’s their own fault. They watched Netflix build its streaming powerhouse right in front of their noses — and even worse, with their assistance. 
That’s one of the stories we tell in this week’s episode of Land of the Giants: The Netflix Effect, which focuses on the impact Netflix has had on Hollywood and on the people who run it and work in it. 
It’s a story that kicks off with a ride down Sunset Boulevard, where Netflix’s presence manifests physically. Billboards that used to advertise movies and TV shows from a variety of studios and networks are now dedicated solely to Netflix products because Netflix purchased the billboards outright. 
But the story really starts in 2008, when Netflix broke into streaming in a big way, through a backdoor: It purchased the digital streaming rights to movies from Disney and Sony — that is, movies you’ve heard of, like Pirates of the Caribbean — from Starz, the pay TV channel. Starz had ambitions for its own streaming service, but those fizzled, which is why you have probably never heard of Vongo. 
And that’s why Netflix got those movies for a song — around $30 million a year — while becoming a pretty good streaming service almost overnight. For context: In 2012, when Netflix wanted to make a new streaming deal for content from Disney, which by then had realized that streaming was a real thing, Netflix paid an estimated $300 million a year.
A contractual loophole let Netflix get Disney’s and Sony’s stuff without cutting deals with  Disney and Sony. But soon enough, media companies were scrambling to sell their stuff directly to Netflix: They saw Netflix as an easy source of nearly free money — if Reed Hastings and company wanted to pay them for old shows and movies they were already selling other places, then they’d be happy to do it.
But that free money wasn’t really free: Netflix took the stuff Hollywood considered its leftovers and built a giant business with it — and ended up competing directly with the established media players, using their own content. Which leads us to today, where the biggest media companies in the world find themselves years behind what used to be a Silicon Valley upstart. 
It’s a fascinating story, and you can hear it now.


Subscribe to Land of the Giants on Apple Podcasts, Google Podcasts, Spotify, Stitcher, or wherever you listen to podcasts.
  
  
  
      




  America’s wealthiest tech billionaires are faring extraordinarily well six months into a historic pandemic, posing a striking contrast with the fate of other Americans during the worst economic downturn since the Great Depression.
When the coronavirus pandemic began to ravage the economy, those who worry about inequality expressed concern that billionaires — and particularly tech billionaires — would amass more power and that the income gaps would grow more dramatic. And their worries appear to have been well founded.
People like Jeff Bezos, Elon Musk, and Steve Ballmer have added tens of billions of dollars to their net worths since the beginning of the calendar year, according to the Bloomberg Billionaires Index. The success of the very richest was punctuated on Monday when Bezos’s net worth grew by $13 billion, the largest single-day jump since Bloomberg began tracking the day-to-day changes in 2012.
It’s easy to lose track of precisely how wealthy the ultra-wealthiest have become. Terms like “billionaires” can generalize and disguise the scale of the fortunes created in today’s economy. A billion here, a billion there — the very rich remain very rich. Exactly how rich can feel irrelevant.
But the particulars of their staggering success matter because at the other end of America’s income inequality divide, the extra money would not feel so irrelevant. More than 30 million Americans are now depending on unemployment benefits, some of which are set to expire at the end of the month. Low-wage workers are especially prone to layoffs. And the pandemic is pounding poorer neighborhoods in particular, where the number of Covid-19 cases is higher.
Voices on the left see this as zero-sum — those extra billions can make a difference if redistributed — and are calling for a remaking of the American economy after this crisis. They would like to see the very wealthy pay more in taxes in order to repair what they believe is an insufficient safety net.
Market-oriented thinkers argue that these billionaires are becoming rich because they are creating value for their shareholders — which is a basic imperative of capitalism — and the billionaires happen to be some of these companies’ largest shareholders. 
That argument, though, is why it is worth assessing the figures in the uppermost echelon of America’s elite. Over the last few weeks in particular, tech fortunes have climbed to new heights. 
That’s true for no one more than Bezos, whose assets in 2020 have climbed by $75 billion; his net worth is now nearing almost $200 billion. 
That historic wealth gain is due to the rise of Amazon, which has proved indispensable as people around the world stay at home in response to the pandemic. Its stock has skyrocketed 70 percent since the start of the year. That’s a boom for him as well as for his ex-wife, MacKenzie Bezos, whose shares in the company have put her on the doorstep of becoming the world’s wealthiest woman, a position held now by Francoise Bettencourt Meyers. On New Year’s Day 2020, MacKenzie Bezos was the world’s 25th wealthiest person — now she’s 13th, with $63 billion to her name, per Bloomberg.
Amazon is not the only big tech company whose relative success has made the rich richer. The S&P 500 may be about flat in 2020, but the stock appreciation at Facebook, Apple, and Google parent company Alphabet has created even more winnings for its billionaires. But the rise of two other tech companies and the billionaires behind them have changed the tippiest of the tippy-top.
Ballmer, the longtime CEO of Microsoft, is not a household name for most Americans, who are far more familiar with his predecessor, Bill Gates. (Many of the country’s richest people are not faces you would recognize if you passed them on the street.) 
But Ballmer is not just any rich person — he has sneakily become America’s fifth-richest person thanks to the enormous growth in Microsoft’s stock, which has almost quadrupled over the last five years. That could draw more scrutiny to Ballmer, who remains in the public eye primarily as the animated courtside presence at the home games of the Los Angeles Clippers, which he owns. Ballmer began the year as the 15th-richest person.
The other tech billionaire who has turned a fortune into a super-sized fortune amid the recession is Musk, the idiosyncratic founder of Tesla and SpaceX. At the start of 2020, Musk ranked as the 35th richest person. But his net worth has nearly tripled over the last seven months, and Musk is now the sixth wealthiest person in the world with almost $75 billion. Tesla’s stock has almost quadrupled this calendar year.
All told, the coronavirus is proving to offer pretty good times for Big Tech’s leaders. Nine of the 15 richest people in the world come from America’s technology sector, according to the Bloomberg rankings, compared to seven at year’s beginning. Almost every tech billionaire is in the black, not the red, this year. 
Not many Americans can say the same. And if the American economy drops further, the tech billionaire will loom — fairly or unfairly — as an easy scapegoat.
 
  
  
  
      




  When Adnan Khan was sent to jail in 2003 at the age of 18, texting wasn’t all that popular. But by the time he was released from prison in January 2019, texting was the norm and social media was everywhere — even behind bars. 
“I’ve seen the videos. I think they’re hilarious,” Khan, the executive director of Re:Store Justice, said on a recent episode of the Reset podcast. 
What Khan is referring to is “prison TikTok,” a hashtag that lets TikTok users view videos shared on the app by people who are incarcerated. “TikTok is one way, and social media is one way, for incarcerated people to show the world who they really are and to humanize themselves,” Khan said.  
The content depicted on prison TikTok varies; some videos show terrible living conditions inside US prisons, like overcrowding and flooding, whereas others depict people in prison uniforms doing renditions of trending TikTok dances.  
But what’s surprising about prison TikTok isn’t that people who are incarcerated appreciate great choreography, just like anyone else. Rather, it’s that folks in prison have access to smartphones at all. 
Cellphones are not allowed in American prisons. And people who are found with contraband in their possession can get hit with serious penalties. 
“You can get charged with a misdemeanor or a felony depending on what jurisdiction you’re in,” said Nazgol Ghandnoosh, a senior research analyst at the Sentencing Project. “And at the very least, you’re going to lose good time credits or harm your chances of parole.”
So why would anyone run the risk of posting on TikTok or calling a family member using a contraband cellphone? 
For Khan, having a cellphone while in prison, even for a short time, was a way to mend his relationship with his mother. It allowed him to have real emotional conversations with his family without being monitored by corrections officers and without having to deal with a 15-minute time limit or the large fees that typically come with using a prison landline. 
“It was through a cellphone where I started to really have these tough conversations of how I felt neglected by her,” he said. “What the cellphone did for me — and we keep using the word ‘cellphone’ like it was the phone itself — obviously, what I’m really talking about is a form of community, consistent communication, and a safe space to have that communication.”
To find out more about prison TikTok and the use of contraband cellphones in prison, listen to the latest episode of Reset below.

Subscribe to Reset on Apple Podcasts, Google Podcasts, Spotify, or wherever you listen to podcasts.
 
  
  
  
      





  Three months ago, Netflix announced that it had seen a huge surge in new subscribers because people were stuck at home around the world, waiting out the Covid-19 pandemic.
So consider this a repeat: Netflix says that it also did great during April, May, and June of this year — because people were stuck at home around the world, waiting out the Covid-19 pandemic.
More specifically: The company signed up another 10 million more subscribers and now has 193 million subscribers worldwide.
The downside, Netflix said, is also something the company said last quarter: It worries that just about everyone who wants to get Netflix this year has already signed up for it, so its growth won’t be as enormous in the second half of the year. 
That would be a problem all of Netflix’s many competitors, from Apple to Disney to AT&T’s HBO Max, would love to have. But while analysts had guessed that the arrival of all those competitors would cut into Netflix’s growth, that hasn’t panned out. And the fact that Wall Street allows Netflix to take on billions in debt, which it uses to buy and make truckloads of TV shows and movies, means that Netflix has been able to keep showing off new products to its customers while some of its cautious competitors have had a hard time keeping up.
But not even Netflix can fend off a pandemic forever. The company says that it will have plenty of new stuff to show people this year, but that next year it will have a “more second-half-weighted content slate in terms of our big titles.” Translation: If you’re still sheltering at home in February, you’re going to have to work harder to find Netflix stuff you want to watch.
Then again, we’re in unchartered waters here. 

  
    Related
  

  
    The inside story of Netflix, the tiny tech company that took over Hollywood
  

Three months ago, Netflix said that it had signed up 16 million subscribers instead of the 7 million it had planned on getting — and that all 9 million of the extra subscribers showed up in March when countries around the world told their citizens to stay home.
At the time, Netflix CEO Reed Hastings said he thought the company might sign up another 7.5 million subscribers in April, May, or June — or it might not, because no one has ever tried financial forecasting during a pandemic before. He described Netflix’s projections as “a bunch of us feeling the wind.”
Here’s what we seem to know now: Netflix seems to have been purpose-built to thrive in a pandemic. As I wrote in April: 

It’s an internet-only business that hasn’t had to interrupt its service in any way, and it’s a subscription-based business that doesn’t make any money from advertising so it doesn’t have to worry about the collapse of that industry.
Netflix is also, for now, in a good position to ride out the pandemic: While it has billions in debt, it is generating lots of cash each month from its subscribers.

One other thing we know: Reed Hastings is no longer the CEO of Netflix, he’s the co-CEO. Hastings announced that he has promoted Ted Sarandos, his longtime head of content, to share the top title with him. It’s a signal that Sarandos is likely to eventually succeed Hastings, though Hastings says he “committed to Netflix for the long term.”
  
  
  
      




  Sens. Elizabeth Warren, Tom Carper, Brian Schatz, and Sheldon Whitehouse are demanding more information from Facebook CEO Mark Zuckerberg about a reported “loophole” for climate misinformation in Facebook’s fact-checking program.
In a letter sent to Zuckerberg on Wednesday, the senators reference a widely reported-on incident from last year that involved a story about climate change models written by two people from an organization called the CO2 Coalition and published by the Washington Examiner. The piece questioned the extent of climate change and was flagged as false by one of Facebook’s third-party fact-checking partners, Science Feedback. Five scientists contributed to the fact-check and concluded that the piece had “very low” scientific credibility and misled people about how climate change models actually work, and the post was labeled as false. The label was later removed after Facebook reiterated to Science Feedback its policies about fact-checking opinion.
“If Facebook is truly ‘committed to fighting the spread of false news on Facebook and Instagram,’ the company must immediately acknowledge in its fact-checking process that the climate crisis is not a matter of opinion and act to close loopholes that allow climate disinformation to spread on its platform,” the senators wrote. They also ask Zuckerberg to explain how disinformation related to Covid-19 and vaccinations differs from disinformation related to climate change.
As the senators’ letter points out, Facebook’s handling of this particular incident raises questions about how the platform handles fact-checking as well as all content on its platform that is labeled as or deemed to be opinion. The lawmakers also raised questions about the extent to which Facebook is actually committed to clamping down on disinformation. Now, Warren and her colleagues want to know more about what types of posts Facebook is exempting from fact-checking because they’re deemed opinion and how Facebook factors in false information about climate change within its overall efforts to combat fake news.
Facebook admits there’s confusion about its approach. Facebook’s policy communications director Andy Stone told the New York Times earlier this week that the company’s opinion exception for fact-checking has existed since 2016. According to the paper, “Mr. Stone said that The Washington Examiner post, originally published as an op-ed, clearly aligned with Facebook’s definition of opinion content and added that fact checkers should have been aware of that classification.” Stone also said that climate change content doesn’t create an imminent threat to peoples’ health or safety.
But following the publication of that article, Facebook representative Liz Bourgeois provided another statement, which was shared with Recode.
“When someone posts content based on false facts — even if it’s an op-ed or editorial — it is still eligible for fact-checking,” Bourgeois said. “We’re working to make this clearer in our guidelines so our fact checkers can use their judgment to determine whether it is an attempt to mask false information under the guise of opinion.”
In short, just because content calls itself opinion doesn’t mean it’s ineligible for fact-checking. The statement comes amid concern that those seeking to spread misinformation, including those that deny climate change, could claim their content was opinion in order to avoid Facebook’s fact-checking process.

There’s also concern over who ultimately gets to decide what content is deserving of a fact-check. On its website, Facebook says that those seeking to appeal fact-checkers’ decisions should go to those fact-checkers directly.  A Facebook spokesperson also told Recode that Science Feedback removed the rating after Facebook clarified guidance about its approach to rating opinions. Meanwhile, a science editor for Science Feedback’s Climate Feedback, Scott Johnson, told Recode in an email: “We were informed by the Facebook team that the Washington Examiner article should be considered opinion and ineligible for a fact-check rating.”
“Even though they claim that it’s an opinion piece, there were many statements of fact in the piece, and those statements were either wrong or misleading,” Andrew Dessler, a climate change expert at Texas A&M who contributed to the Science Feedback fact-check, told Recode. “Facebook is this impenetrable fortress that nobody has any idea — as far as I can tell, at least — how these decisions are made. So I don’t know who made the decision to do it.”
The confusion over what’s is eligible for fact-checking come as Facebook faces accusations of having a double standard for misinformation. While Covid-19 misinformation has been aggressively taken down — for having the threat of imminent harm — climate misinformation isn’t treated the same way. At the same time, the company is still facing wide-ranging criticism over how it handles sources of misinformation and disinformation. 
“[T]he climate crisis and environmental degradation are not matters of opinion,” the senators wrote. “They are existential threats that hurt communities and economies throughout the world — including and especially Black communities and other communities of color — and will continue to do so.” 
This echos other critics of the platform. Earlier this month, climate leaders and several prominent politicians, including Tom Steyer and Stacey Abrams, wrote a letter to Facebook’s Oversight Board demanding that the company close the loophole that allows content to be deemed ineligible for fact-checking after being labeled as opinion. That letter followed a joint report from the newsletters HEATED and Popular Information. At the same time, Facebook is facing an ongoing boycott from advertisers called Stop Hate for Profit, which is led by civil rights leaders urging the company to fight hate speech and misinformation, including climate denialism. 
“I think that Facebook is making a business decision here. This is not any kind of principled stand about opinion,” said Dessler about the confusion surrounding Facebook’s policies. “We’ve gotten ourselves into a terrible situation here where we’ve left it up to corporations to make decisions about what people can say.” 
Update, July 16, 3:15 pm ET: We’ve added more information about Facebook’s fact-checking system and the senators’ letter to this article.
  
  
  
      




  Silicon Valley’s elite are choosing their partisan teams with just over 100 days to go until Election Day — and few appear to be backing President Donald Trump.
The clearest view yet of the breakdown came in fundraising reports released Wednesday. They portray a tech industry that has unmistakably coalesced around Joe Biden, the presumptive Democratic nominee, despite him being the first choice of few during the party primary. Now, juxtaposed against a president long reviled by the power set of Silicon Valley, Biden is sweeping up cash from billionaires, CEOs, and political kingmakers in a show of force.
A who’s who of tech’s rich and powerful cut checks of up to $620,600 — the legal maximum — to a jumbo-sized joint fundraising committee of Biden and Democratic Party groups around the country after April 1, when the primary was effectively over and the fundraising quarter began. The committee’s fundraising report makes clear that Silicon Valley forms a large part of the backbone of Biden’s high-dollar fundraising operation. For instance, of the dozen people who gave the legal maximum to the Biden Victory Fund, half came from the Bay Area.
Among the tech billionaires giving the legal maximum, or close to it: Facebook co-founder Dustin Moskovitz; philanthropist Laurene Powell Jobs; eBay’s first full-time employee, Jeff Skoll; Zynga founder Mark Pincus; and media moguls Barry Diller and Jeffrey Katzenberg. If you scanned the Biden Victory Fund’s report, every few lines you’d find another marquee name who forked over hundreds of thousands of dollars.
The quarterly fundraising report of Trump Victory reads much differently. While high-wattage supporters of the president do exist in Silicon Valley, they have preferred to maintain low profiles in an industry where they fear backlash.
So the billionaires who did cut major checks to Trump stand out even more. The sole tech titan who cut a significant check last quarter to Trump’s joint fundraising group with the Republican Parties was Safra Catz, the CEO of the software giant Oracle. Catz and her husband each donated $125,000 to Trump Victory in the spring, the latest sign of coziness between Trump and Oracle, which perhaps more than any other tech giant has fostered a close relationship with the White House.
In some ways, who was missing from Trump’s quarterly fundraising report was more notable than who was on it. That’s especially true for Peter Thiel, the billionaire who served as Trump’s emissary to Silicon Valley after loudly backing Trump in 2016 to the chagrin of his industry. Thiel, who has reportedly drawn distance from the president and is now a non-factor in Trump’s orbit, did not make a donation to Trump Victory during the three-month period — extending the fundraising snub from Thiel since the last gift he made in October 2018.
Thiel is instead focusing so far on down-ballot races, and on one race in particular. In June, Thiel invested an additional $500,000 in a super PAC that he set up to back Kris Kobach, a hardline immigration official running for the Republican nomination for the US Senate in Kansas. Thiel has now spent $750,000 on behalf of Kobach, whom Democrats at least see as a weak general-election candidate.
Another billionaire leaving a noteworthy mark on the election, the reports reveal, is former Microsoft CEO Steve Ballmer, who is now the world’s fifth-richest person. Ballmer, who is today best known as the exuberant owner of the Los Angeles Clippers, has not been seen as particularly ideological. He’s funded Republicans and Democrats over the years, but in generally modest amounts compared to his billionaire peers.
But Ballmer and his wife, Connie, made a landmark political donation this spring when they gave $7 million to Everytown for Gun Safety, a gun-control group founded by Mike Bloomberg that primarily supports down-ballot Democrats. (Bloomberg, the former New York City mayor, campaigned for the Democratic nomination for president this year.) The donation is the Ballmers’ single-largest disclosed donation ever, more than 200 times as large as Steve Ballmer’s next-biggest check ($32,000). And it came around the same time that Connie Ballmer cut her own single-biggest disclosed check, a half-million dollars to a pro-Biden super PAC.
The Ballmers have long been advocates for gun control. But they have never put a foot down with the same financial force as they did this spring. And in a tech industry shaken by Trump, they’re proving to be not alone.

  
  
  
      




As the US faces a renewed reckoning on racial justice, Facebook has faced unprecedented pressure over the past few weeks to stop the spread of hate speech on its platform.

One of the main people responsible for ratcheting up that pressure is Jonathan Greenblatt, CEO of the Anti-Defamation League. Greenblatt, along with leaders of several other civil rights groups, has organized a historic advertiser boycott of Facebook that has so far prompted more than 1,000 companies, including Starbucks and Unilever, to stop advertising on the social media giant until it makes certain changes. The campaign, called Stop Hate for Profit, is pushing Facebook to appoint a C-level executive with expertise in civil rights and remove Facebook groups devoted to things like Holocaust denialism, among other things.

“We’ve been at this work of fighting anti-Semitism and bigotry in all forms for over 100 years,” Greenblatt told Recode in an interview last week. “And frankly, we believe that Facebook is the front line in fighting hate today.”

Instead of making incremental promises of progress, Facebook instead needs to fundamentally reform, Greenblatt told Recode.

“Mark Zuckerberg has really elevated freedom of expression above all else,” Greenblatt said. “But I think that we need to realize that hateful words can have harmful results.”

Here’s what this prominent civil rights leader said Facebook needs to do at a turning point in its years-long struggle to reduce hate speech.

The following transcript has been lightly edited for clarity and length.

Facebook can’t rely on Washington to tell it how to fight the hate speech flourishing on its platform
Shirin Ghaffary

Facebook is facing unprecedented levels of criticism right now over how it handles hate speech. At the same time, the company has said it wants to stay true to its values around free expression. And there’s pressure from Republicans that accuse Facebook of having [alleged] anti-conservative bias when it moderates political speech. So what do you think the ethical move is here? What do you think Facebook should do?

Jonathan Greenblatt

First of all, I just need to say right up front, ADL is the oldest anti-hate organization in the world. We’ve been at this work of fighting anti-Semitism, bigotry in all forms, for over 100 years. And we’ve been intensely focused on Silicon Valley, certainly, since I started as CEO in 2015. And frankly, we believe that Facebook is the front line in fighting hate today.

I’m laying all this out because we actively work with the company, because you know, I’m a former product manager. I used to run consumer products at a Kleiner Perkins company called Realtor.com. And then I incubated [online volunteer database All for Good] inside Google. So I’ve worked in the Valley, I’ve had teams of engineers, I know full well that the pace of innovation is so great that we can’t leave it to the legislators. We are much better advantaged to working with the companies, and building, as a design principle, anti-hate into these products, embedded into these platforms, making it part of their practices. That’s going to be a more efficient and effective way than waiting for Washington, which can take years.

All that being said as background, and so to be more specific, we’ve worked with Facebook for years.

We work with their engineers, we work with their policy teams. We work with them on a regular basis helping them to delete content, helping them to identify extremists, helping them sort through issues.

All of this is relevant. Because for starters, I don’t agree with the characterization that you laid out — which clearly has been presented by people with an agenda. There is nothing political in pushing back against prejudice.

I’m sorry, I don’t believe that the “alt-right” represents any reasonable rendition of the political spectrum.

This president and his predilections notwithstanding, we are in a new kind of environment, where we have to countenance, “How will this fly with the white supremacists?” And I just don’t buy that. So I think that’s important table-setting. Because when people say, “Oh, I’m worried, slippery slope” — I’m sorry, this is not about a slippery slope. Slander is not a slippery slope. Freedom of expression was not intended to be the freedom to express hateful views that would inspire violence against Black people or Jewish people or other individuals from marginalized communities. And anyone who tells you that, that’s a tell. Anyone who says that, that’s a tell. They have a different agenda. So I’m just saying I don’t even buy that right off the bat.

With that said, we started Stop Hate for Profit because we were not seeing the kind of progress on fighting prejudice, on eliminating extremism, from Facebook. This is not to say that there aren’t well-intentioned people in the organization. There are — I know them, I’ve worked with them.

And yet the company was simply not delivering with the degree of urgency that we felt this issue merited. So the frustration had been building, and it came to a head after the death of George Floyd, who was murdered in the streets of Minneapolis by a police officer. And then we saw boogaloo enthusiasts out in the open on Facebook, literally, coordinating how they were going to disrupt the rallies, the Black Lives Matter rallies, how they were going to subvert them with violence.

Flashing red lights went off, and we brought this to the attention of Facebook and said we need a meeting right away to deal with this. And we didn’t get it. Which maybe in some ways wasn’t so much a surprise. When I said we needed a meeting, we needed to meet with Mark, we needed a clear commitment — we didn’t get it. And I wrote that letter with Jim Steyer of Common Sense Media. We’ve done some work together. There are also child advocates who are concerned about these issues.

And I said to Jim, “Okay, we need to move, we need to mobilize.” And so I reached out to Derrick Johnson of the NAACP, and Rashad Robinson of Color of Change. And Color [of Change] has done a lot of really important work as well, with Silicon Valley. Remember, they put that civil rights audit in place, which was recently published.

Facebook is failing on civil rights because it downplays the real-world consequences of hate speech
Shirin Ghaffary

That leads into my next question. What’s your takeaway of that civil rights audit, if you’ve had a chance to look at it in depth?

Jonathan Greenblatt

I’ve looked at it a little bit. I mean, it’s 100-plus pages, I can’t say I’ve read the whole thing. But I think one of the things that seems very clear to me from my read is that, as if we didn’t know this already, Mark Zuckerberg has really elevated freedom of expression above all else.

But I think that we need to realize that hateful words can have harmful results.

And while Facebook happens in the digital world, it can catalyze real-world consequences: Boogaloo enthusiasts who fired at a federal courthouse in Ohio in May killed a guard standing outside. Or in March, a member of the Aryan Brotherhood organized on a Facebook group.

I don’t know off the top of my head the exact phrasing of Facebook’s mission statement, but it’s something like “bringing the world together.” But I think bringing the Aryan Brotherhood together is not exactly a good thing. So if freedom of expression elevates engaging the Aryan Brotherhood, I think we need to step back and ask ourselves, is this really what the platform is designed to do? And if that is the case, how do we need to iterate the platform so it’s not extending the rights of the Aryan Brotherhood?

Frankly, I don’t think this is hard inasmuch as I don’t think it’s a moral dilemma. I think it’s an ethical conundrum. So I think [that’s] the reason why Stop Hate for Profit has expanded.

When we started Stop Hate for Profit three weeks ago, we had zero companies on deck. Oftentimes these sort of efforts, they’re sort of choreographed in advance. But we had zero companies. And today, we have over 1,000 organizations who have joined: Fortune 500s, and small- to medium-sized businesses and local nonprofits and global NGOs. It’s unbelievable. And I think it is a reflection of the degree of frustration that’s out there about Facebook. It’s not unique to us. There’s a large number of people who are just deeply concerned about the company, in this pattern of behavior that hasn’t changed.

Shirin Ghaffary

And what do you make of Zuckerberg’s comments to employees, that advertisers will return soon enough?

Jonathan Greenblatt

Yeah, I raised this with Mark on the call. We had a conversation. We met with him and I raised this issue. I said basically, “We know what you told the staff, I mean, we all read it in the Information.” And he said, “Oh, you know — it’s taken out of context, you need to read the whole remarks.” And I said, “I did! I read the whole remarks, that’s the point. And it’s why I find this really alarming.”

You know, I think someone said to me, “I’m sorry you had an unproductive call.” And I said, “No, actually it was very productive. It was very productive as much as it was extremely clarifying about what their priorities are.” I mean, we came to the conversation with expectations.

They [Facebook] called the meeting; we didn’t call it.

And the expectation that we had was that we were going to review the recommendations which we published on our website three weeks ago. And again, it’s worth pausing on that for a second. None of the recommendations are all that novel.

None of them are all that new. All of them align with things that ADL has been saying for years, that Color of Change has been saying for years, things that Free Press, and Common Sense, and other people — the NAACP — have been saying.

So we expected we were going to review the recommendations and talk about their implementation, strategy — what they would do and when they would do it.

But we didn’t get any of that. There was no concrete conversation. There was no review of milestones. There was no proposal of time frame. There was really nothing, so we left that call extremely disappointed.

And then, on the other hand, it’s clarifying. Again, we don’t see Facebook prioritizing this in the way that we would expect.

And I think that’s why over 1,000 organizations have joined [the ad boycott]. That’s why more are calling us every day. That’s why more nonprofits want to get on board in terms of joining the coalition, not just taking the ad pause. And so I think this is really a moment in time and an opportunity for Facebook to decide what path they want to walk down. I hope they will reflect upon our conversation and reach a different conclusion than the one that we’ve seen to date.

The ad boycott on Facebook is working, whether or not Facebook admits it
Shirin Ghaffary

Do you think so far that the boycott is working? And what do you say to those who pointed out that a majority of Facebook’s revenue does not come from the top advertisers — that it comes from a huge number of small and medium businesses — and how it would be very hard to get all those people to join a boycott?

Jonathan Greenblatt

Well, look, a couple things to think about. So number one, this was never intended to be a permanent boycott. It wasn’t intended to be a long-term walkout. In fact, as we’ve said to all the participants, this is strictly about a pause on paid advertising.

But as to your question of — has it succeeded? I mean, look, it has definitely succeeded. In a couple different ways. So number one, in Facebook’s 15-plus years, it has never happened before that advertisers have engaged and flexed their muscles in this way. That has never happened. Number two, never before has there been this kind of broad-based public conversation about Facebook practices and fighting hate on the platform.

So we’ve been able to kind of kindle this discussion that’s happening on social media. It’s in every newspaper, it’s on every news show. And the most amazing thing was to log in to Twitter in the morning and type in the hashtag “StopHateforProfit” — I’ve done it a couple times in the past couple weeks — and see all the tweets in foreign languages. I mean, this thing has had global reverberations.

And you know last week, as you probably saw, Reddit pulled down thousands of toxic subreddits. Something they probably should have done long before they did it. YouTube closed down a number of white supremacist channels — Richard Spencer and a few others. Even Twitch, which is a sub of Amazon, shut down the toxic channels. So I think all of these things are happening suddenly and swiftly because of Stop Hate for Profit. So in terms of Facebook realizing it has this vulnerability, in terms of advertising purchasing, they have a kind of power, in terms of creating this public conversation in terms of stimulating other companies to take action. I mean, we’re already succeeding.

I say all of this because, as you were kind of alluding to, Facebook has 8 million customers. It’s a $70 billion behemoth. We never thought that calling for a one-month ad block would put a dent in their P&L. The goal was to make a dent in their practices, and that being our primary objective.

Facebook’s harassment problem is worse than on other platforms
Rebecca Heilweil

Today, how bad is Facebook’s problem with anti-Semitism or other forms of hate that the ADL tracks? And how good is Facebook at taking it down?

Jonathan Greenblatt

The short answer is yes, it’s bad. So what do I mean by that? Well, like today, if you were interested, you could just log in to a Facebook group [like] “Exposing the Rothschilds,” a Facebook group with 133,000 members dedicated to anti-Semitic conspiracies about Jewish malevolence, the likes of which prompted the shooter to go to the Tree of Life synagogue, or the shooter to go to the Poway synagogue.

Rebecca Heilweil

Can you walk through what is the connection between forms of hate that we see online and the kind of violent attacks that actually happen in real life?

Jonathan Greenblatt

We see certainly a corollary relationship between increased activity on social media and increased activity offline.

The recruiting is up, and the propaganda events are up. We do annual reports about online hate and harassment. Again, this is what we do at ADL, we fight hate. So that means we have to track it, we have to analyze it and understand it. And then that allows us to drive policy and other forms of intervention. And so as we track hate offline, we track hate online. You can go to our latest, the 2020 report [on online hate and harassment]. I think it’s like 44 percent of online users report experiencing something in harassment. My recollection is like 28 percent reported a severe, sustained pattern of harassment. That number itself is pretty terrifying, that one out of four people are severely harassed online.

And far and away the place where it happens more than any other platform is Facebook. So, to your question — is anti-Semitism a problem? Is hate a problem? Yes, these are problems. Yes. Our research indicates that these are issues across these different social media services, but they’re particularly pronounced on Facebook. Now Mark, in our meeting and in other venues, decided the fact that Facebook’s AI removes 89 percent of hate content. While that stat is directionally encouraging, it’s not empirically very helpful.

Facebook has to do better than removing 89 percent of harmful content on its platform
Jonathan Greenblatt

We have to ask ourselves, “How is Facebook defining hate content?” It doesn’t seem to me that Facebook groups dedicated to Holocaust denialism are actually adding much to the public conversation. It doesn’t seem to me that QAnon conspiracy groups who spread the fiction that there are pedophilia rings in the basements of pizza parlors. Right, this is a thing. Or that cannibalism is practiced in the Democratic Party. This is a real thing, too. I don’t believe that those QAnon [posts] are adding much to the “community” that Facebook aspires to create. Right? So I think we have to ask ourselves, number one: How is Facebook defining hate content? Because I’m not sure we’re on the same page.

Secondly, I don’t understand what that 89 percent is of, right? So I need to know, what’s the numerator and the denominator? You know, not just what’s the rough percent. And maybe most significantly, I think it’s problematic for an organization of this sophistication, for a company of this capability, that they’re satisfied with and bragging about 89 percent. So Ford Motor Company is one of the members of Stop Hate for Profit. They don’t report, “Eighty-nine percent of our seatbelts work, so we’re good.” Right?

Or, Kind Snacks is another company sitting in Stop Hate for Profit. They don’t say, “Oh, well, 89 percent of our bars don’t contain glass.” Or Levi’s, which is another one of our participants. They don’t get to say, “Well, 89 percent of our jeans aren’t made in sweatshops, so we’re good.”

Again, when we’re talking about companies, the size and sophistication in any other industry, there is no “good enough.”

Right? It’s sort of binary. I think for Facebook, which is literally the most sophisticated advertising platform in the history of capitalism, 89 percent is not nearly good enough. They can microtarget users, you know, to a degree heretofore unimagined: by age, by gender, by geography, by industry, by favorite sports team — I could go on. The idea that they can’t knock off the neo-Nazis? I’m sorry, if this were really a corporate priority, if this were really an urgent issue, there’s a lot more that could be done. And then ultimately what one would do, what one would say, is for example, let’s stick with Facebook groups, alright? It’s one of my recommendations: They should remove all Facebook groups promoting hate.

I used to be an executive at Starbucks. If you had a drink at Starbucks that was making a customer sick … you know what we would do at Starbucks? Or what the executive team would do? It’s a question, I’m not being rhetorical.

Shirin Ghaffary

You would pull it all off the shelf, right?

Jonathan Greenblatt

You’d pull it off the shelf and you’d fix it, you’d figure it out. So Facebook groups promoting the Aryan Brotherhood, promoting boogaloo, promoting other violent and hateful killing conspiracies — you pull it down, and you’ve fixed it once and for all. Would it be disruptive to all the other people using groups? Yes, but that’s how it works in business.

Rebecca Heilweil

Do think it’s fair to say that the ADL’s relationship with Facebook has just broken down over the years? And can you speak to that?

Jonathan Greenblatt

I mean, look, we are still working with them on a regular basis. Yeah, I think we’re very deeply frustrated. … I think the company really needs to ratchet up its game dramatically. And I would start by looking at the recommendations in Stop Hate for Profit. They are simple. They are straightforward.

Again, for a company of this capability, they would not be difficult to execute. It’s a one and done.

Guess what happens when it’s one and done, by the way? Their product is improved. Their platform is safer and more secure for all of its users. You have less racial hatred. You have less anti-Semitism, you have less xenophobia, you have less anti-Muslim bias. I think the outcome here would be constructive and a good thing for the company.

Facebook should stop making exceptions to its rules for politicians such as President Trump
Shirin Ghaffary

Some are asking if Zuckerberg and Facebook are too sympathetic or too hesitant to moderate Trump and if there’s any political motivation there. At some point, Color of Change put out a statement saying it thinks Facebook has “back channels” with Trump and far-right extremists. So what do you make of all that? Do you think there’s truth to this?

Jonathan Greenblatt

Political exemption is one of the things that we think needs to be dealt with. There are hard things — this did not seem like a hard decision. For a company that had $8 billion in profits last year because [of] the advertising dollars that it earned right from companies. I just gotta say, eliminate the political exemption. I think that would please their advertisers who don’t want their ads subsidizing hateful content or violence.

That’s why we have it in there as a recommendation [in the list of demands tied to the Facebook ad boycott]. Number 8: removing misinformation other than voting. This is something Mark talked about in the public remarks he made about two weeks ago. And he said, we’re going to crack down on voter information on Election Day. So I’m like, “Okay, well, that’s great. That’s good. But what about the other 364 days of the year?” Voter misinformation matters every day. And I don’t think there’s anything political in that because the right to vote is a constitutional guarantee of all citizens, irrespective of how they choose to affiliate.

And again, prohibiting calls of violence by any politicians. Again, there’s nothing partisan in that. Implementing that kind of scripture ultimately serves the general public. Which should be applied equally to all politicians, or all elected officials.

Shirin Ghaffary

Do you think that Trump is held to a different standard? Do you think, even compared to other public figures, that he’s held to different standards by Facebook?

Jonathan Greenblatt

Yes, of course he’s held to a different standard. And again, the looters and shooters comment — I can’t imagine any circumstance that should have been permitted, including when it was said by President Trump.

This is why we call for the creation of the civil rights role in the C-suite. Because, of course, Mark Zuckerberg isn’t an expert on civil rights. He’s the CEO, but you should have people around the table who can advise on it.

As you probably saw when they released the audit yesterday, they announced they’re going to create a VP-level position for civil rights. But considering Facebook’s scope and their size and the spread of their services across every segment of society, I think you can’t afford to have civil rights subordinated three levels down. You need someone with this kind of authority at the table, across the table from Mark. Someone who brings the level of experience you need to navigate these complicated issues.

Who should decide what speech is acceptable and unacceptable in 2020?
Shirin Ghaffary

There is a shift happening in terms of what is considered acceptable speech online and what is not. Do you see that as well? And where do you see this headed? Do you think that Facebook should have a role in saying what is acceptable content or not? Do you think that should come from regulators or third parties?

Jonathan Greenblatt

Look, ultimately, in this political environment, you’re not going to see any legislation from Congress, at least for the near future. Any action by the kind of regulatory bodies, the executive branch, like the FTC, or otherwise may play a role — or the DOJ.

That being said, I think it’s going to take, it’s going to have to be a cross-sector effort. We’ve got consumer advocates, civil rights activists, corporate advertisers — and taken together, I hope we can encourage Mark. I mean, Mark can choose to ignore government regulators. He can ignore consumer advocacy. But I can’t imagine that Mark and that Facebook will continue to ignore the corporate advertisers.

Shirin Ghaffary

So it sounds like you think the public should take a role?

Jonathan Greenblatt

I think Stop Hate for Profit is a demonstration of public frustration. And the response reflects public pressure. And I think on top of that, I think this campaign looks poised to grow. Every day we’re [hearing] from people around the world who want to be involved. We’re hearing from other sectors who want to step up. I think this campaign will likely expand and intensify if we don’t see any progress by the end of the month. That would be my guess.

Will you help keep Vox free for all?

Millions of people rely on Vox to understand how the policy decisions made in Washington, from health care to unemployment to housing, could impact their lives. Our work is well-sourced, research-driven, and in-depth. And that kind of work takes resources. Even after the economy recovers, advertising alone will never be enough to support it. If you have already made a contribution to Vox, thank you. If you haven’t, help us keep our journalism free for everyone by making a financial contribution today, from as little as $3.

  Shadow, the startup behind the catastrophic Iowa caucuses earlier this year, has quietly lost its majority funder, rebranded itself, and changed its CEO to help chart a new, more focused direction in 2020.
What was once called Shadow Inc. is now called BlueLink, a recent restructuring that was almost certainly necessary after Shadow became embroiled in controversy for creating the flawed and hastily built app that sowed confusion and significantly delayed the reporting of the results of Iowa’s Democratic caucuses. It’s part of an attempt by the company to move past a massive blunder that not only jeopardized Shadow the company, but also cast shadows over the Democratic Party’s tech and Silicon Valley donors’ abilities to bring their know-how to the party.
“If there’s ever a time for a rebrand,” then-CEO, Gerard Niemira, told Recode in May, “this is it.”
Niemira and the company’s majority investor, a constellation of progressive political groups called Acronym, became some of the most despised forces in Democratic politics after the app’s technical difficulties in February delayed results for a week and robbed the caucus winners of their momentum. Now, ahead of the general election — when the Democratic Party will need its digital efforts to prove error-free — the group is trying to move forward.
“Obviously when something like Iowa happens, it’s important to take stock,” Niemira said.
But the newly rebranded BlueLink will have to accomplish this without ACRONYM, which told Recode that it decided after Iowa in February to “wind down our involvement with Shadow” and that, as of May, it is in the process of divesting from the company.
Niemiera is now also out as CEO of BlueLink as part of a quiet leadership shakeup. Niemiera told Recode in May that he was bringing on more help with Irene Tollinger, a Silicon Valley designer and product executive, who Niemiera said would be BlueLink’s chief operating officer. But just a few months later, on BlueLink’s newly designed website, it is Tollinger listed as CEO — with Niemiera not listed anywhere on its “Team” page. Tollinger confirmed to Recode on Tuesday that Niemiera had stepped down and that she had taken the CEO role. 
Niemiera said in May that Tollinger would oversee what the startup is hoping will be a return to its foundational work: integrating data for campaigns neatly into one place, rather than developing caucus apps beyond its expertise. Niemira said then that he was talking to Tollinger — who calls herself in her Twitter bio an “untangler of thorny problems” — before the Iowa screw-up, but she is expected to oversee its main business line — the data work called LightRail — going forward.
The other pillar of the company’s work, software that allows campaigns to text voters and volunteers, won’t be a priority after the November elections because of the crowd of other progressive vendors that have begun offering similar services, Niemira said. He also vowed to kill one-off projects like the Iowa app. 
“My cardinal sin with the Iowa stuff was distracting the team,” he said.
But even with a more focused company, it’s not clear that the startup that brought you the Iowa caucuses will be able to escape its history. Acronym as of May remained its majority investor, Niemira said, but with Acronym eventually cutting ties, BlueLink will have to find other donors going forward. And while lines like “When a light is shining, Shadows are a constant companion,” were scrubbed from its old website, Niemira seemed well aware that partners will easily be able to figure out that the company remains essentially the same product in a new wrapper. 
It’s a cover, he acknowledged with a knowing laugh, that is “pretty thin.” But that’s better than continuing to run one of the most hated brands in Democratic politics.
“It wasn’t fun for me personally, I can tell you that. It remains not fun,” Niemira said of what happened in Iowa. “But you got to get up and do the work every day, whether it’s coronavirus, bad press, or anything else that comes your way.”
Update, July 14: This article has been updated with the news of BlueLink’s CEO change.

  
  
  
      




  Breaking Bad was one of the biggest hits Netflix never made. 
After old episodes of the drama about high school science teacher and meth dealer Walter White began airing on Netflix, the show got more popular on AMC, its original network. Usually, audiences for TV shows dwindle over time. With Breaking Bad, the opposite happened: Its season four finale had less than 2 million viewers; but when the series finale aired two years later, its audience had grown to 10 million people. 
This is an example of what we call the Netflix Effect — that’s Netflix’s ability to find new and bigger audiences for shows that had languished on traditional networks. 
That’s why Netflix was traditional TV’s frenemy for a relatively brief time period. It could grow the popularity of shows that were still running on traditional TV — while simultaneously teaching TV viewers to stream shows on Netflix.
AMC insists that Netflix was just one of the reasons Breaking Bad got so big — it says that audiences also found the show via video-on-demand and Breaking Bad “marathons” that the network would run. Most other people, including Breaking Bad creator Vince Gilligan, credit Netflix for the boost. 
Netflix’s platform made it easy for viewers to binge multiple episodes in one sitting. They could watch episodes whenever they wanted, rather than waiting for them to air on TV. And, crucially: When they watched Breaking Bad on Netflix, they didn’t see a single commercial. 
Netflix provided a superior way to watch the show, and audiences followed. 
Breaking Bad’s Netflix-powered ascent was good for AMC, and for Netflix, and for Netflix viewers. But this kind of win-win-win was a temporary phase for Netflix and the TV networks. Eventually, the TV guys realized that even though Netflix could boost them in the short term, they were weakening their own business in the long term by training viewers to watch their stuff on Netflix instead of TV. So they started to claw back their programming for their own Netflix-style services that they began launching. But Netflix executives, who anticipated that move, had been busy creating their own shows.
Even in 2020, it’s not always a direct competition. Netflix is spending billions a year on its own shows to pull more viewers away from traditional TV, but the streaming service can sometimes still work with traditional TV. 
Last year, Gilligan finally released a Breaking Bad sequel — El Camino, a movie focused on Walter White’s sidekick Jesse Pinkman. And this time, the TV/Netflix order was reversed: The movie debuted on Netflix last fall and then showed up on AMC a few months later.
On this episode of Land of the Giants: The Netflix Effect, we look at Netflix’s relationship with the traditional media companies it eventually disrupted. It’s complicated! Which makes it perfect for our podcast.

Subscribe to Land of the Giants on Apple Podcasts, Google Podcasts, Spotify, Stitcher, or wherever you listen to podcasts.

  
  
  
      




  Each year, around this time in July, billionaire and former Google CEO Eric Schmidt usually invites a few dozen celebrities, economists, and politicians from around the world to Big Sky country.
On the 5,200-acre grounds of the ultra-exclusive Yellowstone Club retreat, Schmidt brings together the likes of Lady Gaga, Sen. Cory Booker, and about 40 other invitees to hear from leaders who have included some Google executives — and few people know about it.
In the world of billionaires and other elites, these kinds of ritzy Western getaways — often christened “ideas summits” or weekends for “thought leadership”— are not uncommon. But they are usually quasi-public, like the paparazzi-littered Sun Valley deals conference, or at least more broadly known, such as the annual retreat hosted until recently by now-disgraced television host Charlie Rose or Mitt Romney’s summit in Utah.
But this particular conclave — in its eighth year since launching in 2012, though it’s taking a break due to the coronavirus pandemic — has remained highly secretive by design, only vaguely acknowledged in media. But it is coming into focus in a new report from a tech watchdog group, the Tech Transparency Project, which combed through social media posts and flight records connected to Schmidt’s retreat to offer the most detail yet on the weekend. The report argues that this retreat gives Schmidt, until recently one of Google’s highest-profile executives, a way to charm his critics and curry favor for Google — covertly.
“The picture that emerges is Schmidt using the celebrity-studded retreat as a form of soft power, helping Google advance its interests with cowboy hats and intimate concerts rather than the traditional tools of corporate influence,” the group writes in the report. 
Schmidt and Google declined to comment on the event and on the report.
Retreats for the elite are drawing more scrutiny as populist pushback builds to both tech wealth and to the broader notion of how elites shape public opinion. Google is one of the world’s most powerful companies, and Schmidt has long served as its political fixer, the tip of the spear of a tech lobbying operation that spent $150 million over the last decade. (Schmidt reportedly cut his last lingering tie to the company earlier this year.) And the event, at least theoretically, is a possible way for Schmidt to more gingerly build out this sphere of influence.
The event seems to have adopted a veil of secrecy. More than a dozen attendees identified by the Tech Transparency Project — which itself doesn’t fully disclose its backers but says it has no corporate donors — didn’t return requests for comment. Part of the reason the event has effectively remained undercover may be that it’s held at a heavy-security club among the most exclusive locations in the country — “the world’s only private ski, golf and adventure community” that is limited to just about 860 memberships to preserve its exclusivity. Mentions of the Schmidt event are almost entirely nonexistent in the press, save a few in mostly foreign media.
In one of those few apparent public utterances about the event, a fellow Yellowstone Club member and billionaire, Ron Burkle, said Schmidt hand-picks the people who appear at the retreat each year.
The event’s agenda seems to resemble other conferences: Attendees sit in on sessions and interviews, spend time outdoors, attend private concerts from musicians like Leon Bridges, or even jaunt over to the Yellowstone Club’s annual rodeo invitational, which sometimes coincides with the Schmidt event, the report says.
But crucially, Schmidt’s event offers an opportunity for him to not just entertain some of the world’s most influential thinkers, but to occasionally promote Google by bringing Google leaders to the event, according to the authors, such as one of the inventors of Google Glass and a Google employee who worked on its self-driving cars. Some of the event’s past attendees have the power to scrutinize and investigate Google in their roles as lawmakers or journalists. 
Still, there’s no hard evidence that Schmidt’s weekends have directly won him or Google any favors. The group merely raises the possibility that it could soften attitudes toward the tech giant. In a way, that’s part of the point — it’s usually hard to know if retreats sway attendees’ views or relationships with the host. It’s the under-the-wraps nature of Schmidt’s retreat that appears to be unusual.
The report points to Austrian Chancellor Sebastian Kurz and Pulitzer Prize-winning investigative reporter Ronan Farrow as examples of past attendees whose relationships with Google may have been influenced by Schmidt’s events. The report speculates that there is a possible connection between changes in Kurz’s stance on taxing tech companies and his attendance in 2018. A spokesperson for Kurz declined to comment. And though Farrow has published critical pieces about Google since he attended the event, the report calls out one particular story Farrow ran on NBC about ISIS that the Tech Transparency Project felt was soft on Google.
“I view it as part of my job to meet sources in government and business, along with academics, scientists, and fellow journalists,” Farrow told Recode. “That has not and would not influence my reporting. And I welcome any investigative leads on Google or any other major tech company.”
Other senior reporters have also attended the Schmidt weekend in the past, a group that once included Vox’s editor-at-large, Ezra Klein. Klein said the event “seemed to be a pretty standard ideas-style conference, but in Yellowstone.” 
“The guest list and tone definitely reflected an older era when Silicon Valley thought it had a lot to teach Washington, and Washington was more interested in learning,” he told Recode.
The Tech Transparency Project doesn’t have ironclad proof of any policy or opinion change tied to the conference. But Katie Paul, the project’s director, argues that the existence of this retreat should at least be publicly known since, she says, it is effectively a form of lobbying.
“Its secrecy is also something that should be concerning to the American public — in terms of what kinds of influence these companies are trying to wield — if it’s something they feel they need to keep quiet,” she said.




  
  
  
      









  Civil rights leaders are fed up with Facebook. They say the company still allows too much racist, hateful, and violent content to spread on its social media network — and that company executives gave PR “spin” rather than meaningful solutions in a meeting on Tuesday about these issues.
In light of recent protests against racial injustice in the US, Facebook is facing a renewed reckoning over how it handles hate speech on its platform. Civil rights leaders, advertisers, politicians, and even some of Facebook’s own employees are urging the social media giant to make faster progress in its several-years-long “journey” of fighting hate speech. Critics say the company’s own executives have held back this progress because they have failed to commit to concrete fixes or timelines. 
“It was abundantly clear in our meeting today that Mark Zuckerberg and the Facebook team are not yet ready to address the vitriolic hate on their platform,” the Stop Hate for Profit Coalition, a collection of civil rights groups that includes the Anti-Defamation League, the NAACP, Free Press, and Color of Change, said in a statement shortly after Tuesday’s call. The group denounced how Zuckerberg “offered the same old defense of white supremacist, anti-Semitic, islamophobic, and other hateful groups” that it has “heard too many times before.”
In a press call on Tuesday following the meeting with Facebook, the Stop Hate for Profit Coalition said the company had not met any of the coalition’s 10 outstanding demands, which include having Facebook install a C-suite-level executive with civil rights expertise to make key decisions about discrimination; to conduct regular third-party audits of misinformation and hate speech on the platform; and to remove Facebook groups promoting things like white nationalism and anti-Semitism.
“Unfortunately, we got no clarity, no details, no results,” Jonathan Greenblatt, president of the Anti-Defamation League (ADL), said on the press call. “This was disappointing to us. We expected specifics and this is not what we heard.”
A spokesperson for Facebook told Recode in a statement:
“This meeting was an opportunity for us to hear from the campaign organizers and reaffirm our commitment to combating hate on our platform. They want Facebook to be free of hate speech and so do we. That’s why it’s so important that we work to get this right.” Facebook noted that it has “invested billions” in moderating hate speech, created new policies to prohibit voter suppression, and recently banned 250 white supremacist organizations. “We know we will be judged by our actions, not by our words, and are grateful to these groups and many others for their continued engagement,” the statement said. 
In the past few weeks, nearly 1,000 companies, including big-name brands like Unilever and Coca Cola, have agreed to join Stop Hate for Profit’s boycott, pausing their advertising at least through July because of Facebook’s perceived failure to police hate speech on its platforms, whether from everyday users or political figures like Donald Trump. While this boycott represents a small portion of Facebook’s total ad revenue (which largely comes from small- and medium-sized businesses), it’s still bad for the company’s reputation. 
Facebook’s struggles in handling hate speech and misinformation on its platform date back a long time. Many have argued the company’s policies and inaction have contributed to the genocide against the Rohingya people in Myanmar, exacerbated political polarization in the lead-up to the 2016 US presidential election, and facilitated the growth of violent extremist groups like the boogaloo movement in the US.
This criticism of Facebook resurfaced after the company refused to moderate recent controversial posts by President Trump on the platform, such as his “shooting ... looting” comment, which many viewed as inciting violence against Black Lives Matter protesters. By contrast, Twitter flagged an identical post by Trump on its platform with a warning label for glorifying violence.  
In a public note on her Facebook account on Tuesday, Facebook COO Sheryl Sandberg defended the progress Facebook has made on this front over the years and said it’s continuing to improve.
“We have clear policies against hate — and we strive constantly to get better and faster at enforcing them,” Sandberg wrote. “We have made real progress over the years, but this work is never finished and we know what a big responsibility Facebook has to get better at finding and removing hateful content.” 
On Wednesday, Facebook is expected to publicly release the final results of its two-year external audit on civil rights, which tracked how the company addresses hate speech on its platform. But the groups in the Stop Profit for Hate campaign say they doubt the report will prompt the kind of change needed, and so they are continuing to pressure the company publicly to take more forceful action against hate speech.
As Facebook has faced pressure to come down harder against hate speech and misinformation, it’s emphasized its commitment to free speech principles. Zuckerberg has repeatedly said he doesn’t want to be an “arbiter of truth” on thorny issues like political misinformation — an argument he has long used to defend Facebook’s role as a “platform” of open communication rather than a media company that passes editorial judgment about acceptable political and democratic discourse. 
Many Republicans, including President Trump, have made unfounded accusations that Facebook and other social media platforms have an anti-conservative bias. In May, the president issued an executive order attempting to strip social media platforms of legal protections in order to compel them to be less purportedly biased. The executive order is widely seen as legally unenforceable, but it has restarted a political debate about what people should be allowed to say on social media platforms.
On Tuesday’s call, civil rights leaders disputed the notion that stopping hate is a political issue.
“This is not an issue with two sides. There is nothing partisan or political in pushing back on prejudice,” said the ADL’s Greenblatt. 
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



The social media company TikTok has announced that it will no longer operate in Hong Kong, marking the most aggressive move yet by a technology company in response to the implementation of a new national security law that’s already being used to crack down on dissent in the region.
“In light of recent events, we’ve decided to stop operations of the TikTok app in Hong Kong,” a spokesperson for the social media company told Recode. 
TikTok has faced ongoing criticism and skepticism from American lawmakers over its relationship with its Beijing-based parent company, ByteDance, which also runs a similar, separate app in China. On Monday, Secretary of State Mike Pompeo said the United States is considering banning the app, among other Chinese social media apps, though it’s not clear how such a prohibition would be enforced. In response, TikTok said it would never provide user data to the Chinese government, and it would refuse such a request. The company’s recent transparency report does not include any requests for data from China. 
TikTok’s decision comes as other major tech companies — namely Facebook, Google, Microsoft, Twitter, and Zoom — have said they would stop processing Hong Kong government requests for user data. The companies are pushing back against a new national security law that went into effect earlier this month that could quash free speech and dissent in the region. The pushback also represents a rare moment when big American tech companies are contesting China’s tight grip on information in the country. 
“Last Wednesday, when the law took effect, we paused production on any new data requests from Hong Kong authorities, and we’ll continue to review the details of the new law,” a Google spokesperson told Recode.  
Meanwhile, Apple has remained more cautious.
“We’re assessing the new law, which went into effect less than a week ago, and we have not received any content requests since the law went into effect,” a spokesperson for the company told the Guardian, adding that the company typically does not receive requests directly from Hong Kong authorities. 
Backlash against the imposition of the new national security law in Hong Kong has been brewing for over a year. Because Hong Kong has historically operated with some degree of autonomy from China, protesters took to the streets in March 2019 after a proposed bill in Hong Kong would have allowed extradition of suspects to mainland China. More than a year later, the Chinese government imposed the sweeping new national security law and began enforcing it on July 1, when the full text of the law was released for the first time. Among other things, the new law specifically criminalizes “secession, subversion, organization and perpetration of terrorist activities, and collusion with a foreign country or with external elements to endanger national security.” The law also applies online and gives authorities the right to remove posts.
While it’s so far unclear how officials will enforce the new law, Hong Kong police quickly arrested several protesters for doing things like waving Hong Kong independence flags and holding signs calling for Hong Kong’s independence. As many worry about the extent to which the new law will be used to silence dissenters online, social media companies are springing to action.
In response to the law, Facebook specifically cited concerns over the protection of free speech; WhatsApp, which is owned by Facebook, is also pausing requests for user data from Hong Kong authorities. “We believe freedom of expression is a fundamental human right and support the right of people to express themselves without fear for their safety or other repercussions,” a Facebook spokesperson told Recode, adding that the company will continue to analyze the new law and conduct “formal human rights due diligence and consultations with international human rights experts.” 
Facebook, like many tech companies, has worked with the Hong Kong authorities in the past. In the second half of 2019, Facebook produced at least some data in response to just under half of the requests it received from the Hong Kong government. 
Twitter said that, after the national security law went into effect last week, it immediately paused its processing of Hong Kong authorities’ requests for user data and that government-issued requests will be disclosed publicly. A Twitter spokesperson told Recode the company “is committed to protecting the people using our service and their freedom of expression” and will be “reviewing the law to assess its implications, particularly as some of the terms of the law are vague and without clear definition.” 
The secure messaging app Telegram, which has been popular during the Hong Kong protests, announced that it had suspended its process for responding to user data requests from Hong Kong authorities. Meanwhile, the encrypted message app Signal said in a tweet that it would have announced it was stopping but “never started turning over user data.” The company added, “Also, we don’t have any user data to turn over.”
Update: This story has been updated to include statements from TikTok, Apple, and other technology companies.
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  Did you watch Tiger King during the quarantine?
A lot of us did. Netflix’s true-crime documentary about tiger breeders battling it out with animal rights activists was released in late March. Within a month, some 64 million people had watched at least some of the series.
This was during the same time that millions of people were stuck at home for lockdown and decided to subscribe to Netflix.
But did all of those Netflix users watch Tiger King because they wanted to or because their friends told them to? Or was it because Netflix told them to via its vaunted recommendation algorithm?
That’s the question we explore on the latest episode of Land of the Giants: The Netflix Effect. 
And more broadly, we wanted to get a better understanding of how Netflix uses the vast trove of data it collects about its viewers’ habits.
At this point, most of us know that big tech companies often surveil their users and then use the information they gather to decide what their users are likely to see or engage with. The downside of that can be obvious: Just ask Facebook’s and Google’s many critics.
The stakes seem lower for Netflix. For instance, to date, no one has accused the company of subverting democracy in the US. And Netflix says it doesn’t hoover up nearly as much of your data as other big tech companies because it’s not in the ad business and doesn’t need that level of information.
But Netflix does take very detailed notes about the way its 200 million users behave when they watch Netflix.
It says it uses that data to help figure out what you might want to watch — and to figure out the best way to present that choice to you. Even if Netflix thinks you and your friend both want to watch, say, Floor Is Lava, it may show each of you different images or clips from the show based on what it knows about your viewing habits.
But our big question is whether Netflix has a bias about who owns the stuff it shows you.
It’s an increasingly relevant question as media companies that used to license their stuff to Netflix claw that content back so they can power their own competing streaming services. Which is why Friends has left Netflix for HBO Max, and The Office is going to Comcast’s Peacock, and why most of Disney’s TV shows and movies have moved to Disney+.
So it would seem obvious that Netflix would want to promote shows it makes and owns, like Tiger King and Floor Is Lava, because it can keep those shows, as opposed to ones that will eventually leave for another streamer.
Which is why we asked Netflix executives and people outside of the company if that’s actually happening. You can hear their answers on the player below, or on Apple Podcasts, Google Podcasts, Spotify, Stitcher, or wherever you like to listen to podcasts. 


  
  
  
      




  Did you watch Tiger King during the quarantine?
A lot of us did. Netflix’s true-crime documentary about tiger breeders battling it out with animal rights activists was released in late March. Within a month, some 64 million people had watched at least some of the series.
This was during the same time that millions of people were stuck at home for lockdown and decided to subscribe to Netflix.
But did all of those Netflix users watch Tiger King because they wanted to or because their friends told them to? Or was it because Netflix told them to via its vaunted recommendation algorithm?
That’s the question we explore on the latest episode of Land of the Giants: The Netflix Effect. 
And more broadly, we wanted to get a better understanding of how Netflix uses the vast trove of data it collects about its viewers’ habits.
At this point, most of us know that big tech companies often surveil their users and then use the information they gather to decide what their users are likely to see or engage with. The downside of that can be obvious: Just ask Facebook’s and Google’s many critics.
The stakes seem lower for Netflix. For instance, to date, no one has accused the company of subverting democracy in the US. And Netflix says it doesn’t hoover up nearly as much of your data as other big tech companies because it’s not in the ad business and doesn’t need that level of information.
But Netflix does take very detailed notes about the way its 200 million users behave when they watch Netflix.
It says it uses that data to help figure out what you might want to watch — and to figure out the best way to present that choice to you. Even if Netflix thinks you and your friend both want to watch, say, Floor Is Lava, it may show each of you different images or clips from the show based on what it knows about your viewing habits.
But our big question is whether Netflix has a bias about who owns the stuff it shows you.
It’s an increasingly relevant question as media companies that used to license their stuff to Netflix claw that content back so they can power their own competing streaming services. Which is why Friends has left Netflix for HBO Max, and The Office is going to Comcast’s Peacock, and why most of Disney’s TV shows and movies have moved to Disney+.
So it would seem obvious that Netflix would want to promote shows it makes and owns, like Tiger King and Floor Is Lava, because it can keep those shows, as opposed to ones that will eventually leave for another streamer.
Which is why we asked Netflix executives and people outside of the company if that’s actually happening. You can hear their answers on the player below, or on Apple Podcasts, Google Podcasts, Spotify, Stitcher, or wherever you like to listen to podcasts. 


  
  
  
      




  Amid the advertiser boycott of Facebook, one of the Democratic Party’s biggest megadonors, Reid Hoffman, is exploring ways to launch a similar boycott of President Trump and his White House.
Hoffman, the billionaire founder of LinkedIn, could spend as much as $100 million this election cycle and has elbowed his way into becoming one of the most powerful players in Democratic politics. So what issues attract his attention — and, correspondingly, his money — matter because they can help explain where the progressive movement will focus over the summer.
Hoffman foreshadowed what may be his next political move this week when asked about the building protest movement against Facebook over its unwillingness to aggressively moderate Trump’s inflammatory posts. Hoffman volunteered a revealing tidbit about his political thinking.
“Frankly, on the political side, one of the things I’ve been thinking about trying to go stimulate for the next month is an anti-Trump boycott,” Hoffman said in an interview on Tuesday with Anne-Marie Slaughter, the head of New America. “The various forms of enrichment with Mar-a-Lago and all the rest should not be part of an American political system. It should be protested in economic ways, as well as political ways.”
Hoffman didn’t offer much detail about the idea, which is said to be in its early stages. But if companies are thinking about ways to stand up against racial hatred, Hoffman’s team thinks they shouldn’t just boycott Facebook, which broadcasts that hatred, but rather those who voice it, like Trump.
Based on some of his past conversations, the move by Hoffman could resemble an effort like Grab Your Wallet, a campaign that convinced some retailers, including Nordstrom and T.J. Maxx, to drop products associated with Trump and his family. According to Shannon Coulter, the campaign’s leader, Hoffman’s team reached out to Grab Your Wallet and has had extensive talks about a donation since 2017. Hoffman’s team has declined to fund Grab Your Wallet because Coulter said they preferred to focus on electoral politics, and she hasn’t heard from Hoffman’s team in about a year. 
Grab Your Wallet has now broadened its work to focus on boycotting companies with board members or executives that have supported Trump politically. There are signs that this type of organizing works — the attendance at SoulCycle fell after a boycott sprang up to protest that the fitness company’s majority investor, Stephen Ross, was hosting a fundraiser for Trump.
Another possibility is that Hoffman may try to organize some boycott of Trump properties, such as Mar-a-Lago, which he mentioned in the interview. The problem with that approach, though, is that now, five years after Trump announced his run for president, the brands and people that are willing to be associated with Trump properties, including his hotel in Washington, DC, have already weathered much blowback and chosen to stick around.
“Something focused on Mar-a-Lago is unlikely to work, in my opinion, for the same reason it doesn’t work to boycott Trump’s hotels,” Coulter said. “The people who patronize those places are already very much on Team Trump.”
The advertiser boycott of Facebook has perhaps proven a model for boycotts of brands. After Facebook repeatedly declined to moderate Trump’s inflammatory posts about racial justice protests, a group of civil rights organizations launched a campaign called Stop Hate for Profit that has convinced some of the country’s biggest brands to pause advertising on Facebook, even if it’s unclear whether that will make a serious dent in Facebook’s revenue. Facebook recently announced new plans to regulate hate speech.
One of Facebook’s early investors was Hoffman’s venture capital firm, Greylock, and Hoffman and Facebook founder Mark Zuckerberg remain close.
What Hoffman chooses to fund is closely watched in the Democratic Party. While he has had his share of stumbles and detractors in the world of politics, he and his team are the tip of the Silicon Valley spear as the tech community marshals its considerable resources to boost presumptive Democratic presidential nominee Joe Biden.
But there are some signs that the tech community and Biden still don’t see totally eye to eye. Biden has called for the immediate revocation of Section 230, the landmark law that protects publishers like Facebook from liability over what third parties say or do on the platform.
Asked what Hoffman, who has advised Biden on digital campaigning, would say to Biden about Section 230, Hoffman drew some distance from his chosen candidate, saying that there should be “less-absolute-than-publisher liability, but more than no liability.” A viewer of the session asked Hoffman about a recent Washington Post editorial saying that both Biden and Trump — who also wants to revoke Section 230, albeit for different reasons — were wrong.
“Both of those two positions seem to be badly put from an American values perspective,” Hoffman said.


  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Fully autonomous trucking may be one step closer to reality. On Wednesday, the San Diego-based self-driving startup TuSimple announced what it’s calling the world’s first autonomous freight network. That means that the company is laying the groundwork for delivering a lot more of our stuff with self-driving trucks.
TuSimple is hardly the only company working to making fully automated shipping a reality. Several companies, including Aurora, Daimler, and Embark Trucks, are competing for a slice of the future of self-driving freight trucks. Alphabet-owned Waymo confirmed on Tuesday that it will be expanding its own self-driving trucking routes throughout the American Southwest and Texas, following previous tests in Arizona, California, Michigan, and Georgia. 
TuSimple’s expansion plans seem more concrete than some of its competitors. The company is expanding existing shipments with UPS, which has also invested in TuSimple, and the foodservice delivery giant McLane. The major shipping company US Xpress, one of the nation’s largest freight companies, will also start shipping goods through TuSimple, which now has 22 contracted customers. Those companies will ultimately have influence over which routes are digitally mapped out next for self-driving trucks. 
“Imagine if you could influence, back in the day, where a railroad track was being built, and you could build that railroad track right to your front door,” TuSimple president Cheng Lu told Recode. “As a shipper, wouldn’t that give you a big advantage?”
TuSimple plans to expand its routes in the next four years, starting with the addition of new delivery service throughout the Texas triangle of Dallas, Houston, and San Antonio in 2020 and 2021. (The company plans to keep drivers behind the self-driving wheel just in case something goes awry.) By 2023, the company wants its self-driving trucks on routes between Los Angeles and Jacksonville, Florida, and hopes to be shipping nationwide by 2024.
  
  
    
    
      
        
  


  






      
    
    
  
  


Currently, TuSimple has just 40 autonomous trucks in use — though it’s adding 10 more later this year — and its longest route is the 1,000-mile drive between Phoenix and Dallas. TuSimple’s trucks are (eventually) meant to operate without humans; they will be trained on mapped routes, but the goal is to autonomously navigate traffic— and other surprises — on the roads. The challenge is building a trustworthy, AI-powered “virtual driver” that can effectively process information from a wide array of sensors, like lots and lots of high-quality cameras, but also lidar and radar. TuSimple, which was valued at around $1 billion last year, is mainly focused on long-haul trucking and hasn’t spent as much time on self-driving cars. 
That’s a big difference from Waymo, which has focused on last-mile delivery and has already used its smaller self-driving minivans to move packages for UPS and AutoNation. In a call with reporters on Tuesday, Waymo’s commercial lead for trucking, Charlie Jatt, shared how the company plans to move forward with trucking. The strategy centers on demonstrating and testing a platform for self-driving trucks until the technology can be implemented into vehicles and parts produced by other manufacturers. 
“We see ourselves as a technology company, not a trucking or fleet management company,” Jatt said, according to a transcript of the call. “We’re developing the Waymo Driver and then will partner with OEMs to ensure that our technology can be successfully integrated on classic trucks that are being manufactured and sold to the market in the future.” 
Waymo said it is manufacturing some of its own self-driving trucks in Detroit — these are the giant, Class 8 shipping trucks you often see on highways — but the company wouldn’t detail how large the fleet would be. Before the autonomous trucks hit the highway, Waymo sends out smaller vehicles, generally minivans, to collect data and map out routes. Thus far, Waymo says those trucks have previously helped deliver freight for Google’s logistics division in Atlanta, but ultimately, Waymo says it wants to focus on building software and mapping technology. 
While TuSimple, Waymo, and others are making progress, it will be years before the majority of America’s long-haul shipping is carried out by autonomous trucks. Engineers and regulators are still studying how self-driving trucks — and self-driving cars — actually work on the road. But the hope is that the technology will make trucking cheaper, safer, faster, and more fuel-efficient.
Still, these companies’ moves reflect that they’re not looking to destroy trucking as we know it, but instead want to strategically insert their autonomous technology into the manufacturing, shipping, and infrastructure that exists today. That’s probably good for trucking overall. Where it leaves truckers themselves is still unclear. 
Trucking jobs will change amid automation, but it’s not yet clear how 
One of the biggest fears about self-driving trucks is that the industry will ultimately displace the nearly 2 million people working as heavy and tractor-trailer truck drivers. For now, it’s not obvious how many of them will be affected in the near future. Researchers from the Bureau of Labor Statistics estimate that there are just under half a million long-haul drivers whose positions are the most vulnerable to autonomous technology, and point out that humans will still be needed for other, trucking-related tasks, like customer service and loading. Self-driving trucks that are already on the road also still need backup drivers.
Meanwhile, companies like TuSimple say they’re addressing a shortage of and high turnover among long-haul drivers, pointing to research from the American Trucking Associations. There’s also concern over the dangers of truck driving, with risk factors that include long hours behind the wheel, unforgiving weather, and driving under the influence. TuSimple addresses this anxiety in its marketing.


What if we could create a virtual driver who never drinks, never texts, and never gets tired?Find out more on 07.01.2020#autonomousdriving #autonomousvehicles #autonomoustrucks #selfdrivingvehicles #selfdrivingtrucks #trucking #ai pic.twitter.com/2R3XpZNFBf— TuSimple (@TuSimpleAI) June 24, 2020



But Lu, TuSimple’s president, argues that even if the technology does displace drivers, they’ll be able to find other jobs that focus more on first- and last-mile hauls, rather than the longer trips, and other types of jobs that are less exhausting. 
“It’s not like if you’re a long-haul driver, you can only be a long-haul driver,” Lu said. 
TuSimple worked with Pima Community College in Arizona to launch an online autonomous vehicles specialist certification program that’s meant to help truck drivers adapt to the new technology. Brenna Bayles, a Kansas-based driver who owns her own truck and is studying in the Pima program, said that she doesn’t fear automation, despite concerns that AI could take away jobs in the industry. In fact, she wants to own a fleet of self-driving vehicles herself one day. 
“The buggy-whip manufacturers didn’t want to see cars come along,” Bayles told Recode, arguing that self-driving cars will ultimately be safer and cheaper to insure. “They either get with the program and work in that field, in the capacities that they can, or they find another field, but we don’t stop progress.” 
Representatives of other truckers seem less optimistic. According to Norita Taylor, a spokesperson for the Owner-Operator Independent Drivers Association, beyond automation potentially threatening jobs, there are still many unanswered questions as to how self-driving trucks will navigate on-the-road situations typically handled by humans, like emergencies, problems with cargo, and dealing with law enforcement on the road.
“We have a lot of concern that federal regulators are going to put on blinders and push for more technology as the answer to the industry’s problems without considering the negative impacts of those technologies,” she said. 
Kara Deniz, press secretary for the Teamsters, expressed concern that self-driving technology could be used to extend the hours expected of human drivers behind the wheel and limit the rest and breaks that they’re entitled to. The union remains worried about the impact this technology could have on jobs, among concerns of safety and lack of transparency.
“None of these [companies’] speculations take into account the reality of driving,” Deniz said in an email, “or reflect the understanding of the industry from the perspective of someone who actually does the job day in and out.”
But the reality of those jobs certainly could change, especially as companies that employ truck drivers start to look more closely at self-driving technology. While it’s not clear how fast they will get there or what the rules of the road may be, Wednesday’s news is just another sign that the technology is on its way. 
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




  America is now home to almost 800 billionaires, a record high that accounts for more than a quarter of the world’s megarich.
That rise of the billionaire class is made stark in a new report that paints their financial might in the most complete picture yet. It also serves as a vivid snapshot of American income inequality at a time when anti-billionaire sentiment is on the rise and fueling global backlash against ultra-wealthy elites from both the left and the right. So the debate over whether the world’s megarich are too rich is really a debate about whether America’s megarich are too rich.
The number of billionaires in the US reached 788 by the end of 2019, a 12 percent increase from the prior year, according to the report from Wealth-X, which produces the comprehensive annual study. Those American billionaires now control $3.4 trillion in total assets, 14 percent more than they did at the end of 2018.
The country with the next-highest number of billionaires, China, has fewer than half the number of the US. That $3.4 trillion in American billionaires’ net worth is more than the combined total net worths of the billionaires who reside in the next eight countries.
Only three years prior, at the end of 2016, America had 620 billionaires. Those billionaires controlled just $2.6 trillion then. But the rise of Silicon Valley — and its tech giants, which have skyrocketed in value in the years since — has built out the American billionaire class.
The San Francisco area is the third-most-common home for billionaires, up from fifth place in 2016. The other US cities now among the biggest billionaire concentrations are New York (No. 1 overall, with 113) and Los Angeles (No. 7 overall, with 44).
And there are few signs that the tech boom is abating, or at least impacting the bottom line of its billionaire leaders. As the coronavirus wrecks economies around the globe, threatening the lives of low-income and working-class people, the billionaires of the world are doing pretty okay. That’s especially true in the tech sector, where there are 8.4 percent more billionaires around the globe as of May 2020 compared with the end of 2019. That’s the highest uptick of any sector.
That there are more American billionaires than ever helps explain why there is more backlash than we’ve seen in a long time to their war chests and influence. “Billionaire” became an insult in the Democratic presidential primary over the past year, with candidates even debating whether billionaires “should exist.” Conservative populists have lumped in these billionaires as pursuing elite “globalist” policy goals that leave working-class Americans, such as those who voted for Donald Trump, out to dry. And a new chorus of critics have, with some success, flipped one of the assets of billionaires — their commitment to philanthropy — into a liability by turning it into a debate over tax avoidance. 
This public interrogation of billionaires is not merely an academic exercise. It’s having an impact. We now feel as negatively about billionaires as we did during the financial crisis in 2009. If America does enter a depression due to the coronavirus, it makes you wonder just how much worse we’ll feel about the ultra-wealthy, especially now that they’re around in record numbers.
  
  
  
      




  
On Tuesday afternoon, Facebook announced that it had removed more than 200 accounts linked to the violent, anti-government extremist “boogaloo” movement. This move comes after weeks of criticism over the company’s handling of hate speech on its platform. Still, banning the boogaloo accounts does not solve Facebook’s larger hate speech problem.
More than 100 major brands, from Unilever to Verizon, have pulled advertising from the platform after civil rights groups called for a boycott in the past week. Facebook’s efforts to address the controversy included the announcement of increased efforts to prevent voter suppression based on race and ethnicity and a potential audit of its moderation practices. 
The controversy over social media companies and hate speech has intensified in recent weeks as protesters across the US have been fighting for greater racial justice. About a month ago, as protests were first breaking out, Facebook ignited outrage when it decided not to do anything after President Trump posted a comment saying “when the looting starts, the shooting starts” in a post about the protests. This enraged civil rights leaders, as well as some of Facebook’s employees. It also prompted the “Stop Hate for Profit” ad boycott, led by organizations such as the Anti-Defamation League and the NAACP. Three US senators joined the chorus on Tuesday, sending a letter to Facebook asking the company to more strictly enforce its rules on extremist content.
Facebook has long had a policy explicitly forbidding hate speech. The company says it removes 89 percent of hate speech on the platform before it gets reported and has argued that while there are always exceptions at its scale, overall, it’s doing a fine job. A recent report by the European Commission found that Facebook was faster than some of its competitors in addressing instances of hate speech. 
“We do not profit from hate. We have no incentive to have hate on our platform,” Facebook VP Nick Clegg said in a Bloomberg appearance on Tuesday and reiterated in a CNN appearance. 
Regardless of the company’s claims about policing hate speech, recent events are furthering the perception that Facebook simply isn’t doing enough. Specifically, critics have argued that the company is making exceptions for politicians like Trump and that flat-out violent groups like the boogaloo movement can continue to gain traction on the platform. They say that some of the company’s smaller competitors like Reddit and Twitter are more aggressively enforcing rules on hate speech, by banning or moderating accounts linked to President Trump and popular accounts that support him. 
Facebook’s approach fits into an established playbook for the company. The reported audit, for instance, would be the third one the company has commissioned. The company has taken down harmful conspiracy networks on its platform, only to see them pop back up or new ones arise. Meanwhile, the ad boycotts likely won’t have a dire financial impact because the bulk of Facebook’s revenue comes from smaller brands (the top 100 advertisers accounted for little more than 6 percent of its revenue last year). But Facebook and other social media companies at least appear to be responding to this new source of pressure.
“What you’re seeing right now is people are leveraging various mechanisms — either economic or public relations — to push back on policies they don’t like,” Kate Klonick, an assistant professor at St. John’s University School of Law who studies social media and free speech, told Recode. “And you’re seeing the platforms give in.” 
Twitter started a wave of social media companies coming down on Trump
Recent moves by Reddit, Snapchat, Twitch, and YouTube mark a decision by social media companies to start more strictly enforcing the rules around hate speech, weeks after protests around the police killing of George Floyd have caused a national reckoning over systemic racism in the United States. 
In many ways, Twitter started this wave of action when, in late May, it added a warning label for glorifying violence to President Trump’s “shooting … looting” post. This represented a precedent-setting move for social media companies, which have been reluctant to moderate Trump, no matter how incendiary his rhetoric. (Twitter has spent the past two years refining its policies on moderating politicians’ speech.) Facebook then struck a nerve when it responded very differently to the same Trump post on its own platform. The company chose not to moderate the post, arguing that it wasn’t an incitememt of violence but an announcement of state use of force.
Now other platforms are joining in and following Twitter’s lead. 
On Monday, Reddit banned r/The_Donald — a popular message board for Trump fans to share memes, videos, and messages — for consistently breaking its rules around harassment and hate speech. The same day, Twitch, a livestreaming company owned by Amazon, decided to temporarily suspend Trump’s account after it found some of its livestreams included “hateful conduct,” such as a rebroadcast of Trump’s kickoff rally where he said that Mexico was bringing rapists to the United States. Those moves follow Snapchat’s decision earlier in June to stop promoting Trump in its “Discover” section because his account had, in the company’s view, incited racial violence. And YouTube banned several high-profile far-right accounts, including those of white supremacist Richard Spencer and former KKK leader David Duke.
While there will be plenty more examples of hate speech on these platforms that likely go unaddressed, the spate of takedowns and bans could carry serious political consequences. They run against the stated free speech values of early internet forums like Reddit, which have historically tried to be as laissez-faire as possible in their approach to moderating content.
“I have to admit that I’ve struggled with balancing my values as an American, and around free speech and free expression, with my values and the company’s values around common human decency,” Reddit CEO Steve Huffman told reporters on Monday, according to The Verge, announcing the company’s decision to ban r/The_Donald. 
Even Facebook has notably drawn some lines with Trump, taking down a Trump campaign ad featuring Nazi insignia and at least two other Trump-sponsored pieces of Facebook content in the past few months, including a Trump ad that tried to mislead people into filling out a fake census form and a post for copyright infringement. But the company is not reversing course on the president’s “looting ... shooting” post, and while it says it’s open to putting labels on political misinformation, it hasn’t yet done so with Trump.
There’s two-way political pressure on Facebook
Historically, Facebook and other social media companies have been cautious not to overly moderate content in the interest of appearing to protect free expression online. At the same time, President Trump and other Republican politicians have accused social media companies of having “anti-conservative” bias. 
Trump has issued an executive order attempting to overturn Section 230, a landmark internet law that shields social media companies like Facebook from being sued over what people post on the platform. The rationale for overturning Section 230, according to Trump, is that Facebook is supposedly putting its thumb on the scales against Republican content — which is a largely unproven and, many argue, bad-faith claim.
That pressure puts Facebook in a bind. If it moderates popular conservative figures too much, even if those users post extremist or hateful content, that fuels Trump and other Republicans to argue that they’re being unfairly censored.
On the other hand, if Facebook doesn’t do a good enough job moderating white supremacist and other hateful content, Democrats, civil rights leaders, and major advertisers could continue to accuse the company of turning a blind eye to hate.
“I’m not going to pretend that we’re going to get rid of everything that people, you know, react negatively to,” said Facebook’s Clegg on CNN on Tuesday. “Politically, there are folks on the right who think we take down too much content, folks on the left who think we don’t take down enough.”
While all the major platforms have long had policies on hate speech, it has often taken major national events like mass shootings to pressure the companies to put more force around these issues. So we’ll see whether Facebook makes a meaningful change in how the company moderates content or whether it waits for the controversy to blow over, as it has in the past. 
Ultimately, it looks as though Mark Zuckerberg will be the judge when it comes to drawing the line on Facebook restricting hate speech versus protecting free speech.




  
  
  
      




  When Netflix launched in 1997, it was a tiny movies-by-mail operation, run out of a storefront in a Silicon Valley stripmall. It was going up against Blockbuster, a $6 billion behemoth that owned the movie rental business. 
Now Blockbuster is gone, and Netflix is a $20 billion business, one so dominant that giant companies like Disney and AT&T are remaking themselves to chase after it.
Even if you don’t know this story, you know this story: Scrappy digital upstart comes out of nowhere, topples the incumbent, and becomes unstoppable.
Except ... this one almost didn’t happen. Blockbuster, it turns out, ended up doing a very good job of fighting back against Netflix and might well have won, but it made some fundamental mistakes that ended up dooming its future.
If you’re old enough to miss Blockbuster night — the predecessor to Netflix and chill — you can blame Netflix. But you also need to blame Blockbuster.
That’s the story we tell in the newest chapter of Land of the Giants: The Netflix Effect — our new seven-part podcast about Netflix and the impact it has had on Hollywood and the world. This one is part history lesson, part nostalgia trip, and, in part, an acknowledgement that luck plays an enormous part in any company’s success.
We don’t want to spoil everything for you — we’d very much like you to listen to the episode below, or on Apple Podcasts, Google Podcasts, Spotify, Stitcher, or wherever you like to listen to podcasts. But we can point out that Netflix itself thought it didn’t have a chance of unseating Blockbuster. 
That’s why Netflix co-founders Reed Hastings and Marc Randolph went to Blockbuster’s headquarters in 2000 and offered to sell their three-year-old company to their rival for $50 million. Blockbuster ended up trying to beat Netflix instead of buying it. And today Netflix is worth about $200 billion.
It’s good to be lucky.


Subscribe to Land of The Giants on Apple Podcasts, Google Podcasts, Spotify, Stitcher, or wherever you listen to podcasts.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  



Caroline McGinnes might be one of the few people in the world who knows what it’s like to lend someone a face. In a new HBO documentary, her eyes, nose, and mouth help cloak the identities of LGBTQ people in Chechnya, the predominantly Muslim republic in Russia. Essentially, McGinnes volunteered to become a deepfake, in a way few have seen before. 
In Chechnya, LGBTQ people have faced significant persecution, including unlawful detentions, torture, and other forms of abuse. Because survivors can rarely reveal their own identities safely, the team behind the film Welcome to Chechnya turned to the same sort of technology typically seen in deepfake videos. They’re using artificial intelligence to overlay faces of volunteers on top of those of survivors. This application of deepfake-like technology can replace more traditional ways of keeping sources anonymous, like having them sit in a shadow or blurring their faces. The tech also helps better display the emotions of the survivors. 
“Deepfake” has become a shorthand term for a variety of technological techniques, but it’s generally understood as artificial intelligence used to alter video and audio, making it look as though someone is saying or doing something they actually haven’t. The term comes from the name of a Reddit user who deployed machine learning to swap celebrities’ faces into porn videos without their consent. But a broader industry has begun to promote similar forms of AI-assisted media manipulation — sometimes called synthetic media — that aren’t necessarily as nefarious.
Like that deepfake video of Barack Obama (or Mark Zuckerberg or Kim Kardashian), the faces might not look quite right as deepfake videos tend to live in the uncanny valley. Welcome to Chechnya warns viewers that the technology is featured, and the “face doubles” can at times appear blurry, almost like watercolor. For the people whose faces appear in the film, the experience can be “pretty surreal,” according to McGinnes.
“They map out all the spacing on your face,” she told Recode, “and they match everything, eyes, your jawline, everything.” 

Welcome to Chechnya, which debuts on June 30 on HBO and HBO Max, represents a rare positive example of deepfakes. With the help of deepfake technology, the film can shine light on human rights abuses while minimizing the risk for victims involved in the production. 
Right now, deepfake technology is better known for harming, rather than helping, people. Women are by far more likely to be hurt by deepfake technology. One recent report from the research group Deeptrace found that almost all deepfakes found online are in nonconsensual porn videos. Another major fear is that deepfakes can be used to impersonate political figures and to push disinformation campaigns, exacerbating the already rampant problem of fake news on the internet. 
Still, the promise of deepfake-like technology to anonymize people may grow more popular, experts told Recode, complicating the debate over the ethics and the regulation of this controversial application of artificial intelligence.
Deepfakes can provide a cloak of anonymity 
The man behind Welcome to Chechnya’s technology is visual effects expert Ryan Laney, who says the technology used in the film essentially moves faces like marionette puppets. Put simply, the facial movements of those featured in the documentary guide how the faces of the “doubles” move, with the help of deep learning tech. 
“The eyebrows and eye shapes become something like brushstrokes,” Laney explained. “So we took the content of subjects in the film and applied the style of the activists.” (The film’s team refers to people who volunteered their faces as “activists.”)
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        Volunteers had their faces filmed from a variety of angles, and their faces were then mapped onto the faces of people who appeared in Welcome to Chechnya.
      
      
        HBO
      
    
  


Ultimately, he says the idea was to create “a digital prosthetic where 100 percent of the motion, the emotion, and the essence of what the subject is doing is there.” Essentially, the technique would subtly change a person’s eye shape, but not the fact that they were blinking. 
One other challenge was avoiding the uncanny valley, which is a term more often used to discuss how realistic robots should look. To address that question, the film brought in Thalia Wheatley, a social psychologist and neuroscientist at Dartmouth, and graduate student Chris Welker, to test different approaches to the face cloaks and see how pilot audiences responded. For instance, they tried out a cartoon-like adaption of the survivors’ faces that Wheatley likened to the animations from the movie Spiderman: Into the Spiderverse. Those performed the worst, the researchers said.

Another version involved masking the face but keeping the original peoples’ eyes, which the researchers thought could help people connect with the subject. It didn’t really work, either. 
“We think the answer is that you put one person’s eyes in another person’s face and the brain just can’t resolve the incongruity and it just feels unsettling and it kicks people out of the experience,” Wheatley said. “But we don’t know for sure.” 
The idea of using deepfakes to anonymize people is growing more popular
Laney says he’s now working to democratize this new deepfake technology for good. Through a new company, Teus Media, he wants to turn his artificial intelligence into a journalistic “digital veil” for cloaking witnesses in danger, and he says he’s already received interest. But Laney’s not the only one pushing that approach. 
Some startups, such as D-ID and Alethea AI, want people to use deepfake-like avatars to digitally cloak themselves. Researchers at the Norwegian University of Science and Technology and the University of Albany have also worked on similar technology. 

The director of the University of Colorado Denver’s National Center on Media Forensics, Catalin Grigoras, emphasizes that the ethical questions surrounding synthetic media are raised when they appear to take on aspects of reality. No one has an issue with fake faces generated in Hollywood films, he says, but issues emerge, for instance, when they’re used to create false news. As for Welcome to Chechnya, the application of deepfake technology is within reason.
“It’s just a new movie that has this kind of visual effects,” Grigoras said. “They are quite good, but still it is possible to detect them.”
Sam Gregory, the program director of Witness, a human rights nonprofit that focuses on video and technology, says that activists he’s spoken to find it one of the few compelling applications of the technology. Gregory points to women who have used virtual masks on Snapchat to share their experience of sexual assaults through video without revealing their identities. 
“Over the last couple of years, it seemed like one of the most positive potential use deepfakes is to retain human features and the ability to convey emotion and preserve the essential humanity of people who faced horrible abuses,” Gregory said. 
Still, Gregory cautions that the deepfakes used for anonymity are not without ethical questions. For instance, he wonders to what extent a face double should match the identity and characteristics — such as their race, gender, and age — of the person whose identity they’re obscuring. Gregory adds that while this technology might help activists, it could also be used to misrepresent and target them. 
Expect more arguments for legitimate synthetic media 
When asked about the questions surrounding deepfakes, Welcome to Chechnya’s Laney says that his technology doesn’t technically count because “deepfakes as a practice are inherently nonconsensual.” To him, the artificial intelligence used in the film required both the agreement of those filmed to be anonymized and the consent of the activists who volunteered their faces. Laney also emphasized that no one is trying to trick the audience. They know the technology is in place and that it’s being used to communicate the extent to which these people are in danger. 
That echoes what a company called Synthesia has said. The startup, which sells similar synthetic media technology, has committed to not offering its deepfake-like technology for public use and has promised that it will “never re-enact someone without their explicit consent,” including “politicians or celebrities for satirical purposes.” 
Currently, there is no federal law explicitly regulating the production of deepfakes in the United States, though some states have expressed interest in regulating the technology and some say existing laws may already apply. Meanwhile, social media companies like Facebook and Twitter have also attempted to create rules for moderating the use of the technology on their platforms. 
But deepfakes are part of a developing industry. As the technology becomes more prominent, we should expect more people to argue for legitimate use cases — or, at the very least, applications that are not as terrifying as the deepfakes we’re more familiar with. That will inevitably complicate how we choose to regulate them.  
Open Sourced is made possible by Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.

  
  
  
      






    
  
    
    
      
        
  


  






      
    
    
  
  




Google is making it easier to delete the data it collects about you — though you might still have to do a little work to enable the feature. 
The company announced on Wednesday that auto-delete will be the default setting for user account activity settings. That said, this “default” setting only applies to new accounts or existing accounts that now turn on data retention after having it disabled. And the default auto-delete time still gives Google as much as three years of your data, as opposed to manual auto-delete settings that keep as little as three months’ worth. 
Google also announced that its account privacy and security settings will soon be accessible through its search page. You’ll also be able to switch over to Chrome’s Incognito mode in its apps more easily — simply press down on your profile photo for a second or two. Incognito mode lets you browse the internet “privately,” which means Google Chrome won’t save your history or cookies on your computer. It does not, however, mean that the websites you visit or the server you use can’t see what you’re doing. 
The Google announcement comes just a couple days after rival Apple announced some new privacy features for its software. More on that in a second.
If you have a Google account and use Google products like Gmail, YouTube, or Chrome, you’re probably logged in all the time. In this case, your activity while using those apps and services can be tracked by Google, which will then use that data to target ads to you, among other things. Over the years, Google has introduced privacy controls over the data you send the company and has made efforts to make those features more obvious to users. 
You can find most of these privacy controls in your account settings by clicking on “Manage your data & personalization.” From there, you can click on “Manage your activity controls.” This is the section where you can save your web and app activity, location history, and YouTube history if you want Google to use that data to give you what it calls a “more personalized experience.” Or you can just ask Google not to save anything and have an impersonal, but more private, experience.
If you decide that you do, in fact, want the personalized experience, you can still manually delete that data whenever you want or set it to auto-delete after a certain amount of time. With the newly announced changes, Google is trying to make it easier to enjoy the best of both worlds, both private and personalized, by making auto-delete the default setting for web and app activity, location history, and YouTube history. 
And now, the other caveats. If you have an existing account that has these things turned on — and, except for location history, they are on by default — then you’ll still have to turn on auto-delete yourself. This default setting only applies to new accounts or existing accounts that turn data collection back on. Which means millions of users won’t have this feature enabled by default after all. 
  
  
    
    
      
        

<img alt="How to set your Google data to auto-delete." src="https://cdn.vox-cdn.com/uploads/chorus_asset/file/20051063/Google_Safety_AutoDelete_GIF_24fps_JR_007.gif">

      
    
    
  
  
    
      
        How to set your Google data to auto-delete.
      
      
        Google
      
    
  


It appears that Google is also making more of an effort to notify existing users that they have the option to turn on auto-delete. The Google search page, for instance, now has a little notice and link to the setting beneath the main search field. Also, that default auto-delete time still gives Google a big chunk of your history: 18 months for web and app activity and location history, and 36 months — three years! — for YouTube histories.
The timing of this announcement is interesting, given that Apple announced two days ago that, in the upcoming releases of iOS 14 and macOS Big Sur, apps will be required to get user permission to track them. Apple’s operating system updates will also require app developers to post a clear notice telling them what is being tracked. Apple’s move toward greater transparency and control could represent a huge boost for user privacy, one that Google does not yet offer in its Google Play store. 
The data Google collects about its users is a big part of its business model. Google and its parent company, Alphabet, pull in billions of dollars in revenue from ads, which are worth more when they’re targeted to the people most likely to buy the product they sell. So while Google has made some improvements in user privacy and control, it’s had a tough time convincing the general public that it truly cares about keeping their data private. And in this regard, Google has lagged behind some of its peers, like Apple, whose business model relies far more on goods and services than data and ads. 
Given how relatively few accounts will have this default auto-delete feature and the large amount of data that is retained even with it, it’s hard to say how much of a difference Google’s updates will really make in user privacy. But it does show that the company is trying to improve it — or at least make us think it is.
Open Sourced is made possible by the Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      




    
  
    
    
      
        
  


  






      
    
    
  
  




Google is making it easier to delete the data it collects about you — though you might still have to do a little work to enable the feature. 
The company announced on Wednesday that auto-delete will be the default setting for user account activity settings. That said, this “default” setting only applies to new accounts or existing accounts that now turn on data retention after having it disabled. And the default auto-delete time still gives Google as much as three years of your data, as opposed to manual auto-delete settings that keep as little as three months’ worth. 
Google also announced that its account privacy and security settings will soon be accessible through its search page. You’ll also be able to switch over to Chrome’s Incognito mode in its apps more easily — simply press down on your profile photo for a second or two. Incognito mode lets you browse the internet “privately,” which means Google Chrome won’t save your history or cookies on your computer. It does not, however, mean that the websites you visit or the server you use can’t see what you’re doing. 
The Google announcement comes just a couple days after rival Apple announced some new privacy features for its software. More on that in a second.
If you have a Google account and use Google products like Gmail, YouTube, or Chrome, you’re probably logged in all the time. In this case, your activity while using those apps and services can be tracked by Google, which will then use that data to target ads to you, among other things. Over the years, Google has introduced privacy controls over the data you send the company and has made efforts to make those features more obvious to users. 
You can find most of these privacy controls in your account settings by clicking on “Manage your data & personalization.” From there, you can click on “Manage your activity controls.” This is the section where you can save your web and app activity, location history, and YouTube history if you want Google to use that data to give you what it calls a “more personalized experience.” Or you can just ask Google not to save anything and have an impersonal, but more private, experience.
If you decide that you do, in fact, want the personalized experience, you can still manually delete that data whenever you want or set it to auto-delete after a certain amount of time. With the newly announced changes, Google is trying to make it easier to enjoy the best of both worlds, both private and personalized, by making auto-delete the default setting for web and app activity, location history, and YouTube history. 
And now, the other caveats. If you have an existing account that has these things turned on — and, except for location history, they are on by default — then you’ll still have to turn on auto-delete yourself. This default setting only applies to new accounts or existing accounts that turn data collection back on. Which means millions of users won’t have this feature enabled by default after all. 
  
  
    
    
      
        

<img alt="How to set your Google data to auto-delete." src="https://cdn.vox-cdn.com/uploads/chorus_asset/file/20051063/Google_Safety_AutoDelete_GIF_24fps_JR_007.gif">

      
    
    
  
  
    
      
        How to set your Google data to auto-delete.
      
      
        Google
      
    
  


It appears that Google is also making more of an effort to notify existing users that they have the option to turn on auto-delete. The Google search page, for instance, now has a little notice and link to the setting beneath the main search field. Also, that default auto-delete time still gives Google a big chunk of your history: 18 months for web and app activity and location history, and 36 months — three years! — for YouTube histories.
The timing of this announcement is interesting, given that Apple announced two days ago that, in the upcoming releases of iOS 14 and macOS Big Sur, apps will be required to get user permission to track them. Apple’s operating system updates will also require app developers to post a clear notice telling them what is being tracked. Apple’s move toward greater transparency and control could represent a huge boost for user privacy, one that Google does not yet offer in its Google Play store. 
The data Google collects about its users is a big part of its business model. Google and its parent company, Alphabet, pull in billions of dollars in revenue from ads, which are worth more when they’re targeted to the people most likely to buy the product they sell. So while Google has made some improvements in user privacy and control, it’s had a tough time convincing the general public that it truly cares about keeping their data private. And in this regard, Google has lagged behind some of its peers, like Apple, whose business model relies far more on goods and services than data and ads. 
Given how relatively few accounts will have this default auto-delete feature and the large amount of data that is retained even with it, it’s hard to say how much of a difference Google’s updates will really make in user privacy. But it does show that the company is trying to improve it — or at least make us think it is.
Open Sourced is made possible by the Omidyar Network. All Open Sourced content is editorially independent and produced by our journalists.
  
  
  
      





  It’s basically a requirement for any company — and especially for tech companies in the last few decades — to boast about having a unique culture and corporate values.
That doesn’t mean those things have much to do with the way the company operates. 
Netflix, for better or worse, is different: The leaders of the streaming company take their culture very, very seriously. They credit it with the success they’ve had upending the media world and forcing giants like Disney, Apple, and AT&T to chase after Netflix. They expect its 7,000 employees to take it seriously, too.
And so for the first episode of Land of the Giants: The Netflix Effect — our new seven-part podcast about the company and the impact it has made on Hollywood and the world — we wanted to dive into Netflix’s culture.
The company was happy to talk about it. Netflix has long been known for its “culture deck” — a slideshow about its HR philosophy it made public years ago and that has been broadly influential in the startup world. 
And CEO Reed Hastings has a book — No Rules Rules — coming out this fall about Netflix’s culture. He thinks you may want to run your company the way he does.
When we told people outside of Netflix that we were making an episode about the company’s culture, we often got blank looks. But when we told current and former employees about our plan, they got excited. Netflix can be a weird place to work, and most people who don’t work there don’t get how weird it is.
For instance, Netflix uses its own cult-like language, like “keeper test” and “sunshining.” It also pays employees top-of-the-market salaries and gives them perks like the absence of an expense policy — employees are just supposed to use common sense. And it encourages workers to meet with recruiters from other companies so they can figure out what the top pay is for their position.
The company also tells employees that they should think of themselves as members of a pro sports team, not a family. Which means they should expect to be replaced by better performers for their spot if Netflix can find them. And it often goes out of its way to tell employees when a coworker has been dismissed and why.
The downside of that kind of intensity and pressure can be employees who feel overwhelmed and insecure. Wall Street Journal reporters Shalini Ramachandran and Joe Flint did an excellent job in 2018 of documenting the difficulty some Netflix employees had with the company’s culture, which the Journal characterized as “ruthless, demoralizing and transparent to the point of dysfunctional.”
The upside is a company where employees feel they have meaningful autonomy about the way they work, and the power to get things done. 
One of Netflix’s overarching tenets, for instance, is “freedom and responsibility” — the ability to make decisions on your own, with accountability. Chief Content Officer Ted Sarandos said that’s what enabled him to lead a pivot into original content in 2011, by spending $100 million for two seasons of House of Cards, sight unseen and without permission from his boss. 
“I told Reed about the deal after we did it,” Sarandos told us.
Another tenet — “farming for dissent” — came out of one of the company’s biggest failures.  You might remember it as a punchline: Qwikster.
The short version: In 2011, Hastings wanted to move his company from its core DVD-by-mail service to online streaming, which was growing quickly but was still a smaller part of his business. So he tried splitting Netflix into a DVD business and a streaming business named Qwikster. Which meant that if his customers wanted the same services they were already getting before, they would have to subscribe to both and end up paying 60 percent more.
Netflix veterans still wince about the experience: The company was skewered on social media and by SNL. Its stock dropped 70 percent, and more than 700,000 people canceled their subscriptions.

Eventually, Hastings admitted that Qwikster’s name, the price hikes, and the way the company talked about it all had been a huge blunder. He rolled back the changes.
But in Hastings’s narrative, the failure was useful for Netflix’s culture. He thinks that many of his top employees could have told him he was wrong but were too afraid or at least too in awe of their CEO’s former successes to say anything.
“Everyone knows the tale of the self-absorbed, arrogant CEO who doesn’t listen. And there’s an element of that, because we have been so successful at so many things before that,” Hastings told us earlier this year at Netflix’s offices in Los Angeles. “But the more subtle one is that I had been so successful before that most of the executives thought ... ‘But Reed has been right on so many things. I’ll bet he’s right on this one. And I’m just not seeing it.’”
After the debacle, Hastings instituted “farming for dissent,” a formal practice where employees are supposed to run their big ideas by colleagues and have them tell you candidly — on a Google Doc that’s open for everyone to see —  what’s wrong with it. It’s considered integral to the company that your coworkers tell you what they really think of your idea, even if — perhaps especially if — you’re their boss. 
So if that’s the kind of thing you’d like to hear more about, you’re in luck: We’ve got a whole episode you can listen to right now.
And we’ll have six more, covering everything from Netflix’s battle with Blockbuster Video to the way it uses its famed recommendation algorithm to the way it has remade Hollywood, coming up. Please tell us what you think: We’re on Twitter at @ranimolla and @pkafka.



Subscribe to Land of The Giants on Apple Podcasts, Google Podcasts, Spotify, Stitcher, or wherever you listen to podcasts. 
  
  
  
      




  More than 1,600 Google employees have signed an internal petition as of Monday afternoon that calls for their employer to stop selling its software to police. 
The employees’ demands follow a wave of protests calling for racial justice in the US after a spate of killings targeting Black Americans, including the alleged murder of Ahmaud Arbery, an unarmed Black man out for a jog in Glynn County, Georgia, and the police killings of George Floyd in Minneapolis and Breonna Taylor in Louisville, Kentucky, who were both unarmed. 
“We’re disappointed to know that Google is still selling to police forces, and advertises its connection with police forces as somehow progressive, and seeks more expansive sales rather than severing ties with police and joining the millions who want to defang and defund these institutions,” reads the petition, which was started by a group of Google employees called Googlers Against Racism last week and is addressed to Sundar Pichai, the CEO of Google parent company Alphabet. “Why help the institutions responsible for the knee on George Floyd’s neck to be more effective organizationally?”
Google did not respond to a request for comment in time for publication. The company currently employs more than 100,000 employees globally, so this petition’s signees represent a relatively small percentage of its workforce. But it is still one of the most significant showings of internal dissent at the company in the past few months.
It’s not clear how extensively or closely Google works with US police departments. Google currently sells cloud-based software, such as the business version of its Gmail product, through a third-party vendor to at least one police department, the Clarkstown Police Department in Rockland County, New York. According to a customer testimonial page, Google touts its software as being a “catalyst for a culture change” at Clarkstown County Police Department, which was sued in 2017 by Black Lives Matter activists who said the department conducted illegal surveillance on them. 
Google also invests in police AI technology that includes drone surveillance through its venture capital arm, Gradient Ventures. 
The Google employees’ petition is the latest example of workers at major tech companies challenging their employers to do more than simply making donations and issuing statements in support of Black Lives Matter. These employees want their companies to significantly change their business practices to minimize harm to the Black community, both inside their companies and beyond. In recent weeks, some Amazon employees renewed their calls for the company to stop selling its facial recognition technology to police, which it announced it would pause for the next year. And employees at Facebook have called for the company to moderate political speech that could incite racial violence or mislead voters.
“We want to be proud of the company we work for. We want the company we build to speak to our values and how we want to show up in the world,” reads the Google employees’ letter, which calls for the company to “take real steps to help dismantle racism.”
Civil rights advocates have warned that police disproportionately use surveillance technology, such as AI-based facial recognition or drone surveillance, against Black communities — and that these practices could have a chilling effect on free speech if people feel they’re being unjustly surveilled in their everyday lives and when they engage in political activism. 
In the past, Google employees have uncovered details about some of the company’s controversial government contracts and pressured the company to end them, such as Project Maven, the company’s contract with the Pentagon, and its secretive efforts to build a censored search product for China, known as Project Dragonfly. Google did not renew its contract with the Pentagon in June of 2018 and reportedly halted its work on Dragonfly in December of 2018, although a March article from the Intercept reported that the company may be resuming some of that work.
In August 2019, hundreds of employees signed a protest document asking the company not to bid on a project with the US immigration agency Customs and Border Patrol (CBP).  
Monday’s petition about working with police departments is the first notable activist effort that’s happened at Google since November 2019, when Google fired four employee activists, including some who spoke out against the company’s work with CBP due to humanitarian concerns about the agency’s immigrant detention facilities and family separation. Google said it fired these workers for violating its data security policies.
In light of the recent protests against police brutality and racism that have spread across the nation after George Floyd’s death, major tech companies are facing a reckoning about their work with police, agitated by their own employees. Today’s letter at Google is yet another sign of that movement’s advancement.


  
  
  
      




  More than 1,600 Google employees have signed an internal petition as of Monday afternoon that calls for their employer to stop selling its software to police. 
The employees’ demands follow a wave of protests calling for racial justice in the US after a spate of killings targeting Black Americans, including the alleged murder of Ahmaud Arbery, an unarmed Black man out for a jog in Glynn County, Georgia, and the police killings of George Floyd in Minneapolis and Breonna Taylor in Louisville, Kentucky, who were both unarmed. 
“We’re disappointed to know that Google is still selling to police forces, and advertises its connection with police forces as somehow progressive, and seeks more expansive sales rather than severing ties with police and joining the millions who want to defang and defund these institutions,” reads the petition, which was started by a group of Google employees called Googlers Against Racism last week and is addressed to Sundar Pichai, the CEO of Google parent company Alphabet. “Why help the institutions responsible for the knee on George Floyd’s neck to be more effective organizationally?”
Google did not respond to a request for comment in time for publication. The company currently employs more than 100,000 employees globally, so this petition’s signees represent a relatively small percentage of its workforce. But it is still one of the most significant showings of internal dissent at the company in the past few months.
It’s not clear how extensively or closely Google works with US police departments. Google currently sells cloud-based software, such as the business version of its Gmail product, through a third-party vendor to at least one police department, the Clarkstown Police Department in Rockland County, New York. According to a customer testimonial page, Google touts its software as being a “catalyst for a culture change” at Clarkstown County Police Department, which was sued in 2017 by Black Lives Matter activists who said the department conducted illegal surveillance on them. 
Google also invests in police AI technology that includes drone surveillance through its venture capital arm, Gradient Ventures. 
The Google employees’ petition is the latest example of workers at major tech companies challenging their employers to do more than simply making donations and issuing statements in support of Black Lives Matter. These employees want their companies to significantly change their business practices to minimize harm to the Black community, both inside their companies and beyond. In recent weeks, some Amazon employees renewed their calls for the company to stop selling its facial recognition technology to police, which it announced it would pause for the next year. And employees at Facebook have called for the company to moderate political speech that could incite racial violence or mislead voters.
“We want to be proud of the company we work for. We want the company we build to speak to our values and how we want to show up in the world,” reads the Google employees’ letter, which calls for the company to “take real steps to help dismantle racism.”
Civil rights advocates have warned that police disproportionately use surveillance technology, such as AI-based facial recognition or drone surveillance, against Black communities — and that these practices could have a chilling effect on free speech if people feel they’re being unjustly surveilled in their everyday lives and when they engage in political activism. 
In the past, Google employees have uncovered details about some of the company’s controversial government contracts and pressured the company to end them, such as Project Maven, the company’s contract with the Pentagon, and its secretive efforts to build a censored search product for China, known as Project Dragonfly. Google did not renew its contract with the Pentagon in June of 2018 and reportedly halted its work on Dragonfly in December of 2018, although a March article from the Intercept reported that the company may be resuming some of that work.
In August 2019, hundreds of employees signed a protest document asking the company not to bid on a project with the US immigration agency Customs and Border Patrol (CBP).  
Monday’s petition about working with police departments is the first notable activist effort that’s happened at Google since November 2019, when Google fired four employee activists, including some who spoke out against the company’s work with CBP due to humanitarian concerns about the agency’s immigrant detention facilities and family separation. Google said it fired these workers for violating its data security policies.
In light of the recent protests against police brutality and racism that have spread across the nation after George Floyd’s death, major tech companies are facing a reckoning about their work with police, agitated by their own employees. Today’s letter at Google is yet another sign of that movement’s advancement.


  
  
  
      






  Section 230, the law that is often credited as the reason why the internet as we know it exists, could be facing its greatest threat yet. A seemingly coordinated attack on the law is unfolding this week from the Trump administration and Republicans in Congress. It follows complaints that platforms such as Facebook, Twitter, and YouTube unfairly censor conservative speech. Though some are framing the efforts as a way to promote free speech, others say the result will be exactly the opposite.
Following President Trump’s executive order aimed at social media companies he thinks are censoring right-wing voices, the most direct actions taken against Section 230 arrived this week in the form of a new bill from Sen. Josh Hawley and a set of recommendations from Attorney General Bill Barr. 
Hawley, a 40-year-old Republican from Missouri who has made no secret of his intentions regarding Section 230, is proposing a bill that would require large platforms to enforce their rules equally to stop a perceived targeting of conservatives and conservative commentary. Hawley is also rumored to be preparing another Section 230-related bill to add to his growing collection. 
Meanwhile, Barr’s Department of Justice said it is calling for new legislation that, in certain cases, would remove the civil liability protections offered by Section 230. If platforms like Facebook, Google, and Twitter somehow encouraged content that violates federal law, these platforms would be treated as “bad samaritans” and would lose the immunity offered by Section 230. Like Hawley’s bill, the DOJ’s proposed rules would also force platforms to clearly define and equally enforce content rules.
Civil rights advocates say they’re concerned that some of these proposed measures may end up becoming law, leading to all sorts of unintended consequences and stifling speech — which will ultimately punish internet users far more than the websites.
“I do think there is a very serious risk to Section 230 right now,” Kathleen Ruane, senior legislative counsel at the American Civil Liberties Union (ACLU), told Recode. “And they all concern me, not for the platforms, but for users and online free expression.”
Section 230, briefly explained
Section 230 is part of the Communications Decency Act of 1996. It says internet platforms that host third-party content are not civilly liable for that content. There are a few exceptions, such as intellectual property or content related to sex trafficking, but otherwise the law allows platforms to be as hands-off as they want to be with user-generated content.
Here’s an example: If a Twitter user were to tweet something defamatory, the user could be sued for libel, but Twitter itself could not. This law has allowed websites and services that rely on user-generated content to exist and grow. If these sites could be held responsible for the actions of their users, they would either have to strictly moderate everything those users produce — which is impossible at scale — or not host any third-party content at all. Either way, the demise of Section 230 could be the end of sites like Facebook, Twitter, Reddit, YouTube, Yelp, forums, message boards, and basically any platform that’s based on user-generated content. 
The law also gives those services that immunity even if they moderate certain content. This is why, for instance, Twitter can take down tweets that it deems in violation of its terms of service. Sen. Ron Wyden, who was one of the architects of Section 230, has likened these provisions to a sword and shield for platforms. 
But as some of these platforms have increased in size, scope, and power, there has been increasing support on both sides of the aisle to chip away at the law that allowed them to flourish free of much accountability. 
Democrats have supported laws that crack down on websites that facilitate sexual abuse. The Allow States and Victims to Fight Online Sex Trafficking Act (FOSTA) and the Stop Enabling Sex Traffickers Act (SESTA) made platforms legally responsible for third-party content related to sex trafficking. The two bills, known together as FOSTA-SESTA, overwhelmingly passed in the House and Senate, and President Trump signed them into law in 2018. 
More recently, there’s the bipartisan Eliminating Abusive and Rampant Neglect of Internet Technologies Act (EARN IT), which would require companies to follow a yet-to-be-defined set of “best practices” or else lose Section 230 immunity if third parties post child pornography on their platforms. Civil rights advocates worry what those “best practices” will be and how they might stifle all speech.
Josh Hawley’s crusade for a different internet
Many Republicans see altering Section 230 as a way to force platforms to fit their definition of “politically neutral.” Typically, this translates into restricting a website or service’s ability to moderate content. 
This seems to be the goal of Hawley’s bill, which is called the Limiting Section 230 Immunity to Good Samaritans Act. Cosponsored by Republican Sens. Marco Rubio, Mike Braun, Tom Cotton, and Kelly Loeffler, the bill would force large tech companies — that is, companies that have 30 million American users or 300 million users worldwide, as well as $1.5 billion annual revenue — to act in “good faith” when enforcing their content rules. Acting in “good faith” here means that platforms must clearly define what their rules are and enforce them consistently, rather than, say, targeting certain types of political speech, as some conservatives believe they currently do.
Users who feel that their content is being unfairly removed would also have a new tool for reprisal. Hawley’s bill gives individual users who believe they’re being censored the right to sue companies for at least $5,000 as well as attorney’s fees. You can imagine how many people would be happy to take advantage of that, which would give platforms a big incentive to comply lest they be flooded with millions of lawsuits.
“It is impossible to moderate user-generated content at scale perfectly, or even well, and this bill would weaponize mistakes,” Aaron Mackey, staff attorney for the Electronic Frontier Foundation, told Recode. “There are legitimate concerns about the dominance of a handful of online platforms and their power to limit internet users’ speech. But rather than addressing those concerns, this bill bluntly encourages frivolous litigation and will lead to massive trolling.”
This isn’t Republicans’ only recent attempt at limiting Section 230. In 2019, Hawley introduced the Ending Support for Internet Censorship Act, which would have required the Federal Trade Commission to declare platforms unbiased to get Section 230 protections. The same year, Rep. Louie Gohmert introduced the Biased Algorithm Deterrence Act, which would remove Section 230 protections from companies that moderated content using algorithms. Both were responses to conservative complaints that companies including Facebook, Twitter, and Google were selectively enforcing their content guidelines, de-platforming, shadow banning, or otherwise censoring conservatives while mostly leaving liberals alone. Sen. Ted Cruz has also been a vocal critic of platforms in this regard, erroneously asserting that Section 230 includes some kind of political neutrality requirement even though the law doesn’t say anything to that effect.
Those complaints have gained steam recently. Despite being one of the biggest beneficiaries of the influence and reach these platforms can afford, President Trump had a recent tantrum over Twitter’s decision to fact-check two of his tweets, which contained inaccurate information about mail-in ballots. Soon after, Trump signed his executive order aimed at social media companies, which said platforms that go beyond “good faith” content moderation should not be entitled to Section 230 protections. An executive order is not a law and therefore its impact on an actual law is likely limited, but the right’s intention to go after big tech companies was made very clear. 
The Trump administration says, “The time is ripe”
While recent bills in Congress have been markedly divisive, Barr’s proposed reforms manage to incorporate the issues that both Democrats and Republicans have raised with Section 230. The DOJ called this a “productive middle ground.” Note that the department’s proposals are simply suggestions for the laws Congress should enact that would actually change things, but they, like the executive order, signal how and why the Trump administration hopes to go after or control large platforms.
One of Barr’s recommendations is to withhold immunity from “truly bad actors,” which are defined as sites promoting, soliciting, or facilitating content that violates federal law. Sites must also “maintain the ability to assist government authorities to obtain content (i.e. evidence) in a comprehensible, readable, and usable format.” This would be the end of services that use end-to-end encryption, which Barr has a particular problem with, and which civil liberties advocates believe will be the ultimate effect of the EARN IT act. 
There’s also a section that addresses “open discourse and greater transparency.” Here, Barr recommends something along the lines of Hawley’s bill — that platforms must have clear terms of service for what is and isn’t allowed on their platforms and moderate content accordingly. This includes defining “good faith,” similar to Hawley’s bill, as well as removing the part of the law that says platforms can moderate content that is “otherwise objectionable,” as Barr believes the term is too vague and has given platforms the freedom to remove anything simply by saying it’s objectionable in some way.
Wyden was not impressed by the recommendations to change the law he helped create.
“This jumbled mess of a proposal is yet another cynical attempt by the Trump administration to bully the tech companies into letting the president and his cronies post lies and conspiracies on their sites, and is clearly not intended to become law,” the Oregon senator told Recode. “Congress should stay far away from this disingenuous plan that would gut the ability of tech companies to take down hateful slime, spawn endless frivolous lawsuits, and chill Americans’ free speech online.”
In the background of all of this is a growing public sentiment against powerful tech companies due, in part, to how they help spread fake news and the incredible amounts of personal information about us they collect. That has surely emboldened politicians to act accordingly. Not only do we have multiple bills against Section 230, but there are also ongoing efforts to break up the biggest tech companies through antitrust investigations both in the United States and the European Union.
“The Department of Justice has concluded that the time is ripe to realign the scope of Section 230 with the realities	of the modern internet,” the recommendations say.
This all adds up to a very real possibility that Section 230, at least as we know it, won’t be around for much longer. Hawley’s bill, which has no bipartisan support as of now, might go the way his past bills did — that is to say, nowhere. But the EARN IT Act does have bipartisan support and, like FOSTA-SESTA which did pass, targets child sexual abuse. Few politicians may want to vote against a law that says it’s meant to combat child porn, regardless of any unintended consequences.
The consequences of changing Section 230 will inevitably change the internet and what we’re allowed to do on it. Ruane, from the ACLU, points to the impact of FOSTA-SESTA, which she says “has been a complete and total disaster,” and its unintended consequences as a guide for what we can expect. Faced with the new law, online platforms didn’t seek to target specific content that might relate to or facilitate sex trafficking; they simply took down everything sex or sex work-related to ensure they wouldn’t get in trouble.
“It was only supposed to apply to advertisements for sex trafficking. That is absolutely not what happened,” Ruane said. “All platforms adopted much broader content moderation policies that applied to a lot of LGBTQ-related speech, sex education-related speech, and ... sites where [sex workers] built communities where they shared information to maintain safety.”
She added, “It is astonishing to me that that law is being used as an example of what we should do in the future because of all the clear harms that censoring a broad amount of speech has caused.”
As for Wyden, he wrote in a recent op-ed that laws that force platforms to be “politically neutral” may not encourage more speech, as conservatives who favor those laws claim, but rather suppress it. Facebook has taken a similar stance, saying on Wednesday that changing Section 230’s liability protections would “mean less speech of all kinds appearing online.”
Section 230 won’t change tomorrow, if it changes at all. But a series of seemingly coordinated attacks from two of the three branches of government certainly shows some momentum toward the possibility of change. 
On one hand, the internet has profoundly changed since the law was introduced 25 years ago and it’s not unreasonable to believe that the law should change with it. On the other, those changes likely won’t have the impact on the companies they’re targeting that lawmakers and the administration seem to desire. The impact will largely fall on the people who use the platforms those companies run: You.
  
  
  
      




  Another day, another potentially racist dog whistle from our president and his representatives. This time, Facebook — which has been reluctant to take on problematic content from Trump, unlike social media peer Twitter — is taking action and removing it from the platform.
On Wednesday, the Trump campaign placed 88 ads on Facebook — 88 is a number with Nazi connotations — that featured a symbol used by Nazis to denote political prisoners in concentration camps. The Trump campaign denied the reference to any Nazi symbols was intentional and deactivated the ads on Wednesday. (Deactivating the ads meant that they could still be seen on the pages of Trump and others, but Facebook was no longer placing the ads in users’ timelines.) Following tweets and reports about the ads, Facebook removed them on Thursday for violating its policy against “using a banned hate group’s symbol,” the company told Recode.
The symbol in question is an upside-down red triangle, which accompanies text about “dangerous mobs” of “far-left groups” causing mayhem in cities. The ad then asks readers to stand with President Trump against antifa. It ran on pages for Trump, Vice President Pence, and Trump’s official campaign. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        The Trump campaign’s Facebook ad included a symbol associated with Nazism.
      
      
        Trump Make America Great Again Committee
      
    
  


Nazis used different colors of upside-down triangles sewn onto clothing to categorize concentration camp prisoners. The pink triangle, used to denote gay people, is perhaps the best known of these, as it was later reclaimed by the LGTBQ community. Red triangles were used for political prisoners, such as people believed to be communists or social democrats. 
  
  
    
    
      
        
  


  






      
    
    
  
  
    
      
        A list of Nazi prisoner symbols from 1936.
      
      
        United States Holocaust Memorial Museum
      
    
  


The Auschwitz Memorial tweeted its own explanation of the symbol, including an image of it on a camp uniform.


A red triangle that marked 'political prisoners' was the most common category of prisoners registered at the German Nazi #Auschwitz camp. In August 1944, political prisoners constituted 95 percent of camp prisoners'. A letter inside the triangle could mark the nationality. pic.twitter.com/jBuNn0xmL1— Auschwitz Memorial (@AuschwitzMuseum) June 18, 2020



Though the use of red triangles was, as the tweet says, very common, it’s not very well-known. But the use of a red triangle as an antifa symbol, which is what the Trump campaign claimed it was meant to be, is even more obscure. 
Though the campaign said on Twitter that the upside-down red triangle is “widely used” by antifa, it’s not. The image most closely associated with the group is of a red and black flag. Mark Bray, author of Antifa: The Anti-Fascist Handbook, told the New York Times that the triangle was not an antifa symbol, adding that its Nazi origins actually represented a “death threat against leftists.”
When asked for evidence of widespread use of the upside-down red triangle, the Trump campaign pointed to a poster being sold on a website that specializes in merchandise with user-submitted images on it. The campaign also pointed out that there is an upside-down red triangle emoji. 
But that’s not all. Adding to claims that using a Nazi symbol was deliberate is the fact that the campaign ran exactly 88 ads featuring the symbol. The number 88 is a known code for “Heil Hitler.” According to Facebook’s ad library, the campaign placed 30 red triangle ads on the Team Trump page, 30 on Trump’s page, and 28 on Pence’s page. Those add up to 88. It’s certainly possible that the Trump campaign’s decision to go with a very specific number of ads — a number that also happens to have Nazi connections — is a coincidence. Facebook clearly doesn’t think so.
It should be noted that the Trump campaign also placed ads with the same wording but with different symbols attached. These symbols, which include a stop sign, a “slow” sign, and other shapes with warning exclamation points in them, do not appear to have any immediate Nazi reference built in. Similar numbers of those ads were placed on the three pages. 
“Whether aware of the history or meaning, for the Trump campaign to use a symbol — one which is practically identical to that used by the Nazi regime to classify political prisoners in concentration camps — to attack his opponents is offensive and deeply troubling,” Jonathan Greenblatt, the chief executive of the Anti-Defamation League, said in a statement. “It is not difficult for one to criticize their political opponent without using Nazi-era imagery. We implore the Trump campaign to take greater caution and familiarize themselves with the historical context before doing so.”
While Facebook has given Trump and his campaign a long leash in the past, they have run afoul of its ad rules before. In March, the platform removed ads that promoted a “census” (it was a campaign survey) that some users could mistake for the official US census. This was part of Facebook’s big push against census misinformation. Facebook also took down a campaign ad that contained copyrighted music.
Trump and his surrogates have been caught using more obvious dog whistles in the past. Some of these references were so blatant that they don’t really qualify as dog whistles at all. Most famous among them might be the image showing a Star of David next to a picture of Hillary Clinton and money. (Trump claimed the symbol was meant to be a sheriff star, but later replaced it with a circle.) 
Trump has also retweeted accounts associated with Nazism and claims of “white genocide,” including one literally called WhiteGenocideTM. He also tweeted an image of supposed crime statistics that said 81 percent of murders of white people were committed by black people and 97 percent of black murders were committed by black people. These statistics were inaccurate and attributed to a “crime statistics bureau” that doesn’t exist. The image also showed a dark-skinned man holding a gun.
It’s possible that the symbol and the number of ads were a coincidence, given the obscurity of the triangle and the fact that the number of ads was spread across three accounts. But given Trump’s past with racist, anti-Muslim, and anti-Semitic imagery on social media, it’s pretty tough to give his campaign the benefit of the doubt now. Facebook didn’t.

  
  Text
AFTER YEARS of backlash over controversial government work, Google technology will be used to aid the Trump administration’s efforts to fortify the U.S.-Mexico border, according to documents related to a federal contract.

In August, Customs and Border Protection accepted a proposal to use Google Cloud technology to facilitate the use of artificial intelligence deployed by the CBP Innovation Team, known as INVNT. Among other projects, INVNT is working on technologies for a new “virtual” wall along the southern border that combines surveillance towers and drones, blanketing an area with sensors to detect unauthorized entry into the country.

In 2018, Google faced internal turmoil over a contract with the Pentagon to deploy AI-enhanced drone image recognition solutions; the capability sparked employee concern that Google was becoming embroiled in work that could be used for lethal purposes and other human rights concerns. In response to the controversy, Google ended its involvement with the initiative, known as Project Maven, and established a new set of AI principles to govern future government contracts.
NEW YORK STARTUP Dataminr aggressively markets itself as a tool for public safety, giving institutions from local police to the Pentagon the ability to scan the entirety of Twitter using sophisticated machine-learning algorithms. But company insiders say their surveillance efforts were often nothing more than garden-variety racial profiling, powered not primarily by artificial intelligence but by a small army of human analysts conducting endless keyword searches.

In July, The Intercept reported that Dataminr, leveraging its status as an official “Twitter Partner,” surveilled the Black Lives Matter protests that surged across the country in the wake of the police killing of George Floyd. Dataminr’s services were initially designed to help hedge funds turn the first glimmers of breaking news on social media into market-beating trades, enabling something like a supercharged version of professional Twitter dashboard TweetDeck. They have since been adopted by media outlets, the military, police departments, and various other organizations seeking real-time alerts on chaos and strife.
NEW YORK STARTUP Dataminr aggressively markets itself as a tool for public safety, giving institutions from local police to the Pentagon the ability to scan the entirety of Twitter using sophisticated machine-learning algorithms. But company insiders say their surveillance efforts were often nothing more than garden-variety racial profiling, powered not primarily by artificial intelligence but by a small army of human analysts conducting endless keyword searches.

In July, The Intercept reported that Dataminr, leveraging its status as an official “Twitter Partner,” surveilled the Black Lives Matter protests that surged across the country in the wake of the police killing of George Floyd. Dataminr’s services were initially designed to help hedge funds turn the first glimmers of breaking news on social media into market-beating trades, enabling something like a supercharged version of professional Twitter dashboard TweetDeck. They have since been adopted by media outlets, the military, police departments, and various other organizations seeking real-time alerts on chaos and strife.
FACEBOOK CONTRACTORS TASKED with sifting through some of the most heinous and traumatizing content on the internet faced a new hurdle this week when they were told to return to company offices to do their work in person as a pandemic runs rampant around them. Audio obtained by The Intercept suggests that their employer, Accenture, is downplaying the risk of indoor exposure to Covid-19.

When the United States began a patchwork national lockdown in March, Facebook contractors, paid a relatively low hourly wage with few of the generous perks afforded to the company’s full-time staffers, began to feel even more acutely dispensable to the $750 billion company. Beginning this week, as first reported by The Verge, these contractors must now resume working in the same facilities that Facebook’s full-time can safely avoid, having been told that they’ll be permitted to work from home through July 2021. “Based on guidance from health and government experts, as well as decisions drawn from our internal discussions about these matters, we are allowing employees to continue voluntarily working from home until July 2021,” a Facebook spokesperson explained to Business Insider.

Facebook has said that the contractors in question, who must wade through so-called priority zero content encompassing the worst of child sexual abuse and graphic violence, can’t safely do this work from home. Three Facebook moderators employed through Accenture who spoke to The Intercept on the condition of anonymity, because they are not permitted to speak with the press, expressed a profound worry that the company, and their ultimate bosses at Facebook HQ, are once again ignoring their safety in the name of keeping the social network running smoothly.
BY 2013, the Obama administration had concluded that it could not charge WikiLeaks or Julian Assange with crimes related to publishing classified documents — documents that showed, among other things, evidence of U.S. war crimes in Iraq and Afghanistan — without criminalizing investigative journalism itself. President Barack Obama’s Justice Department called this the “New York Times problem,” because if WikiLeaks and Assange were criminals for publishing classified information, the New York Times would be just as guilty.

Five years later, in 2018, the Trump administration indicted Assange anyway. But, rather than charging him with espionage for publishing classified information, they charged him with conspiracy to commit a computer crime, later adding 17 counts of espionage in a superseding May 2019 indictment and expanding on those charges in another superseding indictment in June 2020.
BY 2013, the Obama administration had concluded that it could not charge WikiLeaks or Julian Assange with crimes related to publishing classified documents — documents that showed, among other things, evidence of U.S. war crimes in Iraq and Afghanistan — without criminalizing investigative journalism itself. President Barack Obama’s Justice Department called this the “New York Times problem,” because if WikiLeaks and Assange were criminals for publishing classified information, the New York Times would be just as guilty.

Five years later, in 2018, the Trump administration indicted Assange anyway. But, rather than charging him with espionage for publishing classified information, they charged him with conspiracy to commit a computer crime, later adding 17 counts of espionage in a superseding May 2019 indictment and expanding on those charges in another superseding indictment in June 2020.
WHEN 17-YEAR-OLD Kyle Rittenhouse killed two Black Lives Matter protesters (and wounded a third) in late August in Kenosha, Wisconsin, he instantly became a hero among white nationalist circles, in which the Second Amendment is sacrosanct.

On Wednesday, Rittenhouse and the members of the armed militias that supported him, including the Kenosha Guard and Boogaloo Bois, were named in a federal lawsuit brought under the post-Civil War Reconstruction amendments that aimed to establish Black equality.

The suit also names what it alleges was the militia groups’s most prominent enabler: Facebook. All of the parties, the complaint contends, helped deprive Kenosha Black Lives Matter protesters of their First Amendment right to assemble, thus violating the Black Lives Matter activists’ 14th Amendment right to equal protection.
IN AUGUST, 40 federal agents arrived in Memphis. Some were already on the ground by the time U.S. Attorney Michael Dunavant announced the onset of Operation Legend and the city became, along with St. Louis, the seventh to be targeted by the Justice Department’s heavy-handed initiative to reduce violent crime. Many of the agents are on temporary assignment, working in collaboration with police; nearly half will relocate by November. But they will leave behind a city flush with grant money for local police — and heightened surveillance capabilities.

In Memphis, organizers have long battled police surveillance. The fight came to a head in 2017, when a lawsuit against the city of Memphis revealed years of close surveillance of Black Lives Matter activists and union organizers. “We knew we were being watched and monitored and surveilled,” said Hunter Demster, an activist who was tracked on social media by MPD. The suit was successful, and in 2018, a federal judge ordered an independent monitor to oversee policing in the city. Now, activists there say that Operation Legend is a serious blow.
THE RISE OF the internet-connected home security camera has generally been a boon to police, as owners of these devices can (and frequently do) share footage with cops at the touch of a button. But according to a leaked FBI bulletin, law enforcement has discovered an ironic downside to ubiquitous privatized surveillance: The cameras are alerting residents when police show up to conduct searches.

A November 2019 “technical analysis bulletin” from the FBI provides an overview of “opportunities and challenges” for police from networked security systems like Amazon’s Ring and other “internet of things,” or IoT, devices. Marked unclassified but “law enforcement sensitive” and for official use only, the document was included as part of the BlueLeaks cache of material hacked from the websites of fusion centers and other law enforcement entities.


WHOEVER BROKE INTO 251 law enforcement websites and obtained the BlueLeaks trove of documents appears to have reused decades-old software for opening “backdoors” in web servers.

The use of the widely available backdoors provides evidence that the hacktivist who compromised the sensitive sites, including fusion centers linked to federal agencies, didn’t need to use sophisticated digital attack methods because the sites were not very secure.


WHILE DOCTORS AND politicians still struggle to convince Americans to take the barest of precautions against Covid-19 by wearing a mask, the Department of Homeland Security has an opposite concern, according to an “intelligence note” found among the BlueLeaks trove of law enforcement documents: Masks are breaking police facial recognition.

The rapid global spread and persistent threat of the coronavirus has presented an obvious roadblock to facial recognition’s similar global expansion. Suddenly everyone is covering their faces. Even in ideal conditions, facial recognition technologies often struggle with accuracy and have a particularly dismal track record when it comes to identifying faces that aren’t white or male. Some municipalities, startled by the civil liberties implications of inaccurate and opaque software in the hands of unaccountable and overly aggressive police, have begun banning facial recognition software outright. But the global pandemic may have inadvertently provided a privacy fix of its own — or for police, a brand new crisis.
AFTER FAILING TO PREVENT the terrorist attacks of September 11, 2001, the U.S. government realized it had an information sharing problem. Local, state and federal law enforcement agencies had their own separate surveillance databases that possibly could have prevented the attacks, but they didn’t communicate any of this information with each other. So Congress directed the newly formed Department of Homeland Security to form “fusion centers” across the country, collaborations between federal agencies like DHS and the FBI with state and local police departments, to share intelligence and prevent future terrorist attacks.

Yet in 2012 the Senate found that fusion centers have “not produced useful intelligence to support Federal counterterrorism efforts,” that the majority of the reports fusion centers produced had no connection to terrorism at all, and that the reports were low quality and often not about illegal activity. Fusion centers have also been criticized for privacy and civil liberties violations such as infiltrating and spying on anti-war activists.

Last month, the transparency collective Distributed Denial of Secrets published 269 gigabytes of law enforcement data on its website and using the peer-to-peer file sharing technology BitTorrent. The data, stolen from 251 different law enforcement websites by the hacktivist collective Anonymous, was mostly taken from fusion center websites (including many of those listed on DHS’s website), though some of the hacked websites were for local police departments, police training organizations, members-only associations for cops or retired FBI agents, and law enforcement groups specifically dedicated to investigating organized retail crime, drug trafficking, and working with industry.
NATIONWIDE PROTESTS AGAINST racist policing have brought new scrutiny onto big tech companies like Facebook, which is under boycott by advertisers over hate speech directed at people of color, and Amazon, called out for aiding police surveillance. But Microsoft, which has largely escaped criticism, is knee-deep in services for law enforcement, fostering an ecosystem of companies that provide police with software using Microsoft’s cloud and other platforms. The full story of these ties highlights how the tech sector is increasingly entangled in intimate, ongoing relationships with police departments.

Microsoft’s links to law enforcement agencies have been obscured by the company, whose public response to the outrage that followed the murder of George Floyd has focused on facial recognition software. This misdirects attention away from Microsoft’s own mass surveillance platform for cops, the Domain Awareness System, built for the New York Police Department and later expanded to Atlanta, Brazil, and Singapore. It also obscures that Microsoft has partnered with scores of police surveillance vendors who run their products on a “Government Cloud” supplied by the company’s Azure division and that it is pushing platforms to wire police field operations, including drones, robots, and other devices.
LEVERAGING CLOSE TIES to Twitter, controversial artificial intelligence startup Dataminr helped law enforcement digitally monitor the protests that swept the country following the killing of George Floyd, tipping off police to social media posts with the latest whereabouts and actions of demonstrators, according to documents reviewed by The Intercept and a source with direct knowledge of the matter.

The monitoring seems at odds with claims from both Twitter and Dataminr that neither company would engage in or facilitate domestic surveillance following a string of 2016 controversies. Twitter, up until recently a longtime investor in Dataminr alongside the CIA, provides the company with full access to a content stream known as the “firehose” — a rare privilege among tech firms and one that lets Dataminr, recently valued at over $1.8 billion, scan every public tweet as soon as its author hits send. Both companies denied that the protest monitoring meets the definition of surveillance.

A History of Police Work

Dataminr helps newsrooms, corporations, and governments around the world track crises with superhuman speed as they unfold across social media and the wider web. Through a combination of people and software, the company alerts organizations to chatter around global crises — wars, shootings, riots, disasters, and so forth — so that they’ll have a competitive edge as news is breaking. But the meaning of that competitive edge, the supercharged ability to filter out important events from the noise of hundreds of millions of tweets and posts across social media, will vary drastically based on the customer; the agenda of a newspaper using Dataminr to inform its breaking news coverage won’t be the same as the agendas of a bank or the FBI. It’s this latter category of Dataminr’s business, lucrative government work, that’s had the firm on the defensive in recent years.

In 2016, Twitter was forced to reckon with multiple reports that its platform was being used to enable domestic surveillance, including a Wall Street Journal report on Dataminr’s collaboration with American spy agencies in May; an American Civil Liberties Union report on Geofeedia, a Dataminr competitor, in October; and another ACLU investigation into Dataminr’s federal police surveillance work in December. The company sought to assure the public that attempts to monitor its users for purposes of surveillance were strictly forbidden under its rules, and that any violators would be kicked off the platform. For example, then-VP Chris Moody wrote in a company blog post that “using Twitter’s Public APIs or data products to track or profile protesters and activists is absolutely unacceptable and prohibited.” In a letter to the ACLU, Twitter public policy chief Colin Crowell similarly wrote that “the use of Twitter data for surveillance is strictly prohibited” and that “Datatminr’s product does not provide any government customers with … any form of surveillance.”

Twitter also said that Dataminr, one of its “official partners,” would “no longer support direct access by fusion centers” to information such as tweet locations; fusion centers are controversial facilities dedicated to sharing intelligence between the federal government and local police. Dataminr at the same time announced it would no longer provide a product for conducting geospatial analysis “to those supporting first reponse” and added that such clients did not have “direct firehose access.”

But based on interviews, public records requests, and company documents reviewed by The Intercept, Dataminr continues to enable what is essentially surveillance by U.S. law enforcement entities, contradicting its earlier assurances to the contrary, even if it remains within some of the narrow technical boundaries it outlined four years ago, like not providing direct firehose access, tweet geolocations, or certain access to fusion centers.

Dataminr relayed tweets and other social media content about the George Floyd and Black Lives Matter protests directly to police, apparently across the country. In so doing, it used to great effect its privileged access to Twitter data — despite current terms of service that explicitly bar software developers “from tracking, alerting, or monitoring sensitive events (such as protests, rallies, or community organizing meetings)” via Twitter.
IN THE RUN-UP to the 2020 election, former Vice President Joe Biden’s campaign is putting together a foreign policy team for a potential future administration. Among those described as being part of the team is Avril Haines, former deputy director of the CIA during the Obama administration. According to an NBC News report from last week, Haines has been tapped to work advising on policy, as well as lead the national security and foreign policy team.

In addition to her past national security work and impressive presence in the D.C. think tank world, Haines has in the past described herself as a former consultant for the controversial data-mining firm Palantir. Haines’s biography page at the Brookings Institute, where she is listed as a nonresident senior fellow, boasted of this affiliation until at least last week, when it suddenly no longer appeared on the page.

The nature of the consulting work that Haines did for Palantir is not clear. As of press time, requests for comment to her, the Biden campaign, Palantir, and Brookings were not answered. Prior to being removed from the Brookings page, the connection to the data-mining company was listed alongside a long list of other affiliations that were similarly pared down.
THE FEDERAL BUREAU of Investigation may be watching what you tweet and where people gather.

The federal law enforcement agency’s records show a growing focus on harnessing the latest private sector tools for mass surveillance, including recent contracts with companies that monitor social media posts and collect cellphone location data.

On June 9, after demonstrations around the country erupted over the police killing of George Floyd, the FBI signed an expedited agreement to extend its relationship with Dataminr, a company that monitors social media.

About a week prior, the agency modified an agreement it signed in February with Venntel, Inc., a Virginia technology firm that maps and sells the movements of millions of Americans. The company purchases bulk location data and sells it largely to government agencies.
WITH THE INK still drying on their landmark $52 million settlement with Facebook over trauma they suffered working for the company, many outsourced content moderators are now being told that they must view some of the most horrific and disturbing content on the internet for an extra 48 minutes per day, The Intercept has learned.

Following an unprecedented 2018 lawsuit by ex-Facebook content moderator Selena Scola, who said her daily exposure to depictions of rape, murder, and other gruesome acts caused her to develop post-traumatic stress disorder, Facebook agreed in early May to a $52 million settlement, paid out with $1,000 individual minimums to current and former contractors employed by outsourcing firms like Accenture. Following news of the settlement, Facebook spokesperson Drew Pusateri issued a statement reading, “We are grateful to the people who do this important work to make Facebook a safe environment for everyone. We’re committed to providing them additional support through this settlement and in the future.”

Less than a month after this breakthrough, however, Accenture management informed moderation teams that it had renegotiated its contract with Facebook, affecting at least hundreds of North American content workers who would now have to increase their exposure to exactly the sort of extreme content at the heart of the settlement, according to internal company communications reviewed by The Intercept and interviews with multiple affected workers.

The new hours were announced at the tail end of May and beginning of June via emails sent by Accenture management to the firm’s content moderation teams, including those responsible for reviewing Child Exploitation Imagery, or CEI, generally graphic depictions of sexually abused children, and Inappropriate Interactions with Children, or IIC, typically conversations in which adults message minors in an attempt to “groom” them for later sexual abuse or exchange sexually explicit images. The Intercept reviewed multiple versions of this email, apparently based off a template created by Accenture. It refers to the new contract between the two companies as the “Golden SoW,” short for “Statement of Work,” and its wording strongly suggests that stipulations in the renewed contract led to 48-minute increases in the so-called “Safety flows” that handle Facebook posts containing depictions of child abuse.
WITH THE INK still drying on their landmark $52 million settlement with Facebook over trauma they suffered working for the company, many outsourced content moderators are now being told that they must view some of the most horrific and disturbing content on the internet for an extra 48 minutes per day, The Intercept has learned.

Following an unprecedented 2018 lawsuit by ex-Facebook content moderator Selena Scola, who said her daily exposure to depictions of rape, murder, and other gruesome acts caused her to develop post-traumatic stress disorder, Facebook agreed in early May to a $52 million settlement, paid out with $1,000 individual minimums to current and former contractors employed by outsourcing firms like Accenture. Following news of the settlement, Facebook spokesperson Drew Pusateri issued a statement reading, “We are grateful to the people who do this important work to make Facebook a safe environment for everyone. We’re committed to providing them additional support through this settlement and in the future.”

Less than a month after this breakthrough, however, Accenture management informed moderation teams that it had renegotiated its contract with Facebook, affecting at least hundreds of North American content workers who would now have to increase their exposure to exactly the sort of extreme content at the heart of the settlement, according to internal company communications reviewed by The Intercept and interviews with multiple affected workers.

The new hours were announced at the tail end of May and beginning of June via emails sent by Accenture management to the firm’s content moderation teams, including those responsible for reviewing Child Exploitation Imagery, or CEI, generally graphic depictions of sexually abused children, and Inappropriate Interactions with Children, or IIC, typically conversations in which adults message minors in an attempt to “groom” them for later sexual abuse or exchange sexually explicit images. The Intercept reviewed multiple versions of this email, apparently based off a template created by Accenture. It refers to the new contract between the two companies as the “Golden SoW,” short for “Statement of Work,” and its wording strongly suggests that stipulations in the renewed contract led to 48-minute increases in the so-called “Safety flows” that handle Facebook posts containing depictions of child abuse.
DURING AN internal presentation at Facebook on Wednesday, the company debuted features for Facebook Workplace, an intranet-style chat and office collaboration product similar to Slack.

On Facebook Workplace, employees see a stream of content similar to a news feed, with automatically generated trending topics based on what people are posting about. One of the new tools debuted by Facebook allows administrators to remove and block certain trending topics among employees.

The presentation discussed the “benefits” of “content control.” And it offered one example of a topic employers might find it useful to blacklist: the word “unionize.”

SURVEILLANCE FIRMS around the world are licking their lips at a once-in-a-lifetime opportunity to cash in on the coronavirus by repositioning one of their most invasive products: the tracking bracelet.

Body monitors are associated with criminality and guilt in the popular imagination, the accessories of Wall Street crooks under house arrest and menace-to-society parolees. Unlike smartphones, de facto tracking devices in their own right, strapped-on trackers are expressly designed to be attached to the body and exist solely to report the user’s whereabouts and interactions to one or more third parties; they don’t play podcasts or tell you how many steps you took that day to sweeten the surveillance.

But a climate of perpetual bio-anxiety has paved the way for broader acceptance of carceral technologies, with a wave of companies trying to sell tracking accessories to business owners eager to reopen under the aegis of responsible social distancing and to governments hoping to keep a closer eye on people under quarantine.

EARLIER THIS MONTH, Facebook debuted its group video chat offering, Messenger Rooms, to a world under widespread pandemic lockdown, one that’s in large part replaced face-to-face meetings with streamed conversations. The chief beneficiary of this shift, Zoom, has spent months as a punching bag for privacy advocates, so Facebook was quick to assure users that it had “built Rooms with privacy in mind” and that “we don’t watch or listen to your audio or video calls.”

But today, well over a week after the rollout and nearly a month after Facebook announced and offered the privacy assurances about Messenger Rooms, it’s impossible to determine exactly what information will be collected about you and your life if you decide to use the product. The company’s public documentation of Messenger Rooms, including a post focused on privacy, offers very few details, although the privacy post promises, narrowly, that “audio and video from Rooms won’t be used to inform ads.” Facebook’s communications department spent weeks researching my questions about Messenger Rooms privacy, only to come back with few answers, and offering instead only links to a spate of vague policies that predate the product.
EARLIER THIS MONTH, Facebook debuted its group video chat offering, Messenger Rooms, to a world under widespread pandemic lockdown, one that’s in large part replaced face-to-face meetings with streamed conversations. The chief beneficiary of this shift, Zoom, has spent months as a punching bag for privacy advocates, so Facebook was quick to assure users that it had “built Rooms with privacy in mind” and that “we don’t watch or listen to your audio or video calls.”

But today, well over a week after the rollout and nearly a month after Facebook announced and offered the privacy assurances about Messenger Rooms, it’s impossible to determine exactly what information will be collected about you and your life if you decide to use the product. The company’s public documentation of Messenger Rooms, including a post focused on privacy, offers very few details, although the privacy post promises, narrowly, that “audio and video from Rooms won’t be used to inform ads.” Facebook’s communications department spent weeks researching my questions about Messenger Rooms privacy, only to come back with few answers, and offering instead only links to a spate of vague policies that predate the product.
BLUETOOTH HAS SPENT much of its life ignobly associated with crummy headphones, byzantine connection procedures, and car stereo systems that never quite seem to work right. Now this wireless technology concocted in the ’90s to help PCs and mobile phones communicate is being asked to step up and save the planet from a global pandemic. According to its two co-inventors, there could be some issues.

Named for the 10th century king Harald “Bluetooth” Gormsson, famous in Scandinavia for uniting (and Christianizing) the Danes, the humble, oft-derided wireless technology included in some form in nearly every portable device from the past decade and beyond is central to coronavirus contact tracing apps pushed by Apple, Google, and governments across the world. Banking on the standard’s ubiquity, and considerably improved reliability since the ’90s, these entities hope to turn billions of Bluetooth-enabled devices into an army of public health automatons that can map anyone who came into contact with someone who tests positive for Covid-19.
A WINDOW MAY soon open for banks and lenders to use robocalls during the coronavirus crisis. Backed by a push to provide consumers with economic relief, a more expansive exemption could lead to unsolicited debt collection and marketing.

A law passed in 1991 created a working definition of unsolicited robocalls. Last year, in response to outrage over hundreds of millions of unwanted robocalls, Congress passed a new law giving the Federal Communications Commission enhanced power to crack down on the practice with stepped-up penalties for spam callers. Generally, consumers can only receive lawful automated calls if they have opted in or provided their phone number for a financial service.

Now, the coronavirus crisis has sparked an unusual alliance of consumer advocates and the banking industry to come together to seek exemptions to the 1991 definition — and therefore the penalties that accompany enforcement. Together, the unlikely allies called for reinstating the limited use of automatic telephone dialing systems for prerecorded or artificial voice calls made without the consent of consumers.

Normally political foes, the National Consumer Law Center and the American Bankers Association came together to ask the FCC for an expedited, limited exemption to anti-robocalling regulations to allow for automated calls designed explicitly for financial relief. The exemption would allow alerts to inform consumers of loan modifications or payment forbearance options during the Covid-19 epidemic.
JAIL AND PRISON officials in at least three states are using software to scan inmate calls for mentions of the coronavirus, a move advocacy groups believe paves the way for abuse while raising stark questions about carceral health care.

The monitoring software was created by LEO Technologies, a Los Angeles company backed primarily by scandal-plagued Republican fundraiser Elliott Broidy. Known as Verus, it was first deployed several years ago to forestall suicide attempts, mine calls for investigative tips, and for a range of other purposes. In recent weeks, it has been marketed as a system “that can mitigate the effects of the COVID-19 pandemic across our nation’s jail and prison facilities” by alerting prison authorities to sickness-related conversations between inmates and the outside world.
A SMARTPHONE TRACKING firm helping Donald Trump clinch his 2020 presidential reelection recently told investors it’s identified a promising new profit opportunity: the global coronavirus pandemic.

Phunware is part of a vast galaxy of obscure advertising technology companies that help clients follow and target their customers — to “capitalize on users’ daily digital trail,” as Phunware’s site puts it. By embedding Phunware code in their app, a developer can easily glean detailed records of where a user goes and what they do, creating a rich behavioral history to sell on to others.
FACEBOOK INFORMATION TECHNOLOGY contractors have been told their physical presence is required to set up laptops for new hires and other remote employees, and have been given letters to carry on their commutes stating that they are helping to provide “essential services” amid the Covid-19 pandemic.

The contractors, who are employed through global staffing firm Astreya, serve Facebook offices across the country, including in the cities of New York; Austin, Texas; and, Menlo Park, California, all of which require nonessential workers to remain at home. Facebook is eager to get hundreds of laptops and phones set up and shipped to its newly remote workforce across the country, two contractors told The Intercept, a task delegated to them at the same time the social network has emphasized a companywide work-from-home initiative.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 

LEIA EM PORTUGUÊS 
LIKE OTHER tech firms scrambling in the face of the Covid-19 pandemic, Facebook is encouraging staff worldwide to work from home, part of a so-called social distancing strategy to slow the new coronavirus’s spread. But some in the social network’s army of contract workers, already often treated like second-class employees, have complained that they have no such luxury and are being asked to choose between their jobs and their health.

Discussions from Facebook’s internal employee forum reviewed by The Intercept reveal a state of confusion, fear, and resentment, with many precariously employed hourly contract workers stating that, contrary to statements to them from Facebook, they are barred by their actual employers from working from home, despite the technical feasibility and clear public health benefits of doing so.
IN 2013, U.S. Immigration and Customs Enforcement quietly began using a software tool to recommend whether people arrested over immigration violations should be let go after 48 hours or detained. The software’s algorithm supposedly pored over a variety of risk factors before outputting a decision.

A new lawsuit, however, filed by the New York Civil Liberties Union and Bronx Defenders, alleges that the algorithm doesn’t really make a decision, at least not one that can result in a detainee being released. Instead, the groups said, it’s an unconstitutional cudgel that’s been rigged to detain virtually everyone ICE’s New York Field Office brings in, even when the government itself believes they present a minimal threat to public safety.
IN 2013, U.S. Immigration and Customs Enforcement quietly began using a software tool to recommend whether people arrested over immigration violations should be let go after 48 hours or detained. The software’s algorithm supposedly pored over a variety of risk factors before outputting a decision.

A new lawsuit, however, filed by the New York Civil Liberties Union and Bronx Defenders, alleges that the algorithm doesn’t really make a decision, at least not one that can result in a detainee being released. Instead, the groups said, it’s an unconstitutional cudgel that’s been rigged to detain virtually everyone ICE’s New York Field Office brings in, even when the government itself believes they present a minimal threat to public safety.
A POLICE INVESTIGATOR in Spain is trying to solve a crime, but she only has an image of a suspect’s face, caught by a nearby security camera. European police have long had access to fingerprint and DNA databases throughout the 27 countries of the European Union and, in certain cases, the United States. But soon, that investigator may be able to also search a network of police face databases spanning the whole of Europe and the U.S.

According to leaked internal European Union documents, the EU could soon be creating a network of national police facial recognition databases. A report drawn up by the national police forces of 10 EU member states, led by Austria, calls for the introduction of EU legislation to introduce and interconnect such databases in every member state. The report, which The Intercept obtained from a European official who is concerned about the network’s development, was circulated among EU and national officials in November 2019. If previous data-sharing arrangements are a guide, the new facial recognition network will likely be connected to similar databases in the U.S., creating what privacy researchers are calling a massive transatlantic consolidation of biometric data.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 

THERE’S WIDESPREAD CONCERN that video cameras will use facial recognition software to track our every public move. Far less remarked upon — but every bit as alarming — is the exponential expansion of “smart” video surveillance networks.

Private businesses and homes are starting to plug their cameras into police networks, and rapid advances in artificial intelligence are investing closed-circuit television, or CCTV, networks with the power for total public surveillance. In the not-so-distant future, police forces, stores, and city administrators hope to film your every move — and interpret it using video analytics.

The rise of all-seeing smart camera networks is an alarming development that threatens civil rights and liberties throughout the world. Law enforcement agencies have a long history of using surveillance against marginalized communities, and studies show surveillance chills freedom of expression — ill effects that could spread as camera networks grow larger and more sophisticated.

To understand the situation we’re facing, we have to understand the rise of the video surveillance industrial complex — its history, its power players, and its future trajectory. It begins with the proliferation of cameras for police and security, and ends with a powerful new industry imperative: complete visual surveillance of public space.

Video Management Systems and Plug-in Surveillance Networks

In their first decades of existence, CCTV cameras were low-resolution analog devices that recorded onto tapes. Businesses or city authorities deployed them to film a small area of interest. Few cameras were placed in public, and the power to track people was limited: If police wanted to pursue a person of interest, they had to spend hours collecting footage by foot from nearby locations.

In the late 1990s, video surveillance became more advanced. A company called Axis Communications invented the first internet-enabled surveillance camera, which converted moving images to digital data. New businesses like Milestone Systems built Video Management Systems, or VMS, to organize video information into databases. VMS providers created new features like motion sensor technology that alerted guards when a person was caught on camera in a restricted area.

As time marched on, video surveillance spread. On one account, about 50 years ago, the United Kingdom had somewhere north of 60 permanent CCTV cameras installed nationwide. Today, the U.K. has over 6 million such devices, while the U.S. has tens of millions. According to marketing firm IHS Markit, 1 billion cameras will be watching the world by the end of 2021, with the United States rivaling China’s per person camera penetration rate. Police can now track people across multiple cameras from a command-and-control center, desktop, or smartphone.

While it is possible to link thousands of cameras in a VMS, it is also expensive. To increase the amount of CCTVs available, cities recently came up with a clever hack: encouraging businesses and residents to place privately owned cameras on their police network — what I call “plug-in surveillance networks.”
ON NOVEMBER 21, the Ukrainian business publication Vector published a genuine regional success story: An Amazon research lab in Kyiv, affiliated with the company’s Ring home security division, was receiving a “rebrand” makeover and a broader new role within the company. The office was already involved “in many other Amazon projects,” a lab manager told Vector. “We are no longer part of a small startup,” he said in Ukranian, “but a full-fledged R&D center working for one of the world’s largest corporations.”

Ring Ukraine has repeatedly drawn scrutiny and criticism over the past year. In November, five U.S. senators, in a letter to Amazon CEO Jeff Bezos, released the day before the Vector story, had raised concerns about the Kyiv office’s access to Ring home security footage and other private information and asked whether a foreign government could access the material as well. The letter cited an Intercept report that many employees in the office were provided blanket, inappropriate access to a web server containing customer video files. In response, Ring revealed that it had fired four employees over “access to Ring video data” that “exceeded what was necessary for their job functions.” The company did not say what, if anything, it was doing prevent such incidents in the future.
AS SEVEN University of Puerto Rico students prepare to face trial in February for participating in a nonviolent protest more than two years ago, documents released to their defense attorneys reveal that Facebook granted the island’s Justice Department access to a trove of private information from student news publications. The department’s sweeping search warrant was part of a hunt for crimes committed by members of the youth anti-austerity movement, and it has raised fears among civil liberties advocates of a return to a period of Puerto Rico’s history when police routinely targeted citizens for surveillance on the basis of their political interests.

It was April 2017, and for weeks, University of Puerto Rico students had been holding a school-wide strike protesting austerity policies that were poised to defund public services across the island to satisfy the government’s creditors. When the university’s governing board gathered on April 27 to discuss $241 million in budget cuts, the students demanded to be let in. The board refused, locking the doors to the building where the meeting was being held. But the students stormed in anyway, pushing past security.

The action unfolded in real time on Facebook, as three student media outlets, Diálogo UPR, Pulso Estudiantil UPR, and Centro de Comunicación Estudiantil, livestreamed the protest. The students surrounded the board members and shut down the meeting, demanding that the board sign a commitment to rejecting the budget cuts. The action, one of many that took place on campus and in the streets, was over within half an hour. A glass door, some furniture, and a lamp were allegedly broken or damaged. No one was injured, and no one was arrested. But the secretary of Puerto Rico’s Justice Department, now-Gov. Wanda Vázquez, pledged to investigate the incident and arrest lawbreakers.

Two weeks later, students who had assumed leadership roles in the wider strike received citations ordering them to appear in court. When they showed up, they were handcuffed, paraded before media crews, and charged with a host of crimes related to the boardroom protest, the most severe of which — rioting and burglary — were later dropped. The remaining charges, including violating the right to assemble, aggravated restriction of freedom, and violence or intimidation against a public authority, each carry between six months and three years in prison. The seven students go to trial on February 7.

How exactly Vázquez’s Justice Department determined which students to charge out of the dozens who participated in the protest has remained a mystery to defense attorneys. The lawyers’ suspicion: that the case isn’t about crimes committed in the boardroom that day, but rather an attempt to penalize the political activity of some of the most active student organizers. The seven facing trial were members of the student strikers’ negotiating committee as well as political organizations critical of the government.




JUST DAYS AFTER Facebook and one of its contractors, Accenture, sent teams responsible for content moderation back to their offices amid concerns about the coronavirus pandemic, one worker at the office tested positive for Covid-19, according to an internal email viewed by The Intercept.

According to a notification email sent to contractors working out of Accenture’s Facebook facility in Austin, Texas — where hourly contractors deal with the social media giant’s most graphic forms of violence and sexual abuse — the office has already been hit with a positive case. “We have learned that one of our people working at Facebook Domain 8 on the 12th floor has tested positive for COVID-19,” the email reads. “This individual was last in the office on 10/13, became symptomatic on 10/14 and received a positive test result on 10/16. Currently, this person is in self-quarantine.”
FEDERAL AGENTS from the Department of Homeland Security and the Justice Department used “a sophisticated cell phone cloning attack—the details of which remain classified—to intercept protesters’ phone communications” in Portland this summer, Ken Klippenstein reported this week in The Nation. Put aside for the moment that, if the report is true, federal agents conducted sophisticated electronic surveillance against American protesters, an alarming breach of constitutional rights. Do ordinary people have any hope of defending their privacy and freedom of assembly against threats like this?

Yes, they do. Here are two simple things you can do to help mitigate this type of threat:

As much as possible, and especially in the context of activism, use an encrypted messaging app like Signal — and get everyone you work with to use it too — to protect your SMS text messages, texting groups, and voice and video calls.
Prevent other people from using your SIM card by setting a SIM PIN on your phone. There are instructions on how to do this below.
How SIM Cloning Works

Without more details, it’s hard to be entirely sure what type of surveillance was used, but The Nation’s mention of “cell phone cloning” makes me think it was a SIM cloning attack. This involves duplicating a small chip used by virtually every cellphone to link itself to its owner’s phone number and account; this small chip is the subscriber identity module, more commonly known as SIM.

Here’s how SIM cloning would work:

First, the feds would need physical access to their target’s phone; for example, they could arrest their target at a protest, temporarily confiscating their phone.
Then they would pop out the SIM card from the phone, a process designed to be easy, since end users often have reasons to replace the card (such as traveling abroad and needing a local SIM card to access the local cellular network, or when switching cellular providers).
The feds would then copy their target’s SIM card data onto a blank SIM card (this presents some challenges, as I explain below), and then put the original SIM card back without their target knowing.

SIM cards contain a secret encryption key that is used to encrypt data between the phone and cellphone towers. They’re designed so that this key can be used (like when you receive a text or call someone) but so the key itself can’t be extracted.

But it’s still possible to extract the key from the SIM card, by cracking it. Older SIM cards used a weaker encryption algorithm and could be cracked quickly and easily, but newer SIM cards use stronger encryption and might take days or significantly longer to crack. It’s possible that this is why the details of the type of surveillance used in Portland “remain classified.” Do federal agencies know of a way to quickly extract encryption keys from SIM cards? (On the other hand, it’s also possible that “cell phone cloning” doesn’t describe SIM cloning at all but something else instead, like extracting files from the phone itself instead of data from the SIM card.)
Since May, as protesters around the country have marched against police brutality and in support of the Black Lives Matter movement, activists have spotted a recurring presence in the skies: mysterious planes and helicopters hovering overhead, apparently conducting surveillance on protesters. A press release from the Justice Department at the end of May revealed that the Drug Enforcement Agency and U.S. Marshals Service were asked by the Justice Department to provide unspecified support to law enforcement during protests. A few days later, a memo obtained by BuzzFeed News offered a little more insight on the matter; it revealed that shortly after protests began in various cities, the DEA had sought special authority from the Justice Department to covertly spy on Black Lives Matter protesters on behalf of law enforcement. 
Although the press release and memo didn’t say what form the support and surveillance would take, it’s likely that the two agencies were being asked to assist police for a particular reason. Both the DEA and the Marshals possess airplanes outfitted with so-called stingrays or dirtboxes: powerful technologies capable of tracking mobile phones or, depending on how they’re configured, collecting data and communications from mobile phones in bulk.

WHILE DOCTORS AND politicians still struggle to convince Americans to take the barest of precautions against Covid-19 by wearing a mask, the Department of Homeland Security has an opposite concern, according to an “intelligence note” found among the BlueLeaks trove of law enforcement documents: Masks are breaking police facial recognition.

The rapid global spread and persistent threat of the coronavirus has presented an obvious roadblock to facial recognition’s similar global expansion. Suddenly everyone is covering their faces. Even in ideal conditions, facial recognition technologies often struggle with accuracy and have a particularly dismal track record when it comes to identifying faces that aren’t white or male. Some municipalities, startled by the civil liberties implications of inaccurate and opaque software in the hands of unaccountable and overly aggressive police, have begun banning facial recognition software outright. But the global pandemic may have inadvertently provided a privacy fix of its own — or for police, a brand new crisis.
LEVERAGING CLOSE TIES to Twitter, controversial artificial intelligence startup Dataminr helped law enforcement digitally monitor the protests that swept the country following the killing of George Floyd, tipping off police to social media posts with the latest whereabouts and actions of demonstrators, according to documents reviewed by The Intercept and a source with direct knowledge of the matter.

The monitoring seems at odds with claims from both Twitter and Dataminr that neither company would engage in or facilitate domestic surveillance following a string of 2016 controversies. Twitter, up until recently a longtime investor in Dataminr alongside the CIA, provides the company with full access to a content stream known as the “firehose” — a rare privilege among tech firms and one that lets Dataminr, recently valued at over $1.8 billion, scan every public tweet as soon as its author hits send. Both companies denied that the protest monitoring meets the definition of surveillance.

A History of Police Work

Dataminr helps newsrooms, corporations, and governments around the world track crises with superhuman speed as they unfold across social media and the wider web. Through a combination of people and software, the company alerts organizations to chatter around global crises — wars, shootings, riots, disasters, and so forth — so that they’ll have a competitive edge as news is breaking. But the meaning of that competitive edge, the supercharged ability to filter out important events from the noise of hundreds of millions of tweets and posts across social media, will vary drastically based on the customer; the agenda of a newspaper using Dataminr to inform its breaking news coverage won’t be the same as the agendas of a bank or the FBI. It’s this latter category of Dataminr’s business, lucrative government work, that’s had the firm on the defensive in recent years.

In 2016, Twitter was forced to reckon with multiple reports that its platform was being used to enable domestic surveillance, including a Wall Street Journal report on Dataminr’s collaboration with American spy agencies in May; an American Civil Liberties Union report on Geofeedia, a Dataminr competitor, in October; and another ACLU investigation into Dataminr’s federal police surveillance work in December. The company sought to assure the public that attempts to monitor its users for purposes of surveillance were strictly forbidden under its rules, and that any violators would be kicked off the platform. For example, then-VP Chris Moody wrote in a company blog post that “using Twitter’s Public APIs or data products to track or profile protesters and activists is absolutely unacceptable and prohibited.” In a letter to the ACLU, Twitter public policy chief Colin Crowell similarly wrote that “the use of Twitter data for surveillance is strictly prohibited” and that “Datatminr’s product does not provide any government customers with … any form of surveillance.”

Twitter also said that Dataminr, one of its “official partners,” would “no longer support direct access by fusion centers” to information such as tweet locations; fusion centers are controversial facilities dedicated to sharing intelligence between the federal government and local police. Dataminr at the same time announced it would no longer provide a product for conducting geospatial analysis “to those supporting first reponse” and added that such clients did not have “direct firehose access.”

But based on interviews, public records requests, and company documents reviewed by The Intercept, Dataminr continues to enable what is essentially surveillance by U.S. law enforcement entities, contradicting its earlier assurances to the contrary, even if it remains within some of the narrow technical boundaries it outlined four years ago, like not providing direct firehose access, tweet geolocations, or certain access to fusion centers.

Dataminr relayed tweets and other social media content about the George Floyd and Black Lives Matter protests directly to police, apparently across the country. In so doing, it used to great effect its privileged access to Twitter data — despite current terms of service that explicitly bar software developers “from tracking, alerting, or monitoring sensitive events (such as protests, rallies, or community organizing meetings)” via Twitter.


CITIZEN, A MOBILE APP that alerts people to nearby emergencies, is testing the reintroduction of a controversial feature that lets users report crimes and incidents on their own by live streaming video.

Created by New York-based startup sp0n, Citizen first launched under the name “Vigilante” in 2016 in New York City, broadcasting alerts of 911 calls to users in the vicinity and allowing those users to send live video from incident scenes, comment on alerts, and report incidents on their own. In a splashy launch video with the hashtag #CrimeNoMore, several young men were depicted rushing to aid a woman who was chased by a menacing stranger; the video instructs users not to “interfere with the crime,” but then adds, “Good luck out there!” Vigilante was met with swift backlash from the public and police departments, and Apple soon pulled the app from its store. At that time, the New York Police Department issued a statement saying, “Crimes in progress should be handled by the NYPD and not a vigilante with a cell phone.”

Several months later, the app rebranded as Citizen, removed the incident reporting feature, and said it was shifting its focus to “safety” and “avoiding crime” — a far cry from its prior positioning.

Citizen’s return to public crime reporting has not been publicized, but is documented on the company’s user support website. The app’s latest version in Apple and Google’s app stores also includes the description: “Keep Your Community Safe: Report incidents right when they happen to protect the people around you.”
PRESIDENT DONALD TRUMP’S reelection effort has retained the services of a technology company that specializes in the mass collection of smartphone location data, which can be used to track voters for political targeting purposes.

Phunware, an Austin, Texas-based firm, announced the connection in a little-noticed press release in October, touting “new and existing customer wins including American Made Media Consultants,” the consulting firm set up this year by Trump campaign manager Brad Parscale to handle advertising services for a variety of official Trump reelection PACs. The release noted that the deal was signed in conjunction with the Trump-Pence 2020 reelection effort.

A growing subset of advertising firms rely on data brokers that use third-party apps — from popular mobile games to apps used for checking the weather, perfecting a selfie, and online banking — to harvest vast troves of information about potential voters. Phunware, in a section of its website, discusses the company’s ability to obtain GPS location data and the Wi-Fi network used by an individual, as well as user data that can infer an “individual’s gender, age, lifestyle preferences” — potential tools for identifying and influencing voters.

The company claims to offer a wide range of services based on user location data. Individuals who attend a political rally or protest can be identified as potential targets for ads, a technique known as geofencing. Location data can provide insights into how long a shopper spends at a particular clothing store, type of religious venues, or the night clubs they tend to frequent.

“Unfortunately Phunware does not comment on customer-specific data or information,” wrote Brent Brightwell, a spokesperson for Phunware, when contacted about the company’s work with Trump campaign. “Please contact the Trump reelection campaign directly should you have any questions about their activities or efforts.” The Trump campaign did not respond to a request for comment.
A FILMMAKER WORKING on a documentary that’s critical of U.S. policies. A writer who operates a pseudonymous Twitter account to evade an authoritarian regime in their home country. An activist who uses Facebook to organize protests at the U.S.-Mexico border.

These are the kinds of people who might not want U.S. immigration agents poring over their social media profiles before deciding whether they should be allowed into the country. Yet that’s exactly what the State Department now requires as part of the Trump administration’s “extreme vetting” of millions of visa applicants. As of May, people who need a visa to enter the U.S. have to disclose any social media handles they’ve used over the past five years on 20 platforms, from Instagram and Twitter to YouTube and Weibo (the Chinese microblogging service). If they don’t, their visas could be denied.
In partnership with
RING, AMAZON’S CRIMEFIGHTING surveillance camera division, has crafted plans to use facial recognition software and its ever-expanding network of home security cameras to create AI-enabled neighborhood “watch lists,” according to internal documents reviewed by The Intercept.

The planning materials envision a seamless system whereby a Ring owner would be automatically alerted when an individual deemed “suspicious” was captured in their camera’s frame, something described as a “suspicious activity prompt.”

It’s unclear who would have access to these neighborhood watch lists, if implemented, or how exactly they would be compiled, but the documents refer repeatedly to law enforcement, and Ring has forged partnerships with police departments throughout the U.S., raising the possibility that the lists could be used to aid local authorities. The documents indicate that the lists would be available in Ring’s Neighbors app, through which Ring camera owners discuss potential porch and garage security threats with others nearby.

Ring spokesperson Yassi Shahmiri told The Intercept that “the features described are not in development or in use and Ring does not use facial recognition technology,” but would not answer further questions.
SHUTTERSTOCK, THE WELL-KNOWN online purveyor of stock images and photographs, is the latest U.S. company to willingly support China’s censorship regime, blocking searches that might offend the country’s authoritarian government, The Intercept has learned.

The publicly traded company built a $639 million-per-year business on the strength of its vast — sometimes comically vast — catalog of images depicting virtually anything a blogger or advertiser could imagine. The company now does business in more than 150 countries. But in China, there is now a very small, very significant gap in Shutterstock’s offerings. In early September, Shutterstock engineers were given a new goal: The creation of a search blacklist that would wipe from query results images associated with keywords forbidden by the Chinese government. Under the new system, which The Intercept is told went into effect last month, anyone with a mainland Chinese IP address searching Shutterstock for “President Xi,” “Chairman Mao,” “Taiwan flag,” “dictator,” “yellow umbrella,” or “Chinese flag” will receive no results at all. Variations of these terms, including “umbrella movement” — the precursor to the mass pro-democracy protests currently gripping Hong Kong — are also banned.

Shutterstock’s decision to silently aid China’s censorship agenda comes at a time of heightened scrutiny into the relationship between corporate America and President Xi Jinping’s authoritarian regime. Household names like Apple, Blizzard Entertainment, the NBA, and Google have all garnered harsh criticism for letting the policy directives of the Communist Party of China, and the gilded promise of a billion customers, dictate company strategy. Deciding to censor is a particularly stark inversion of values for Shutterstock, which markets itself as an enabler of creative expression.

The photo company’s relationship with China dates back to at least 2014, when it struck a distribution deal with ZCool, a Chinese social network and portfolio site for visual artists. Last year Shutterstock announced a $15 million investment in ZCool, noting that owing to the partnership, “Shutterstock’s content now powers large technology platforms in China such as Tencent Social Ads,” an online advertising subsidiary of the tremendously popular Chinese internet conglomerate Tencent.

Shutterstock’s censorship feature appears to have been immediately controversial within the company, prompting more than 180 Shutterstock workers to sign a petition against the search blacklist and accuse the company of trading its values for access to the lucrative Chinese market. Chinese internet users already struggle to discuss even the tamest of taboo subjects; now, it seemed, the situation would get a little worse, with the aid of yet another willing American company.

“Yes, we’re a creative photo and video marketplace, but we are also an editorial news hub,” one Shutterstock employee told The Intercept. “Want to write a story about the protests in Hong Kong? They never existed. Want to write about Taiwan? It never existed. Xi Jinping is NOT a dictator because he specifically said so. This is dark shit.”

The text of the petition, provided to The Intercept, can be read in full below.

Shutterstock’s founder and CEO Jon Oringer replied to the petition several days later; those hoping for a change of heart were to be disappointed. Shutterstock’s pro-censorship compromise with the Chinese government was justified, Oringer argued, because to refuse to do business in China rather than help the country’s government expand its information control scheme would be the real act of craven corporate turpitude: “Do we make the majority of our content available to China’s 1.3 billion citizens or do we take away their ability to access it entirely? We ultimately believe, consistent with our brand promise, it is more valuable for storytellers to have access to our collection to creatively and impactfully tell their stories.” Shutterstock with a bespoke censorship feature was “more empowering” and “will better serve the people of China than the alternative,” Oringer continued.

Oringer’s company-wide response is also reproduced below.

Following Oringer’s letter and the implementation of the search term blacklist, some employees fear the use of censorship at the company will grow: “He offered no consolation in terms of what our actions will be when China requests to add an X number more search terms to the censorship list,” the Shutterstock staffer told The Intercept, “or if another country comes to us with a similar request. We are devastated.”
LEIA EM PORTUGUÊS 
IS THE HUMAN race approaching its demise? The question itself may sound hyperbolic — or like a throwback to the rapture and apocalypse. Yet there is reason to believe that such fears are no longer so overblown. The threat of climate change is forcing millions around the world to realistically confront a future in which their lives, at a minimum, look radically worse than they are today. At the same time, emerging technologies of genetic engineering and artificial intelligence are giving a small, technocratic elite the power to radically alter homo sapiens to the point where the species no longer resembles itself. Whether through ecological collapse or technological change, human beings are fast approaching a dangerous precipice.
INVESTIGATIVE JOURNALIST Thomas Peele remembers the day he learned that he and fellow reporters at the East Bay Times and the San Jose Mercury News won the Pulitzer Prize for their “relentless” coverage of the deadly Ghost Ship warehouse fire in Oakland, California.

Peele was stunned and, of course, elated that he and his team would be honored for their dogged work covering a tragedy that shook their region.

It was April 2017. One week later, MediaNews Group, owner of the two papers and scores of others around the country, announced plans to move the Bay Area’s copy desk work to Southern California, triggering 20 layoffs from a shrunken roster of fewer than 100 employees in the East Bay newsroom.

In a flash, almost a quarter of the award-winning editorial staff was gone.

“It was a kick in the teeth,” said Peele, who is an officer for the NewsGuild, the union that represents workers at the papers.

The chain’s owners and executives, he said, “didn’t even send a pizza.” But they did send layoff notices, along with “word that our papers would be edited 400 miles away by people we had no relationship with, and who had little knowledge about our region of the state. And we were basically told some stories wouldn’t be copy edited at all.”

“Pulitzer?” he asked. “What Pulitzer?’

It was all part of a growing trend of shifting tasks such as proofreading, copy editing, and page design to “hubs” that would save money. Meanwhile, the papers were profitable, not to mention Pulitzer-winning — but the hedge fund that owned their parent company wanted to skim more off the top.

Two years later, MediaNews Group, better known by its trade name Digital First Media, has decided that shutting down pressrooms, eliminating jobs, and concentrating design and printing into regional hubs hasn’t cut costs enough.

Now it’s outsourcing California news design to the Philippines, paying pennies on the dollar for work that once employed professionals who lived in the communities they served.

An employee in the production department of the design hub, who asked not to be named because of fears he’d be laid off, said Digital First once assured workers that the hubs were key to helping the papers survive. By his count, just before the Philippines contract went into effect in May, the company cut its Southern California design and copy editing pool by a half-dozen people — all on the heels of 65 layoffs across the company’s Southern California newspapers a year earlier.

“They said it was to save money,” he said. “So once we heard everything was being outsourced, we were confused, because we were supposed to be that.”

Computers Covering City Hall?

At the Denver Post, the company is pushing the envelope even further. In bargaining talks with union leaders this summer, Digital First pushed for the right to use artificial intelligence to cover high school sports. They also hope to allow computers to “gather and publish” municipal government news, including “local news stories from suburban communities, school districts and other governmental districts,” according to a company proposal obtained by The Intercept. Denver Post union official Tony Mulligan said the company has already selected a vendor and budgeted money for the prep sports transition.

Of course, artificial intelligence and data mining don’t automatically lead to job loss, and they can be useful tools in the hands of skilled journalists, said Ken Doctor, a media analyst with Nieman Lab.

“The problem is the tools are being used by those who are primarily looking at cost-cutting,” he said. “Actual journalism requires judgment.”

In the midst of this unending budget slashing, Digital First papers are making plenty of money. They netted profits of $159 million in fiscal year 2017, according to Doctor. In fact, Digital First may be the most profitable daily newspaper chain in the industry, earning an average 17 percent annual profit margin in a field where 7 or 8 percent is considered viable. Some of its papers have earned as much as 30 percent, Doctor reported.

In press releases and statements filed with the Securities and Exchange Commission this year, Digital First confirmed its “increased profitability,” which it said grew from 11.6 percent to 16.2 percent between fiscal years 2015 and 2018.

But apparently even “increased profitability” isn’t enough. That’s because Digital First is controlled by the New York hedge fund Alden Global Capital.

Since 2012, when the privately held firm took control of the chain’s hundreds of dailies and weeklies, Alden has treated them like a personal ATM, admitting in court documents that it siphoned hundreds of millions in cash from local papers across the country to gamble on unrelated businesses — many losing propositions — such the Fred’s Pharmacy chain, which on September 9 declared bankruptcy and announced that it was liquidating all its stores and assets. Other court filings show that it invested in Payless ShoeSource, which this year shuttered its more than 2,000 stores.

Under Alden’s stewardship, those companies have shuttered thousands of stores, eliminating a combined 22,000 jobs, while Alden executives and associates took in six-figure compensation packages.

This is a typical strategy for what are known as vulture funds, which push these companies into bankruptcy, preferring to make their money by selling off assets like real estate, extracting maximum profits by slashing payrolls, paying themselves handsomely by serving on the company’s boards, and charging a 2 percent management fee (which, in Alden’s case, adds up to $20 million a year).

Shuttering Newsrooms

I’ve seen firsthand what Alden can do to a company. I worked for one of its papers, the Monterey Herald, until 2015. (I now freelance as a reporter and editor for various media outlets, including websites published by the NewsGuild, the union that represents workers at 13 Digital First newspapers.)

At the Herald, it was bad enough when the hot water heater broke and never got fixed. Then employees had to put plants under the leaky roof to keep the break room from flooding. Then the presses were shut down, the building sold, and we moved into smaller rented digs.

But at least we had an office.

In 2013, Alden quietly set up a mysterious firm called Twenty Lake Holdings to sell off its newspapers’ real estate, which often includes historic buildings in prime downtown locations. In towns from Longmont, Colorado, to Pottstown, Pennsylvania, Alden has closed Digital First offices entirely, forcing reporters to work from other papers’ buildings, printing plants miles from the towns they cover — or from their homes, cars or coffee shops.

Bill Ross, executive director of the NewsGuild of Greater Philadelphia, said that in his region, three Pennsylvania newspapers have closed their offices — in Pottstown, Norristown, and West Chester. These are the papers that, according to Doctor, are earning that stunning 30 percent profit margin.

Evan Brandt, a veteran reporter at the Pottstown Mercury, now works in his attic, “up here with the Christmas decorations.”

Brandt said the biggest challenge is getting breaking news to his community. “The cops reporter doesn’t live in town, so that’s 20 minutes away.”

Local News Going Overseas

In Monrovia, 20 miles northeast of Los Angeles, a Digital First production hub of 33 full-time and three freelance employees puts together 12 daily papers that include the L.A. Daily News, the Orange County Register, the San Jose Mercury News, and a swath of smaller California papers, along with dozens of weekly papers.

But now, even that consolidation effort is apparently not saving enough. Since May, Alden has been shifting design work to the Philippines. Through an outsourcing company called AffinityX, more than 40 California weekly newspapers once designed in Monrovia are now produced in Manila.

In an August 23 email to staff, Digital First’s Southern California News Group managing editor Helayne Perry wrote that “AffinityX has taken over the design of our opinion pages.” There are “rumblings” that page two and local news sections will likely be next, the Monrovia design hub employee said.

Requests for comment to Digital First management went unanswered. An AffinityX spokesperson said company executives declined to comment.
THE FOREIGN INTELLIGENCE Surveillance Court found that the FBI may have violated the rights of potentially millions of Americans — including its own agents and informants — by improperly searching through information obtained by the National Security Agency’s mass surveillance program.

U.S. District Court Judge James E. Boasberg, who serves in the District of Columbia and the FISA court, made his sweeping and condemnatory assessment in October 2018 in a 138-page ruling, which was declassified by the U.S. government this week.
FACEBOOK CEO MARK ZUCKERBERG will be the sole witness to testify before the House Financial Services Committee on October 23 during a hearing on the company’s plans to launch its own cryptocurrency.

Members of the committee had been in talks over whether to allow Zuckerberg to skip the hearing and instead hear from Facebook COO Sheryl Sandberg, but several members pushed for the CEO to appear instead.

The hearing, titled “An Examination of Facebook and Its Impact on the Financial Services and Housing Sectors,” comes three months after Chair Maxine Waters and committee Democrats sent a letter to Facebook calling on the company to suspend plans to launch the cryptocurrency, Libra. The company is also working on a program called Calibra, which would function as a digital wallet for the cryptocurrency.
FACEBOOK CEO MARK ZUCKERBERG will be the sole witness to testify before the House Financial Services Committee on October 23 during a hearing on the company’s plans to launch its own cryptocurrency.

Members of the committee had been in talks over whether to allow Zuckerberg to skip the hearing and instead hear from Facebook COO Sheryl Sandberg, but several members pushed for the CEO to appear instead.

The hearing, titled “An Examination of Facebook and Its Impact on the Financial Services and Housing Sectors,” comes three months after Chair Maxine Waters and committee Democrats sent a letter to Facebook calling on the company to suspend plans to launch the cryptocurrency, Libra. The company is also working on a program called Calibra, which would function as a digital wallet for the cryptocurrency.
TWITTER HELPED TO promote Chinese government propaganda and disinformation about the country’s controversial internment camps in the Xinjiang region, a review of the company’s advertising records reveals.

The social media company today announced a policy change that would bar such promotion following an inquiry from The Intercept and an earlier controversy over similar propaganda related to demonstrations in Hong Kong.

In Xinjiang, a western province in China, the United Nations has estimated that 1 million ethnic minority Muslim Uighurs — including children, pregnant women, elderly people, and people with disabilities — have been detained under the pretext of fighting extremism. According to Human Rights Watch, Chinese authorities are “committing human rights abuses in Xinjiang on a scale unseen in the country in decades.”
GOOGLE IS SET to re-staff its Cairo office, which more or less went dormant in 2014, following the military coup that brought President Abdel Fattah el-Sisi to power in Egypt. The move comes against the backdrop of well-documented abuses by the Sisi government against dissidents and activists, which it facilitates using mass and targeted internet surveillance, and by blocking news, human rights, and blogging websites.

Google said it would begin recruiting full-time staff for the office after a meeting between Egyptian ministers and Google staff led by Google MENA head Lino Cattaruzzi, according to a June press release from the Egyptian government. The company also recently consulted with the Egyptian government on a data protection bill. And it is in talks to partner with the Egyptian government to expand its “Maharat min Google,” or “Skills From Google,” program, which has provided digital training for entrepreneurs through partner organizations over the past year. The expansion would be overseen by a government ministry.

Google’s renewed engagement with Egypt comes just a year after the company sparked outrage when The Intercept revealed that Google planned to develop a censored search engine for use in China, which it code-named Dragonfly. When Google had previously ended its search services in China in 2010, co-founder Sergey Brin referenced the government’s poor tolerance for dissent as a reason for the pullout. Executives say Dragonfly has been shelved, after harsh criticism from Google employees, advocacy groups, and the U.S. Congress.
NEARLY 1,500 MILES from the Menlo Park headquarters of Facebook, at a company outpost in Austin, Texas, moderators toil around the clock to screen and scrub some the most gruesome, hateful, and heinous posts that make their way onto the social network and its photo-sharing subsidiary, Instagram. They are required to view as many as 800 pieces of disturbing content in a single shift, and routinely turn to on-site counselors to help cope with the procession of stomach-turning images, videos, and text. But some members of this invisible army have complained, in a statement widely circulated within Facebook, that the outsourcing giant that officially employs them, Accenture, has repeatedly attempted to violate the confidentiality of these therapy sessions.

The moderators work from within a special section for outsourced staffers at Facebook Austin. The Texas outpost is designed to mimic the look and feel of the company’s famously opulent Silicon Valley digs, but Accenture workers say they’re reminded daily of their secondary status and denied perks, prestige, and basic respect. This second-class tier at Facebook, a sort of international shadow workforce, has been well documented in the media, from Manila to Arizona, and it’s not clear whether the company has done anything to address it beyond issuing defensive PR statements. Moderators in Austin say their job is a brutalizing slog and that Facebook remains largely indifferent to their struggles. Access to on-site counseling is one of the few bright points for this workforce.
Since June, people in China have been unable to read The Intercept, after the country’s government apparently banned our website, along with those of several other media organizations. Today, we are happy to announce a workaround that will allow people in China to circumvent the restrictions, access our full site, and continue to read our award-winning journalism.


THEY CALL IT the Silent Talker. It is a virtual policeman designed to strengthen Europe’s borders, subjecting travelers to a lie detector test before they are allowed to pass through customs.

Prior to your arrival at the airport, using your own computer, you log on to a website, upload an image of your passport, and are greeted by an avatar of a brown-haired man wearing a navy blue uniform.

“What is your surname?” he asks. “What is your citizenship and the purpose of your trip?” You provide your answers verbally to those and other questions, and the virtual policeman uses your webcam to scan your face and eye movements for signs of lying.

At the end of the interview, the system provides you with a QR code that you have to show to a guard when you arrive at the border. The guard scans the code using a handheld tablet device, takes your fingerprints, and reviews the facial image captured by the avatar to check if it corresponds with your passport. The guard’s tablet displays a score out of 100, telling him whether the machine has judged you to be truthful or not.

A person judged to have tried to deceive the system is categorized as “high risk” or “medium risk,” dependent on the number of questions they are found to have falsely answered. Our reporter — the first journalist to test the system before crossing the Serbian-Hungarian border earlier this year — provided honest responses to all questions but was deemed to be a liar by the machine, with four false answers out of 16 and a score of 48. The Hungarian policeman who assessed our reporter’s lie detector results said the system suggested that she should be subject to further checks, though these were not carried out.

Travelers who are deemed dangerous can be denied entry, though in most cases they would never know if the avatar test had contributed to such a decision. The results of the test are not usually disclosed to the traveler; The Intercept obtained a copy of our reporter’s test only after filing a data access request under European privacy laws.
LAST YEAR,GOOGLE faced internal revolt from many employees over its handling of Project Maven, a secretive contract between the company and the Department of Defense to use artificial intelligence to improve the military’s drone targeting capabilities. After a series of internal, worker-led protests and resignations following reporting by The Intercept and Gizmodo, the company said it would wind down the drone project and promised a more transparent approach to similar work in the future.

Now, a number of Google workers are voicing concerns that the Mountain View, California-based search giant is continuing to deploy cutting-edge AI technology to the Pentagon and law enforcement customers.


IT IS THE size of a small suitcase and can be placed discreetly in the back of a car. When the device is powered up, it begins secretly monitoring hundreds of cellphones in the vicinity, recording people’s private conversations and vacuuming up their text messages.

The device is one of several spy tools manufactured by a Chinese company called Semptian, which has supplied the equipment to authoritarian governments in the Middle East and North Africa, according to two sources with knowledge of the company’s operations.

As The Intercept first reported on Thursday, since 2015, Semptian has been using American technology to help build more powerful surveillance and censorship equipment, which it sells to governments under the guise of a front company called iNext.
AN AMERICAN ORGANIZATION founded by tech giants Google and IBM is working with a company that is helping China’s authoritarian government conduct mass surveillance against its citizens, The Intercept can reveal.

The OpenPower Foundation — a nonprofit led by Google and IBM executives with the aim of trying to “drive innovation” — has set up a collaboration between IBM, Chinese company Semptian, and U.S. chip manufacturer Xilinx. Together, they have worked to advance a breed of microprocessors that enable computers to analyze vast amounts of data more efficiently.

Shenzhen-based Semptian is using the devices to enhance the capabilities of internet surveillance and censorship technology it provides to human rights-abusing security agencies in China, according to sources and documents. A company employee said that its technology is being used to covertly monitor the internet activity of 200 million people.

Semptian, Google, and Xilinx did not respond to requests for comment. The OpenPower Foundation said in a statement that it “does not become involved, or seek to be informed, about the individual business strategies, goals or activities of its members,” due to antitrust and competition laws. An IBM spokesperson said that his company “has not worked with Semptian on joint technology development,” but declined to answer further questions. A source familiar with Semptian’s operations said that Semptian had worked with IBM through a collaborative cloud platform called SuperVessel, which is maintained by an IBM research unit in China.
JUST MONTHS BEFORE millions of its internal documents were stolen and dumped on the internet, the Tennessee-based surveillance company Perceptics was preparing to pitch New York’s transit authority on how it could help enforce impending “congestion pricing” rules, according to leaked documents reviewed by The Intercept. The pitch, as outlined in the files, went well beyond mere toll enforcement and into profiling New Yorkers’ travel patterns and companions, creating what experts describe as major privacy risks.

Congestion pricing, on the face of it, doesn’t seem like it would present a privacy risk — it’s a traffic policy, after all, not some new NYPD initiative. The plan is to essentially tax the cars that clog Manhattan’s streets and route the proceeds to public transportation, providing both a deterrent against and palliative for traffic. There won’t be any congestion pricing toll booths: The fee will be assessed automatically and electronically, potentially by photographing the license plates of passing cars and sending the plate owner a bill in the mail. This requires cameras running around the clock, dutifully recording every car that comes and goes. And this, Perceptics claims, is where the company truly shines.

According to an internal presentation released by the Perceptics hacker and reviewed by The Intercept, the company pitched New York’s Metropolitan Transportation Authority, or MTA, in February of this year on how Perceptics’ car-scanning camera arrays, already deployed and honed in areas like the Mexican border and an assortment of U.S. military installations, could help the MTA track down drivers. It’s unknown how the plan was received by the MTA, which administers public transit, bridges, and tolls for New York City and some of its surrounding suburbs, but leaked Perceptics emails show that the company shipped camera hardware to the MTA’s Bridges and Tunnels division for a live demonstration.

Perceptics did not respond to a request for comment. An MTA spokesperson told The Intercept that “all details are still to be determined” regarding congestion pricing enforcement.

The presentation document, titled “Smart Imaging Solutions for New York City Congestion Pricing,” makes clear that Perceptics wants to “produce vehicle-specific profiles” using cameras and “unique machine learning algorithms,” allowing the city to immediately recognize and build travel histories of every car in the congestion zone. Law enforcement and surveillance experts said the system described goes far beyond what would ever be necessary to mail scofflaws traffic tickets. Instead, it is an entirely new sort of surveillance apparatus that tracks deeply personal information like “customer travel patterns and travel consistency,” the number of passengers in the car, or “likely trip purpose,” and associates this information with a unique fingerprint of every vehicle that passes by Perceptics’ cameras.

Allie Bohm, a policy counsel with the New York Civil Liberties Union, described the Perceptics plan as an “incredibly privacy-invasive proposal” that “raises all sorts of associational and First Amendment concerns.” Bohm expressed particular alarm about the possibility of a congestion pricing enforcement system eventually feeding data into the NYPD’s existing surveillance regime. “The NYPD has fancied itself an intelligence agency for a very long time,” said Bohm. “These are folks who are pioneering some really, at best, questionable, and, at worst, alarming programs of surveillance and of drawing conclusions from innocuous behavior.”

The MTA will not deploy congestion pricing before 2021 and has yet to select a tolling vendor. But whether Perceptics wins a contract or not, its idea to bring to the heart of Manhattan military-grade surveillance technology — already provided to Saudi Special Forces and the Jordanian army, according to a Perceptics document — is an example of how something as innocuous-sounding as congestion pricing can turn into a surveillance sprawl.
IN APRIL 2018, Facebook CEO Mark Zuckerberg sat before members of both houses of Congress and told them his company respected the privacy of the roughly two billion people who use it. “Privacy” remained largely undefined throughout Zuckerberg’s televised flagellations, but he mentioned the concept more than two dozen times, including when he told the Senate’s Judiciary and Commerce committees, “We have a broader responsibility to protect people’s privacy even beyond” a consent decree from federal privacy regulators, and when he told the House Energy and Commerce Committee, “We believe that everyone around the world deserves good privacy controls.” A year later, Zuckerberg claimed in interviews and essays to have discovered the religion of personal privacy and vowed to rebuild the company in its image.

But only months after Zuckerberg first outlined his “privacy-focused vision for social networking” in a 3,000-word post on the social network he founded, his lawyers were explaining to a California judge that privacy on Facebook is nonexistent.

The courtroom debate, first reported by Law360, took place as Facebook tried to scuttle litigation from users upset that their personal data was shared without their knowledge with the consultancy Cambridge Analytica and later with advisers to Donald Trump’s campaign. The full transcript of the proceedings — which has been quoted from only briefly — reveal one of the most stunning examples of corporate doublespeak certainly in Facebook’s history.

Representing Facebook before U.S. District Judge Vince Chhabria was Orin Snyder of Gibson Dunn & Crutcher, who claimed that the plaintiffs’ charges of privacy invasion were invalid because Facebook users have no expectation of privacy on Facebook. The simple act of using Facebook, Snyder claimed, negated any user’s expectation of privacy:

There is no privacy interest, because by sharing with a hundred friends on a social media platform, which is an affirmative social act to publish, to disclose, to share ostensibly private information with a hundred people, you have just, under centuries of common law, under the judgment of Congress, under the SCA, negated any reasonable expectation of privacy.

An outside party can’t violate what you yourself destroyed, Snyder seemed to suggest. Snyder was emphatic in his description of Facebook as a sort of privacy anti-matter, going so far as to claim that “the social act of broadcasting your private information to 100 people negates, as a matter of law, any reasonable expectation of privacy.” You’d be hard-pressed to come up with a more elegant, concise description of Facebook than “the social act of broadcasting your private information” to people. So not only is it Facebook’s legal position that you’re not entitled to any expectation of privacy, but it’s your fault that the expectation went poof the moment you started using the site (or at least once you connected with 100 Facebook “friends”).

Judge Chhabria was skeptical of Snyder’s privacy nonexistence argument at times, which he rejected as treating personal privacy as a binary, “like either you have a full expectation of privacy, or you have no expectation of privacy at all,” the judge put it at one point. Chhabria continued with a relatable hypothetical:

If I share [information] with ten people, that doesn’t eliminate my expectation of privacy. It might diminish it, but it doesn’t eliminate it. And if I share something with ten people on the understanding that the entity that is helping me share it will not further disseminate it to a thousand companies, I don’t understand why I don’t have — why that’s not a violation of my expectation of privacy.

Snyder responded with an incredible metaphor for how Facebook sees your use of its services — legally, at least:

Let me give you a hypothetical of my own. I go into a classroom and invite a hundred friends. This courtroom. I invite a hundred friends, I rent out the courtroom, and I have a party. And I disclose — And I disclose something private about myself to a hundred people, friends and colleagues. Those friends then rent out a 100,000-person arena, and they rebroadcast those to 100,000 people. I have no cause of action because by going to a hundred people and saying my private truths, I have negated any reasonable expectation of privacy, because the case law is clear.

And there it is, in broad daylight: Using Facebook is a depressing party taking place in a courtroom, for some reason, that’s being simultaneously broadcasted to a 100,000-person arena on a sort of time delay. If you show up at the party, don’t be mad when your photo winds up on the Jumbotron. That is literally the company’s legal position.

Again and again, Snyder blames the targets of surveillance capitalism for their own surveillance:

This is why every parent says to their child, “Do not post it on Facebook if you don’t want to read about it tomorrow morning in the school newspaper,” or, as I tell my young associates if I were going to be giving them an orientation, “Do not put anything on social media that you don’t want to read in the Law Journal in the morning.” There is no expectation of privacy when you go on a social media platform, the purpose of which, when you are set to friends, is to share and communicate things with a large group of people, a hundred people.

At one point Chhabria asked, seemingly unable to believe Snyder’s argument himself, “If Facebook promises not to disseminate anything that you send to your hundred friends, and Facebook breaks that promise and disseminates your photographs to a thousand corporations, that would not be a serious privacy invasion?

Snyder didn’t blink: “Facebook does not consider that to be actionable, as a matter of law under California law.”

Facebook’s counsel did seem to concede one possibility for the existence of privacy on Facebook: someone who uses Facebook completely contrary to the way it’s designed and to the way it has always been marketed. “If you really want to be private,” Snyder proposed to the court, “there are people who have archival Facebook pages that are like their own private mausoleum. It’s only set to [be visible by] me, and it’s for the purpose of repository, you know, of your private information, and no one will ever see that.” So these are your possible valid legal statuses as a Facebook user: You’re either plugged into the 100,0000-person perpetual surveillance Coachella or living in a digital “mausoleum.” But if you ever decide to fling open the doors of your private data crypt and, say, share a little content on Facebook with friends, as the company has been pushing us for the past 13 years, Snyder says you’re out of luck:

Once you go to friends, the gig is over because you’ve just gone — taken a hundred people and pronounced your personal likes and dislikes. In fact, the very act of liking something and showing your friends that you like something is a non-private act. It’s the whole premise of Facebook and social media, is to render not private your likes, your dislikes, your expressions. When I tag someone in a photo, it’s to tell people, not keep private, that I’m sitting on a park bench with John Smith. So it’s the opposite of private when you do that.

Facebook’s stance that if one truly wants to keep something private, they should keep it far from Facebook is odd — odder, still, given the fact that the company publishes an extremely detailed privacy policy, perhaps only meant for those huddling in private mausoleums where such a principle still exists.

“Facebook was built to bring people closer together,” reads the start of the company’s “Privacy Principles.” “We help you connect with friends and family, discover local events and find groups to join.” Not mentioned is that if you do any of that, it’s Facebook’s official opinion that you’ve “negated” your claim to any privacy whatsoever. The list of principles reads like a bad joke after studying Snyder’s courtroom theorizing: “We design privacy into our products from the outset” seems hard to reconcile with “Once you go to friends, the gig is over.” It’s similarly hard to take “We give you control of your privacy” seriously after hearing, through Snyder, that because Facebook users “shared the information … you’ve lost control over the information and its subsequent disclosure.”
OPERATIVES AT A controversial cybersecurity firm working for the United Arab Emirates government discussed targeting The Intercept and breaching the computers of its employees, according to two sources, including a member of the hacking team who said they were present at a meeting to plan for such an attack.

The firm, DarkMatter, brought ex-National Security Agency hackers and other U.S. intelligence and military veterans together with Emirati analysts to compromise the computers of political dissidents at home and abroad, including American citizens, Reuters revealed in January. The news agency also reported that the FBI is investigating DarkMatter’s use of American hacking expertise and the possibility that it was wielded against Americans.

The campaign against dissidents and critics of the Emirati government, code-named Project Raven, began in Baltimore. A 2016 Intercept article by reporter Jenna McLaughlin revealed how the Maryland-based computer security firm CyberPoint assembled a team of Americans for a contract to hone UAE’s budding hacking and surveillance capabilities, leaving some recruits unsettled. Much of the CyberPoint team was later poached by DarkMatter, a firm with close ties to the Emirati government and headquartered just two floors from the Emirati equivalent of the NSA, the National Electronic Security Authority (which later became the Signals Intelligence Agency). One of McLaughlin’s sources described the episode as something of a “hostile takeover” by the UAE government. A subsequent story by McLaughlin  for Foreign Policy detailed how American spies at DarkMatter had been crucial in building the UAE’s intelligence apparatus. The NESA would go on to become Project Raven’s primary “client,” responsible for handing down groups and organizations to be targeted and compromised.
A MEMBER OF Project Veritas gave testimony in a federal court case indicating that the right-wing group, known for its undercover videos, violates Facebook policies designed to counter systematic deception by Russian troll farms and other groups. The deposition raises questions over whether Facebook will deter American operatives who use the platform to strategically deceive and damage political opponents as vigorously as it has Iranian and Russian propagandists. But is the company capable of doing so without just creating more problems?

Close observers of Veritas and Facebook, including one at a research lab that works with the social network, said the testimony shows the group is clearly violating policies against what Facebook refers to as “coordinated inauthentic behavior.” The company formally defined such behavior in a December 2018 video featuring its cybersecurity policy chief Nathaniel Gleicher, who said it “is when groups of pages or people work together to mislead others about who they are or what they’re doing.” The designation, Gleicher added, is applied by Facebook to a group not “because of the content they’re sharing” but rather only “because of their deceptive behavior.” That is, using Facebook to dupe people is all it takes to fit the company’s institutional definition of coordinated inauthentic behavior.

In practice, “coordinated inauthentic behavior” has become a sort of catchall label for untoward meddling on Facebook, snagging everyone from Burmese military officers to Russian meme spammers. But curbing such activity has become a very public crusade for Facebook in the wake of its prominent role as a platform for the spread of disinformation, propaganda, and outright hoaxes during the 2016 presidential campaign. This past January, Gleicher announced the removal of coordinated inauthentic behavior from Iran, which spread when operatives “coordinated with one another and used fake accounts to misrepresent themselves,” thus triggering a Facebook ban. Similarly, in a 2017 update on Facebook’s internal investigation into Russian online propaganda efforts, the company’s then-head of security Alex Stamos assured the world’s democracies the company was providing “technology improvements for detecting fake accounts,” including “changes to help us more efficiently detect and stop inauthentic accounts at the time they are being created.”

Throughout all of this, coordinated inauthentic behavior has remained more or less synonymous with “foreign actors” and “nation-states,” the cloak-and-dagger stuff of an increasingly militarized internet filled with enemies of the Western Democracy who seek to subvert it from abroad.

Project Veritas, a hybrid of an opposition research shop and a ranting YouTube channel, has taken pride in its ability to deceive since its creation in 2010. With conservative backers like Peter Thiel, the Koch brothers, and the Trump Foundation, the group and its founder James O’Keefe have worked relentlessly to target and malign individuals at institutions they deem leftist, whether it’s Planned Parenthood (reportedly targeted by O’Keefe posing as a young teen’s 23-year-old boyfriend), George Soros (the progressive philanthropist whose professional circle Veritas tried and spectacularly failed to infiltrate), or the Washington Post (whose reporter was offered a fake story on Alabama Senate candidate Roy Moore). O’Keefe has long attempted to position himself in the context of dogged, daring, traditional journalism, describing Veritas’s efforts as “investigative” reporting executed by “undercover journalists.” But his efforts are often executed by what the New Yorker has called “amateurish spies” — their efforts against the Post and Soros resembled a Three Stooges bit — and packaged with mendacious editing, duplicitous production, and outright lying, making Veritas’s audience as much a victim of its productions as the subjects. Debates over who or what is to be considered “real journalism” are almost always counterproductive and contrived, but Veritas stands out for the shamelessness with which it pursues nakedly partisan ends.

There is, of course, a proud tradition of undercover journalism executed unequivocally in the name of informing the public. Writers like Barbara Ehrenreich and Shane Bauer have taken jobs they were not otherwise interested in in order to reveal injustices in society’s margins, and some of the most damning details of the Cambridge Analytica scandal were exposed by a reporter with the UK’s Channel 4 posing as a foreign politician interested in the company’s services. This reporting involved lying, sure — or at least the withholding of true intent, and a willingness to let others deceive themselves — but only as a means to a truthful end. The distinction between these reporters and Veritas operatives may be that the end the latter group seeks, the final media product, is typically just another act of partisan misdirection that doesn’t withstand further scrutiny.

Neither Project Veritas nor Facebook commented for this story.

“Legend Building” by Project Veritas

Project Veritas has systematically deceived not just targets on the left and viewers on the right but Facebook users as well (their official page has over 200,000 followers) at a time when the company is publicly dedicated to fighting this sort of systemic duplicity. That’s a wrinkle that raises questions about Facebook’s commitment to rooting out coordinated inauthentic behavior closer to home — Thiel sits on the company’s board — not to mention Project Veritas’s presence on social media.
THE CHINESE GOVERNMENT appears to have launched a major new internet crackdown, blocking the country’s citizens from accessing The Intercept’s website and those of at least seven other Western news organizations.

On Friday, people in China began reporting that they could not access the websites of The Intercept, The Guardian, the Washington Post, HuffPost, NBC News, the Christian Science Monitor, the Toronto Star, and Breitbart News.

It is unclear exactly when the censorship came into effect or the reasons for it. But Tuesday marked the 30th anniversary of the Tiananmen Square massacre, and Chinese authorities have reportedly increased levels of online censorship to coincide with the event.

Charlie Smith, co-founder of GreatFire.org, an organization that monitors Chinese government internet censorship, said that the apparent crackdown on Western news sites represented a significant new development and described it as a “censorship Black Friday.”

“This frenzied activity could indicate that the authorities are accelerating their push to sever the link between Chinese citizens and any news source that falls outside of the influence of The Party,” said Smith, referencing the ruling Communist Party regime.

For years, China has blocked several Western news organizations after they have published stories that reflect negatively on the government. The New York Times, Bloomberg, the Wall Street Journal, and Reuters have all previously been censored, rendering their websites inaccessible in the country.

China operates an internet censorship system known as the Great Firewall, which uses filtering equipment to stop people in the country from accessing content published on banned websites that are operated outside China’s borders.

It is possible to circumvent the censorship using tools such as a virtual private network, or VPN. However, use of technology that bypasses the Great Firewall is banned — and people in the country who sell access to these services have been jailed.
THE MESSAGES ARRIVED suddenly and then he went quiet. “My identity is leaked,” he said. “I am worried about my safety.”

The Chinese dissident artist Badiucao had been busy preparing an exhibition in Hong Kong to celebrate Free Expression Week, a series of events organized by rights groups. His show was partly inspired by Google’s plan to build a censored search engine in China, and was set to include work that the artist had created skewering the U.S. tech giant for cooperating with the Communist Party regime’s suppression of internet freedom.

But just days before the exhibition was set to launch last year, at a high-profile event featuring members of Russian punk-activist group Pussy Riot, it was canceled by organizers. Badiucao had received threats from the Chinese government and soon went into hiding.

It was a nightmare scenario for the artist, one of China’s most prolific political satirists, who has never revealed his real name. Somehow, police in China had discovered who he was — and they were trying to track him down.
IN A FEDERAL lawsuit, the tech giant Oracle has provided new details to support its accusation that Amazon secretly negotiated a job offer with a then-Department of Defense official who helped shape the procurement process for a massive federal contract for which Amazon was a key bidder.

Amazon Web Services and Microsoft are now the two finalists to win the highly contested $10 billion contract for what is known as the Joint Enterprise Defense Infrastructure, or JEDI. The deal, one of the largest federal contracts in U.S. history, would pay one company to provide cloud computing services in support of Defense Department operations around the world.

But the contract has been hotly contested since the department began soliciting proposals last year. Two of Amazon’s competitors, IBM and Oracle, filed complaints with the Government Accountability Office saying that the winner-take-all process unfairly favored Amazon, which is seen as an industry leader in cloud computing. When its claim was rejected, Oracle sued the government in the U.S. Court of Federal Claims.

Since the court battle began in 2018, Oracle has aggressively lodged conflict-of-interest accusations involving a former DOD official named Deap Ubhi, who left the department in 2017 to take a job at Amazon. In a court motion filed on Friday, Oracle alleged that while Ubhi worked on the preliminary research for the JEDI program in the late summer and fall of 2017, he was also engaged in a secret job negotiation with Amazon for months, complete with salary discussions, offers of signing bonuses, and lucrative stock options.

The motion further alleges that Ubhi did not recuse himself from the JEDI program until weeks after verbally accepting a job offer from Amazon and that he continued to receive information about Amazon’s competitors and participate in meetings about technical requirements, despite a government regulation that forbids such conflicts of interest.

“Neither Ubhi nor [Amazon Web Services] disclosed the employment discussions or job offer to DOD — not when the employment discussions started, not when the informal job offer occurred, not when the formal offer occurred, and not even when Ubhi accepted the offer,” Oracle’s motion reads.

As America’s technology companies have continued to outpace the Pentagon, the Defense Department has looked to recruit talent from Silicon Valley to help enhance its information technology.

Ubhi is a venture capitalist and technology entrepreneur who worked for Amazon before his time in government. He took a job working on a Defense Department initiative aimed at collaborating with Silicon Valley to modernize the Pentagon’s information technology systems. After working as part of a four-person team to help shape the Pentagon JEDI procurement process, he left the department and returned to Amazon in November 2017.

A spokesperson for Amazon Web Services declined to comment and declined to make Ubhi available for an interview, citing ongoing litigation. Elissa Smith, a spokesperson for the Department of Defense, also told The Intercept that “we don’t comment on pending litigation.”

In a previous court filing, U.S. government lawyers accused Oracle of a “broad fishing expedition primarily [intended] to find support for its claim that the solicitation at issue is tainted by alleged conflicts of interest.”

According to Oracle’s motion on Friday, Ubhi began job negotiations with Amazon in August 2017, while he was working on the early stages of the JEDI program. Oracle claims says that “deep discussions” about employment began in late September and that Ubhi “verbally committed” to take the job on October 4. But according to the filing, Ubhi did not recuse himself until October 31, 2017. Oracle alleges that he continued to influence the program in the meantime.

Under the Procurement Integrity Act, government officials who are “contacted by a [contract] bidder about non-federal employment” have two options: They must either report the contact and reject the offer of employment or promptly recuse themselves from any contract proceedings.

“Contracts should be awarded fairly based on merit,” Mandy Smithberger, director of the Center for Defense Information at the Project on Government Oversight, told The Intercept. “The Procurement Integrity Act seeks to ensure that job offers and other financial conflicts of interest don’t influence that process.”

Last year, a Defense Department review found that “there were four instances where [department] individuals with potential financial conflicts of interest” had worked on the JEDI program, according to court records, but the Pentagon concluded that this hadn’t unfairly impacted the contracting process. Two follow-up reviews — one by the GAO in November 2018 and another by the Defense Department in April 2019 — came to similar conclusions.

The second Pentagon review came after the department said that it had received “new information” about Ubhi and would investigate it. According to Oracle’s motion on Friday, the “new information” came from a “belated submission from [Amazon]” to the DOD’s contracting officer that finally acknowledged the monthslong employment talks.

According to Oracle, Ubhi provided a “false narrative” to the contracting officer at the time of his recusal, saying that he was stepping away from the project because Amazon had offered to acquire a company that Ubhi had a stake in. That was a pretext to mask the fact he had been negotiating for months to obtain a job at the company, Oracle’s filing said.

The filing also alleges that between Ubhi’s verbal commitment to accept Amazon’s offer and his recusal from JEDI, he continued to participate in Pentagon meetings about the project’s technical requirements and to receive submissions from Amazon competitors. It also alleges that Ubhi downloaded material from a JEDI project Google Drive to his own laptop.

In its filings, Oracle has argued that Ubhi was instrumental in persuading the Pentagon to seek services from a single vendor — a decision widely seen to improve Amazon’s chances. Oracle cites workplace messages on the platform Slack in which Ubhi tries to persuade his colleagues to come around to that view, but the company does not cite any messages suggesting what his reasons or motive may have been.




AMONG THE MEGA-CORPORATIONS that surveil you, your cellphone carrier has always been one of the keenest monitors, in constant contact with the one small device you keep on you at almost every moment. A confidential Facebook document reviewed by The Intercept shows that the social network courts carriers, along with phone makers — some 100 different companies in 50 countries — by offering the use of even more surveillance data, pulled straight from your smartphone by Facebook itself.

Offered to select Facebook partners, the data includes not just technical information about Facebook members’ devices and use of Wi-Fi and cellular networks, but also their past locations, interests, and even their social groups. This data is sourced not just from the company’s main iOS and Android apps, but from Instagram and Messenger as well. The data has been used by Facebook partners to assess their standing against competitors, including customers lost to and won from them, but also for more controversial uses like racially targeted ads.

Some experts are particularly alarmed that Facebook has marketed the use of the information — and appears to have helped directly facilitate its use, along with other Facebook data — for the purpose of screening customers on the basis of likely creditworthiness. Such use could potentially run afoul of federal law, which tightly governs credit assessments.

Facebook said it does not provide creditworthiness services and that the data it provides to cellphone carriers and makers does not go beyond what it was already collecting for other uses.

Facebook’s cellphone partnerships are particularly worrisome because of the extensive surveillance powers already enjoyed by carriers like AT&T and T-Mobile: Just as your internet service provider is capable of watching the data that bounces between your home and the wider world, telecommunications companies have a privileged vantage point from which they can glean a great deal of information about how, when, and where you’re using your phone. AT&T, for example, states plainly in its privacy policy that it collects and stores information “about the websites you visit and the mobile applications you use on our networks.” Paired with carriers’ calling and texting oversight, that accounts for just about everything you’d do on your smartphone.

An Inside Look at “Actionable Insights”

You’d think that degree of continuous monitoring would be more than sufficient for a communications mammoth to operate its business — and perhaps for a while it was. But Facebook’s “Actionable Insights,” a corporate data-sharing program, suggests that even the incredible visibility telecoms have into your daily life isn’t enough — and Zuckerberg et al. can do them one better. Actionable Insights was announced last year in an innocuous, easy-to-miss post on Facebook’s engineering blog. The article, titled “Announcing tools to help partners improve connectivity,” strongly suggested that the program was primarily aimed at solving weak cellular data connections around the world. “To address this problem,” the post began, “we are building a diverse set of technologies, products, and partnerships designed to expand the boundaries of existing connectivity quality and performance, catalyze new market segments, and bring better access to the unconnected.” What sort of monster would stand against better access for the unconnected?

The blog post makes only a brief mention of Actionable Insights’ second, less altruistic purpose: “enabling better business decisions” through “analytics tools.” According to materials reviewed by The Intercept and a source directly familiar with the program, the real boon of Actionable Insights lies not in its ability to fix spotty connections, but to help chosen corporations use your personal data to buy more tightly targeted advertising.

The source, who discussed Actionable Insights on the condition of anonymity because they were not permitted to speak to the press, explained that Facebook has offered the service to carriers and phone makers ostensibly of free charge, with access to Actionable Insights granted as a sweetener for advertising relationships. According to the source, the underlying value of granting such gratis access to Actionable Insights in these cases isn’t simply to help better service cell customers with weak signals, but also to ensure that telecoms and phone makers keep buying  more and more carefully targeted Facebook ads. It’s exactly this sort of quasi-transactional data access that’s become a hallmark of Facebook’s business, allowing the company to plausibly deny that it ever sells your data while still leveraging it for revenue. Facebook may not be “selling” data through Actionable Insights in the most baldly literal sense of the word — there’s no briefcase filled with hard drives being swapped for one containing cash — but the relationship based on spending and monetization certainly fits the spirit of a sale. A Facebook spokesperson declined to answer whether the company charges for Actionable Insights access.

The confidential Facebook document provides an overview of Actionable Insights and espouses its benefits to potential corporate users. It shows how the program, ostensibly created to help improve underserved cellular customers, is pulling in far more data than how many bars you’re getting. According to one portion of the presentation, the Facebook mobile app harvests and packages eight different categories of information for use by over 100 different telecom companies in over 50 different countries around the world, including usage data from the phones of children as young as 13. These categories include use of video, demographics, location, use of Wi-Fi and cellular networks, personal interests, device information, and friend homophily, an academic term of art. A 2017 article on social media friendship from the Journal of the Society of Multivariate Experimental Psychology defined “homophily” in this context as “the tendency of nodes to form relations with those who are similar to themselves.” In other words, Facebook is using your phone to not only provide behavioral data about you to cellphone carriers, but about your friends as well.

From these eight categories alone, a third party could learn an extraordinary amount about patterns of users’ daily life, and although the document claims that the data collected through the program is “aggregated and anonymized,” academic studies have found time and again that so-called anonymized user data can be easily de-anonymized. Today, such claims of anonymization and aggregation are essentially boilerplate from companies who wager you’ll be comfortable with them possessing a mammoth trove of personal observations and behavioral predictions about your past and future if the underlying data is sufficiently neutered and grouped with your neighbor’s.

A Facebook spokesperson told The Intercept that Actionable Insights doesn’t collect any data from user devices that wasn’t already being collected anyway. Rather, this spokesperson said Actionable Insights repackages the data in novel ways useful to third-party advertisers in the telecom and smartphone industries.

Material reviewed by The Intercept show demographic information presented in a dashboard-style view, with maps showing customer locations at the county and city level. A Facebook spokesperson said they “didn’t think it goes more specific than zip code.” But armed with location data beamed straight from your phone, Facebook could technically provide customer location accurate to a range of several meters, indoors or out.

Targeting By Race and Likely Creditworthiness

Despite Facebook’s repeated assurances that user information is completely anonymized and aggregated, the Actionable Insights materials undermine this claim. One Actionable Insights case study from the overview document promotes how an unnamed North American cellular carrier had previously used its Actionable Insights access to target a specific, unnamed racial group. Facebook’s targeting of “multicultural affinity groups,” as the company formerly referred to race, was discontinued in 2017 after the targeting practice was widely criticized as potentially discriminatory.

Another case study described how Actionable Insights can be used to single out individual customers on the basis of creditworthiness. In this example, Facebook explained how one of its advertising clients, based outside the U.S., wanted to exclude individuals from future promotional offers on the basis of their credit. Using data provided through Actionable Insights, a Data Science Strategist, a role for which Facebook continues to hire, was able to generate profiles of customers with desirable and undesirable credit standings. The advertising client then used these profiles to target or exclude Facebook users who resembled these profiles.
AMONG THE MEGA-CORPORATIONS that surveil you, your cellphone carrier has always been one of the keenest monitors, in constant contact with the one small device you keep on you at almost every moment. A confidential Facebook document reviewed by The Intercept shows that the social network courts carriers, along with phone makers — some 100 different companies in 50 countries — by offering the use of even more surveillance data, pulled straight from your smartphone by Facebook itself.

Offered to select Facebook partners, the data includes not just technical information about Facebook members’ devices and use of Wi-Fi and cellular networks, but also their past locations, interests, and even their social groups. This data is sourced not just from the company’s main iOS and Android apps, but from Instagram and Messenger as well. The data has been used by Facebook partners to assess their standing against competitors, including customers lost to and won from them, but also for more controversial uses like racially targeted ads.

Some experts are particularly alarmed that Facebook has marketed the use of the information — and appears to have helped directly facilitate its use, along with other Facebook data — for the purpose of screening customers on the basis of likely creditworthiness. Such use could potentially run afoul of federal law, which tightly governs credit assessments.

Facebook said it does not provide creditworthiness services and that the data it provides to cellphone carriers and makers does not go beyond what it was already collecting for other uses.

Facebook’s cellphone partnerships are particularly worrisome because of the extensive surveillance powers already enjoyed by carriers like AT&T and T-Mobile: Just as your internet service provider is capable of watching the data that bounces between your home and the wider world, telecommunications companies have a privileged vantage point from which they can glean a great deal of information about how, when, and where you’re using your phone. AT&T, for example, states plainly in its privacy policy that it collects and stores information “about the websites you visit and the mobile applications you use on our networks.” Paired with carriers’ calling and texting oversight, that accounts for just about everything you’d do on your smartphone.

An Inside Look at “Actionable Insights”

You’d think that degree of continuous monitoring would be more than sufficient for a communications mammoth to operate its business — and perhaps for a while it was. But Facebook’s “Actionable Insights,” a corporate data-sharing program, suggests that even the incredible visibility telecoms have into your daily life isn’t enough — and Zuckerberg et al. can do them one better. Actionable Insights was announced last year in an innocuous, easy-to-miss post on Facebook’s engineering blog. The article, titled “Announcing tools to help partners improve connectivity,” strongly suggested that the program was primarily aimed at solving weak cellular data connections around the world. “To address this problem,” the post began, “we are building a diverse set of technologies, products, and partnerships designed to expand the boundaries of existing connectivity quality and performance, catalyze new market segments, and bring better access to the unconnected.” What sort of monster would stand against better access for the unconnected?

The blog post makes only a brief mention of Actionable Insights’ second, less altruistic purpose: “enabling better business decisions” through “analytics tools.” According to materials reviewed by The Intercept and a source directly familiar with the program, the real boon of Actionable Insights lies not in its ability to fix spotty connections, but to help chosen corporations use your personal data to buy more tightly targeted advertising.

The source, who discussed Actionable Insights on the condition of anonymity because they were not permitted to speak to the press, explained that Facebook has offered the service to carriers and phone makers ostensibly of free charge, with access to Actionable Insights granted as a sweetener for advertising relationships. According to the source, the underlying value of granting such gratis access to Actionable Insights in these cases isn’t simply to help better service cell customers with weak signals, but also to ensure that telecoms and phone makers keep buying  more and more carefully targeted Facebook ads. It’s exactly this sort of quasi-transactional data access that’s become a hallmark of Facebook’s business, allowing the company to plausibly deny that it ever sells your data while still leveraging it for revenue. Facebook may not be “selling” data through Actionable Insights in the most baldly literal sense of the word — there’s no briefcase filled with hard drives being swapped for one containing cash — but the relationship based on spending and monetization certainly fits the spirit of a sale. A Facebook spokesperson declined to answer whether the company charges for Actionable Insights access.

The confidential Facebook document provides an overview of Actionable Insights and espouses its benefits to potential corporate users. It shows how the program, ostensibly created to help improve underserved cellular customers, is pulling in far more data than how many bars you’re getting. According to one portion of the presentation, the Facebook mobile app harvests and packages eight different categories of information for use by over 100 different telecom companies in over 50 different countries around the world, including usage data from the phones of children as young as 13. These categories include use of video, demographics, location, use of Wi-Fi and cellular networks, personal interests, device information, and friend homophily, an academic term of art. A 2017 article on social media friendship from the Journal of the Society of Multivariate Experimental Psychology defined “homophily” in this context as “the tendency of nodes to form relations with those who are similar to themselves.” In other words, Facebook is using your phone to not only provide behavioral data about you to cellphone carriers, but about your friends as well.

From these eight categories alone, a third party could learn an extraordinary amount about patterns of users’ daily life, and although the document claims that the data collected through the program is “aggregated and anonymized,” academic studies have found time and again that so-called anonymized user data can be easily de-anonymized. Today, such claims of anonymization and aggregation are essentially boilerplate from companies who wager you’ll be comfortable with them possessing a mammoth trove of personal observations and behavioral predictions about your past and future if the underlying data is sufficiently neutered and grouped with your neighbor’s.

A Facebook spokesperson told The Intercept that Actionable Insights doesn’t collect any data from user devices that wasn’t already being collected anyway. Rather, this spokesperson said Actionable Insights repackages the data in novel ways useful to third-party advertisers in the telecom and smartphone industries.

Material reviewed by The Intercept show demographic information presented in a dashboard-style view, with maps showing customer locations at the county and city level. A Facebook spokesperson said they “didn’t think it goes more specific than zip code.” But armed with location data beamed straight from your phone, Facebook could technically provide customer location accurate to a range of several meters, indoors or out.

Targeting By Race and Likely Creditworthiness

Despite Facebook’s repeated assurances that user information is completely anonymized and aggregated, the Actionable Insights materials undermine this claim. One Actionable Insights case study from the overview document promotes how an unnamed North American cellular carrier had previously used its Actionable Insights access to target a specific, unnamed racial group. Facebook’s targeting of “multicultural affinity groups,” as the company formerly referred to race, was discontinued in 2017 after the targeting practice was widely criticized as potentially discriminatory.

Another case study described how Actionable Insights can be used to single out individual customers on the basis of creditworthiness. In this example, Facebook explained how one of its advertising clients, based outside the U.S., wanted to exclude individuals from future promotional offers on the basis of their credit. Using data provided through Actionable Insights, a Data Science Strategist, a role for which Facebook continues to hire, was able to generate profiles of customers with desirable and undesirable credit standings. The advertising client then used these profiles to target or exclude Facebook users who resembled these profiles.
IMAGINE YOU’RE HIKING through the woods near a border. Suddenly, you hear a mechanical buzzing, like a gigantic bee. Two quadcopters have spotted you and swoop in for a closer look. Antennae on both drones and on a nearby autonomous ground vehicle pick up the radio frequencies coming from the cell phone in your pocket. They send the signals to a central server, which triangulates your exact location and feeds it back to the drones. The robots close in.

Cameras and other sensors on the machines recognize you as human and try to ascertain your intentions. Are you a threat? Are you illegally crossing a border? Do you have a gun? Are you engaging in acts of terrorism or organized crime? The machines send video feeds to their human operator, a border guard in an office miles away, who checks the videos and decides that you are not a risk. The border guard pushes a button, and the robots disengage and continue on their patrol.

This is not science fiction. The European Union is financing a project to develop drones piloted by artificial intelligence and designed to autonomously patrol Europe’s borders. The drones will operate in swarms, coordinating and corroborating information among fleets of quadcopters, small fixed-wing airplanes, ground vehicles, submarines, and boats. Developers of the project, known as Roborder, say the robots will be able to identify humans and independently decide whether they represent a threat. If they determine that you may have committed a crime, they will notify border police.

President Donald Trump has used the specter of criminals crossing the southern border to stir nationalist political sentiment and energize his base. In Europe, two years after the height of the migration crisis that brought more than a million people to the continent, mostly from the Middle East and Africa, immigration remains a hot-button issue, even as the number of new arrivals has dropped. Political parties across the European Union are winning elections on anti-immigrant platforms and enacting increasingly restrictive border policies. Tech ethicists and privacy advocates worry that Roborder and projects like it outsource too much law enforcement work to nonhuman actors and could easily be weaponized against people in border areas.
IMAGINE YOU’RE HIKING through the woods near a border. Suddenly, you hear a mechanical buzzing, like a gigantic bee. Two quadcopters have spotted you and swoop in for a closer look. Antennae on both drones and on a nearby autonomous ground vehicle pick up the radio frequencies coming from the cell phone in your pocket. They send the signals to a central server, which triangulates your exact location and feeds it back to the drones. The robots close in.

Cameras and other sensors on the machines recognize you as human and try to ascertain your intentions. Are you a threat? Are you illegally crossing a border? Do you have a gun? Are you engaging in acts of terrorism or organized crime? The machines send video feeds to their human operator, a border guard in an office miles away, who checks the videos and decides that you are not a risk. The border guard pushes a button, and the robots disengage and continue on their patrol.

This is not science fiction. The European Union is financing a project to develop drones piloted by artificial intelligence and designed to autonomously patrol Europe’s borders. The drones will operate in swarms, coordinating and corroborating information among fleets of quadcopters, small fixed-wing airplanes, ground vehicles, submarines, and boats. Developers of the project, known as Roborder, say the robots will be able to identify humans and independently decide whether they represent a threat. If they determine that you may have committed a crime, they will notify border police.

President Donald Trump has used the specter of criminals crossing the southern border to stir nationalist political sentiment and energize his base. In Europe, two years after the height of the migration crisis that brought more than a million people to the continent, mostly from the Middle East and Africa, immigration remains a hot-button issue, even as the number of new arrivals has dropped. Political parties across the European Union are winning elections on anti-immigrant platforms and enacting increasingly restrictive border policies. Tech ethicists and privacy advocates worry that Roborder and projects like it outsource too much law enforcement work to nonhuman actors and could easily be weaponized against people in border areas.
PALANTIR, THE CIA-FUNDED data analysis company founded by billionaire Trump adviser Peter Thiel, provided software at the center of a 2017 operation targeting unaccompanied children and their families, newly released Homeland Security documents show.

The documents undercut prior statements from Palantir, in which the company tried to draw a clean line between the wing of ICE devoted strictly to deportations and the enforcement of immigration laws, and its $38 million contract with Homeland Security Investigations, or HSI, a component of ICE with a far broader criminal enforcement mandate. Asked about the contract renewal by the New York Times, a Palantir spokesperson stated:

“There are two major divisions of ICE with two distinct mandates: Homeland Security Investigations, or H.S.I., is responsible for cross-border criminal investigations. The other major directorate, Enforcement and Removal Operations, or E.R.O., is responsible for interior civil immigration enforcement, including deportation and detention of undocumented immigrants. We do not work for E.R.O.”

Documents obtained through Freedom of Information Act litigation and provided to The Intercept show that this claim, that Palantir software is strictly involved in criminal investigations as opposed to deportations, is false. The discrepancy between the private intelligence firm’s public assertion and the reality conveyed in the newly-released documents was first identified by Mijente, an advocacy organization that has closely tracked Palantir’s murky role in immigration enforcement. Far from detached support in “cross-border criminal investigations,” the materials released this week confirm the role Palantir technology played in facilitating hundreds of arrests, only a small fraction of which led to criminal prosecutions.
DURING A GROUP DINNER in a small town in Norway in 2015, at an international conference for investigative journalists, a Ukrainian reporter told me that he used both Gmail and Mail.ru, Russia’s most popular email provider. “Every time I write an email,” he said, “I have to decide if I want Obama to read it, or if I want Putin to read it.”

It may be hyperbolic to suggest that world leaders personally comb through individual email accounts, but the reporter’s point stands: When you use services like Gmail, Mail.ru, Facebook, Dropbox, Slack, or any other site that stores your data, they will hand your private information to governments when compelled to do so and in some cases, merely when asked. Last year, the Supreme Court ruled that the government usually needs a warrant to access private data held by third-party companies. But even with new legal protection, email remains all too easy for governments to quietly obtain. Many companies, like Facebook, have shared personal information even more widely, with private entities. When your personal data is stored on a company’s servers, as with the email in your Gmail account, there are no technical barriers to the host company sharing it when it sees fit.

Google provided private information to government agencies around the world more than 60,000 times in 2017, often turning over data from multiple Google accounts at once, according to its transparency report. And that doesn’t include over 100,000 Google accounts from which the company gave data in response to secret orders from the Foreign Intelligence Surveillance Court, a U.S. national security tribunal whose meetings and decisions are kept from the public. Mail.ru doesn’t provide a transparency report, but the situation is no doubt much worse in Russia: All Russian internet companies are required to retain data they collect about their users and to hand it to FSB, a Russian spy agency, if asked.
THE BERNIE SANDERS campaign kicked off its massive volunteer program this weekend by holding nearly 5,000 house parties across the country and unveiling a new organizing app that gives campaign supporters a way to share political information on friends, family, and neighbors. 

Sanders’s strategy to emerge from the crowded primary field revolves around energizing and empowering his army of supporters, and giving them easy-to-use tools in the hopes of expanding the electoral map in both the primary and general elections. More than 60,000 people attended the events, which took place in every state and more than 30 countries outside the U.S., according to the campaign.

Sanders, along with campaign manager Faiz Shakir and campaign co-chair Nina Turner, addressed supporters through a pre-recorded broadcast that was streamed at the parties. “So let’s do it, let’s run a historic grassroots campaign,” Sanders told supporters. “And when we do that, the 1 percent can spend all of the money that they want. We’re gonna beat them.”

The campaign’s new organizing tool, called BERN, helps volunteers track potential supporters and voters, allowing them to log the name and background of anyone they talk to, from friends and family members to a stranger on the street. The app will also help volunteers know how to participate in the Democratic primary or caucus in their state and register voters.

On friend-to-friend mode, supporters are asked to add the name, city, and state of everyone they know, information that is then matched to their voter record. The app also asks about the person’s level of support, union membership, and other candidates they might vote for.

Some critics have called the app invasive, arguing that the database of personal information could open non-supporters up to harassment. Though much of the information the app requests is publicly available, critics say that having the data neatly compiled — while not giving people a way to opt out of it — presents safety concerns.

The skepticism appears rooted in (hostility to Sanders and) a basic lack of familiarity with how campaigns work. Voter rolls are public, and the Democratic Party has long been aggregating additional information about voters to aid with fundraising and turnout operations, data that all major campaigns have access to. The difference is that the Sanders app democratizes the process with the goal of expanding the electorate, while the party operations are aimed at identifying existing supporters so they can be motivated to vote. The party data is generally available to campaign volunteers, but because Sanders lowers the bar to volunteering, more people will now have access to the data. The goal, though, is to get more people to vote for Sanders, not to attack Sanders opponents.

To that end, they’ll be relying heavily on supporters.“We don’t think, in the national office, that we have all of the answers,” Sanders said. “Trust me, we don’t. Every person out there knows your own community better than we do. Can you put on a concert, can you have a potluck event? Whatever it may be, bring people together. Develop a sense of community, reach out to people who might feel uncomfortable about being involved in politics.”

Sanders has a list of 1.1 million people who’ve pledged to volunteer so far, meaning roughly 6 percent showed up to a house party over the weekend. Sanders told them that the goal is to have volunteers engaging on social media in addition to the “old-fashioned stuff,” like knocking on doors and handing out literature. Unlike the typical political campaign, where volunteers work under the supervision of paid campaign staff, Sanders volunteers will be given the tools to help grow the movement at an exponential scale, free of the restraints of traditional top-down campaigns.

“And remember,” Sanders said. “It’s not Bernie! It’s us! Don’t forget that: Us! Us! Us!”

Sanders supporters, with and without previous organizing experience, gathered this weekend in libraries, living rooms, restaurants, and classrooms. They wore Bernie shirts, made Bernie signs, Bernie cookies, and Bernie cakes. Some groups even received a surprise phone call from the candidate himself.

In Oakland, Bay Area Muslims for Bernie held its party at a local Palestinian street food restaurant. A group of around 25 people, which included supporters from Egypt, Iran, Morocco, Yemen, Afghanistan, and around the United States, joined the organizing kickoff. “We even had refugees attend who cannot vote but still wanted to support and promote Bernie’s message,” Reyhaneh Rajabzadeh told The Intercept in a message.
AFTER YEARS OF ignoring the issue, lawmakers on Capitol Hill are suddenly engaged in a furious fight over enacting national legislation to establish basic online privacy rights for consumers. As with the crafting of much legislation dealing with complicated issues, legislators are relying on experts to help codify the consumer protections.

In a twist that is all too familiar in Washington, D.C., however, many of the groups that have positioned themselves as expert voices on consumer privacy are pushing for a bill that hews closely to tech industry interests. Lawmakers who are famously ignorant on technology issues are hearing largely from an army of industry lobbyists and experts funded by social media companies, online platforms, data brokers, advertisers, and telecommunication giants — the very same corporate interests that profit from the collection and sale of internet data.

Take the Center for Democracy and Technology, one of the most prominent privacy-centered Beltway think tanks. The group is considered to be well-respected among congressional staffers, routinely testifies before committees on privacy legislation, and is a prime mover in the national online privacy bill discussion.

Late last year, the organization circulated draft federal privacy legislation that would nullify major state-level regulations. In March, when the Senate Judiciary Committee held its first hearing of the session on how to formulate a federal consumer privacy standard, the center’s Privacy and Data Project Director Michelle Richardson testified.

The Center for Democracy and Technology is also awash in corporate money from the tech sector. Amazon, Verizon, and Google are among the corporate donors that each provide over $200,000 to the group. AT&T, Uber, and Twitter are also major donors.

Last Wednesday, the group hosted its annual gala, known as “Tech Prom,” which brought together lobbyists and government affairs officials from leading Silicon Valley and telecom firms. Facebook, Google, Amazon, and Microsoft purchased tables at the event and served as sponsors, a privilege that came in exchange for a $35,000 donation to the center.
IT IS A Chinese state-owned company that is implicated in disturbing human rights violations. But that has not stopped Hikvision from gaining a major foothold in the United Kingdom. Through a network of corporate partners, the Hangzhou-based security firm has supplied its surveillance cameras for use on the British parliamentary estate, as well as to police, hospitals, schools, and universities throughout the country, according to sources and procurement records.

Hikvision, whose technology the U.S. government recently banned federal agencies from purchasing, is generating millions of dollars in annual revenue selling its technology to British companies and organizations. At the same time, it has been helping to establish an oppressive surveillance state in the Xinjiang region of China, where the Uighur ethnic minorities have been held in secret internment camps.

British politicians are raising concerns about the technology — and are calling for an embargo on further purchases of it — on the grounds that Hikvision is complicit in human rights abuses and also represents a national security risk, as it is feared that Chinese intelligence agencies could potentially tap into camera feeds in sensitive locations. Some of the company’s cameras record audio and are connected to the internet, meaning that they can be monitored from anywhere in the world.

In January, the cameras were scheduled to be installed inside London’s Portcullis House, according to Adm. Lord Alan West, a member of the U.K. Parliament’s second chamber, the House of Lords. Portcullis House is an office building in Westminster used by more than 200 members of Parliament and 400 of their staff to carry out their daily work, which routinely involves discussion of confidential national security, economic, and foreign policy issues.

West told The Intercept that someone who was “concerned that this was happening” tipped him off about a contract that would equip the building with Hikvision surveillance equipment. He said he subsequently complained about the matter to authorities within the parliamentary estate.

“It seems to me to be extremely worrying — it’s rather like being able to get a Mata Hari into each office,” he said, referring to the Dutch exotic dancer who was accused of spying for Germany during World War I. “Are we sure we are happy with Chinese CCTV in members of Parliament’s offices, listening to what they say to their constituents, listening to what ministers say, filming the documents on their desks?”

A Parliament spokesperson denied the existence of a contract involving Hikvision and said that there was no plan to “install any additional cameras at Portcullis House this year.”

A source familiar with security on parts of the parliamentary estate, which, in addition to Portcullis House, consists of the Palace of Westminster, the Norman Shaw buildings, and Big Ben, told The Intercept that Hikvision’s equipment had “absolutely” been used there in the past. The source said they could not confirm whether any Hikvision cameras were currently active, as there are hundreds of cameras fitted both in and around all parliamentary and government buildings in the area.
HOW EXACTLY FACEBOOK decides who sees what is one of the great pieces of forbidden knowledge in the information age, hidden away behind nondisclosure agreements, trade secrecy law, and a general culture of opacity. New research from experts at Northeastern University, the University of Southern California, and the public-interest advocacy group Upturn doesn’t reveal how Facebook’s targeting algorithms work, but does show an alarming outcome: They appear to deliver certain ads, including for housing and employment, in a way that aligns with race and gender stereotypes — even when advertisers ask for the ads to be exposed a broad, inclusive audience.

There are two basic steps to advertising on Facebook. The first is taken by advertisers when they choose certain segments of the Facebook population to target: Canadian women who enjoy badminton and Weezer, lacrosse dads over 40 with an interest in white genocide, and so forth. The second is taken by Facebook, when it makes an ad show up on certain peoples’ screens, reconciling the advertiser’s targeting preferences with the flow of people through Facebook’s apps and webpages in a given period of time. Advertisers can see which audiences ended up viewing the ad, but are never permitted to know the underlying logic of how those precise audiences were selected.

The new research focuses on the second step of advertising on Facebook, the process of ad delivery, rather than on ad targeting. Essentially, the researchers created ads without any demographic target at all and watched where Facebook placed them. The results, said the researchers, were disturbing:

Critically, we observe significant skew in delivery along gender and racial lines for “real” ads for employment and housing opportunities despite neutral targeting parameters. Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive.

Rather than targeting a demographic niche, the researchers requested only that their ads reach Facebook users in the United States, leaving matters of ethnicity and gender entirely up to Facebook’s black box. As Facebook itself tells potential advertisers, “We try to show people the ads that are most pertinent to them.” What exactly does the company’s ad-targeting black box, left to its own devices, consider pertinent? Are Facebook’s ad-serving algorithms as prone to bias like so many others? The answer will not surprise you.

For one portion of the study, researchers ran ads for a wide variety of job listings in North Carolina, from janitors to nurses to lawyers, without any further demographic targeting options. With all other things being equal, the study found that “Facebook delivered our ads for jobs in the lumber industry to an audience that was 72% white and 90% men, supermarket cashier positions to an audience of 85% women, and jobs with taxi companies to a 75% black audience even though the target audience we specified was identical for all ads.” Ad displays for “artificial intelligence developer” listings also skewed white, while listings for secretarial work overwhelmingly found their way to female Facebook users.

Although Facebook doesn’t permit advertisers to view the racial composition of an ad’s viewers, the researchers said they were able to confidently infer these numbers by cross-referencing the indicators Facebook does provide, particularly regions where users live, which in some states can be cross-referenced with race data held in voter registration records.

In the case of housing ads — an area Facebook has already shown in the past has potential for discriminatory abuse — the results were also heavily skewed along racial lines. “In our experiments,” the researchers wrote, “Facebook delivered our broadly targeted ads for houses for sale to audiences of 75% white users, when ads for rentals were shown to a more demographically balanced audience.” In other cases, the study found that “Facebook delivered some of our housing ads to audiences of over 85% white users while they delivered other ads to over 65% Black users (depending on the content of the ad) even though the ads were targeted identically.”

Facebook appeared to algorithmically reinforce stereotypes even in the case of simple, rather boring stock photos, indicating that not only does Facebook automatically scan and classify images on the site as being more “relevant” to men or women, but changes who sees the ad based on whether it includes a picture of, say, a football or a flower. The research took a selection of stereotypically gendered images — a military scene and an MMA fight on the stereotypically male side, a rose as stereotypically female — and altered them so that they would be invisible to the human eye (marking the images as transparent “alpha” channels, in technical terms). They then used these invisible pictures in ads run without any gender-based targeting, yet found Facebook, presumably after analyzing the images with software, made retrograde, gender-based decisions on how to deliver them: Ads with stereotypical macho images were shown mostly to men, even though the men had no idea what they were looking at. The study concluded that “Facebook has an automated image classification mechanism in place that is used to steer different ads towards different subsets of the user population.” In other words, the bias was on Facebook’s end, not in the eye of the beholder.
HOW EXACTLY FACEBOOK decides who sees what is one of the great pieces of forbidden knowledge in the information age, hidden away behind nondisclosure agreements, trade secrecy law, and a general culture of opacity. New research from experts at Northeastern University, the University of Southern California, and the public-interest advocacy group Upturn doesn’t reveal how Facebook’s targeting algorithms work, but does show an alarming outcome: They appear to deliver certain ads, including for housing and employment, in a way that aligns with race and gender stereotypes — even when advertisers ask for the ads to be exposed a broad, inclusive audience.

There are two basic steps to advertising on Facebook. The first is taken by advertisers when they choose certain segments of the Facebook population to target: Canadian women who enjoy badminton and Weezer, lacrosse dads over 40 with an interest in white genocide, and so forth. The second is taken by Facebook, when it makes an ad show up on certain peoples’ screens, reconciling the advertiser’s targeting preferences with the flow of people through Facebook’s apps and webpages in a given period of time. Advertisers can see which audiences ended up viewing the ad, but are never permitted to know the underlying logic of how those precise audiences were selected.

The new research focuses on the second step of advertising on Facebook, the process of ad delivery, rather than on ad targeting. Essentially, the researchers created ads without any demographic target at all and watched where Facebook placed them. The results, said the researchers, were disturbing:

Critically, we observe significant skew in delivery along gender and racial lines for “real” ads for employment and housing opportunities despite neutral targeting parameters. Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive.

Rather than targeting a demographic niche, the researchers requested only that their ads reach Facebook users in the United States, leaving matters of ethnicity and gender entirely up to Facebook’s black box. As Facebook itself tells potential advertisers, “We try to show people the ads that are most pertinent to them.” What exactly does the company’s ad-targeting black box, left to its own devices, consider pertinent? Are Facebook’s ad-serving algorithms as prone to bias like so many others? The answer will not surprise you.

For one portion of the study, researchers ran ads for a wide variety of job listings in North Carolina, from janitors to nurses to lawyers, without any further demographic targeting options. With all other things being equal, the study found that “Facebook delivered our ads for jobs in the lumber industry to an audience that was 72% white and 90% men, supermarket cashier positions to an audience of 85% women, and jobs with taxi companies to a 75% black audience even though the target audience we specified was identical for all ads.” Ad displays for “artificial intelligence developer” listings also skewed white, while listings for secretarial work overwhelmingly found their way to female Facebook users.

Although Facebook doesn’t permit advertisers to view the racial composition of an ad’s viewers, the researchers said they were able to confidently infer these numbers by cross-referencing the indicators Facebook does provide, particularly regions where users live, which in some states can be cross-referenced with race data held in voter registration records.

In the case of housing ads — an area Facebook has already shown in the past has potential for discriminatory abuse — the results were also heavily skewed along racial lines. “In our experiments,” the researchers wrote, “Facebook delivered our broadly targeted ads for houses for sale to audiences of 75% white users, when ads for rentals were shown to a more demographically balanced audience.” In other cases, the study found that “Facebook delivered some of our housing ads to audiences of over 85% white users while they delivered other ads to over 65% Black users (depending on the content of the ad) even though the ads were targeted identically.”

Facebook appeared to algorithmically reinforce stereotypes even in the case of simple, rather boring stock photos, indicating that not only does Facebook automatically scan and classify images on the site as being more “relevant” to men or women, but changes who sees the ad based on whether it includes a picture of, say, a football or a flower. The research took a selection of stereotypically gendered images — a military scene and an MMA fight on the stereotypically male side, a rose as stereotypically female — and altered them so that they would be invisible to the human eye (marking the images as transparent “alpha” channels, in technical terms). They then used these invisible pictures in ads run without any gender-based targeting, yet found Facebook, presumably after analyzing the images with software, made retrograde, gender-based decisions on how to deliver them: Ads with stereotypical macho images were shown mostly to men, even though the men had no idea what they were looking at. The study concluded that “Facebook has an automated image classification mechanism in place that is used to steer different ads towards different subsets of the user population.” In other words, the bias was on Facebook’s end, not in the eye of the beholder.
SEN. ELIZABETH WARREN’S plan to break up tech giants Amazon, Google, Facebook, and Apple has given concentrated corporate power its most prominent political platform since the 1912 presidential election — and we’re still nearly a year away from the first round of primary voting. This tracks with the rising awareness of the corrosiveness of monopoly power generally and those tech giants specifically.

Whether such policy boldness means anything in a brand-obsessed political landscape will be determined when ballots are cast. But it is undeniably driving a policy discussion that the next Democratic presidential nominee, no matter who it is, will likely take up. In that context, the debate over Warren’s plan is critical, as it prefigures the trajectory of each and every challenge to corporate dominance.

First, many critiques will come from those with a direct stake in the outcome — in this case, Big Tech-funded individuals or organizations, which are so ubiquitous as to create an echo chamber. Second, the critiques will highlight the “radical” nature of the changes, setting them at odds with American history, even though Warren’s central proposal — to structurally separate business lines in an effort to eliminate anti-competitive conduct and foster competition — has a century-old pedigree. And third, we’ll be assured that the cure is worse than the disease, that Warren’s ideas would destroy everything from online shopping to the smartphone, a perspective that relies on deliberate misinterpretation.

This roadmap for discrediting policy solutions that confront power should be easy enough to spot by now, and will be employed long into the future. So it’s worth breaking down how it works.

The Manufacturing of Dissent

“The issue is not the size and current market dominance of these [tech] companies,” wrote the American Enterprise Institute’s Michael Strain for Bloomberg, in response to the Warren plan. “If anything, politicians should be celebrating these companies as crown jewels of the U.S. economy.”

Strain’s employer, AEI, is funded in part by Google, according to the company’s transparency page. This is not noted in Strain’s Bloomberg op-ed. But AEI and its writers have done several critical pieces about Warren’s proposal, as well a California privacy regulation that also imposes stricter rules on Big Tech. All of these opinion articles indirectly benefit one of AEI’s donors.

The episode points to a significant trend of writers and scholars opining on the Warren plan while conflicted by the overwhelming amounts of Big Tech cash that have infested Washington. Google’s list of organizations to whom it has donated is massive, and combined with Facebook and Amazon’s dominance of Washington, it’s hard to find anyone with a critical eye toward Big Tech regulation who doesn’t have something to disclose.

Rich Lowry of National Review unleashed a pack of industry talking points to explain how Big Tech “helps create a strong American society.” National Review takes Google money. Here’s a similar sentiment dragging the Warren plan on the pages of National Review, from a senior fellow at the Competitive Enterprise Institute, which also takes Google money. The American Action Forum seems to dislike the Warren plan; the group, well, takes Google money.

Geoffrey Manne and Alec Stapp condemn Warren for “wanting to turn the Internet into a sewer.” Manne’s organization, the International Center for Law and Economics, has taken a boatload of Google money; as of 2015, he had contributed to at least eight white papers commissioned or funded by Google that endorsed Google’s policy positions, in addition to being a frequent pro-Google commentator in news articles and congressional testimony. Stapp, before hooking up with Manne at ICLE, worked at the Mercatus Center at George Mason University, another recipient of Google funds. A former Manne co-author, Joshua Wright, worked at George Mason University and has been periodically on and off the Google payroll in between government work.

Manne and Stapp’s piece got the pile-on treatment on Twitter from representatives of the Google-funded Cato Institute and Niskanen Center; Stapp previously worked at Niskanen. Several venture capitalists who currently rely on Big Tech for exit strategies for their companies also gave the thumbs-up to the piece.

The Computer and Communications Industry Association, a trade group that includes Amazon, Facebook, and Google among its members, uses a subsidiary named Springboard to hurl critiques at regulatory tech policies. In addition to the aforementioned articles from AEI and National Review, Springboard points to the opinions of a partner at Andreessen Horowitz, an early investor in Facebook, and the CCIA’s own vice president for Law an Policy — which amounts to CCIA linking to itself as outside confirmation of its beliefs.

These linkages are virtually endless and show an incestuous network of academics, think-tankers, advocacy organizations, and trade groups, all of which happen to agree on every issue important to Big Tech. The money supports extending the prominence and megaphone of these organizations, and with nearly unlimited pocketbooks, it creates the impression of a tsunami of support for the industry.

A “Radical” Idea That’s Been Around for Over a Century

The core of Warren’s plan, which for now is just a proposal on Medium rather than legislation, involves what is known as “structural separation.” Companies with over $25 billion in annual global revenue that operate platforms — connectors between people, people and advertisers, or people and merchants — would not be allowed to both own the platform and also participate as a seller on that platform. The classic example would be Amazon’s marketplace, where Amazon also operates its own line of Amazon Basics, competing with its third-party sellers. Google’s ad exchange also competes on Google with ad tech companies, and would need to be spun off. The same would go for Google’s local search, which routinely deprioritizes recommendation sites like Yelp.

The idea is that these entities get preferential treatment from the platform they own, giving Basics, Google ad tech, and Google Search an unfair advantage and extending the platform’s dominance. Only the biggest companies would have to structurally separate; smaller platforms would still have to meet a standard of fair, reasonable, and nondiscriminatory treatment for participants on the platform and its users.

This forced divestiture of tech platforms’ other business lines has been described as radical. Manne and Stapp claim it will turn the internet into your sewer service — mainly because Warren uses the word “utility” to describe regulated platforms.

Jeff Bezos didn’t come up with the idea of owning a marketplace and using it to sell your own stuff at an unfair advantage against rivals. Reading Railroad, for example, became the largest company in the world by owning the rails that carried anthracite coal, as well as the coal mines along the route. Rival coal producers that wanted to use the lines got less favorable rates, fell behind, and got swallowed up by Reading Railroad.

Congress put a stop to it in 1906 by adopting the Hepburn Act, which prevented the railroads from carrying products that they owned. This forced the Reading Railroad to divest the P&R Coal and Iron Company, the subsidiary that owned the coal mines. Warren is merely following a long history of structural separation that began when Teddy Roosevelt was president.

Theater owners were not allowed to also produce and distribute films after the Supreme Court’s Paramount decision in 1948. Television networks were prevented from owning the programming they ran in prime time, under the Financial Interest and Syndication, or “fin-syn,” rules imposed by the Federal Communications Commission in 1970. In telecom, AT&T was heavily circumscribed and restricted to common-carrier telephone service, banning the company from capitalizing on innovations from Bell Labs and forcing compulsory licensing of those patents in 1956, which created the modern electronics industry. Banks were structurally separated between investment and deposit-taking commercial lines after the Glass-Steagall reforms. Rep. David Cicilline, D-R.I., the chair of the House Judiciary’s antitrust subcommittee, has analogized a structural separation in tech as a Glass-Steagall type of rule.

These structural separations have widespread goals: diversity, financial stability, decentralization of power, and innovation. “We owe the internet to structural separation,” said Harold Feld, a senior vice president with Public Knowledge, referring to the Carterfone decisions, where the FCC allowed people to connect their own devices, like a modem, to the telephone network. “Clearly this has a long and successful history in telecom.”

Some of these restrictions, like those on banks or television stations, have been dismantled. And there are cases of companies selling products in a store while also owning the store: Kirkland products at Costco are ubiquitous, for example. But as Lina Khan, scholar and staffer for Cicilline’s antitrust subcommittee, has pointed out, the key question is whether the platform, be it brick and mortar or digital, is creating a bottleneck by privileging its own products over rivals. And there’s a lot of evidence that Amazon in particular does just that, reacting to high-selling products by creating a generic version, and down-ranking the competitor in its search. Because half of all e-commerce is sold on Amazon, competitors have few alternatives but to sell in what feels like a rigged marketplace.

India has already instituted a Warren-like rule to prevent e-commerce platforms from selling their own products on the platform. “We should go back and understand the wisdom of that kind of separation,” said Peter Carstensen, a professor emeritus at the University of Wisconsin Law School. “We would never want the interstate highway system to be owned by Walmart. It simplifies the market functions if you separate them out.”

Another benefit to structural separation is the relative ease of regulation. Instead of well-paid economists fighting it out over what constitutes anti-competitive conduct or restraint of trade, large companies simply can’t compete with rivals on their own platform, because of the threat of market power.

The Warren plan sets a rather arbitrary number of $25 billion in annual revenue as the dividing line for that power, a kind of substitute for the technocratic determination. This has angered critics: Andy Kessler at the Wall Street Journal denied that antitrust law has anything to do with bigness.

The idea that John Sherman, author of the Sherman Antitrust Act, was not concerned with bigness would come as news to Sherman, who once said, “If we will not endure a king as a political power, we should not endure a king over the production, transportation, and sale of any of the necessaries of life.” Warren’s campaign sees the $25 billion figure as a clean way to assist regulators with pinpointing market dominance. “It has the benefit of a clear rule,” said one senior campaign adviser, who was not authorized to speak on the record. “We should presume if a company with over $25 billion in revenue is operating a marketplace, it has power and leverage.”

While agnostic on the specific dividing number, Feld gave Warren’s team credit “for trying to come up with something that makes sense.” Others are not as thrilled about it. But their arguments often misconstrue the Warren plan.

Assuming the Worst

Ben Thompson, a former Apple and Microsoft analyst who writes about the business of technology, had one of the sharpest critiques of the Warren proposal, and it starts with denying Warren’s claim on the history of technology. Warren has credited the Microsoft trial for creating space for the modern tech giants to emerge, something Thompson mocks. “Bing was not even launched until 2009, eight years after the Microsoft case was settled. MSN Search, its predecessor, did launch in 1998, but with licensed search results from Inktomi and AltaVista; Microsoft didn’t launch its own web crawler until 2005.”

This view neglects the politics of the U.S. trial against Microsoft, which put a dominant company under pressure and wary of extending that dominance into the then-emerging web services arena. As Gary Reback, who represented Netscape against Microsoft in the 1990s, has often said, including to me in a 2017 interview, “The trial is the remedy.” By exposing Microsoft’s machinations to the nation, it made the company gun-shy to choke off competition, Reback argues.

“The only way to get to Google was the Microsoft browser,” he said. “Microsoft could have put up a big red sign saying this site is unsafe. It could have killed Google in the cradle, but didn’t. The reason why, and this is from Microsoft people, is they had this public trial. It wasn’t worth it as a company.”

Feld concurred that Microsoft’s behavior changed after the public spotlight of the trial, and the kind of aggressive actions to shut down competitors largely stopped. You can apply this to IBM’s antitrust issues in the 1970s and ’80s opening space for Apple, and AOL’s forced interoperability of Instant Messenger in 2001 giving room to social media. “Big companies are sensitive to this stuff; after they’ve been burned, they do generally play it safe,” Feld said, noting that big cable hasn’t had such a spotlight and they managed to crush TiVo swiftly and completely. So while Thompson focuses on specific Microsoft business decisions, he ignores the political context.

Thompson also warns that applying the structural separation standard to Apple, as Warren confirmed in an interview at South by Southwest, would lead to smartphones shipped without any applications. “Was Apple breaking the law when they shipped the first iPhone with only first-party apps?” Thompson asks. “At what point did delivering an acceptable consumer experience out-of-the-box cross the line into abusing a dominant position? This argument may make sense in theory but it makes zero sense in reality.”

This argument also has zero bearing on what Warren’s talking about. Whether Apple is unfairly tying or bundling its own apps onto its phones at purchase is a question for existing antitrust laws — it was the question in the Microsoft case, in fact. “The ordinary rules apply in that case,” said the senior Warren adviser. “The key thing we’re talking about is the marketplace.”

Contrary to what critics have claimed, Apple would not have to divest from the App Store completely under Warren’s plan, nor would the security benefits of Apple managing what goes onto its phone wither away. Apple would merely be disallowed from selling its own apps next to competing ones. This would hardly destroy Apple, largely a phone and hardware manufacturer and not primarily an app-maker. It would allow competition on the platform.

Apple does have a legitimate antitrust problem with the App Store, as Thompson acknowledges. Spotify has complained to the European Union that Apple takes a 30 percent cut from all revenues from its iPhone app, while preventing it from emailing users directly or allowing upgrades. This indirectly benefits Apple Music, Spotify says. Apple has accused Spotify of using “misleading rhetoric” in its complaint.

Spotify wants changes to Apple’s conduct on the App Store — which is not only fair game for traditional antitrust that can identify anti-competitive impositions of market power, but is also part of Warren’s plan, which mandates fair and nondiscriminatory treatment to marketplace participants. And it shows how Warren is highlighting a consumer welfare issue: If Spotify has to absorb a 30 percent transfer of revenues to Apple for use of its iPhone customers, it likely has to raise prices, and it cannot offer services like upgrades directly.
SEN. ELIZABETH WARREN’S plan to break up tech giants Amazon, Google, Facebook, and Apple has given concentrated corporate power its most prominent political platform since the 1912 presidential election — and we’re still nearly a year away from the first round of primary voting. This tracks with the rising awareness of the corrosiveness of monopoly power generally and those tech giants specifically.

Whether such policy boldness means anything in a brand-obsessed political landscape will be determined when ballots are cast. But it is undeniably driving a policy discussion that the next Democratic presidential nominee, no matter who it is, will likely take up. In that context, the debate over Warren’s plan is critical, as it prefigures the trajectory of each and every challenge to corporate dominance.

First, many critiques will come from those with a direct stake in the outcome — in this case, Big Tech-funded individuals or organizations, which are so ubiquitous as to create an echo chamber. Second, the critiques will highlight the “radical” nature of the changes, setting them at odds with American history, even though Warren’s central proposal — to structurally separate business lines in an effort to eliminate anti-competitive conduct and foster competition — has a century-old pedigree. And third, we’ll be assured that the cure is worse than the disease, that Warren’s ideas would destroy everything from online shopping to the smartphone, a perspective that relies on deliberate misinterpretation.

This roadmap for discrediting policy solutions that confront power should be easy enough to spot by now, and will be employed long into the future. So it’s worth breaking down how it works.

The Manufacturing of Dissent

“The issue is not the size and current market dominance of these [tech] companies,” wrote the American Enterprise Institute’s Michael Strain for Bloomberg, in response to the Warren plan. “If anything, politicians should be celebrating these companies as crown jewels of the U.S. economy.”

Strain’s employer, AEI, is funded in part by Google, according to the company’s transparency page. This is not noted in Strain’s Bloomberg op-ed. But AEI and its writers have done several critical pieces about Warren’s proposal, as well a California privacy regulation that also imposes stricter rules on Big Tech. All of these opinion articles indirectly benefit one of AEI’s donors.

The episode points to a significant trend of writers and scholars opining on the Warren plan while conflicted by the overwhelming amounts of Big Tech cash that have infested Washington. Google’s list of organizations to whom it has donated is massive, and combined with Facebook and Amazon’s dominance of Washington, it’s hard to find anyone with a critical eye toward Big Tech regulation who doesn’t have something to disclose.

Rich Lowry of National Review unleashed a pack of industry talking points to explain how Big Tech “helps create a strong American society.” National Review takes Google money. Here’s a similar sentiment dragging the Warren plan on the pages of National Review, from a senior fellow at the Competitive Enterprise Institute, which also takes Google money. The American Action Forum seems to dislike the Warren plan; the group, well, takes Google money.

Geoffrey Manne and Alec Stapp condemn Warren for “wanting to turn the Internet into a sewer.” Manne’s organization, the International Center for Law and Economics, has taken a boatload of Google money; as of 2015, he had contributed to at least eight white papers commissioned or funded by Google that endorsed Google’s policy positions, in addition to being a frequent pro-Google commentator in news articles and congressional testimony. Stapp, before hooking up with Manne at ICLE, worked at the Mercatus Center at George Mason University, another recipient of Google funds. A former Manne co-author, Joshua Wright, worked at George Mason University and has been periodically on and off the Google payroll in between government work.

Manne and Stapp’s piece got the pile-on treatment on Twitter from representatives of the Google-funded Cato Institute and Niskanen Center; Stapp previously worked at Niskanen. Several venture capitalists who currently rely on Big Tech for exit strategies for their companies also gave the thumbs-up to the piece.

The Computer and Communications Industry Association, a trade group that includes Amazon, Facebook, and Google among its members, uses a subsidiary named Springboard to hurl critiques at regulatory tech policies. In addition to the aforementioned articles from AEI and National Review, Springboard points to the opinions of a partner at Andreessen Horowitz, an early investor in Facebook, and the CCIA’s own vice president for Law an Policy — which amounts to CCIA linking to itself as outside confirmation of its beliefs.

These linkages are virtually endless and show an incestuous network of academics, think-tankers, advocacy organizations, and trade groups, all of which happen to agree on every issue important to Big Tech. The money supports extending the prominence and megaphone of these organizations, and with nearly unlimited pocketbooks, it creates the impression of a tsunami of support for the industry.

A “Radical” Idea That’s Been Around for Over a Century

The core of Warren’s plan, which for now is just a proposal on Medium rather than legislation, involves what is known as “structural separation.” Companies with over $25 billion in annual global revenue that operate platforms — connectors between people, people and advertisers, or people and merchants — would not be allowed to both own the platform and also participate as a seller on that platform. The classic example would be Amazon’s marketplace, where Amazon also operates its own line of Amazon Basics, competing with its third-party sellers. Google’s ad exchange also competes on Google with ad tech companies, and would need to be spun off. The same would go for Google’s local search, which routinely deprioritizes recommendation sites like Yelp.

The idea is that these entities get preferential treatment from the platform they own, giving Basics, Google ad tech, and Google Search an unfair advantage and extending the platform’s dominance. Only the biggest companies would have to structurally separate; smaller platforms would still have to meet a standard of fair, reasonable, and nondiscriminatory treatment for participants on the platform and its users.

This forced divestiture of tech platforms’ other business lines has been described as radical. Manne and Stapp claim it will turn the internet into your sewer service — mainly because Warren uses the word “utility” to describe regulated platforms.

Jeff Bezos didn’t come up with the idea of owning a marketplace and using it to sell your own stuff at an unfair advantage against rivals. Reading Railroad, for example, became the largest company in the world by owning the rails that carried anthracite coal, as well as the coal mines along the route. Rival coal producers that wanted to use the lines got less favorable rates, fell behind, and got swallowed up by Reading Railroad.

Congress put a stop to it in 1906 by adopting the Hepburn Act, which prevented the railroads from carrying products that they owned. This forced the Reading Railroad to divest the P&R Coal and Iron Company, the subsidiary that owned the coal mines. Warren is merely following a long history of structural separation that began when Teddy Roosevelt was president.

Theater owners were not allowed to also produce and distribute films after the Supreme Court’s Paramount decision in 1948. Television networks were prevented from owning the programming they ran in prime time, under the Financial Interest and Syndication, or “fin-syn,” rules imposed by the Federal Communications Commission in 1970. In telecom, AT&T was heavily circumscribed and restricted to common-carrier telephone service, banning the company from capitalizing on innovations from Bell Labs and forcing compulsory licensing of those patents in 1956, which created the modern electronics industry. Banks were structurally separated between investment and deposit-taking commercial lines after the Glass-Steagall reforms. Rep. David Cicilline, D-R.I., the chair of the House Judiciary’s antitrust subcommittee, has analogized a structural separation in tech as a Glass-Steagall type of rule.

These structural separations have widespread goals: diversity, financial stability, decentralization of power, and innovation. “We owe the internet to structural separation,” said Harold Feld, a senior vice president with Public Knowledge, referring to the Carterfone decisions, where the FCC allowed people to connect their own devices, like a modem, to the telephone network. “Clearly this has a long and successful history in telecom.”

Some of these restrictions, like those on banks or television stations, have been dismantled. And there are cases of companies selling products in a store while also owning the store: Kirkland products at Costco are ubiquitous, for example. But as Lina Khan, scholar and staffer for Cicilline’s antitrust subcommittee, has pointed out, the key question is whether the platform, be it brick and mortar or digital, is creating a bottleneck by privileging its own products over rivals. And there’s a lot of evidence that Amazon in particular does just that, reacting to high-selling products by creating a generic version, and down-ranking the competitor in its search. Because half of all e-commerce is sold on Amazon, competitors have few alternatives but to sell in what feels like a rigged marketplace.

India has already instituted a Warren-like rule to prevent e-commerce platforms from selling their own products on the platform. “We should go back and understand the wisdom of that kind of separation,” said Peter Carstensen, a professor emeritus at the University of Wisconsin Law School. “We would never want the interstate highway system to be owned by Walmart. It simplifies the market functions if you separate them out.”

Another benefit to structural separation is the relative ease of regulation. Instead of well-paid economists fighting it out over what constitutes anti-competitive conduct or restraint of trade, large companies simply can’t compete with rivals on their own platform, because of the threat of market power.

The Warren plan sets a rather arbitrary number of $25 billion in annual revenue as the dividing line for that power, a kind of substitute for the technocratic determination. This has angered critics: Andy Kessler at the Wall Street Journal denied that antitrust law has anything to do with bigness.

The idea that John Sherman, author of the Sherman Antitrust Act, was not concerned with bigness would come as news to Sherman, who once said, “If we will not endure a king as a political power, we should not endure a king over the production, transportation, and sale of any of the necessaries of life.” Warren’s campaign sees the $25 billion figure as a clean way to assist regulators with pinpointing market dominance. “It has the benefit of a clear rule,” said one senior campaign adviser, who was not authorized to speak on the record. “We should presume if a company with over $25 billion in revenue is operating a marketplace, it has power and leverage.”

While agnostic on the specific dividing number, Feld gave Warren’s team credit “for trying to come up with something that makes sense.” Others are not as thrilled about it. But their arguments often misconstrue the Warren plan.

Assuming the Worst

Ben Thompson, a former Apple and Microsoft analyst who writes about the business of technology, had one of the sharpest critiques of the Warren proposal, and it starts with denying Warren’s claim on the history of technology. Warren has credited the Microsoft trial for creating space for the modern tech giants to emerge, something Thompson mocks. “Bing was not even launched until 2009, eight years after the Microsoft case was settled. MSN Search, its predecessor, did launch in 1998, but with licensed search results from Inktomi and AltaVista; Microsoft didn’t launch its own web crawler until 2005.”

This view neglects the politics of the U.S. trial against Microsoft, which put a dominant company under pressure and wary of extending that dominance into the then-emerging web services arena. As Gary Reback, who represented Netscape against Microsoft in the 1990s, has often said, including to me in a 2017 interview, “The trial is the remedy.” By exposing Microsoft’s machinations to the nation, it made the company gun-shy to choke off competition, Reback argues.

“The only way to get to Google was the Microsoft browser,” he said. “Microsoft could have put up a big red sign saying this site is unsafe. It could have killed Google in the cradle, but didn’t. The reason why, and this is from Microsoft people, is they had this public trial. It wasn’t worth it as a company.”

Feld concurred that Microsoft’s behavior changed after the public spotlight of the trial, and the kind of aggressive actions to shut down competitors largely stopped. You can apply this to IBM’s antitrust issues in the 1970s and ’80s opening space for Apple, and AOL’s forced interoperability of Instant Messenger in 2001 giving room to social media. “Big companies are sensitive to this stuff; after they’ve been burned, they do generally play it safe,” Feld said, noting that big cable hasn’t had such a spotlight and they managed to crush TiVo swiftly and completely. So while Thompson focuses on specific Microsoft business decisions, he ignores the political context.

Thompson also warns that applying the structural separation standard to Apple, as Warren confirmed in an interview at South by Southwest, would lead to smartphones shipped without any applications. “Was Apple breaking the law when they shipped the first iPhone with only first-party apps?” Thompson asks. “At what point did delivering an acceptable consumer experience out-of-the-box cross the line into abusing a dominant position? This argument may make sense in theory but it makes zero sense in reality.”

This argument also has zero bearing on what Warren’s talking about. Whether Apple is unfairly tying or bundling its own apps onto its phones at purchase is a question for existing antitrust laws — it was the question in the Microsoft case, in fact. “The ordinary rules apply in that case,” said the senior Warren adviser. “The key thing we’re talking about is the marketplace.”

Contrary to what critics have claimed, Apple would not have to divest from the App Store completely under Warren’s plan, nor would the security benefits of Apple managing what goes onto its phone wither away. Apple would merely be disallowed from selling its own apps next to competing ones. This would hardly destroy Apple, largely a phone and hardware manufacturer and not primarily an app-maker. It would allow competition on the platform.

Apple does have a legitimate antitrust problem with the App Store, as Thompson acknowledges. Spotify has complained to the European Union that Apple takes a 30 percent cut from all revenues from its iPhone app, while preventing it from emailing users directly or allowing upgrades. This indirectly benefits Apple Music, Spotify says. Apple has accused Spotify of using “misleading rhetoric” in its complaint.

Spotify wants changes to Apple’s conduct on the App Store — which is not only fair game for traditional antitrust that can identify anti-competitive impositions of market power, but is also part of Warren’s plan, which mandates fair and nondiscriminatory treatment to marketplace participants. And it shows how Warren is highlighting a consumer welfare issue: If Spotify has to absorb a 30 percent transfer of revenues to Apple for use of its iPhone customers, it likely has to raise prices, and it cannot offer services like upgrades directly.
IN SEPTEMBER 2017, Aileen Black wrote an email to her colleagues at Google. Black, who led sales to the U.S. government, worried that details of the company’s work to help the military guide lethal drones would become public through the Freedom of Information Act. “We will call tomorrow to reinforce the need to keep Google under the radar,” Black wrote.

According to a Pentagon memo signed last year, however, no one at Google needed worry: All 5,000 pages of documents about Google’s work on the drone effort, known as Project Maven, are barred from public disclosure, because they constitute “critical infrastructure security information.”

One government transparency advocate said the memo is part of a recent wave of federal decisions that  keep sensitive documents secret on that same basis — thus allowing agencies to quickly deny document requests.
In partnership with
In partnership with



GOOGLE EMPLOYEES HAVE carried out their own investigation into the company’s plan to launch a censored search engine for China and say they are concerned that development of the project remains ongoing, The Intercept can reveal.

Late last year, bosses moved engineers away from working on the controversial project, known as Dragonfly, and said that there were no current plans to launch it. However, a group of employees at the company was unsatisfied with the lack of information from leadership on the issue — and took matters into their own hands.

The group has identified ongoing work on a batch of code that is associated with the China search engine, according to three Google sources. The development has stoked anger inside Google offices, where many of the company’s 88,000 workforce previously protested against plans to launch the search engine, which was designed to censor broad categories of information associated with human rights, democracy, religion, and peaceful protest.
AFTER YEARS of backlash over controversial government work, Google technology will be used to aid the Trump administration’s efforts to fortify the U.S.-Mexico border, according to documents related to a federal contract.

In August, Customs and Border Protection accepted a proposal to use Google Cloud technology to facilitate the use of artificial intelligence deployed by the CBP Innovation Team, known as INVNT. Among other projects, INVNT is working on technologies for a new “virtual” wall along the southern border that combines surveillance towers and drones, blanketing an area with sensors to detect unauthorized entry into the country.

In 2018, Google faced internal turmoil over a contract with the Pentagon to deploy AI-enhanced drone image recognition solutions; the capability sparked employee concern that Google was becoming embroiled in work that could be used for lethal purposes and other human rights concerns. In response to the controversy, Google ended its involvement with the initiative, known as Project Maven, and established a new set of AI principles to govern future government contracts.
THE RISE OF the internet-connected home security camera has generally been a boon to police, as owners of these devices can (and frequently do) share footage with cops at the touch of a button. But according to a leaked FBI bulletin, law enforcement has discovered an ironic downside to ubiquitous privatized surveillance: The cameras are alerting residents when police show up to conduct searches.

A November 2019 “technical analysis bulletin” from the FBI provides an overview of “opportunities and challenges” for police from networked security systems like Amazon’s Ring and other “internet of things,” or IoT, devices. Marked unclassified but “law enforcement sensitive” and for official use only, the document was included as part of the BlueLeaks cache of material hacked from the websites of fusion centers and other law enforcement entities.
SURVEILLANCE FIRMS around the world are licking their lips at a once-in-a-lifetime opportunity to cash in on the coronavirus by repositioning one of their most invasive products: the tracking bracelet.

Body monitors are associated with criminality and guilt in the popular imagination, the accessories of Wall Street crooks under house arrest and menace-to-society parolees. Unlike smartphones, de facto tracking devices in their own right, strapped-on trackers are expressly designed to be attached to the body and exist solely to report the user’s whereabouts and interactions to one or more third parties; they don’t play podcasts or tell you how many steps you took that day to sweeten the surveillance.

But a climate of perpetual bio-anxiety has paved the way for broader acceptance of carceral technologies, with a wave of companies trying to sell tracking accessories to business owners eager to reopen under the aegis of responsible social distancing and to governments hoping to keep a closer eye on people under quarantine.
LEIA EM PORTUGUÊS 

LEIA EM PORTUGUÊS 
AS SEVEN University of Puerto Rico students prepare to face trial in February for participating in a nonviolent protest more than two years ago, documents released to their defense attorneys reveal that Facebook granted the island’s Justice Department access to a trove of private information from student news publications. The department’s sweeping search warrant was part of a hunt for crimes committed by members of the youth anti-austerity movement, and it has raised fears among civil liberties advocates of a return to a period of Puerto Rico’s history when police routinely targeted citizens for surveillance on the basis of their political interests.

It was April 2017, and for weeks, University of Puerto Rico students had been holding a school-wide strike protesting austerity policies that were poised to defund public services across the island to satisfy the government’s creditors. When the university’s governing board gathered on April 27 to discuss $241 million in budget cuts, the students demanded to be let in. The board refused, locking the doors to the building where the meeting was being held. But the students stormed in anyway, pushing past security.

The action unfolded in real time on Facebook, as three student media outlets, Diálogo UPR, Pulso Estudiantil UPR, and Centro de Comunicación Estudiantil, livestreamed the protest. The students surrounded the board members and shut down the meeting, demanding that the board sign a commitment to rejecting the budget cuts. The action, one of many that took place on campus and in the streets, was over within half an hour. A glass door, some furniture, and a lamp were allegedly broken or damaged. No one was injured, and no one was arrested. But the secretary of Puerto Rico’s Justice Department, now-Gov. Wanda Vázquez, pledged to investigate the incident and arrest lawbreakers.

Two weeks later, students who had assumed leadership roles in the wider strike received citations ordering them to appear in court. When they showed up, they were handcuffed, paraded before media crews, and charged with a host of crimes related to the boardroom protest, the most severe of which — rioting and burglary — were later dropped. The remaining charges, including violating the right to assemble, aggravated restriction of freedom, and violence or intimidation against a public authority, each carry between six months and three years in prison. The seven students go to trial on February 7.

How exactly Vázquez’s Justice Department determined which students to charge out of the dozens who participated in the protest has remained a mystery to defense attorneys. The lawyers’ suspicion: that the case isn’t about crimes committed in the boardroom that day, but rather an attempt to penalize the political activity of some of the most active student organizers. The seven facing trial were members of the student strikers’ negotiating committee as well as political organizations critical of the government.

SHUTTERSTOCK, THE WELL-KNOWN online purveyor of stock images and photographs, is the latest U.S. company to willingly support China’s censorship regime, blocking searches that might offend the country’s authoritarian government, The Intercept has learned.

The publicly traded company built a $639 million-per-year business on the strength of its vast — sometimes comically vast — catalog of images depicting virtually anything a blogger or advertiser could imagine. The company now does business in more than 150 countries. But in China, there is now a very small, very significant gap in Shutterstock’s offerings. In early September, Shutterstock engineers were given a new goal: The creation of a search blacklist that would wipe from query results images associated with keywords forbidden by the Chinese government. Under the new system, which The Intercept is told went into effect last month, anyone with a mainland Chinese IP address searching Shutterstock for “President Xi,” “Chairman Mao,” “Taiwan flag,” “dictator,” “yellow umbrella,” or “Chinese flag” will receive no results at all. Variations of these terms, including “umbrella movement” — the precursor to the mass pro-democracy protests currently gripping Hong Kong — are also banned.

Shutterstock’s decision to silently aid China’s censorship agenda comes at a time of heightened scrutiny into the relationship between corporate America and President Xi Jinping’s authoritarian regime. Household names like Apple, Blizzard Entertainment, the NBA, and Google have all garnered harsh criticism for letting the policy directives of the Communist Party of China, and the gilded promise of a billion customers, dictate company strategy. Deciding to censor is a particularly stark inversion of values for Shutterstock, which markets itself as an enabler of creative expression.

The photo company’s relationship with China dates back to at least 2014, when it struck a distribution deal with ZCool, a Chinese social network and portfolio site for visual artists. Last year Shutterstock announced a $15 million investment in ZCool, noting that owing to the partnership, “Shutterstock’s content now powers large technology platforms in China such as Tencent Social Ads,” an online advertising subsidiary of the tremendously popular Chinese internet conglomerate Tencent.

Shutterstock’s censorship feature appears to have been immediately controversial within the company, prompting more than 180 Shutterstock workers to sign a petition against the search blacklist and accuse the company of trading its values for access to the lucrative Chinese market. Chinese internet users already struggle to discuss even the tamest of taboo subjects; now, it seemed, the situation would get a little worse, with the aid of yet another willing American company.

“Yes, we’re a creative photo and video marketplace, but we are also an editorial news hub,” one Shutterstock employee told The Intercept. “Want to write a story about the protests in Hong Kong? They never existed. Want to write about Taiwan? It never existed. Xi Jinping is NOT a dictator because he specifically said so. This is dark shit.”

The text of the petition, provided to The Intercept, can be read in full below.

Shutterstock’s founder and CEO Jon Oringer replied to the petition several days later; those hoping for a change of heart were to be disappointed. Shutterstock’s pro-censorship compromise with the Chinese government was justified, Oringer argued, because to refuse to do business in China rather than help the country’s government expand its information control scheme would be the real act of craven corporate turpitude: “Do we make the majority of our content available to China’s 1.3 billion citizens or do we take away their ability to access it entirely? We ultimately believe, consistent with our brand promise, it is more valuable for storytellers to have access to our collection to creatively and impactfully tell their stories.” Shutterstock with a bespoke censorship feature was “more empowering” and “will better serve the people of China than the alternative,” Oringer continued.

Oringer’s company-wide response is also reproduced below.

Following Oringer’s letter and the implementation of the search term blacklist, some employees fear the use of censorship at the company will grow: “He offered no consolation in terms of what our actions will be when China requests to add an X number more search terms to the censorship list,” the Shutterstock staffer told The Intercept, “or if another country comes to us with a similar request. We are devastated.”
FEW PEOPLE HAD ever heard of Perceptics, a Tennessee-based subcontractor that sells license plate readers to U.S. Customs and Border Protection, before last month, when news emerged that the company had been hacked and that sensitive data — including images of license plates and drivers — had been released on the dark web.

The hack is just the sort of privacy breach that civil liberties advocates have long warned could come from massive government data collection, especially when it is contracted out to private firms. And it comes at a time when the CBP is under scrutiny for monitoring activists and journalists at the U.S.-Mexico border and airports.

Yet while photos of faces and license plates of some 100,000 U.S. drivers are now freely available online, the CEO of Perceptics, John Dalton, claimed in an email a few years ago that “CBP has none of the privacy concerns at the border that all agencies have inland.”

Writing to one of his company’s lobbyists in 2013, Dalton suggested that the border agency offered Perceptics an opportunity to make greater use of license plate images, stating, “Data mining and looking at traffic patterns/abnormalities are strong analytics for CBP, and could be for others.” Dalton appeared to be referring to the CBP’s relatively unfettered powers of search and seizure within 100 miles of the border. In contrast, for agencies other than CBP, “there is much concern with ACLU state level lawsuits and elsewhere around privacy issues, so this is a live challenge,” he wrote.
JUST MONTHS BEFORE millions of its internal documents were stolen and dumped on the internet, the Tennessee-based surveillance company Perceptics was preparing to pitch New York’s transit authority on how it could help enforce impending “congestion pricing” rules, according to leaked documents reviewed by The Intercept. The pitch, as outlined in the files, went well beyond mere toll enforcement and into profiling New Yorkers’ travel patterns and companions, creating what experts describe as major privacy risks.

Congestion pricing, on the face of it, doesn’t seem like it would present a privacy risk — it’s a traffic policy, after all, not some new NYPD initiative. The plan is to essentially tax the cars that clog Manhattan’s streets and route the proceeds to public transportation, providing both a deterrent against and palliative for traffic. There won’t be any congestion pricing toll booths: The fee will be assessed automatically and electronically, potentially by photographing the license plates of passing cars and sending the plate owner a bill in the mail. This requires cameras running around the clock, dutifully recording every car that comes and goes. And this, Perceptics claims, is where the company truly shines.

According to an internal presentation released by the Perceptics hacker and reviewed by The Intercept, the company pitched New York’s Metropolitan Transportation Authority, or MTA, in February of this year on how Perceptics’ car-scanning camera arrays, already deployed and honed in areas like the Mexican border and an assortment of U.S. military installations, could help the MTA track down drivers. It’s unknown how the plan was received by the MTA, which administers public transit, bridges, and tolls for New York City and some of its surrounding suburbs, but leaked Perceptics emails show that the company shipped camera hardware to the MTA’s Bridges and Tunnels division for a live demonstration.

Perceptics did not respond to a request for comment. An MTA spokesperson told The Intercept that “all details are still to be determined” regarding congestion pricing enforcement.

The presentation document, titled “Smart Imaging Solutions for New York City Congestion Pricing,” makes clear that Perceptics wants to “produce vehicle-specific profiles” using cameras and “unique machine learning algorithms,” allowing the city to immediately recognize and build travel histories of every car in the congestion zone. Law enforcement and surveillance experts said the system described goes far beyond what would ever be necessary to mail scofflaws traffic tickets. Instead, it is an entirely new sort of surveillance apparatus that tracks deeply personal information like “customer travel patterns and travel consistency,” the number of passengers in the car, or “likely trip purpose,” and associates this information with a unique fingerprint of every vehicle that passes by Perceptics’ cameras.

Allie Bohm, a policy counsel with the New York Civil Liberties Union, described the Perceptics plan as an “incredibly privacy-invasive proposal” that “raises all sorts of associational and First Amendment concerns.” Bohm expressed particular alarm about the possibility of a congestion pricing enforcement system eventually feeding data into the NYPD’s existing surveillance regime. “The NYPD has fancied itself an intelligence agency for a very long time,” said Bohm. “These are folks who are pioneering some really, at best, questionable, and, at worst, alarming programs of surveillance and of drawing conclusions from innocuous behavior.”

The MTA will not deploy congestion pricing before 2021 and has yet to select a tolling vendor. But whether Perceptics wins a contract or not, its idea to bring to the heart of Manhattan military-grade surveillance technology — already provided to Saudi Special Forces and the Jordanian army, according to a Perceptics document — is an example of how something as innocuous-sounding as congestion pricing can turn into a surveillance sprawl.
A MEMBER OF Project Veritas gave testimony in a federal court case indicating that the right-wing group, known for its undercover videos, violates Facebook policies designed to counter systematic deception by Russian troll farms and other groups. The deposition raises questions over whether Facebook will deter American operatives who use the platform to strategically deceive and damage political opponents as vigorously as it has Iranian and Russian propagandists. But is the company capable of doing so without just creating more problems?

Close observers of Veritas and Facebook, including one at a research lab that works with the social network, said the testimony shows the group is clearly violating policies against what Facebook refers to as “coordinated inauthentic behavior.” The company formally defined such behavior in a December 2018 video featuring its cybersecurity policy chief Nathaniel Gleicher, who said it “is when groups of pages or people work together to mislead others about who they are or what they’re doing.” The designation, Gleicher added, is applied by Facebook to a group not “because of the content they’re sharing” but rather only “because of their deceptive behavior.” That is, using Facebook to dupe people is all it takes to fit the company’s institutional definition of coordinated inauthentic behavior.

In practice, “coordinated inauthentic behavior” has become a sort of catchall label for untoward meddling on Facebook, snagging everyone from Burmese military officers to Russian meme spammers. But curbing such activity has become a very public crusade for Facebook in the wake of its prominent role as a platform for the spread of disinformation, propaganda, and outright hoaxes during the 2016 presidential campaign. This past January, Gleicher announced the removal of coordinated inauthentic behavior from Iran, which spread when operatives “coordinated with one another and used fake accounts to misrepresent themselves,” thus triggering a Facebook ban. Similarly, in a 2017 update on Facebook’s internal investigation into Russian online propaganda efforts, the company’s then-head of security Alex Stamos assured the world’s democracies the company was providing “technology improvements for detecting fake accounts,” including “changes to help us more efficiently detect and stop inauthentic accounts at the time they are being created.”

Throughout all of this, coordinated inauthentic behavior has remained more or less synonymous with “foreign actors” and “nation-states,” the cloak-and-dagger stuff of an increasingly militarized internet filled with enemies of the Western Democracy who seek to subvert it from abroad.

Project Veritas, a hybrid of an opposition research shop and a ranting YouTube channel, has taken pride in its ability to deceive since its creation in 2010. With conservative backers like Peter Thiel, the Koch brothers, and the Trump Foundation, the group and its founder James O’Keefe have worked relentlessly to target and malign individuals at institutions they deem leftist, whether it’s Planned Parenthood (reportedly targeted by O’Keefe posing as a young teen’s 23-year-old boyfriend), George Soros (the progressive philanthropist whose professional circle Veritas tried and spectacularly failed to infiltrate), or the Washington Post (whose reporter was offered a fake story on Alabama Senate candidate Roy Moore). O’Keefe has long attempted to position himself in the context of dogged, daring, traditional journalism, describing Veritas’s efforts as “investigative” reporting executed by “undercover journalists.” But his efforts are often executed by what the New Yorker has called “amateurish spies” — their efforts against the Post and Soros resembled a Three Stooges bit — and packaged with mendacious editing, duplicitous production, and outright lying, making Veritas’s audience as much a victim of its productions as the subjects. Debates over who or what is to be considered “real journalism” are almost always counterproductive and contrived, but Veritas stands out for the shamelessness with which it pursues nakedly partisan ends.

There is, of course, a proud tradition of undercover journalism executed unequivocally in the name of informing the public. Writers like Barbara Ehrenreich and Shane Bauer have taken jobs they were not otherwise interested in in order to reveal injustices in society’s margins, and some of the most damning details of the Cambridge Analytica scandal were exposed by a reporter with the UK’s Channel 4 posing as a foreign politician interested in the company’s services. This reporting involved lying, sure — or at least the withholding of true intent, and a willingness to let others deceive themselves — but only as a means to a truthful end. The distinction between these reporters and Veritas operatives may be that the end the latter group seeks, the final media product, is typically just another act of partisan misdirection that doesn’t withstand further scrutiny.

Neither Project Veritas nor Facebook commented for this story.

“Legend Building” by Project Veritas

Project Veritas has systematically deceived not just targets on the left and viewers on the right but Facebook users as well (their official page has over 200,000 followers) at a time when the company is publicly dedicated to fighting this sort of systemic duplicity. That’s a wrinkle that raises questions about Facebook’s commitment to rooting out coordinated inauthentic behavior closer to home — Thiel sits on the company’s board — not to mention Project Veritas’s presence on social media.
IN A FEDERAL lawsuit, the tech giant Oracle has provided new details to support its accusation that Amazon secretly negotiated a job offer with a then-Department of Defense official who helped shape the procurement process for a massive federal contract for which Amazon was a key bidder.

Amazon Web Services and Microsoft are now the two finalists to win the highly contested $10 billion contract for what is known as the Joint Enterprise Defense Infrastructure, or JEDI. The deal, one of the largest federal contracts in U.S. history, would pay one company to provide cloud computing services in support of Defense Department operations around the world.

But the contract has been hotly contested since the department began soliciting proposals last year. Two of Amazon’s competitors, IBM and Oracle, filed complaints with the Government Accountability Office saying that the winner-take-all process unfairly favored Amazon, which is seen as an industry leader in cloud computing. When its claim was rejected, Oracle sued the government in the U.S. Court of Federal Claims.

Since the court battle began in 2018, Oracle has aggressively lodged conflict-of-interest accusations involving a former DOD official named Deap Ubhi, who left the department in 2017 to take a job at Amazon. In a court motion filed on Friday, Oracle alleged that while Ubhi worked on the preliminary research for the JEDI program in the late summer and fall of 2017, he was also engaged in a secret job negotiation with Amazon for months, complete with salary discussions, offers of signing bonuses, and lucrative stock options.

The motion further alleges that Ubhi did not recuse himself from the JEDI program until weeks after verbally accepting a job offer from Amazon and that he continued to receive information about Amazon’s competitors and participate in meetings about technical requirements, despite a government regulation that forbids such conflicts of interest.

“Neither Ubhi nor [Amazon Web Services] disclosed the employment discussions or job offer to DOD — not when the employment discussions started, not when the informal job offer occurred, not when the formal offer occurred, and not even when Ubhi accepted the offer,” Oracle’s motion reads.

As America’s technology companies have continued to outpace the Pentagon, the Defense Department has looked to recruit talent from Silicon Valley to help enhance its information technology.

Ubhi is a venture capitalist and technology entrepreneur who worked for Amazon before his time in government. He took a job working on a Defense Department initiative aimed at collaborating with Silicon Valley to modernize the Pentagon’s information technology systems. After working as part of a four-person team to help shape the Pentagon JEDI procurement process, he left the department and returned to Amazon in November 2017.

A spokesperson for Amazon Web Services declined to comment and declined to make Ubhi available for an interview, citing ongoing litigation. Elissa Smith, a spokesperson for the Department of Defense, also told The Intercept that “we don’t comment on pending litigation.”

In a previous court filing, U.S. government lawyers accused Oracle of a “broad fishing expedition primarily [intended] to find support for its claim that the solicitation at issue is tainted by alleged conflicts of interest.”

According to Oracle’s motion on Friday, Ubhi began job negotiations with Amazon in August 2017, while he was working on the early stages of the JEDI program. Oracle claims says that “deep discussions” about employment began in late September and that Ubhi “verbally committed” to take the job on October 4. But according to the filing, Ubhi did not recuse himself until October 31, 2017. Oracle alleges that he continued to influence the program in the meantime.

Under the Procurement Integrity Act, government officials who are “contacted by a [contract] bidder about non-federal employment” have two options: They must either report the contact and reject the offer of employment or promptly recuse themselves from any contract proceedings.

“Contracts should be awarded fairly based on merit,” Mandy Smithberger, director of the Center for Defense Information at the Project on Government Oversight, told The Intercept. “The Procurement Integrity Act seeks to ensure that job offers and other financial conflicts of interest don’t influence that process.”

Last year, a Defense Department review found that “there were four instances where [department] individuals with potential financial conflicts of interest” had worked on the JEDI program, according to court records, but the Pentagon concluded that this hadn’t unfairly impacted the contracting process. Two follow-up reviews — one by the GAO in November 2018 and another by the Defense Department in April 2019 — came to similar conclusions.

The second Pentagon review came after the department said that it had received “new information” about Ubhi and would investigate it. According to Oracle’s motion on Friday, the “new information” came from a “belated submission from [Amazon]” to the DOD’s contracting officer that finally acknowledged the monthslong employment talks.

According to Oracle, Ubhi provided a “false narrative” to the contracting officer at the time of his recusal, saying that he was stepping away from the project because Amazon had offered to acquire a company that Ubhi had a stake in. That was a pretext to mask the fact he had been negotiating for months to obtain a job at the company, Oracle’s filing said.

The filing also alleges that between Ubhi’s verbal commitment to accept Amazon’s offer and his recusal from JEDI, he continued to participate in Pentagon meetings about the project’s technical requirements and to receive submissions from Amazon competitors. It also alleges that Ubhi downloaded material from a JEDI project Google Drive to his own laptop.

In its filings, Oracle has argued that Ubhi was instrumental in persuading the Pentagon to seek services from a single vendor — a decision widely seen to improve Amazon’s chances. Oracle cites workplace messages on the platform Slack in which Ubhi tries to persuade his colleagues to come around to that view, but the company does not cite any messages suggesting what his reasons or motive may have been.
In partnership with

FORMER GOOGLE CEO Eric Schmidt has defended the company’s plan to build a censored version of its search engine in China.

In an interview with the BBC on Monday, Schmidt said that he wasn’t involved in decisions to build the censored search platform, code-named Dragonfly. But he insisted that there were “many benefits” to working with China and said he was an advocate of operating in the country because he believed that it could “help change China to be more open.”

As The Intercept first revealed in August, Google developed a prototype of the censored search engine that was designed to remove content that China’s ruling Communist Party regime deems sensitive. The search engine would have blacklisted thousands of words and phrases, including terms such as “human rights,” “student protest,” and “Nobel Prize” in Mandarin.

The revelations prompted a wave of protests inside and outside of Google, with employees, activists, and prominent lawmakers demanding an end to the project. Google subsequently stated that it had ceased work on Dragonfly and moved employees to new projects.
LAST YEAR, A coalition of privacy advocates and child psychologists warned against putting an Amazon Alexa speaker anywhere near your child on the fairly reasonable grounds that developing minds shouldn’t befriend always-on surveillance devices, no matter how cute the packaging. Now, a group of privacy researchers, attorneys, and U.S. senators are calling on the Federal Trade Commission to investigate Amazon’s alleged violations of COPPA, a law protecting the littlest users of all.

COPPA, the Children’s Online Privacy Protection Act, regulates how companies can collect and use data on users who might have trouble spelling “privacy,” let alone understand it enough to consent to relinquishing it. COPPA is the reason why so many sites, like Facebook, simply don’t allow children under 13 to sign up. Amazon, on the other hand, decided to court children for its data collection business, releasing the Amazon Echo Dot Kids Edition, an always-listening “smart speaker” that retains all of the functions of its adult counterpart, but tucks them inside a candy-colored shell. The kiddo speaker also adds child-specific features, like the ability to have Amazon’s virtual assistant Alexa read your child a story in her disembodied robo-voice, or play child-geared content from sources like Cartoon Network and Nickelodeon.

A new complaint drafted by the Campaign for a Commercial-Free Childhood, the consumer privacy group Center for Digital Democracy, and Georgetown University’s Institute for Public Representation says that Amazon is committing a litany of COPPA violations through the Echo Dot Kids Edition, and calls on the FTC to investigate.

Amazon’s COPPA violations, according to the complaint, include failure to provide parental notice and obtain parental consent for online services related to the kids’ Echo Dot, failure to tell parents that they have a right to review personal information submitted by their child, and failure to provide parents a way to delete such information or opt out of its collection.

Across 96 pages, the complaint gets more specific, offering examples of how Amazon dodges, obscures, and otherwise neglects its duties to parents. Given that the attorneys who drafted the complaint were confused by Amazon’s byzantine policies, it’s hard to imagine average parents faring much better:

Even if parents were for some reason motivated to seek out the website version of the Children’s Privacy Disclosure, the hyperlinked Privacy Notice is long, confusingly written, and contains a lot of unrelated material. It is unclear what, if any, parts apply to the Echo Dot Kids Edition, and some of the information seems to contradict the Children’s Privacy Disclosure. For example, the Privacy Notice discloses that Amazon collects “search term and search result information from some searches conducted through the Web search features offered by our subsidiary, Alexa Internet,” but it does not say whether it does so when a child is using the Echo Dot Kids Edition.

Perhaps most troubling is what the complaint says about Amazon’s treatment of child voice recordings, which aren’t supposed to be stored indefinitely, per recent FTC guidance: “The answer is clear: No, the company can’t keep it. Under Section 312.10 of COPPA, you’re allowed to retain children’s personal information ‘for only as long as is reasonably necessary to fulfill the purpose for which the information was collected.'”

But Amazon takes a different approach, the complaint explains: “In response to a Congressional inquiry about how long it keeps recordings and other information collected from children, however, Amazon responded: ‘Voice recordings are retained for the parent’s review until the parent deletes them.'” In other words, Amazon is keeping a child’s Amazon queries stored indefinitely, not “for only as long as is reasonably necessary.”

GOOGLE EXECUTIVES ARE carrying out a secret internal assessment of work on a censored search engine for China, The Intercept has learned.

A small group of top managers at the internet giant are conducting a “performance review” of the controversial effort to build the search platform, known as Dragonfly, which was designed to blacklist information about human rights, democracy, religion, and peaceful protest.

Performance reviews at Google are undertaken annually to evaluate employees’ output and development. They are usually carried out in an open, peer review-style process: Workers grade each other’s projects and the results are then assessed by management, who can reward employees with promotion if they are deemed ready to progress at the company.

In the case of Dragonfly, however, the peer review aspect has been removed, subverting the normal procedure. In a move described as highly unusual by two Google sources, executives set up a separate group of closed “review committees,” comprised of senior managers who had all previously been briefed about the China search engine.

ACROSS THE WORLD, the reputation of elites and their institutions is in free fall. A flood of online information has given the public unprecedented access to elite individuals in politics, media, academia, science, business, and an array of other fields. Thanks to tools like social media, the activist public has greater proximity to its supposed mandarin class than ever before. What this newfound intimacy has revealed has not always been flattering. Many of those who had been held up as elites in their fields have, upon closer examination by the public, been revealed as mediocre, incompetent, buffoonish, and, in some cases, possibly unhinged. At the same time, the public, for all its passion, has also revealed itself to be vulnerable to conspiracy theories, disinformation, and outbreaks of hysteria.
FOLLOWING MONTHS OF protests from its employees, Google announced last summer that it would not renew its contract with the military on Project Maven, an initiative to use artificial intelligence to improve the targeting and surveillance capabilities of drones on the battlefield.

In an email sent this week by Kent Walker, Google’s senior vice president for global affairs, the Silicon Valley giant appeared to hedge on its commitment to fully cut ties with the drone initiative.


ON MARCH 17, 2016, Ring CEO Jamie Siminoff emailed out a company-wide declaration of war. The message, under the subject line “Going to war,” made two things clear to the home surveillance company’s hundreds of employees: Everyone was getting free camouflage-print T-shirts (“They look awesome,” assured Siminoff), and the company’s new mission was to use consumer electronics to fight crime. “We are going to war with anyone that wants to harm a neighborhood,” Siminoff wrote — and indeed Ring made it easier for police and worried neighbors to get their hands on footage from Ring home cameras. Internal documents and video reviewed by The Intercept show why this merging of private tech business and public law enforcement has troubling privacy implications.

This first declaration of startup militancy — which Siminoff would later refer to as “Ring War I” or simply “RW1” — would be followed by more, equally clumsy attempts at corporate galvanization, some aimed at competitors or lackluster customer support. But the RW1 email is striking in how baldly it lays out the priorities and values of Ring, a company now owned by Amazon and facing strident criticism over its mishandling of customer data, as previously reported by The Intercept and The Information.

Ring and Siminoff, who still leads the company, haven’t been shy about their focus on crime-fighting. In fact, Ring’s emphasis not only on personal peace of mind, but also active crime-fighting has been instrumental in differentiating its cloud-connected doorbell and household surveillance gear from those made by its competitors. Ring products come with access to a social app called Neighbors that allows customers to not just to keep tabs on their own property, but also to share information about suspicious-looking individuals and alleged criminality with the rest of the block. In other words, Ring’s cameras aren’t just for keeping tabs on your own stoop or garage — they work to create a private-sector security bubble around entire residential areas, a neighborhood watch for the era of the so-called smart home.


SHOSHANA ZUBOFF’S “The Age of Surveillance Capitalism” is already drawing comparisons to seminal socioeconomic investigations like Rachel Carson’s “Silent Spring” and Karl Marx’s “Capital.” Zuboff’s book deserves these comparisons and more: Like the former, it’s an alarming exposé about how business interests have poisoned our world, and like the latter, it provides a framework to understand and combat that poison. But “The Age of Surveillance Capitalism,” named for the now-popular term Zuboff herself coined five years ago, is also a masterwork of horror. It’s hard to recall a book that left me as haunted as Zuboff’s, with its descriptions of the gothic algorithmic daemons that follow us at nearly every instant of every hour of every day to suck us dry of metadata. Even those who’ve made an effort to track the technology that tracks us over the last decade or so will be chilled to their core by Zuboff, unable to look at their surroundings the same way.
A NEW WEBSITE exposes the extent to which Apple cooperates with Chinese government internet censorship, blocking access to Western news sources, information about human rights and religious freedoms, and privacy-enhancing apps that would circumvent the country’s pervasive online surveillance regime.

The new site, AppleCensorship.com, allows users to check which apps are not accessible to people in China through Apple’s app store, indicating those that have been banned. It was created by researchers at GreatFire.org, an organization that monitors Chinese government internet censorship.

In late 2017, Apple admitted to U.S. senators that it had removed from its app store in China more than 600 “virtual private network” apps that allow users to evade censorship and online spying. But the company never disclosed which specific apps it removed — nor did it reveal other services it had pulled from its app store at the behest of China’s authoritarian government.

In addition to the hundreds of VPN apps, Apple is currently preventing its users in China from downloading apps from news organizations, including the New York Times, Radio Free Asia, Tibetan News, and Voice of Tibet. It is also blocking censorship circumvention tools like Tor and Psiphon; Google’s search app and Google Earth; an app called Bitter Winter, which provides information about human rights and religious freedoms in China; and an app operated by the Central Tibetan Authority, which provides information about Tibetan human rights and social issues.

Some bans – such as those of certain VPN apps and the Times – have received media coverage in the past, but many never generate news headlines. Charlie Smith, a co-founder of GreatFire.org, told The Intercept that the group was motivated to launch the website because “Apple provides little transparency into what it censors in its app store. Most developers find out their app has been censored after they see a drop in China traffic and try to figure out if there is a problem. We wanted to bring transparency to what they are censoring.”
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
GOOGLE IS FACING a new campaign of global protests over its plan to launch a censored version of its search engine in China.

On Friday, a coalition of Chinese, Tibetan, Uighur, and human rights groups organized demonstrations outside Google’s offices in the U.S., U.K., Canada, India, Mexico, Chile, Argentina, Sweden, Switzerland, and Denmark.
THE “SMART HOME” of the 21st century isn’t just supposed to be a monument to convenience, we’re told, but also to protection, a Tony Stark-like bubble of vigilant algorithms and internet-connected sensors working ceaselessly to watch over us. But for some who’ve welcomed in Amazon’s Ring security cameras, there have been more than just algorithms watching through the lens, according to sources alarmed by Ring’s dismal privacy practices.

Ring has a history of lax, sloppy oversight when it comes to deciding who has access to some of the most precious, intimate data belonging to any person: a live, high-definition feed from around — and perhaps inside — their house. The company has marketed its line of miniature cameras, designed to be mounted as doorbells, in garages, and on bookshelves, not only as a means of keeping tabs on your home while you’re away, but of creating a sort of privatized neighborhood watch, a constellation of overlapping camera feeds that will help police detect and apprehend burglars (and worse) as they approach. “Our mission to reduce crime in neighborhoods has been at the core of everything we do at Ring,” founder and CEO Jamie Siminoff wrote last spring to commemorate the company’s reported $1 billion acquisition payday from Amazon, a company with its own recent history of troubling facial recognition practices. The marketing is working; Ring is a consumer hit and a press darling.

Despite its mission to keep people and their property secure, the company’s treatment of customer video feeds has been anything but, people familiar with the company’s practices told The Intercept. Beginning in 2016, according to one source, Ring provided its Ukraine-based research and development team virtually unfettered access to a folder on Amazon’s S3 cloud storage service that contained every video created by every Ring camera around the world. This would amount to an enormous list of highly sensitive files that could be easily browsed and viewed. Downloading and sharing these customer video files would have required little more than a click. The Information, which has aggressively covered Ring’s security lapses, reported on these practices last month.

At the time the Ukrainian access was provided, the video files were left unencrypted, the source said, because of Ring leadership’s “sense that encryption would make the company less valuable,” owing to the expense of implementing encryption and lost revenue opportunities due to restricted access. The Ukraine team was also provided with a corresponding database that linked each specific video file to corresponding specific Ring customers.

LEIA EM PORTUGUÊS 
GOOGLE CEO SUNDAR PICHAI came under fire from lawmakers on Tuesday over the company’s secretive plan to launch a censored search engine in China.

During a hearing held by the House Judiciary Committee, Pichai faced sustained questions over the China plan, known as Dragonfly, which would blacklist broad categories of information about democracy, human rights, and peaceful protest.

The hearing began with an opening statement from Rep. Kevin McCarthy, R-Calif., who said launching a censored search engine in China would “strengthen China’s system of surveillance and repression.” McCarthy questioned whether it was the role of American companies to be “instruments of freedom or instruments of control.”

Pichai read prepared remarks, stating “even as we expand into new markets, we never forget our American roots.” He added: “I lead this company without political bias and work to ensure that our products continue to operate that way. To do otherwise would go against our core principles and our business interests.”
GOOGLE IS FACING a renewed wave of criticism from human rights groups over its controversial plan to launch a censored search engine in China.

A coalition of more than 60 leading groups from countries across the world have joined forces to blast the internet giant for failing to address concerns about the secretive China project, known as Dragonfly. They come from countries including China, the United States, the United Kingdom, Argentina, Bolivia, Chile, France, Kazakhstan, Mexico, Norway, Pakistan, Palestine, Romania, Syria, Tibet, and Vietnam.

A prototype for the censored search engine was designed to blacklist broad categories of information about human rights, democracy, and peaceful protest. It would link Chinese users’ searches to their personal cellphone number and store people’s search records inside the data centers of a Chinese company in Beijing or Shanghai, which would be accessible to China’s authoritarian Communist Party government.

If the plan proceeds, “there is a real risk that Google would directly assist the Chinese government in arresting or imprisoning people simply for expressing their views online, making the company complicit in human rights violations,” the human rights groups wrote in a letter that will be sent to Google’s leadership on Tuesday.

The letter highlights mounting anger and frustration within the human rights community that Google has rebuffed concerns about Dragonfly, concerns that have been widely raised both inside and outside the company since The Intercept first revealed the plan in August. The groups say in their 900-word missive that Google’s China strategy is “reckless,” piling pressure on CEO Sundar Pichai, who is due to appear Tuesday before the House Judiciary Committee, where he will likely face questions on Dragonfly.
GOOGLE IS FACING a renewed wave of criticism from human rights groups over its controversial plan to launch a censored search engine in China.

A coalition of more than 60 leading groups from countries across the world have joined forces to blast the internet giant for failing to address concerns about the secretive China project, known as Dragonfly. They come from countries including China, the United States, the United Kingdom, Argentina, Bolivia, Chile, France, Kazakhstan, Mexico, Norway, Pakistan, Palestine, Romania, Syria, Tibet, and Vietnam.

A prototype for the censored search engine was designed to blacklist broad categories of information about human rights, democracy, and peaceful protest. It would link Chinese users’ searches to their personal cellphone number and store people’s search records inside the data centers of a Chinese company in Beijing or Shanghai, which would be accessible to China’s authoritarian Communist Party government.

If the plan proceeds, “there is a real risk that Google would directly assist the Chinese government in arresting or imprisoning people simply for expressing their views online, making the company complicit in human rights violations,” the human rights groups wrote in a letter that will be sent to Google’s leadership on Tuesday.

The letter highlights mounting anger and frustration within the human rights community that Google has rebuffed concerns about Dragonfly, concerns that have been widely raised both inside and outside the company since The Intercept first revealed the plan in August. The groups say in their 900-word missive that Google’s China strategy is “reckless,” piling pressure on CEO Sundar Pichai, who is due to appear Tuesday before the House Judiciary Committee, where he will likely face questions on Dragonfly.
LEIA EM PORTUGUÊS 
LAST MONTH, A famed hacker who has been serving a 10-year prison sentence since 2012 was accused by a guard at a federal detention center of “minor assault,” landing the so-called hacktivist in solitary confinement, according to advocates. The guard at Michigan’s Federal Correctional Institute-Milan made the accusation against Jeremy Hammond — the activist associated with hacking groups Anonymous and LulzSec and best know for hacking private intelligence firm Stratfor and leaking documents to WikiLeaks — on either November 19 or 20. Hammond has been held in solitary confinement ever since, according to the Jeremy Hammond Support Network.

The guard claims that Hammond hit him with a door, “stood his ground,” and pushed his shoulder into the guard. The head of Hammond’s support network said the prison guard’s account is an overblown. “Jeremy says that he was exiting his unit through a door that has no windows and could not see the guard on the other side, and as he’s exiting, bumped the guard with the door,” Grace North told The Intercept. “The guard immediately grabbed Jeremy and threw him up against the wall and dragged him down to solitary, with no handcuffs, without calling for backup, which is against prison protocol, and Jeremy has been there ever since.”

North’s version of events also portrays the guard as overly aggressive: After the guard was hit with the door, North said, he asked Hammond if he “wanted to go.”
LAST MONTH, A famed hacker who has been serving a 10-year prison sentence since 2012 was accused by a guard at a federal detention center of “minor assault,” landing the so-called hacktivist in solitary confinement, according to advocates. The guard at Michigan’s Federal Correctional Institute-Milan made the accusation against Jeremy Hammond — the activist associated with hacking groups Anonymous and LulzSec and best know for hacking private intelligence firm Stratfor and leaking documents to WikiLeaks — on either November 19 or 20. Hammond has been held in solitary confinement ever since, according to the Jeremy Hammond Support Network.

The guard claims that Hammond hit him with a door, “stood his ground,” and pushed his shoulder into the guard. The head of Hammond’s support network said the prison guard’s account is an overblown. “Jeremy says that he was exiting his unit through a door that has no windows and could not see the guard on the other side, and as he’s exiting, bumped the guard with the door,” Grace North told The Intercept. “The guard immediately grabbed Jeremy and threw him up against the wall and dragged him down to solitary, with no handcuffs, without calling for backup, which is against prison protocol, and Jeremy has been there ever since.”

North’s version of events also portrays the guard as overly aggressive: After the guard was hit with the door, North said, he asked Hammond if he “wanted to go.”

THE SECRECY SURROUNDING the work was unheard of at Google. It was not unusual for planned new products to be closely guarded ahead of launch. But this time was different. The objective, code-named Dragonfly, was to build a search engine for China that would censor broad categories of information about human rights, democracy, and peaceful protest.

In February 2017, during one of the first group meetings about Dragonfly at Google’s Mountain View headquarters in California, some of those present were left stunned by what they heard. Senior executives disclosed that the search system’s infrastructure would be reliant upon a Chinese partner company with data centers likely in Beijing or Shanghai.

Locating core parts of the search system on the Chinese mainland meant that people’s search records would be easily accessible to China’s authoritarian government, which has broad surveillance powers that it routinely deploys to target activists, journalists, and political opponents.

Yonatan Zunger, then a 14-year veteran of Google and one of the leading engineers at the company, was among a small group who had been asked to work on Dragonfly. He was present at some of the early meetings and said he pointed out to executives managing the project that Chinese people could be at risk of interrogation or detention if they were found to have used Google to seek out information banned by the government.

Scott Beaumont, Google’s head of operations in China and one of the key architects of Dragonfly, did not view Zunger’s concerns as significant enough to merit a change of course, according to four people who worked on the project. Beaumont and other executives then shut out members of the company’s security and privacy team from key meetings about the search engine, the four people said, and tried to sideline a privacy review of the plan that sought to address potential human rights abuses.

Zunger — who left his position at Google last year — is one of the four people who spoke to The Intercept for this story. He is the first person with direct involvement in Dragonfly to go on the record about the project. The other three who spoke to The Intercept are still employed by Google and agreed to share information on the condition of anonymity because they were not authorized to talk to the media. Their accounts provide extraordinary insight into how Google bosses worked to suppress employee criticism of the censored search engine and reveal deep fractures inside the company over the China plan dating back almost two years.

Google’s leadership considered Dragonfly so sensitive that they would often communicate only verbally about it and would not take written notes during high-level meetings to reduce the paper trail, two sources said. Only a few hundred of Google’s 88,000 workforce were briefed about the censorship plan. Some engineers and other staff who were informed about the project were told that they risked losing their jobs if they dared to discuss it with colleagues who were themselves not working on Dragonfly.

“They [leadership] were determined to prevent leaks about Dragonfly from spreading through the company,” said a current Google employee with knowledge of the project. “Their biggest fear was that internal opposition would slow our operations.”
LEIA EM PORTUGUÊS 
AMNESTY INTERNATIONAL HAS announced a new protest campaign calling on Google to cancel its controversial plan to launch a censored search engine in China.

The human rights group on Monday launched a petition against the search engine and said that on Tuesday, it will stage demonstrations outside Google offices in the United States, the United Kingdom, Australia, Canada, Germany, Hong Kong, the Netherlands, and Spain. Google’s plan for China would “irreparably damage internet users’ trust in the tech company,” Amnesty said in statement, and “would set a dangerous precedent for tech companies enabling rights abuses by governments.”

As The Intercept first reported in August, Google secretly developed the censored search engine as part of a project code-named Dragonfly. It was designed to blacklist words and phrases such as “human rights,” “Nobel Prize,” and “student protest.” The search platform would link Chinese users’ search records to their cellphone numbers and share people’s search histories with a Chinese partner company. The search records would in turn be accessible to China’s authoritarian government, which has broad surveillance and data-seizing powers that it routinely uses to identify and arrest activists and critics.

“This is a watershed moment for Google,” said Joe Westby, Amnesty International’s researcher on technology and human rights. “As the world’s No. 1 search engine, it should be fighting for an internet where information is freely accessible to everyone, not backing the Chinese government’s dystopian alternative.”
NINE HUMAN RIGHTS and civil liberties organizations sent a letter to the U.S. Justice Department today objecting to a potential agreement between the United States and the United Kingdom that would give British law enforcement broad access to data held by U.S. technology companies.

The possible agreement stems from the Clarifying Lawful Overseas Use of Data Act, or CLOUD Act, for which Justice Department officials have lobbied since 2016 and which President Donald Trump signed into law in March.

In addition to requiring American tech companies to provide data on U.S. citizens when served with a warrant, the CLOUD Act allows for so-called executive agreements between the president and foreign governments. These agreements, the first of which would be with the United Kingdom, would empower foreign law enforcement agencies to order U.S. tech companies to produce data about individual users without a warrant, so long as the search target is not a U.S. citizen or resident.

The Electronic Frontier Foundation, one of the organizations that signed the letter of protest, has described a possible scenario for how a U.K. police service might obtain data under the CLOUD Act: “London investigators want the private Slack messages of a Londoner they suspect of bank fraud. The London police could go directly to Slack, a U.S. company, to request and collect those messages. The London police would receive no prior judicial review for this request. The London police could avoid notifying U.S. law enforcement about this request. The London police would not need a probable cause warrant for this collection.”

But this form of international data-sharing could put Americans’ privacy at risk and expose citizens to potential Fourth Amendment abuses, critics say.

While the CLOUD Act requires that foreign police services not “intentionally target a United States person or a person located in the United States,” the law does not stop foreign police agencies from receiving communications of U.S. citizens or residents. Using the Electronic Frontier Foundation’s example of a Londoner communicating on Slack, any communications between the targeted British citizen and Americans would also be turned over to London police.

“The phrase ‘intentionally target’ creates a large loophole; people in the U.S. and U.S. persons overseas could easily get caught in the dragnet,” said Sarah St.Vincent, an investigator with Human Rights Watch, another signatory to the Justice Department letter. Although such so-called minimization procedures are ostensibly in place to prevent foreign governments from ensnaring U.S. users, St.Vincent told The Intercept that she rejects the notion that they “should be reassuring to anyone,” as “procedures are not laws,” but rather safeguards. “I don’t see any mechanism in here to ensure that those are strictly applied and inspected,” St.Vincent added.

The CLOUD Act also leaves open the possibility that a foreign police agency could obtain, without a warrant, incriminating communications from a U.S. citizen, which could then be shared with U.S. law enforcement. Data obtained in this way could not be used as evidence in a U.S. court, because its collection would violate Fourth Amendment protections. But local, state, or federal law enforcement agencies could reacquire the communications after obtaining a warrant — a controversial law enforcement practice known as “parallel construction.”

Federal law enforcement agencies, including the FBI and the Drug Enforcement Administration, already use parallel construction to launder information acquired from the warrantless wiretapping programs exposed by National Security Agency whistleblower Edward Snowden. In November 2017, The Intercept reported how the FBI used parallel construction to enter information first obtained through the government’s mass surveillance programs into evidence in terrorism trials. In these cases, prosecutors did not disclose to the courts that investigators had obtained the evidence from warrantless surveillance and then re-obtained it using legitimate warrants.

A Human Rights Watch report released in January documented how the DEA set up a unit called the Special Operations Division to receive raw intelligence from the NSA and disseminate leads to field agents. Agents on the ground were instructed to conceal the source of their information and find other ways to justify searches and broader investigations.

“The CLOUD Act would specifically allow the U.K. authorities to pass data belonging to U.S. persons back to the U.S. authorities if it ‘relates to significant harm, or the threat thereof, to the United States or United States persons’ — quite a significant loophole,” St.Vincent said. “The U.S. authorities can’t deliberately set up this end run around the Fourth Amendment themselves, but they’re free to sit back and receive whatever the U.K. sees fit to share.”

DESPITE THE ACT’S worrying implications for user privacy, the American tech vanguard has embraced it. In February, Google, Apple, Facebook, Microsoft, and Oath (formerly Yahoo) wrote to four U.S. senators detailing their support for the legislation, claiming that CLOUD “reflects a growing consensus in favor of protecting Internet users,” and “would be notable progress to protect consumers’ rights and would reduce conflicts of law.” In September, Reuters reported that Apple was building an “online tool” that would allow police around the world to more easily request the company’s user data.

Only after the CLOUD Act was passed did Microsoft, one of the law’s early boosters, address questions of personal privacy and offer assurances that the framework would not be abused. In a September blog post, Microsoft president and top lawyer Brad Smith announced “six principles that have driven, and will continue to drive, our advocacy as governments reform their laws and negotiate international agreements,” including a “universal right” for users to be notified if their data is accessed, and the ability to “challenge unlawful and inappropriate demands for user data.” Even so, Smith’s post states unequivocally that Microsoft believes the “passage of the CLOUD Act created the foundation for a new generation of international agreements that allows governments to engage with each other to create lasting rules to protect privacy.” Other tech firms have simply remained silent.
IS IT POSSIBLE to tell whether someone is a criminal just from looking at their face or listening to the sound of their voice? The idea may seem ludicrous, like something out of science fiction — Big Brother in “1984” detects any unconscious look “that carried with it the suggestion of abnormality” — and yet, some companies have recently begun to answer this question in the affirmative. AC Global Risk, a startup founded in 2016, claims to be able to determine your level of “risk” as an employee or an asylum-seeker based not on what you say, but how you say it.

The California-based company offers an automated screening system known as a Remote Risk Assessment, or RRA. Here’s how it works: Clients of AC Global Risk help develop automated, yes-or-no interview questions. The group of people selected for a given screening then answer these simple questions in their native language during a 10-minute interview that can be conducted over the phone. The RRA then measures the characteristics of their voice to produce an evaluation report that scores each individual on a spectrum from low to high risk. CEO Alex Martin has said that the company’s proprietary risk analysis can “forever change for the better how human risk is measured.”

AC Global Risk, which boasts the consulting firm of Robert Gates, Condoleezza Rice, and Stephen Hadley on its advisory board, has advertised contracts with the U.S. Special Operations Command in Afghanistan, the Ugandan Wildlife Authority, and the security teams at Palantir, Apple, Facebook, and Google, among others. The extensive use of risk screening in these and other markets, Martin has said, has proven that it is “highly accurate, scalable, cost-effective, and capable of high throughput.” AC Global Risk claims that its RRA system can simultaneously process hundreds of individuals anywhere in the world. Now, in response to President Donald Trump’s calls for the “extreme vetting” of immigrants, the company has pitched itself as the ultimate solution for “the monumental refugee crisis the U.S. and other countries are currently experiencing.”

It’s a proposal that would seem to appeal to the U.S. Department of Homeland Security. The DHS has already funded research to develop similar AI technology for the border. The program, known as the Automated Virtual Agent for Truth Assessments in Real-Time, or AVATAR, used artificial intelligence to measure changes in the voice, posture, and facial gestures of travelers in order to flag those who appeared untruthful or seemed to pose a potential risk. In 2012 it was tested by volunteers at the U.S.-Mexico border. The European Union has also funded research into technology that would reduce “the workload and subjective errors caused by human agents.”

Some of the leading experts in vocal analytics, algorithmic bias, and machine learning find the trend toward digital polygraph tests troubling, pointing to the faulty methodology of companies like AC Global Risk. “There is some information in dynamic changes in the voice and they’re detecting it. This is perfectly plausible,” explained Alex Todorov, a Princeton University psychologist who studies the science of social perception and first impressions. “But the question is, How unambiguous is this information at detecting the category of people they’ve defined as risky? There is always ambiguity in these kinds of signals.”

Over the past year, the American Civil Liberties Union and others have reported that Border Patrol agents have been seizing people from Greyhound buses based on their appearance or accent. Because Customs and Border Protection agents already use information about how someone speaks or looks as a pretext to search individuals in the 100-mile border zone, or to deny individuals entry to the U.S., experts fear that vocal emotion detection software could make such biases routine, pervasive, and seemingly “objective.”

AC Global Risk declined to respond to repeated requests for comment for this article. The company also did not respond to a list of detailed questions about how the technology works. In public appearances, however, Martin has claimed that the company’s proprietary analytical processes can determine someone’s risk level with greater than 97 percent accuracy. (AVATAR, meanwhile, claims an accuracy rate of between 60 and 70 percent.) Several leading audiovisual experts who reviewed AC Global Risk’s publicly available materials for The Intercept used the word “bullshit” or “bogus” to describe the company’s claims. “From an ethical point of view, it’s very dubious and shady to give the impression that recognizing deception from only the voice can be done with any accuracy,” said Björn Schuller, a professor at the University of Augsburg who has led the field’s major academic challenge event to advance the state of the art in vocal emotion detection. “Anyone who says they can do this should themselves be seen as a risk.”
LEIA EM PORTUGUÊS 
IN THE VIDEO game aisle of a Walmart Supercenter, Eric, 43, is refreshing his phone. The Superman logo on his T-shirt has been reworked into a hammer and sickle. He’s waiting to hear back from a stranger based, as far as he knows, in Kenya. “He just offered to pay $400,” Eric says. “I don’t feel 100 percent on him, but I don’t have anything real to base it on.”

Eric is a currency broker. But he doesn’t work on Wall Street, or at the Chicago Mercantile Exchange, or with any financial institution. He’s one of the thousands of Americans who have built a career out of the bitcoin phenomenon, whether out of necessity or a sense of entrepreneurship. It’s enabled Eric to stop toiling as an information technology engineer for software companies and set his own hours. “Never in my life had I thought I’d work for myself,” Eric explains.

His operation mainly consists of a phone and a couple of apps. But it also requires daily visits to his version of the trading floor: Walmart. Eric visits big box stores so often that he knows the self-checkout machines (his preferred method of payment) better than the clerks who work there. He goes to Walmart to buy gift cards, which serve as a medium of exchange and a means of protection from being duped by his clients. But the whole scheme relies on exploiting Walmart’s rather lax standard on gift card purchases. Eric believes, through his up-close experience, that these lax standards have been facilitating rampant gift card fraud, which has risen to epidemic levels as of late.

Eric will end up netting $12 on this transaction, but he did it at a discount, to make sure he had a customer. He wanted to make sure he could demonstrate to a reporter precisely how it’s done, on the condition that his real name not be used. The goal, he said, was to expose vulnerabilities in Walmart’s gift card policy, so that the company would take action. On Tuesday, Walmart did just that, sort of, announcing new rules in the way it treats such cards, upending a world that Eric offered The Intercept an invitation to explore.

The new policy will make it difficult for Eric to make ends meet, but he’s glad it happened. “I’m a socialist in a capitalist world,” he explains.

ERIC BECAME FASCINATED with bitcoin while using it for offshore gambling sites. That led him to his trading business, which he’s operated for over three years.

Here’s how it works: Eric sells bitcoin to buyers all over the world through a peer-to-peer marketplace called Paxful, charging a premium for the cryptocurrency. When Eric started his business a couple of years ago, he could charge a 100 percent markup — selling $300 in bitcoin for $600 — because buyers expected the cryptocurrency’s price to elevate, and had few options to obtain it. Nowadays, with more volatility and more competition among traders, that has dropped to around 30 percent. Paxful takes a small slice of each transaction on their service, usually about 1 percent.

In exchange for the bitcoin, the buyers typically swap gift cards, which are less traceable than dollars or yuan and, thanks to globalization, broadly useful all over the world. No physical card changes hands; Eric will get the number from the gift card and load it into an app called GiftMe, which generates a barcode that can be used at a checkout counter.

Sometimes the transaction stops there: Eric can use a gift card to buy essentials. But repeated trading requires an endless stream of bitcoin, more than he could mine or purchase on his own. So he has to use the gift cards to acquire more bitcoin, making his money on the spread between what he’s charged to buy it and what he charges to sell it.

As an intermediate step, Eric uses the gift cards he receives to buy other gift cards. This is important for a couple of reasons. First, certain gift cards are desirable to resellers overseas. Cards for electronics are especially attractive — iTunes, PlayStation, and Steam in particular. Those can be sold to customers in Brazil, Pakistan, and elsewhere. Even particular denominations are seen as attractive, because they can be more easily sold on the street. “If I take all these $10 cards, they won’t have any in stock for a month,” Eric says, rummaging through the gift card displays, explaining why he’s willing to travel 45 minutes to Walmarts in his area, just looking for the right cards.

Buying new gift cards also performs a kind of asset protection function for Eric. Scams are commonplace in the gift card trading world. Eric tries to engage in due diligence with his buyers, checking their prior transactions on Paxful. (If there are disputes about transactions, Paxful will help settle them with a court-like process.) He delays the release of bitcoin to his clients until he spends every last cent physically in a store, to ensure that the transaction is legitimate. But exchanging gift cards for gift cards adds to his security. “It’s so I have a card that no one but me and whoever I’m selling it to has seen,” he says. “I do try to avoid getting burned, but also try to avoid being part of burning someone else.”

Armed with fresh cards, Eric then sells them to accumulate more bitcoin. Again, the cards don’t actually change hands; Eric sends the codes through WhatsApp or another messaging system. The margin between what Eric charges for bitcoin and what others charge him represents his profit. Eric, because of how long he’s been in the business, has become adept at finding the best spreads, whereas his customers just want their bitcoin. On his trade with the Kenyan for the $400 gift card, he charged a 28 percent markup, and flipped the cards for bitcoin at a 25 percent markup. He walked away with a 3 percent margin, or $12.

If Eric can replicate that, he can make a living wage; some days, he’ll sell as much as $12,000 in bitcoin, which can translate to $300 or $400. Eric estimates that thousands of people exist in this informal economy, whether selling gift cards on the street, to pawn shops, or through online exchanges. Illicitly acquired gift cards have even been used to pay for opioids.

Technically speaking, reselling gift cards violates the terms of service. Everything else Eric does merely involves trading one legal thing of value for another. But he is very open about the nature of the world he traffics in. “I might sound to you like a scammer,” he says. “Don’t worry, I won’t take it the wrong way.”

The prevalence of illicit activity with gift cards, which invites a crackdown, has posed a threat to his career, with the only saving grace being the relative indifference at Walmart.

WALMART WAS NOT originally Eric’s favorite store to carry out his business. “I was on a first-name basis with everybody at Best Buy and Target for a long time,” he says. But over the summer, he went to ring up an order at a self-checkout counter at Target, and discovered that the company no longer allowed people to buy gift cards with other gift cards. “It changed overnight. The people at the stores were blindsided.” Days later, the same thing happened with Best Buy.

The companies, which do a lot of business in gift cards, had good reason to restrict purchases. A Federal Trade Commission bulletin in May warned Americans of an epidemic of gift card fraud. Specifically, the FTC highlighted callers claiming to be with the IRS or a family member and asking for payments in gift cards. Unsuspecting victims then buy gift cards and hand over the codes. Scammers can use them to either buy goods and services, or flip them in the resale market while simultaneously draining them of funds, making money twice on the same card.

This is just one way gift cards can lead to fraud. “Another way would be hacking accounts: People get the account numbers and take the balance off them,” said Joyce Carter, a vice president with Member Access Pacific, which manages gift and other card programs for businesses and credit unions. This can be done manually, by stealing cards at stores or memorizing card numbers at the sale display, or through more sophisticated means, like computer-aided phishing for card numbers or counterfeiting. There is rich variety in the scams.

Buying gift cards with other gift cards launders fraudulent or illegally obtained card codes into new, legitimate ones. It’s a common method for scam artists. “That type of fraud is happening a lot,” said Carter. According to the National Retail Federation, organized retail crime, which includes but is not limited to gift card fraud, costs retailers $726,351 for every $1 billion in sales.

Though retailers have traditionally not been on the hook for gift card fraud, that liability has shifted somewhat with the advent of chip readers at retail outlets. In particular, merchants that allow customers to swipe cards instead of reading them with chips face chargebacks from issuers on counterfeit transactions. Because of these new rules, retailers started to require that all gift card purchases be made in cash.

In a statement, Target spokesperson Danielle Schumann said the company “takes a comprehensive approach to preventing gift card scams that includes partnerships with law enforcement, technology enhancements and team member training.” That includes barring gift card purchases with other gift cards, as laid out on the Target website. Best Buy did not respond to inquiries, but their website notes that gift card purchases are limited to $500, and Eric’s experience indicates that they no longer let customers make gift-card-for-gift-card swaps.

Eric claimed to me that Walmart, prior to this week, had no restrictions on purchasing gift cards with gift cards. And indeed, I watched him pull it off.

After Eric got the $400 Walmart gift card code from the bitcoin buyer in Kenya, he pasted it into the GiftMe app, generating a bar code. He picked up seven $10 PlayStation cards, and a handful of $20 and $30 cards from PlayStation and Steam. We went to pay at the counter, and the sales clerk methodically scanned and activated the stack of gift cards. Eric was able to pay with the $400 Walmart gift card he’d purchased seconds earlier from someone halfway around the world. He didn’t have to show ID or the physical card. “You have a nice day,” Eric said to the clerk upon leaving.

Eric paid for the gift card used for his purchases. But it could just as easily have been a counterfeit, or a clone, or a stolen card, laundered through Walmart’s transaction into something legitimate. Back in the parking lot, Eric told me, “I walk out of Walmart all the time thinking that someone like me, with less scruples, could be walking out, too, without getting hassled.”

When I asked Carter about Walmart’s practices, she responded that the retailer should be more mindful of fraud. “If I’m buying more than five or six gift cards, [Walmart] might want to think about security,” she said.

Though Walmart has relatively more liability for gift card shenanigans, Carter said the burden remains disproportionately placed on consumer victims, and in particular the issuers that allow their card to be sold at Walmart. “They’re the ones taking the most risk. Most of the time the retailer doesn’t have anything to lose when it comes to fraud.”

Eric’s other interactions with Walmart left him skeptical that the company has much interest in preventing gift card fraud. Recently, a scammer tried to sell him two $1,000 Walmart gift cards, and when he checked them, he saw they were already being spent down online. “These were online orders that hadn’t shipped yet; Walmart could stop the orders to stop the rip-off,” he said. “They had no interest. I got all the way to a supervisor, and he said, ‘We don’t involve ourselves in a consumer’s personal business.’ But it’s fraud! They’re being used to facilitate fraud.”

In other words, Eric says, Walmart views gift cards as cash, and it doesn’t go around wondering whether purchases at their stores are made with stolen dollars. This is true of most retailers. “To them, revenue is revenue,” Eric said. “They very specifically don’t give a fuck about crimes involving their store if they’re not directly liable or directly hurt.” It can also be difficult to solve those crimes, putting a Walmart employee in the position of adjudicating which owner of the $1,000 gift card has the legal right to use it.

In October, Walmart spokesperson Randy Hargrove said in a statement to The Intercept, “We take this issue seriously and have measures designed to help guard against these types of crimes. Like many retailers, we are looking at this issue, the controls we have in place, and we are continuously working to enhance our gift card program to better serve and protect customers.”

It turns out that Walmart was more involved in reforming its practices than I knew. After a year-long investigation by the attorneys general of Pennsylvania and New York, Walmart, Best Buy, and Target announced new nationwide policies to deal with gift cards. Reuters reported that the changes would lead to “prohibiting the redemption of store-branded gift cards for other gift cards” — exactly what Eric wanted to happen.

That’s not quite right. As Pennsylvania Attorney General Josh Shapiro detailed in a press release, the main changes included limits to the monetary value that gift cards are sold for, and how much money can be loaded onto a gift card. As for trading gift cards for gift cards, there are restrictions on the purchase of iTunes, Steam, or Google Play gift cards, some of the main ones sold into the black market.

 
In partnership with
AT THE BEGINNING of October, Amazon was quietly issued a patent that would allow its virtual assistant Alexa to decipher a user’s physical characteristics and emotional state based on their voice. Characteristics, or “voice features,” like language accent, ethnic origin, emotion, gender, age, and background noise would be immediately extracted and tagged to the user’s data file to help deliver more targeted advertising.

The algorithm would also consider a customer’s physical location — based on their IP address, primary shipping address, and browser settings — to help determine their accent. Should Amazon’s patent become a reality, or if accent detection is already possible, it would introduce questions of surveillance and privacy violations, as well as possible discriminatory advertising, experts said.

The civil rights issues raised by the patent are similar to those around facial recognition, another technology Amazon has used as an anchor of its artificial intelligence strategy, and one that it controversially marketed to law enforcement. Like facial recognition, voice analysis underlines how existing laws and privacy safeguards simply aren’t capable of protecting users from new categories of data collection — or government spying, for that matter. Unlike facial recognition, voice analysis relies not on cameras in public spaces, but microphones inside smart speakers in our homes. It also raises its own thorny issues around advertising that targets or excludes certain groups of people based on derived characteristics like nationality, native language, and so on (the sort of controversy that Facebook has stumbled into again and again).
AT THE BEGINNING of October, Amazon was quietly issued a patent that would allow its virtual assistant Alexa to decipher a user’s physical characteristics and emotional state based on their voice. Characteristics, or “voice features,” like language accent, ethnic origin, emotion, gender, age, and background noise would be immediately extracted and tagged to the user’s data file to help deliver more targeted advertising.

The algorithm would also consider a customer’s physical location — based on their IP address, primary shipping address, and browser settings — to help determine their accent. Should Amazon’s patent become a reality, or if accent detection is already possible, it would introduce questions of surveillance and privacy violations, as well as possible discriminatory advertising, experts said.

The civil rights issues raised by the patent are similar to those around facial recognition, another technology Amazon has used as an anchor of its artificial intelligence strategy, and one that it controversially marketed to law enforcement. Like facial recognition, voice analysis underlines how existing laws and privacy safeguards simply aren’t capable of protecting users from new categories of data collection — or government spying, for that matter. Unlike facial recognition, voice analysis relies not on cameras in public spaces, but microphones inside smart speakers in our homes. It also raises its own thorny issues around advertising that targets or excludes certain groups of people based on derived characteristics like nationality, native language, and so on (the sort of controversy that Facebook has stumbled into again and again).
SO MUCH ATTENTION in the midterm elections this year has focused on the gubernatorial race in Georgia between Republican Brian Kemp and Democrat Stacey Abrams that the race for secretary of state, the office Kemp is vacating, has gone largely ignored.

It’s arguably the more important race, since this is the office that will control the state’s voter registration database and any purges made to the voter roll going forward. Equally important, it’s the office that will be responsible for programming all of the state’s currently paperless voting machines that can’t be audited, though Georgia will be looking to replace these machines with an undetermined model next year. Both of these factors could make Georgia a hotbed for voter suppression tactics and vote-counting integrity in the 2020 presidential elections, experts said.
APPARENTLY FUELED BY anti-Semitism and the bogus narrative that outside forces are scheming to exterminate the white race, Robert Bowers murdered 11 Jewish congregants as they gathered inside their Pittsburgh synagogue, federal prosecutors allege. But despite long-running international efforts to debunk the idea of a “white genocide,” Facebook was still selling advertisers the ability to market to those with an interest in that myth just days after the bloodshed.

Earlier this week, The Intercept was able to select “white genocide conspiracy theory” as a pre-defined “detailed targeting” criterion on the social network to promote two articles to an interest group that Facebook pegged at 168,000 users large and defined as “people who have expressed an interest or like pages related to White genocide conspiracy theory.” The paid promotion was approved by Facebook’s advertising wing. After we contacted the company for comment, Facebook promptly deleted the targeting category, apologized, and said it should have never existed in the first place.

Our reporting technique was the same as one used by the investigative news outlet ProPublica to report, just over one year ago, that in addition to soccer dads and Ariana Grande fans, “the world’s largest social network enabled advertisers to direct their pitches to the news feeds of almost 2,300 people who expressed interest in the topics of ‘Jew hater,’ ‘How to burn jews,’ or, ‘History of “why jews ruin the world.”’” The report exposed how little Facebook was doing to vet marketers, who pay the company to leverage personal information and inclinations in order to gain users’ attention — and who provide the foundation for its entire business model. At the time, ProPublica noted that Facebook “said it would explore ways to fix the problem, such as limiting the number of categories available or scrutinizing them before they are displayed to buyers.” Rob Leathern, a Facebook product manager, assured the public, “We know we have more work to do, so we’re also building new guardrails in our product and review processes to prevent other issues like this from happening in the future.”

Leathern’s “new guardrails” don’t seem to have prevented Facebook from manually approving our ad buy the same day it was submitted, despite its explicit labeling as “White Supremacy – Test.”
GOOGLE CEO SUNDAR PICHAI has refused to answer a list of questions from U.S. lawmakers about the company’s secretive plan for a censored search engine in China.

In a letter newly obtained by The Intercept, Pichai told a bipartisan group of six senators that Google could have “broad benefits inside and outside of China,” but said he could not share details about the censored search engine because it “remains unclear” whether the company “would or could release a search service” in the country.

Pichai’s letter contradicts the company’s search engine chief, Ben Gomes, who informed staff during a private meeting that the company was aiming to release the platform in China between January and April 2019. Gomes told employees working on the Chinese search engine that they should get it ready to be “brought off the shelf and quickly deployed.”

According to sources and confidential Google documents, the search engine for China, codenamed Dragonfly, was designed to comply with the strict censorship regime imposed by China’s ruling Communist Party. It would restrict people’s access to broad categories of information, blacklisting phrases like “human rights,” “student protest,” and “Nobel Prize.”

The Chinese platform was designed to link people’s searches to their phone number, track their location, and then share that data with a Chinese partner company. This would make it easy to track individual users’ searches, raising concerns that any person in China using Google to seek out information banned by the government could be at risk of interrogation or detention if security agencies were to obtain copies of their search records.
WHILE THE WORLD is grappling with the apparent grisly murder of Saudi dissident and Washington Post journalist Jamal Khashoggi, the Saudi government decided to announce a new band of influential Western allies, some plucked from the uppermost echelon of Silicon Valley, who would serve on an advisory board for Neom, the Saudi government’s improbable, exorbitant plan to build a “mega city” in the desert.

But almost as soon as his participation was revealed, Sam Altman, head of famed venture capital firm Y Combinator, announced that he is “suspending” his role with Neom, while two others on the star-studded list denied that they were participating.

Altman, along with legendary tech investor Marc Andreessen, notorious Uber founder (and ousted ex-CEO) Travis Kalanick, IDEO CEO Tim Brown, and Dan Doctoroff of Sidewalk Labs, a subsidiary of Google-owner Alphabet, were among those listed as members of the new board. Given that the United States itself is now forced into a momentarily uncomfortable spot given its longtime affection for and deep political ties to the Saudis, this was a less than ideal time for Americans to come out as friends of the kingdom.
A NEW REPORT from the U.S. Government Accountability Office brings both good and bad news. For governments around the world that might like to sabotage America’s military technology, the good news is that this would be all too easy to do: Testers at the Department of Defense “routinely found mission-critical cyber vulnerabilities in nearly all weapon systems that were under development” over a five-year period, the report said. For Americans, the bad news is that up until very recently, no one seemed to care enough to fix these security holes.

In 1991, the report noted, the U.S. National Research Council warned that “system disruptions will increase” as the use of computers and networks grows and as adversaries attack them. The Pentagon more or less ignored this and at least five subsequent warnings on the subject, according to the GAO, and hasn’t made a serious effort to safeguard the vast patchwork of software that controls planes, ships, missiles, and other advanced ordnance against hackers.

The sweeping report drew on nearly 30 years of published research, including recent assessments of the cybersecurity of specific weapon systems, as well as interviews with personnel from the Department of Defense, the National Security Agency, and weapons-testing bodies. It covered a broad span of American weapons, examining systems at all of the service branches and in space.

The report found that “mission-critical cyber vulnerabilities” cropped up routinely during weapons development and that test teams “easily” took over real systems without detection “using relatively simple tools and techniques,” exploiting “basic issues such as poor password management and unencrypted communications.” Testers could also download and delete data, in one cases exfiltrating 100 gigabytes of material, and could tap into operators’ terminals, in one instance popping up computer dialogs asking the operators “to insert two quarters to continue.” But a malicious attacker could pull off much worse than jokes about quarters, warns the GAO: “In one case, the test team took control of the operators’ terminals. They could see, in real-time, what the operators were seeing on their screens and could manipulate the system.”

Posing as surrogates for, say, Russian or Chinese military hackers, testers sometimes found easy victories. “In some cases,” the GAO found, “simply scanning a system caused parts of the system to shut down,” while one “test team was able to guess an administrator password in nine seconds.” The testers found embarrassing, elementary screw-ups of the sort that would get a middle school computer lab administrator in trouble, to say nothing of someone safeguarding lethal weapon systems. For example, “multiple weapon systems used commercial or open source software, but did not change the default password when the software was installed, which allowed test teams to look up the password on the Internet.”
“WE HAVE TO be focused on what we want to enable,” said Ben Gomes, Google’s search engine chief. “And then when the opening happens, we are ready for it.”

It was Wednesday, July 18, and Gomes was addressing a team of Google employees who were working on a secretive project to develop a censored search engine for China, which would blacklist phrases like “human rights,” “student protest,” and “Nobel Prize.”

“You have taken on something extremely important to the company,” Gomes declared, according to a transcript of his comments obtained by The Intercept. “I have to admit it has been a difficult journey. But I do think a very important and worthwhile one. And I wish ourselves the best of luck in actually reaching our destination as soon as possible.”

Gomes joked about the unpredictability of President Donald Trump and groaned about the ongoing trade war between the U.S. and China, which has slowed down Google’s negotiations with Communist Party officials in Beijing, whose approval Google requires to launch the censored search engine.
“WE HAVE TO be focused on what we want to enable,” said Ben Gomes, Google’s search engine chief. “And then when the opening happens, we are ready for it.”

It was Wednesday, July 18, and Gomes was addressing a team of Google employees who were working on a secretive project to develop a censored search engine for China, which would blacklist phrases like “human rights,” “student protest,” and “Nobel Prize.”

“You have taken on something extremely important to the company,” Gomes declared, according to a transcript of his comments obtained by The Intercept. “I have to admit it has been a difficult journey. But I do think a very important and worthwhile one. And I wish ourselves the best of luck in actually reaching our destination as soon as possible.”

Gomes joked about the unpredictability of President Donald Trump and groaned about the ongoing trade war between the U.S. and China, which has slowed down Google’s negotiations with Communist Party officials in Beijing, whose approval Google requires to launch the censored search engine.
AFTER CALIFORNIA PASSED the most sweeping online privacy law in the nation this summer, big tech went back to the state legislature to weaken it. While that effort fizzled before the end of the state’s legislative session, a more insidious strategy emerged this week: going around California and appealing to Congress.

Alastair Mactaggart, who led the California effort, told The Intercept that a Wednesday hearing in Congress left him concerned that Congress might pre-empt the state legislation at the behest of giant tech firms.

“Tech has had zero regulation,” Mactaggart said in an interview. “For them it’s been this Wild West of being able to monetize information any which way. They will pull out all the stops to try to get back to where they were.”

Mactaggart, a Bay Area real estate developer, became an unlikely activist when he bankrolled a ballot measure that, among other things, would require tech companies to reveal what personal information they collected on users, allow users to opt out of the sale of their data to third parties, and impose fines for data breaches. Tech firms fought it vigorously, but were hampered by a series of scandals like Facebook’s release of data to Cambridge Analytica, and widespread popular support for some limits on persistent surveillance.

Tech firms, fearing being locked into a policy they could only change at the ballot, encouraged the state legislature to get involved. The California House and Senate passed a law substantially similar to the initiative, with unanimous support in both chambers. Gov. Jerry Brown signed it in June.

The law doesn’t take effect until January 2020, and big tech’s hope was to water it down before that date. Industry representatives went to Congress this week after failing to get California lawmakers to narrow the definition of “selling” data, which would have rendered some of the protections of the law moot.

The Senate Commerce Committee held a hearing on Wednesday where leaders of six tech and telecom companies — Amazon, Google, Apple, Twitter, AT&T, and Charter Communications — endorsed a federal consumer privacy standard. In their conception, this would pre-empt any state data protection laws.

It’s an eerie echo of the federal pre-emption that Wall Street banks received and used to great effect during the run-up to the housing bubble. In 2002, Georgia passed an anti-predatory lending law, and both the Office of Thrift Supervision and the Office of the Comptroller of the Currency ruled that banks they regulated simply did not have to comply with it. This created a chilling effect, as states declined to crack down on the rampant fraud in the mortgage industry. And the financial crisis was the result.

At the hearing, the companies criticized the California statute, in particular its definition of “personal information,” which they consider overbroad. Amazon vice president Andrew DeVore called the statute “confusing and difficult to comply with,” adding that it “may actually undermine important privacy-protective practices.” Alone among the witnesses, Apple’s Bud Tribble did say that while federal legislation should pre-empt state law, it must also “meet the bar of protecting consumers meaningfully.” However, he did not elaborate on what that protection should entail.

EIGHT YEARS AGO, when Google announced that it would pull out of China, the company released a statement explaining that the Chinese government had been “crystal clear” that “self-censorship is a non-negotiable legal requirement” for operating in the country.

But now that the firm is considering a relaunch in China with a new search platform, an initiative codenamed Project Dragonfly, Google is hedging on whether they believe the Chinese government censors its citizens.

Keith Enright, Google’s chief privacy officer, discussed the China project under questioning from several senators during a commerce committee hearing on Wednesday.

“In your opinion, does China engage in censoring its citizens?” asked Sen. Ted Cruz, R-Texas, during the hearing.

“As the privacy representative of Google,” said Enright, “I am not sure I have an informed opinion on that question.”
OMNIPRESENT FACIAL RECOGNITION has become a golden goose for law enforcement agencies around the world. In the United States, few are as eager as the Department of Homeland Security. American airports are currently being used as laboratories for a new tool that would automatically scan your face — and confirm your identity with U.S. Customs and Border Protection — as you prepare to board a flight, despite the near-unanimous objections from privacy advocates and civil libertarians, who call such scans invasive and pointless.

According to a new report on the Biometric Entry-Exit Program by DHS itself, we can add another objection: Your flight could be late.

Although the new report, published by Homeland Security’s Office of the Inspector General, is overwhelmingly supportive in its evaluation of airport-based biometric surveillance — the practice of a computer detecting your face and pairing it with everything else in the system — the agency notes some hurdles from a recent test code-named “Sprint 8.” Among them, the report notes with palpable frustration, was that airlines insist on letting their passengers depart on time, rather than subjecting them to a Homeland Security surveillance prototype plagued by technical issues and slowdowns:

Demanding flight departure schedules posed other operational problems that significantly hampered biometric matching of passengers during the pilot in 2017. Typically, when incoming flights arrived behind schedule, the time allotted for boarding departing flights was reduced. In these cases, CBP allowed airlines to bypass biometric processing in order to save time. As such, passengers could proceed with presenting their boarding passes to gate agents without being photographed and biometrically matched by CBP first. We observed this scenario at the Atlanta Hartsfield-Jackson International Airport when an airline suspended the biometric matching process early to avoid a flight delay. This resulted in approximately 120 passengers boarding the flight without biometric confirmation.
A SCIENTIST WHO quit Google over its plan to build a censored search engine in China has told U.S. senators that some company employees may have “actively subverted” an internal privacy review of the system.

Jack Poulson resigned from Google in August after The Intercept reported that a group of the internet giant’s staffers was secretly working on a search engine for China that would remove content about subjects such as human rights, democracy, peaceful protest, and religion. “I view our intent to capitulate to censorship and surveillance demands in exchange for access to the Chinese market as a forfeiture of our values and governmental negotiating position across the globe,” Poulson told his bosses.

Now, Poulson has sent a letter to members of the Senate Committee on Commerce, Science, and Transportation ahead of a hearing on Wednesday at which Keith Enright, Google’s chief privacy officer, is scheduled to appear. Despite a major internal and external backlash over a period of almost two months, Google has so far refused to publicly address questions about its China censorship plan, code-named Dragonfly. The appearance of Enright on Capitol Hill is likely to be the first time a representative of the company is forced to provide answers about the project.

In his letter to the senators, Poulson said that there has been “a pattern of unethical and unaccountable decision making from company leadership” at Google. He called on the lawmakers to pressure Enright to respond to concerns raised by 14 leading human rights groups, who said in late August that Dragonfly could result in Google “directly contributing to, or [becoming] complicit in, human rights violations.”
THE UNITED NATIONS accidentally published passwords, internal documents, and technical details about websites when it misconfigured popular project management service Trello, issue tracking app Jira, and office suite Google Docs.

The mistakes made sensitive material available online to anyone with the proper link, rather than only to specific users who should have access. Affected data included credentials for a U.N. file server, the video conferencing system at the U.N.’s language school, and a web development environment for the U.N.’s Office for the Coordination of Humanitarian Affairs. Security researcher Kushagra Pathak discovered the accidental leak and notified the U.N. about what he found a little over a month ago. As of today, much of the material appears to have been taken down.

In an online chat, Pathak said he found the sensitive information by running searches on Google. The searches, in turn, produced public Trello pages, some of which contained links to the public Google Docs and Jira pages.

Trello projects are organized into “boards” that contain lists of tasks called “cards.” Boards can be public or private. After finding one public Trello board run by the U.N., Pathak found additional public U.N. boards by using “tricks like by checking if the users of one Trello board are also active on some other boards and so on.” One U.N. Trello board contained links to an issue tracker hosted on Jira, which itself contained even more sensitive information. Pathak also discovered links to documents hosted on Google Docs and Google Drive that were configured to be accessible to anyone who knew their web addresses. Some of these documents contained passwords.
FACEBOOK’S TOTAL INABILITY to keep itself from being a convenient tool for genocidal incitement in Myanmar has been well-covered, now a case study in how a company with such immense global power can so completely fail to use it for good. But a new report released this week by the United Nations fact-finding mission in Myanmar, where calls for the slaughter of Muslims have enjoyed all the convenience of a modern Facebook signal boost, makes clear just how unprepared the company was for its role in an ethnic massacre.

In a recent New Yorker profile of Facebook founder and CEO Mark Zuckerberg, he responds to his company’s role in the crisis — which the U.N. has described as “determining” — with all the urgency and guilt of a botched restaurant order: “I think, fundamentally, we’ve been slow at the same thing in a number of areas, because it’s actually the same problem. But, yeah, I think the situation in Myanmar is terrible.” Zuckerberg added that the company needs to “move from what is fundamentally a reactive model” when it comes to blocking content that’s fueled what the U.N. described last year as a “textbook example of ethnic cleansing.”

The new report reveals just how broken this “reactive model” truly is.

According to the 479-page document, and as flagged in a broader Guardian story this week, “the Mission itself experienced a slow and ineffective response from Facebook when it used the standard reporting mechanism to alert the company to a post targeting a human rights defender for his alleged cooperation with the Mission.” What follows is the most clear-cut imaginable violation of Facebook’s rules, followed by the most abject failure to enforce them when it mattered most:

The post described the individual as a “national traitor”, consistently adding the adjective “Muslim”. It was shared and re-posted over 1,000 times. Numerous comments to the post explicitly called for the person to be killed, in unequivocal terms: “Beggar-dog species. As long as we are feeling sorry for them, our country is not at peace. These dogs need to be completely removed.” “If this animal is still around, find him and kill him. There needs to be government officials in NGOs.” “Wherever they are, Muslim animals don’t know to be faithful to the country.” “He is a Muslim. Muslims are dogs and need to be shot.” “Don’t leave him alive. Remove his whole race. Time is ticking.” The Mission reported this post to Facebook on four occasions; in each instance the response received was that the post was examined but “doesn’t go against one of [Facebook’s] specific Community Standards”. The Mission subsequently sent a message to an official Facebook email account about the matter but did not receive a response. The post was finally removed several weeks later but only through the support of a contact at Facebook, not through the official channel. Several months later, however, the Mission found at least 16 re-posts of the original post still circulating on Facebook. In the weeks and months after the post went online, the human rights defender received multiple death threats from Facebook users, warnings from neighbours, friends, taxi drivers and other contacts that they had seen his photo and the posts on Facebook, and strong suggestions that the post was an early warning. His family members were also threatened. The Mission has seen many similar cases where individuals, usually human rights defenders or journalists, become the target of an online hate campaign that incites or threatens violence.
GOOGLE BOSSES HAVE forced employees to delete a confidential memo circulating inside the company that revealed explosive details about a plan to launch a censored search engine in China, The Intercept has learned.

The memo, authored by a Google engineer who was asked to work on the project, disclosed that the search system, codenamed Dragonfly, would require users to log in to perform searches, track their location — and share the resulting history with a Chinese partner who would have “unilateral access” to the data.

The memo was shared earlier this month among a group of Google employees who have been organizing internal protests over the censored search system, which has been designed to remove content that China’s authoritarian Communist Party regime views as sensitive, such as information about democracy, human rights, and peaceful protest.

According to three sources familiar with the incident, Google leadership discovered the memo and were furious that secret details about the China censorship were being passed between employees who were not supposed to have any knowledge about it. Subsequently, Google human resources personnel emailed employees who were believed to have accessed or saved copies of the memo and ordered them to immediately delete it from their computers. Emails demanding deletion of the memo contained “pixel trackers” that notified human resource managers when their messages had been read, recipients determined.
GOOGLE BUILT A prototype of a censored search engine for China that links users’ searches to their personal phone numbers, thus making it easier for the Chinese government to monitor people’s queries, The Intercept can reveal.

The search engine, codenamed Dragonfly, was designed for Android devices, and would remove content deemed sensitive by China’s ruling Communist Party regime, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest.

Previously undisclosed details about the plan, obtained by The Intercept on Friday, show that Google compiled a censorship blacklist that included terms such as “human rights,” “student protest,” and “Nobel Prize” in Mandarin.

Leading human rights groups have criticized Dragonfly, saying that it could result in the company “directly contributing to, or [becoming] complicit in, human rights violations.” A central concern expressed by the groups is that, beyond the censorship, user data stored by Google on the Chinese mainland could be accessible to Chinese authorities, who routinely target political activists and journalists.

Sources familiar with the project said that prototypes of the search engine linked the search app on a user’s Android smartphone with their phone number. This means individual people’s searches could be easily tracked – and any user seeking out information banned by the government could potentially be at risk of interrogation or detention if security agencies were to obtain the search records from Google.
GOOGLE BUILT A prototype of a censored search engine for China that links users’ searches to their personal phone numbers, thus making it easier for the Chinese government to monitor people’s queries, The Intercept can reveal.

The search engine, codenamed Dragonfly, was designed for Android devices, and would remove content deemed sensitive by China’s ruling Communist Party regime, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest.

Previously undisclosed details about the plan, obtained by The Intercept on Friday, show that Google compiled a censorship blacklist that included terms such as “human rights,” “student protest,” and “Nobel Prize” in Mandarin.

Leading human rights groups have criticized Dragonfly, saying that it could result in the company “directly contributing to, or [becoming] complicit in, human rights violations.” A central concern expressed by the groups is that, beyond the censorship, user data stored by Google on the Chinese mainland could be accessible to Chinese authorities, who routinely target political activists and journalists.

Sources familiar with the project said that prototypes of the search engine linked the search app on a user’s Android smartphone with their phone number. This means individual people’s searches could be easily tracked – and any user seeking out information banned by the government could potentially be at risk of interrogation or detention if security agencies were to obtain the search records from Google.
LINKNYC KIOSKS HAVE become a familiar eyesore to New Yorkers. Over 1,600 of these towering, nine-and-a-half-foot monoliths — their double-sided screens festooned with ads and fun facts — have been installed across the city since early 2016. Mayor Bill de Blasio has celebrated their ability to provide “the fastest and largest municipal Wi-Fi network in the world” as “a critical step toward a more equal, open, and connected city for every New Yorker, in every borough.” Anyone can use the kiosks’ Android tablets to search for directions and services; they are also equipped with charging stations, 911 buttons, and phones for free domestic calls.

But even as the kiosks have provided important services to connect New Yorkers, they may also represent a troubling expansion of the city’s surveillance network, potentially connecting every borough to a new level of invasive monitoring. Each kiosk has three cameras, 30 sensors, and heightened sight lines for viewing above crowds.

Since plans for LinkNYC were first unveiled, journalists, residents, and civil liberties experts have raised concerns that the internet kiosks might be storing sensitive data about its users and possibly tracking their movements. For the last two years, the American Civil Liberties Union, Electronic Frontier Foundation, and a small but vocal group of activists — including ReThink LinkNYC, a grassroots anti-surveillance group, and the anonymous Stop LinkNYC coalition — have highlighted the kiosk’s potential to track locations, collect personal information, and fuel mass surveillance.

Now an undergraduate researcher has discovered indications in LinkNYC code — accidentally made public on the internet — that LinkNYC may be actively planning to track users’ locations.
LINKNYC KIOSKS HAVE become a familiar eyesore to New Yorkers. Over 1,600 of these towering, nine-and-a-half-foot monoliths — their double-sided screens festooned with ads and fun facts — have been installed across the city since early 2016. Mayor Bill de Blasio has celebrated their ability to provide “the fastest and largest municipal Wi-Fi network in the world” as “a critical step toward a more equal, open, and connected city for every New Yorker, in every borough.” Anyone can use the kiosks’ Android tablets to search for directions and services; they are also equipped with charging stations, 911 buttons, and phones for free domestic calls.

But even as the kiosks have provided important services to connect New Yorkers, they may also represent a troubling expansion of the city’s surveillance network, potentially connecting every borough to a new level of invasive monitoring. Each kiosk has three cameras, 30 sensors, and heightened sight lines for viewing above crowds.

Since plans for LinkNYC were first unveiled, journalists, residents, and civil liberties experts have raised concerns that the internet kiosks might be storing sensitive data about its users and possibly tracking their movements. For the last two years, the American Civil Liberties Union, Electronic Frontier Foundation, and a small but vocal group of activists — including ReThink LinkNYC, a grassroots anti-surveillance group, and the anonymous Stop LinkNYC coalition — have highlighted the kiosk’s potential to track locations, collect personal information, and fuel mass surveillance.

Now an undergraduate researcher has discovered indications in LinkNYC code — accidentally made public on the internet — that LinkNYC may be actively planning to track users’ locations.
FACEBOOK CHIEF OPERATING officer Sheryl Sandberg draped herself in the star-spangled banner of American principles before today’s Senate Select Intelligence Committee hearing on social media. Sandberg proclaimed that democratic values of free expression were integral to the company’s conscience. “We would only operate in a country where we could do so in keeping with our values,” she went on. Either this was a lie told under oath, or Facebook has some pretty lousy values.
LEADING HUMAN RIGHTS groups are calling on Google to cancel its plan to launch a censored version of its search engine in China, which they said would violate the freedom of expression and privacy rights of millions of internet users in the country.

A coalition of 14 organizations — including Amnesty International, Human Rights Watch, Reporters Without Borders, Access Now, the Committee to Protect Journalists, the Electronic Frontier Foundation, the Center for Democracy and Technology, PEN International, and Human Rights in China — issued the demand Tuesday in an open letter addressed to the internet giant’s CEO, Sundar Pichai. The groups said the censored search engine represents “an alarming capitulation by Google on human rights” and could result in the company “directly contributing to, or [becoming] complicit in, human rights violations.”

The letter is the latest major development in an ongoing backlash over the censored search platform, code-named Dragonfly, which was first revealed by The Intercept earlier this month. The censored search engine would remove content that China’s ruling Communist Party regime views as sensitive, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to confidential Google documents.

Google launched a censored search engine in China in 2006, but ceased operating the service in the country in 2010, citing Chinese government efforts to limit free speech, block websites, and hack Google’s computer systems. The open letter released Tuesday asks Google to reaffirm the commitment it made in 2010 to no longer provide censored search in China.
ON AUGUST 13, FACEBOOK shut down the English-language page of Telesur, blocking access for roughly half a million followers of the leftist media network until it was abruptly reinstated two days later. Facebook has provided three different explanations for the temporary disappearing, all contradicting one another, and not a single one making sense.

Telesur was created by Venezuela’s then-President Hugo Chávez in 2005 and co-funded by hemispheric neighbors Cuba, Bolivia, Nicaragua, and Uruguay — Argentina pulled support for the web and cable property in 2016. As a state-owned media property, it exists somewhere on the same continuum as RT and Al Jazeera, though like the former, Telesur has been criticized as a nakedly partisan governmental mouthpiece, and like the latter, it does engage in real news reporting. But putting aside questions of bias and agenda, Telesur does seem to exist on a separate plane than, say, Infowars, which exists primarily to peddle its particular, patently false genre of right-wing paranoia fan fiction packaged as news (and brain pills), as opposed to some garden-variety political agenda. Unlike RT, Telesur hasn’t been singled-out for a role in laundering disinformation for military intelligence purposes, nor is it a hoax factory, à la Alex Jones.

So it was unexpected when Telesur English blinked out of existence on the 13th, and even stranger when Facebook struggled to explain its own actions. At the time of its suspension, Telesur received this boilerplate message from Facebook:

Hello,

Your Page “teleSUR English” has been removed for violating our Terms of Use. A Facebook Page is a distinct presence used solely for business or promotional purposes. Among other things, Pages that are hateful, threatening or obscene are not allowed. We also take down Pages that attack an individual or group, or that are set up by an unauthorised individual. If your Page was removed for any of the above reasons, it will not be reinstated. Continued misuse of Facebook’s features could result in the permanent loss of your account.

The Facebook Team

Later that day, a Facebook customer support agent told the network that the suspension appeared to be due to a technical glitch — a go-to explanation for the company — rather than a violation of the company’s Terms of Use, adding that the issue was “under analysis by the engineering department.”
GOOGLE BOSSES HAVE broken their silence on the company’s plan to launch a censored search engine in China amid mounting internal protests over the project.

On Thursday, CEO Sundar Pichai admitted to employees during an all-hands meeting that the censorship project – code-named Dragonfly – had been “in an exploration stage for quite a while now,” according to two sources who heard his remarks. Pichai emphasized his belief that Google should return to China, but claimed that the company was “not close to launching a search product in China.” Facing employee criticism for shrouding Dragonfly in secrecy, Pichai vowed that “we’ll definitely be transparent as we get closer to actually having a plan of record.”

Google co-founder Sergey Brin also spoke at Thursday’s meeting — and remarkably stated that he knew nothing about Dragonfly until The Intercept exposed it earlier this month. Back in 2006, Google launched a censored search engine in China. But four years later, in March 2010, it pulled the service out of the country, citing Chinese government efforts to limit free speech, block websites, and hack Google’s computer systems. At that time, Brin was a vocal opponent of the censorship. During Thursday’s meeting, Brin told Google employees that Dragonfly would have “certain trade-offs” but said the process was “slow-going and complicated.”

Both Pichai and Brin’s remarks to Google employees raise a number of questions. Pichai’s attempt to portray Dragonfly as an “exploratory” project contradicts internal Google documents and statements issued by senior Google officials on Dragonfly that The Intercept has seen.
GOOGLE EMPLOYEES ARE demanding answers from the company’s leadership amid growing internal protests over plans to launch a censored search engine in China.

Staff inside the internet giant’s offices have agreed that the censorship project raises “urgent moral and ethical issues” and have circulated a letter saying so, calling on bosses to disclose more about the company’s work in China, which they say is shrouded in too much secrecy, according to three sources with knowledge of the matter.

The internal furor began after The Intercept earlier this month revealed details about the censored search engine, which would remove content that China’s authoritarian government views as sensitive, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, leaked Google documents disclosed. The search platform is to be launched via an Android app, pending approval from Chinese officials.

The censorship plan – code-named Dragonfly – was not widely known within Google. Prior to its public exposure, only a few hundred of Google’s 88,000 employees had been briefed about the project – around 0.35 percent of the total workforce. When the news spread through the company’s offices across the world, many employees expressed anger and confusion.

Now, a letter has been circulated among staff calling for Google’s leadership to recognize that there is a “code yellow” situation – a kind of internal alert that signifies a crisis is unfolding. The letter suggests that the Dragonfly initiative violates an internal Google artificial intelligence ethical code, which says that the company will not build or deploy technologies “whose purpose contravenes widely accepted principles of international law and human rights.”
BY MISCONFIGURING PAGES on Trello, a popular project management website, the governments of the United Kingdom and Canada exposed to the entire internet details of software bugs and security plans, as well as passwords for servers, official internet domains, conference calls, and an event-planning system.

The U.K. government also exposed a small quantity of code for running a government website, as well as a limited number of emails. All told, between the two governments, a total of 50 Trello pages, known on the site as “boards,” were published on the open web and indexed by Google.

The computer researcher who found the sensitive material, Kushagra Pathak, had disclosed just this past April a wide swath of additional private data exposed to the public on Trello, which is widely used by software developers, among others. That earlier disclosure revealed how, on dozens of public Trello boards run by various organizations and individuals, the information available included email and social media credentials, as well as specific information on unfixed bugs and security vulnerabilities. Pathak even found an NGO sharing login details to a donor management software database, which in turn contained, he said, personally identifiable information and financial records on donors. In both the April and new security research, the sensitive data on Trello was tracked down starting with a simple Google query.

The data exposures underscore how easy it has become to improperly leak sensitive data in the era of cloud computing. More broadly, they show how the use and development of software has become a complex endeavor, involving a wide range of independent online systems, and how this complexity itself represents a security risk, encouraging users and developers to take shortcuts intended to cut through the morass. Tools like Trello can help master the tangle of development in a safe and constructive way, but can also be misused.
BLACK HAT HAS established its reputation as a world-famous hacker conference by drawing attention to the complex problems in cybersecurity that no one else has solved, or even noticed. For the last two decades, its packed discussions, known as briefings, have made headlines by featuring highly technical experts revealing previously unknown security vulnerabilities. In recent years, hackers have demonstrated their ingenuity in overcoming a smart gun’s protections, tampering with voting machines, and shutting down critical city infrastructure.

But last week, for the first time in Black Hat’s history, the conference invited speakers to address gender discrimination, sexual assault, mental health, and substance abuse. The conference’s inaugural Community Track briefings provided a window into problems in the cybersecurity world that have long been hidden in plain sight. At the Mandala Bay Convention Center in Las Vegas, certified rape crisis counselors spoke alongside engineers and emergency physicians about some of the challenges facing hackers as people.

Many leading cybersecurity conferences, such as Black Hat, Def Con, and RSA, have seemed reluctant to outgrow their beginnings as boys’ clubs, even as their attendees have become more professional and diverse. Over the last decade, journalists, hackers, and advocates have documented a range of abusive incidents at these events. Earlier this year, I spoke to two dozen women who worked in cybersecurity, many of whom had reported incidents of harassment only to be dismissed or ignored by organizers of these events. Some said that the systemic nature of sexism at these annual events felt like a feature, not a bug. In this landscape, Black Hat’s Community Track — along with an expanded range of initiatives to support working mothers, survivors of sexual assault, queer hackers, and recovering alcoholics, among others — represented a welcome step.

Countering Stigma and Silence

Cybersecurity is, by all accounts, an emotionally demanding field. In a briefing on burnout, depression, and suicide in the hacker community, Christian Dameff, a physician, and Jay Radcliffe, a security researcher, explained the unique stressors that often accompany jobs in the information security sector, such as social isolation and abnormal sleep schedules. They cited an Information Systems Security Association study from 2018, in which 68 percent of respondents described work-life balance as a major problem. Contributing to this, they said, was a talent shortage that increased demands on an already overworked staff. The field’s self-image of strength and toughness, Radcliffe said, could also serve to further isolate employees from seeking help.

In her talk on addiction in infosec, Jamie Tomasello, an engineer at Duo Security, detailed the relationship between stress and alcoholism. She described the particular ways in which the imperative to drink overlapped with career opportunities — and an occasionally toxic conference culture. “I built rapport, trust, and respect while drinking,” she said. “I was included in conversations and projects that I wouldn’t have been in without that glass in hand.” As a recovering alcoholic, she noted, it could be difficult to attend conferences like Black Hat that were fueled by networking and afterparties at bars. She offered alternatives for managers and companies hoping to organize more inclusive events for employees struggling with alcoholism, and praised the introduction of sobriety meetings. Employee wellness programs, she stressed, needed “to extend beyond health, food, gym memberships.”

In their respective talks on the importance of neurodiversity, Joe Slowik, a veteran with post-traumatic stress disorder who now works in network defense, and Rhett Greenhagen, a senior security researcher for McAfee’s Advanced Programs Group who has Asperger’s, each echoed this call for empathy. Slowik said that he had “rage-submitted” his talk, “Demystifying PTSD in Information Security,” to the conference after coming across an article that failed to distinguish between burnout, high stress, and an actual PTSD diagnosis. He pushed back against a “one-size-fits-all” approach to dealing with survivors of sexual and military trauma. Alienation, depression, and disengagement were common symptoms, he said, and he described his daily work as giving him his confidence back. “Don’t shun, ignore, or pity. Engage,” he advised those who might work with colleagues with PTSD.

Greenhagen described the ways in which being a person with Asperger’s gave him an interest in pattern recognition — “It is extremely hard for us to not solve a puzzle,” he noted — and a major leg up as a network security analyst. While the evidence is chiefly anecdotal, it is suspected that there is a prevalence of hackers on the autism spectrum. But for all the pleasures of the demanding work, Greenhagen also acknowledged some serious downsides to working on a team. Sensory distractions and small talk interfered with his ability to do his job — an experience that was echoed by hackers with an autism spectrum disorder diagnosis who took part in an informal survey conducted by Stacy Thayer, a psychologist who spoke alongside Greenhagen. “I don’t think I’ll ever have a normal social interaction with other co-workers,” Greenhagen said. “Either there were people who absolutely adored me, even if they found stupid crap I did hilarious. Or there were people who couldn’t stand me. What made it livable was that it wasn’t a huge percentage. I had more people stand up for me and realize I have shortcomings.”

The briefings focused on mental health were by turns moving and vexing. Some of the men emphasized soul-baring, engaging their captive audience in a personal story, at the expense of skill-building. Race was notably absent as a topic of discussion. So too were the ways in which diagnoses such as alcoholism, PTSD, burnout, and Asperger’s might differently affect people across genders and identities. Given the graphic nature of the discussions about suicide in the PTSD and burnout talks, trigger warnings would have been prudent. But it was precisely the elementary nature of some of these discussions that testified to their novelty in the community — and hence their necessity.

GOOGLE’S FORMER HEAD of free expression issues in Asia has slammed the internet giant’s plan to launch a censored search engine in China, calling it a “stupid move” that would violate widely held human rights principles.

As The Intercept first reported last week, Google has been quietly developing a search platform for China that would remove content that China’s authoritarian government views as sensitive, such as information about political opponents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to internal Google documents.

Lokman Tsui, Google’s head of free expression for Asia and the Pacific between 2011 and 2014, read the leaked censorship plans and said he was disturbed by the details. “This is just a really bad idea, a stupid, stupid move,” he told The Intercept in an interview. “I feel compelled to speak out and say that this is not right.”
GOOGLE’S FORMER HEAD of free expression issues in Asia has slammed the internet giant’s plan to launch a censored search engine in China, calling it a “stupid move” that would violate widely held human rights principles.

As The Intercept first reported last week, Google has been quietly developing a search platform for China that would remove content that China’s authoritarian government views as sensitive, such as information about political opponents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to internal Google documents.

Lokman Tsui, Google’s head of free expression for Asia and the Pacific between 2011 and 2014, read the leaked censorship plans and said he was disturbed by the details. “This is just a really bad idea, a stupid, stupid move,” he told The Intercept in an interview. “I feel compelled to speak out and say that this is not right.”
A BIPARTISAN GROUP of six U.S. senators is demanding that Google CEO Sundar Pichai explain the company’s plan to launch a censored version of its search engine in China.

Since spring 2017, the internet giant has been developing a censored Android search app to launch in the country as part of a secretive project code-named Dragonfly, The Intercept revealed on Wednesday. The app would manipulate search results in accordance with strict censorship rules in China that are mandated by the ruling Communist Party regime, which restricts people’s access to information about political opponents, free speech, democracy, human rights, and peaceful protest. The censored Google search has been designed to “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to internal Google documents.

In a letter sent to Pichai on Friday, the six lawmakers called the Google plan “deeply troubling” and said that it “risks making Google complicit in human rights abuses related to China’s rigorous censorship regime.” The letter was led by Sen. Marco Rubio, R-Fla., and also signed by Sens. Mark Warner, D-Va., Tom Cotton, R-Ark., Ron Wyden, D-Ore., Cory Gardner, R-Colo., and Robert Menendez, D-N.J.

The senators write: “It is a coup for the Chinese government and Communist Party to force Google—the biggest search engine in the world—to comply with their onerous censorship requirements, and sets a worrying precedent for other companies seeking to do business in China without compromising their core values.”
GOOGLE BOSSES WERE scrambling to contain leaks and internal anger on Wednesday after the company’s confidential plan to launch a censored version of its search engine in China was revealed by The Intercept.

Just a few hundred of Google’s massive 88,000-strong workforce had been briefed on the project prior to the revelations, which triggered a wave of disquiet that spread through the internet giant’s offices across the world.

Company managers responded by swiftly trying to shut down employees’ access to any documents that contained information about the China censorship project, according to Google insiders who witnessed the backlash.

“Everyone’s access to documents got turned off, and is being turned on [on a] document-by-document basis,” said one source. “There’s been total radio silence from leadership, which is making a lot of people upset and scared. … Our internal meme site and Google Plus are full of talk, and people are a.n.g.r.y.”

On a message board forum for Google employees, one staff member posted a link to The Intercept’s story alongside a note saying that they and two other members of their team had been asked to work on the Chinese censorship project, code-named Dragonfly.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
THE LATEST TECHNOLOGIES promise cops the ability to whip out a smartphone, take a snapshot of a passerby, and instantly learn if that person is in an immigration or gang database.

A federal broadband program, designed after 9/11 to improve first responder communication during emergencies, will enhance this sort of capability and integrate it into an internet “super highway” built specifically for police and public safety. The program, called FirstNet, is already expanding the surveillance options available to law enforcement agencies across the country.

According to publicly available documents, as well as interviews with program participants, stakeholders, and government researchers, FirstNet will help agencies like U.S. Customs and Border Protection communicate with local police, deliver more information to officers’ hands, accelerate the nascent law enforcement app industry, and provide public safety agencies with new privileges and powers over AT&T’s commercial broadband network.

The program will also hasten these agencies’ migration from public radio frequencies to encrypted broadband networks, potentially eliminating one resource that local newsrooms and citizens have historically relied upon to monitor police and first responders.

FirstNet is a public-private partnership that creates a dedicated lane for public safety agencies within AT&T’s existing broadband network. As of January, all U.S. states had opted in to FirstNet, meaning that they agreed not to build their own competing broadband lanes for law enforcement and public safety. Then, in March, AT&T announced that FirstNet’s core — the infrastructure that isolates police traffic from the commercial network — had become operational at last.

“It’s like having a super highway that only public safety can use,” the company wrote in a press release.

Why FirstNet?

Part of FirstNet’s mission is to create a virtual space that allows any federal, state or local law enforcement or public safety agency to communicate seamlessly with any other. Therefore, convincing as many agencies as possible to sign up for the program is key to its success.

FirstNet recently pitched U.S. Customs and Border Protection to convince the agency to subscribe to the network. In a white paper, FirstNet claims its services will provide CBP access to “photographs, real-time audio/video feeds, and databases from other state, local, or Federal agencies … to aid in the identification and apprehension of terrorists, undocumented aliens, and smugglers.” These capabilities would be offered “in times of crisis or simply day-to-day operations.”

In the pitch, FirstNet also promises to help agents “connect to critical databases to identify whether detained persons have been previously apprehended for violating immigration law by quickly and efficiently collecting biographic (e.g., name, date of birth, place of birth) and biometric information (e.g., 10-print fingerprints, photo image), which are submitted remotely to said databases.” The document also promotes FirstNet’s support of other data-heavy technologies, such as live video streaming from drones.

AT&T and FirstNet did not respond to questions about whether CBP or any other federal agency has subscribed to the program. (A recent press release indicates that some federal agencies are currently using the system, but it does not name them.) CBP did not respond to requests for comment.

Local law enforcement officials are well-aware of the new capabilities that FirstNet is offering their departments. Domingo Herraiz, programs director at the International Association of Chiefs of Police, is excited about the heightened access to federal data FirstNet promises. Herraiz told The Intercept that FirstNet will place information from fusion centers, which enable criminal intelligence-sharing between government agencies, at the fingertips of local officers. “You could have gang databases,” he said. “It’s not there [on officers’ phones] today, but it will be.”

A “Private Tunnel” for Law Enforcement and First Responders

The concept behind FirstNet — a broadband network dedicated to public safety — was inspired by the National Commission on Terrorist Attacks Upon the United States (the so-called 9/11 Commission). Its 2004 report determined that streamlined communication between different agencies and jurisdictions could have saved lives in the aftermath of the attacks on the World Trade Center. The report blamed the use of separate radio frequencies by police and firefighters for the deaths of firefighters who didn’t get the message to evacuate before the north tower collapsed.
SILICON VALLEY FIRMS seeking lucrative business opportunities with the Pentagon face a range of obstacles, not least the morally fraught choice of enabling a military led by President Donald Trump with the latest technological solutions. Enter a group of former high-level officials from the Obama administration, who are helping to bridge the divide between tech firms and the Defense Department through a new company called WestExec Advisors.

“Think Scowcroft Group, Kissinger, RiceHadleyGates, Albright, but my generation,” explained Michèle Flournoy, referencing the myriad of consulting firms founded by former top national security and foreign policy officials, during an interview on Capitol Hill. Flournoy herself is the former under secretary of defense for policy and one of the co-founders of WestExec Advisors.

The sort of initiatives WestExec is posturing itself to spearhead, however, have grown controversial. In recent months, Google has faced an internal rebellion over its work with the Defense Department to deploy cutting-edge artificial intelligence technology for drone warfare, part of a Pentagon initiative known as Project Maven. The internal uprising led to Google executives announcing last month that the firm would not renew the military contract when it expires next year. And WestExec has found itself at the center of the storm, with the consultancy’s officials deeply involved with the project and wading into the media firestorm that was set off by the Google employees’ objections.
ON FRIDAY, Special Counsel Robert Mueller, as part of his investigation into interference with the 2016 presidential election, charged 12 Russian military intelligence officers with conducting “large-scale cyber operations to interfere with the 2016 U.S. presidential election.” The indictment contains a surprising amount of technical information about alleged Russian cyberattacks against a range of U.S. political targets, including the Democratic Congressional Campaign Committee, the Democratic National Committee, members of Hillary Clinton’s presidential campaign, the Illinois (probably) State Board of Elections, and an American election vendor, apparently VR Systems, and its government customers.

While the indictment only describes the U.S. government’s charges in this case, the specific technical evidence presented is compelling and paints by far the most detailed and plausible picture yet of what exactly occurred in 2016.

It also sheds light on what the U.S. government is capable of doing when it investigates cyberattacks, as well as how Russia’s Main Intelligence Directorate of the General Staff, or GRU, allegedly conducted the attacks — which it denies — and what operational security mistakes they made. Here are what I find to be the most compelling takeaways from the indictment.

AMID HIGH-PROFILE Supreme Court rulings like the lifeline given to the practice of gerrymandering, the endorsement of Trump’s Muslim travel ban, the gutting of public sector unions, and the defense of bakers who don’t want to serve gay people, the case of Ohio v. American Express may get overlooked.

But it could prove to be one of the most consequential rulings of the decade, serving as a broad immunity cloak for Silicon Valley giants — and perhaps others — in their quest to utterly dominate the global political economy.

Which is weird, because it was a case about credit card fees.

In a 5-4 ruling along party lines on Monday, the court, featuring Justice Neil Gorsuch rather than Judge Merrick Garland, ruled that American Express did not violate antitrust laws when it wrote into its contracts with retailers that they could not offer discounts or enticements to get customers to use other forms of payment.
IN THE FACE of the Trump administration’s neglect and indifference toward the reunification of the thousands of immigrant families it has forcibly separated, some lawmakers, activists, and celebrities have called for the use of DNA testing, along with other biometrics, as a means to return some 3,700 children to their parents. So far, at least two direct-to-consumer genealogy companies have heeded those calls. MyHeritage and 23andMe have both offered to donate DNA sampling kits for the purposes of verifying kinship.

“In light of the humanitarian tragedy that has taken place, in which children have been separated from their parents, we have decided to rise to the challenge and take the lead in helping these families,” Gilad Japhet, founder and CEO of MyHeritage, said in a press statement. The company is offering 5,000 free DNA tests for separated parents and children, and it plans to distribute the kits through government agencies and NGOs. Both MyHeritage and 23andMe have said that its DNA results will be processed confidentially and not shared with any third parties.
LOBBYISTS FOR THE largest technology and telecommunications firms have only three days to prevent the California Consumer Privacy Act, or CCPA, a ballot initiative that would usher in the strongest consumer privacy standards in the country, from going before state voters this November.

The initiative allows consumers to opt out of the sale and collection of their personal data, and vastly expands the definition of personal information to include geolocation, biometrics, and browsing history. The initiative also allows consumers to pursue legal action for violations of the law.

The idea that Californians might gain sweeping new privacy rights has spooked Silicon Valley, internet service providers, and other industries that increasingly rely on data collection, leading to a lobbying push to defeat the initiative before it gains traction. Their best hope may be to convince the sponsors of the initiative, including San Francisco real estate developer Alastair Mactaggart, to pull the proposal in exchange for compromise privacy legislation, AB 375, which would achieve some of the same goals of the initiative. Lawmakers behind the legislation, led by state Assembly Member Ed Chau, D-Monterey Park, and state Sen. Robert Hertzberg, D-Van Nuys, have promised to swiftly pass their bill this week if sponsors withdraw the CCPA.

Emails obtained by The Intercept reveal that tech giants are fighting behind the scenes to water down the privacy legislation, hoping to prevent an expensive and potentially losing ballot fight this year.

Andrea Deveau, a lobbyist for TechNet, a trade group for Google, Facebook, and other tech companies, has continually updated an ad-hoc business lobbying coalition formed to defeat the CCPA. In an update sent on Sunday evening, Deveau provided a “compilation of feedback re: the most problematic aspects of AB 375.”

In her update, she listed a vast array of changes lobbyists are still seeking, including a rewrite of the privacy law’s description of what counts as personal information, changes to the conditions under which a consumer can seek legal action, the preservation of arbitration clauses in consumer contracts, and the removal of the mandate that firms display a button on their homepage giving consumers a clear way of opting out of data collection, among other changes.
LAST WEEK, INTERPOL held a final project review of its speaker identification system, a four-year, 10 million euro project that has recently come to completion. The Speaker Identification Integrated Project, what they call SiiP, marks a major development in the international expansion of voice biometrics for law enforcement uses — and raises red flags when it comes to privacy.

Speaker identification works by taking samples of a known voice, capturing its unique and behavioral features, and then turning these features into an algorithmic template that’s known as a voice print or voice model. With enough voice prints and samples collected in its global audio database, Interpol’s speaker identification system will be able to upload an unknown voice and, regardless of the language it is speaking, match it to a list of likely candidates. SiiP’s database allow uploads and downloads of samples from 192 law enforcement agencies across the world.

SiiP will join Interpol’s existing fingerprint and face databases, and its key advantage will be to facilitate a quick identification process — say, of a kidnapper making a phone call — even in the absence of other identifiers. The platform also boasts the ability to filter voice samples by gender, age, language, and accent. When the audio recordings are taken from similar acoustical environments, accuracy rates can be extremely high.
IN A LANDMARK privacy decision, the Supreme Court ruled 5-4 on Friday that police must get a warrant in order to obtain your cellphone’s location data over an extended period of time.

The decision is a major victory for privacy advocates, who have long argued that the law has failed to keep pace with the amount of intrusive data we voluntarily hand over to private companies.

Chief Justice John Roberts joined the liberal justices on the court, declaring that even though the data is held by a third party, the government still needs a warrant to obtain it.

“We decline to grant the state unrestricted access to a wireless carrier’s database of physical location information,” said Roberts, writing for the majority. “In light of the deeply revealing nature of [cell-site location information], its depth, breadth, and comprehensive reach, and the inescapable and automatic nature of its collection, the fact that such information is gathered by a third party does not make it any less deserving of Fourth Amendment protection.”

The court made the ruling in the case of Timothy Carpenter, who was convicted in 2013 of robbing Radio Shack and T-Mobile stores in Michigan and Ohio. In order to build their case, the FBI obtained 127 days’ worth of location information for Carpenter’s cellphone – almost 12,900 location points – which they used to place him at the scene of the robberies.
IN A LANDMARK privacy decision, the Supreme Court ruled 5-4 on Friday that police must get a warrant in order to obtain your cellphone’s location data over an extended period of time.

The decision is a major victory for privacy advocates, who have long argued that the law has failed to keep pace with the amount of intrusive data we voluntarily hand over to private companies.

Chief Justice John Roberts joined the liberal justices on the court, declaring that even though the data is held by a third party, the government still needs a warrant to obtain it.

“We decline to grant the state unrestricted access to a wireless carrier’s database of physical location information,” said Roberts, writing for the majority. “In light of the deeply revealing nature of [cell-site location information], its depth, breadth, and comprehensive reach, and the inescapable and automatic nature of its collection, the fact that such information is gathered by a third party does not make it any less deserving of Fourth Amendment protection.”

The court made the ruling in the case of Timothy Carpenter, who was convicted in 2013 of robbing Radio Shack and T-Mobile stores in Michigan and Ohio. In order to build their case, the FBI obtained 127 days’ worth of location information for Carpenter’s cellphone – almost 12,900 location points – which they used to place him at the scene of the robberies.
EARLIER THIS YEAR, it was reported that Elliott Broidy, previously known for his conviction in a state bribery case and his role as a top Donald Trump fundraiser, proffered meetings with the president to foreign regimes who were also potential clients of his defense firm Circinus. Little is known about Circinus, but purported company documents obtained by The Intercept contain plans to peddle social media surveillance software to repressive regimes.

The Circinus website paints the contractor as a red-blooded defender of U.S. national security: “Are you a patriot determined to keep our country — both government and private industry — safe?” its careers page reads. Circinus’s executive roster boasts experience in U.S. special forces, Homeland Security, and military intelligence. But the documents, a series of pitch decks, indicate that the company was prepared to sell what’s described as a suite of sophisticated internet-mining tools to the governments of Cyprus, Romania, Tunisia, and the United Arab Emirates, touting the ability to detect and identify online “detractors.” The recent histories of Tunisia and the UAE are rife with human rights abuses, including crackdowns against political dissent.
THE INTERCEPT AND European media partners are joining with The Signals Network, a whistleblower support organization, to solicit information from those who want to speak out against data-related malpractice in the technology sector.

Through this collaboration, The Intercept will work with German newspaper Die Zeit, French news website Mediapart, British newspaper The Daily Telegraph, and the global news platform WikiTribune to obtain and investigate information regarding the abuse of personal data — whether by a social network, health care company, government, marketing firm, or any other of the myriad entities that collect and process such data on an enormous scale. The Signals Network, a nonprofit founded by French media executive Gilles Raymond, said on its website that it can, in “selected … appropriate cases,” offer support to whistleblowers, including legal aid, psychological counseling, and safe shelter in case of physical threats.

As part of the effort, the news partners will share a contact number on the secure messaging platform Signal and share tips submitted to a set of email addresses.

Sources that come forward will have access to not just the newsrooms of the above news organizations, but the chance to reach their combined audiences of over 46 million readers in three languages. Those interested in contacting the consortium can find more details here, and should read these guidelines for submitting information.
THE INTERCEPT AND European media partners are joining with The Signals Network, a whistleblower support organization, to solicit information from those who want to speak out against data-related malpractice in the technology sector.

Through this collaboration, The Intercept will work with German newspaper Die Zeit, French news website Mediapart, British newspaper The Daily Telegraph, and the global news platform WikiTribune to obtain and investigate information regarding the abuse of personal data — whether by a social network, health care company, government, marketing firm, or any other of the myriad entities that collect and process such data on an enormous scale. The Signals Network, a nonprofit founded by French media executive Gilles Raymond, said on its website that it can, in “selected … appropriate cases,” offer support to whistleblowers, including legal aid, psychological counseling, and safe shelter in case of physical threats.

As part of the effort, the news partners will share a contact number on the secure messaging platform Signal and share tips submitted to a set of email addresses.

Sources that come forward will have access to not just the newsrooms of the above news organizations, but the chance to reach their combined audiences of over 46 million readers in three languages. Those interested in contacting the consortium can find more details here, and should read these guidelines for submitting information.
CAMILLE TUUTTI CAN’T remember all the times she’s been harassed. A prominent information technology journalist and editor, Tuutti feels that her friendly and outgoing personality — a necessity in her line of work — has often been misinterpreted by men in her field as an invitation for inappropriate behavior, especially at top cybersecurity conferences, where binge drinking is encouraged. Drunk men have often put their arms around her and her colleagues. She has been asked out “a million times.” Someone tried to kiss her the first time she met him.

This April, at RSA, a leading cybersecurity conference held in San Francisco, she was walking the showroom with a male colleague when a male stranger asked her what she was wearing to bed. She noticed, too, that vendors at the show assumed that she didn’t know what she was talking about and that her colleague did. And despite organizers’ previous attempts to implement a dress code, many of the booths featured “booth babes” — scantily clad models hired to attract men to vendors’ wares. “It was so tone-deaf, especially in 2018 and especially in the wake of #MeToo,” Tuutti said.

The casual sexism Tuutti encountered at RSA is not atypical of big-league hacker and cybersecurity conferences. While there are no precise statistics available about harassment at these events, anecdotal reports like Tuutti’s have been widespread and documented for years.

The Intercept spoke to nearly two dozen women across the industry who recounted experiences ranging from uncomfortable to traumatic at conferences such as Def Con and Black Hat, held each year in Las Vegas, and RSA, held worldwide. The women who spoke to The Intercept had encountered a variety of offenses, from suggestive commentary and drunken come-ons to groping and assault. Some of the women, among whom are renowned journalists, CEOs, diversity advocates, and hackers, said that even if their own status had shielded them from some of the worst behavior, they had all heard troubling stories from younger colleagues, peers, and friends.
GOOGLE EXECUTIVES ANNOUNCED to company staff this morning that the tech giant won’t renew its contract to work on Project Maven, the controversial Pentagon program designed to provide the military with artificial intelligence technology used to help drone operators identify images on the battlefield. Google will continue work on the project through March 2019, according to multiple people with knowledge of the announcement, but once the 18-month contract concludes, it will not be renewed.

The company, however, has not committed to forego signing other military contracts dealing with artificial intelligence, according to multiple people with knowledge of the decision. Google declined to comment for this story.
JUST DAYS BEFORE the 2016 presidential election, hackers identified by the National Security Agency as working for Russia attempted to breach American voting systems. Among their specific targets were the computers of state voting officials, which they had hoped to compromise with malware-laden emails, according to an intelligence report published previously by The Intercept.

Now we know what those emails looked like.

An image of the malicious email, provided to The Intercept in response to a public records request in North Carolina, reveals precisely how hackers, who the NSA believed were working for Russian military intelligence, impersonated a Florida-based e-voting vendor and attempted to trick its customers into opening malware-packed Microsoft Word files.
JUST DAYS BEFORE the 2016 presidential election, hackers identified by the National Security Agency as working for Russia attempted to breach American voting systems. Among their specific targets were the computers of state voting officials, which they had hoped to compromise with malware-laden emails, according to an intelligence report published previously by The Intercept.

Now we know what those emails looked like.

An image of the malicious email, provided to The Intercept in response to a public records request in North Carolina, reveals precisely how hackers, who the NSA believed were working for Russian military intelligence, impersonated a Florida-based e-voting vendor and attempted to trick its customers into opening malware-packed Microsoft Word files.
A WASHINGTON-BASED ADVOCACY organization that purports to be a voice for startup tech companies is actually a sock puppet for Google, according to a report released Wednesday that details numerous links between the two.

According to the report, startup advocacy group Engine has at least seven former Google employees and consultants on its board of directors and advisory board. Its three founders all previously worked at Google; they founded a startup incubator that Google eventually bought. Google has given Engine an undisclosed amount of funding over the past five years. The two share a lobbying firm called S-3 Group that has worked for both Engine and Google. The initial launch party for Engine in 2011 had attendees RSVP to a Google email address, which is reserved primarily for employees, unlike the Gmail address that is offered to the public.

On numerous issues, from patent reform, anti-piracy efforts, and high-skilled immigration, to the recent changes to Section 230 of the Communications Decency Act, Engine’s advocacy and Google’s stated policy preferences are in alignment, the report explains. Google even funded a research paper that Engine later released.

“Public officials need to be aware that this so-called startup advocacy group is really in bed with Silicon Valley’s foremost D.C. influence machine, whose interests are often in conflict with those of disruptive entrepreneurs,” said Daniel Stevens of the Campaign for Accountability, which released the report. There are no clean hands here: The Campaign for Accountability gets major funding from Oracle, a chief antagonist to Google.

A Google spokesperson said the company is “happy to support Engine’s work” to represent the views of startups in Washington policy debates. “While we often agree on policy matters, Engine is an independent organization just like the other groups we support,” the spokesperson said.

Google publicly discloses its funding support for Engine on its website, “in contrast to the Campaign for Accountability, which declines to list its corporate funders and has been instrumental in Oracle’s long-running legal grudge against Google,” the spokesperson said.

Ken Gleuck, a senior vice president in Oracle’s Washington office, said after publication that Google’s charge was off-base and that Oracle had nothing to do with the report. “Before reading your story, Oracle had no idea Engine even existed, nor did we have any knowledge or involvement with this report,” he said. “While we are flattered, Google should not assume we are behind every bad story about Google. We’d run out of 20 percent time if all we did was out Google front groups. Are we also responsible for the Red Wedding and plastic straws?”
OFFICIALS AT THE Lockport, New York, school district have purchased face recognition technology as part of a purported effort to prevent school shootings. Starting in September, all 10 of Lockport District’s school buildings, just north of Buffalo, will be outfitted with a surveillance system that can identify faces and objects. The software, known as Aegis, was developed by SN Technologies Corp., a Canadian biometrics firm that specifically advertises to schools. It can be used to alert officials to whenever sex offenders, suspended students, fired employees, suspected gang members, or anyone else placed on a school’s “blacklist” enters the premises. Aegis also sends alerts any time one of the “top 10” most popular guns used in school shootings appears in view of a camera.

The district is spending most of its recent $4 million state “Smart School” grant on these and other enhancements to its security systems, including bullet-proof greeter windows and a mass notification system, according to the Niagra Gazette. “We always have to be on our guard. We can’t let our guard down,” Lockport Superintendent Michelle T. Bradley told the Buffalo News. “For the Board of Education and the Lockport City School District, this is the No. 1 priority: school security.”
Update: Since this article was published, GPGTools released version 2018.2 which appears to successfully mitigate the OpenPGP EFAIL attack for macOS High Sierra users. If you use macOS High Sierra, Apple Mail, and GPGTools, it should be safe to use PGP again if you update to the latest version of everything. If you use an older version of macOS, GPGTools is still vulnerable.


IT’S BEEN NEARLY two weeks since a group of European researchers published a paper describing “EFAIL,” a set of critical software vulnerabilities that allow encrypted email messages to be stolen from within the inbox. And developers of email clients and encryption plug-ins are still scrambling to come up with a permanent fix.

Apple Mail is the email client that comes free with every Mac computer, and an open source project called GPGTools allows Apple Mail to smoothly encrypt and decrypt messages using the 23-year-old PGP standard. The day the EFAIL paper was published, GPGTools instructed users to workaround EFAIL by changing a setting in Apple Mail to disable loading remote content:
In partnership with
WHAT’S THE BEST way to keep adults from questioning the use of a deeply problematic product? Get them started when they’re too young to question anything. Amazon has a new addition to its line of voice-commanded artificial intelligence Alexa assistants, marketed for use by children as young as 5 years old, who can barely grasp a box of juice, let alone digital privacy. Now, a coalition of children’s privacy and psychology advocates are warning parents away from Amazon’s latest, cutest device, saying it could normalize surveillance and harm children’s mental development.

The Echo Dot for kids is functionally identical to the Echo Dot for adults, except that it’s brightly colored and inexplicably costs $30 more than the grown-up version. Cosmetics aside, Echo Dot is still an AI-powered microphone that listens constantly for an activation keyword, relays a user’s voice to remote servers where it is analyzed and processed opaquely, and then responds to an increasingly long list of commands; on its packaging, Amazon highlights commands like “tell me a story” and “start SpongeBob.” Dot for kids will not only perpetually listen to and entertain your children, but attempt to teach them manners in your stead: “Alexa even provides positive feedback when kids ask questions and remember to say ‘please,'” says Amazon.
LEIA EM PORTUGUÊS 
DIGITAL SECURITY SPECIALISTS like me get some version of this question all the time: “I think my laptop may have been infected with malware. Can you check?”

We dread this sort of query because modern computer exploits are as complex, clever, and hard to reason about as modern computers — particularly if someone has the ability to physically access your device, as is routinely the case with laptops, especially when traveling. So while it’s definitely possible to detect certain types of tampering, it isn’t always trivial. And even in controlled environments, it’s impossible to give a laptop a clean bill of health with full confidence – it’s always possible that it was tampered with in a way you did not think to check.

The issue of tampering is particularly relevant for human rights workers, activists, journalists, and software developers, all of whom hold sensitive data sought by powerful potential attackers. People in these vocations are often keenly aware of the security of their laptops while traveling – after all, laptops store critical secrets like communication with sources, lists of contacts, password databases, and encryption keys used to vouch for source code you write, or to give you access to remote servers.
LEIA EM PORTUGUÊS 
AFTER WATCHING the Facebook founder and CEO’s 48-hour trip to Capitol Hill, there are two possible conclusions: either Mark Zuckerberg deliberately misled Congress, or Mark Zuckerberg knows very little about his own company. Both are bad.

Again and again, before both Senate and House committees, Zuckerberg pleaded ignorance about the company he created and has controlled for 14 years. Zuckerberg wasn’t dodging questions about obscure corners of the company or corporate minutiae, but the most plainly fundamental aspects of Facebook’s business and privacy policies. Rather than the congressional beatdown many had expected, the most striking aspect of Zuckerberg’s testimony wasn’t his painful apologias or excuse-spinning, but his ability to spend nearly 10 hours saying almost nothing. The hearings may prove to be a sea change moment for Facebook and the greater data-mining industrial complex, but it would be hard to say the public learned much of anything.

When Sen. Kamala Harris asked Zuckerberg, on the subject of Cambridge Analytica, whether the company had any conversations about whether to inform the 87 million users affected, the CEO replied, “I don’t know if there were any conversations at Facebook overall because I wasn’t in a lot of them,” and finally “I don’t remember a conversation like that.”

When asked by Sen. Maria Cantwell whether Facebook employees had helped with Cambridge Analytica’s work: “Senator, I don’t know.”

When asked about the role of Palantir, a data-mining defense contractor co-founded by Facebook board member and early Zuckerberg ally Peter Thiel: “I’m not really that familiar with what Palantir does.”

Zuckerberg acted similarly confused when asked whether Facebook does things it openly says it does on its own website. When Sen. Roger Wicker asked Zuckerberg if he could confirm whether “Facebook can track a user’s internet browsing activity, even after that user has logged off of the Facebook platform,” the CEO replied, “Senator — I — I want to make sure I get this accurate, so it would probably be better to have my team follow up afterwards.”

The answer is categorically, unequivocally yes, according to Facebook.com: “If you’re logged out or don’t have a Facebook account and visit a website with the Like button or another social plugin, your browser sends us a more limited set of info.”

When Sen. Roy Blunt asked Zuckerberg whether Facebook tracks users across devices (say, from their iPhone to their iPad), he replied that he was “not sure of the answer to that question.”

Meanwhile, on Facebook.com:
FOUNDED IN 1936, the Advertising Research Foundation “has been the standard-bearer for unbiased quality in research on advertising, media and marketing,” according to its website, and works to spread “unifying standards and best practices” throughout the ad industry. Last year, the ARF presented Cambridge Analytica with its highest honor.

The current scandal engulfing both Facebook and Cambridge Analytica, the shadowy British political consultancy that exfiltrated and exploited 50 million profiles from the social network, centers mostly around how the data was acquired, not how it was used. This is due in part to the fact that before the pilfered profile revelations, Cambridge Analytica enjoyed the praise of its peers in the marketing world, which viewed it as a band of innovators.
Because of errors inserted during editing, the original version of this article contained the mistaken assertion that ICE used private Facebook data to track unauthorized immigrants. The story has been corrected. See below for full correction.

CAMBRIDGE ANALYTICA MAY have had access to the personal information of tens of millions of unwitting Americans, but a genuine debate has emerged about whether the company had the sophistication to put that data effectively to use on behalf of Donald Trump’s presidential campaign.

But one other organization that has ready access to Facebook’s trove of personal data has a much better track record of using such information effectively: U.S. Immigration and Customs Enforcement.

ICE, the federal agency tasked with Trump’s program of mass deportation, uses backend Facebook data to locate and track suspects, according to a string of emails and documents obtained by The Intercept through a public records request. The hunt for one particular suspect provides a rare window into how ICE agents use social media and powerful data analytics tools to find targets.
LEIA EM PORTUGUÊS 
ON WEDNESDAY, HOUSE Democrats on the Intelligence Committee released a memo laying out the steps they would have taken had they been in charge of the Trump-Russia investigation — and steps they may take if and when they gain subpoena power by taking over the House of Representatives in November.

The possibility became much more likely on Tuesday night, as Democrats pulled off an upset victory in a Pennsylvania special election for a congressional seat that President Donald Trump had carried by nearly 20 points, and that Democrats hadn’t even bothered to contest in the last two cycles. If the pattern holds, Democrats have a strong chance of picking up the nearly two-dozen seats they need to win the House — and the subpoena power that comes with it.

Down on Page 20 of the memo is a pair of ideas that could put Congress on a collision course with privacy advocates in Silicon Valley. “Apple: The Committee should seek records reflecting downloaded encrypted messaging apps for certain key individuals,” the memo suggests. “The Committee should likewise issue a subpoena to WhatsApp for messages exchanged between key witnesses of interest.”

The committee said that it would also seek to find out “all messaging applications that Mr. [Jared] Kushner used during the campaign as well as the presidential transition, including but not limited to SMS, iMessage, Whatsapp, Facebook Messenger, Signal, Slack, Instagram, and Snapchat.”

The committee may also consider adding ProtonMail, the encrypted email service, to that list. One White House staffer, Ryan P. McAvoy, jotted his ProtonMail passwords and his address on a piece of White House stationery and left it at a bus stop near the White House. A source found it there and provided it to The Intercept, which confirmed its authenticity. (McAvoy did not respond to requests for comment.)

Following publication of this story, Irina Marcopol, a spokesperson for ProtonMail, forwarded a statement from the company’s CEO, Andy Yen. “Don’t be a password idiot,” Yen suggested. “In other words, don’t be this guy.”
Yen went on to note that whether a White House staffer is using encrypted or unencrypted email isn’t the important question, at least from the perspective of retention of records. The use of any personal email account for official business would run afoul of that.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
GOOGLE HAS QUIETLY secured a contract to work on the Defense Department’s new algorithmic warfare initiative, providing assistance with a pilot project to apply its artificial intelligence solutions to drone targeting.

The military contract with Google is routed through a Northern Virginia technology staffing company called ECS Federal, obscuring the relationship from the public.

The contract, first reported Tuesday by Gizmodo, is part of a rapid push by the Pentagon to deploy state-of-the-art artificial intelligence technology to improve combat performance.

Google, which has made strides in applying its proprietary deep learning tools to improve language translation, and vision recognition, has a cross-team collaboration within the company to work on the AI drone project.

The team, The Intercept has learned, is working to develop deep learning technology to help drone analysts interpret the vast image data vacuumed up from the military’s fleet of 1,100 drones to better target bombing strikes against the Islamic State.

The race to adopt cutting-edge AI technology was announced in April 2017 by then-Deputy Defense Secretary Robert Work, who unveiled an ambitious plan called the Algorithmic Warfare Cross-Functional Team, code-named Project Maven. The initiative, Work wrote in an agency-wide memo, is designed to “accelerate DoD’s integration of big data and machine learning” and “turn the enormous volume of data available to DoD into actionable intelligence and insights at speed.”

The first phase of Project Maven, which incorporates multiple teams from across the Defense Department, is an effort to automate the identification and classification of images taken by drones — cars, buildings, people — providing analysts with increased ability to make informed decisions on the battlefield.
FOR A MOMENT, it seemed the hackers had slipped up and exposed their identities. It was the summer of 2013, and European investigators were looking into an unprecedented breach of Belgium’s telecommunications infrastructure. They believed they were on the trail of the people responsible. But it would soon become clear that they were chasing ghosts – fake names that had been invented by British spies.

The hack had targeted Belgacom, Belgium’s largest telecommunications provider, which serves millions of people across Europe. The company’s employees had noticed their email accounts were not receiving messages. On closer inspection, they made a startling discovery: Belgacom’s internal computer systems had been infected with one of the most advanced pieces of malware security experts had ever seen.

As The Intercept reported in 2014, the hack turned out to have been perpetrated by U.K. surveillance agency Government Communications Headquarters, better known as GCHQ. The British spies hacked into Belgacom employees’ computers and then penetrated the company’s internal systems. In an eavesdropping mission called “Operation Socialist,” GCHQ planted bugs inside the most sensitive parts of Belgacom’s networks and tapped into communications processed by the company.
ON A THURSDAY afternoon in November 2015, a light snow was falling outside the windows of the Ecuadorian embassy in London, despite the relatively warm weather, and Julian Assange was inside, sitting at his computer and pondering the upcoming 2016 presidential election in the United States.

In little more than a year, WikiLeaks would be engulfed in a scandal over how it came to publish internal emails that damaged Hillary Clinton’s presidential campaign, and the extent to which it worked with Russian hackers or Donald Trump’s campaign to do so. But in the fall of 2015, Trump was polling at less than 30 percent among Republican voters, neck-and-neck with neurosurgeon Ben Carson, and Assange spoke freely about why WikiLeaks wanted Clinton and the Democrats to lose the election.

“We believe it would be much better for GOP to win,” he typed into a private Twitter direct message group to an assortment of WikiLeaks’ most loyal supporters on Twitter. “Dems+Media+liberals woudl then form a block to reign in their worst qualities,” he wrote. “With Hillary in charge, GOP will be pushing for her worst qualities., dems+media+neoliberals will be mute.” He paused for two minutes before adding, “She’s a bright, well connected, sadistic sociopath.”
THE U.K. GOVERNMENT’S mass surveillance powers were deemed unlawful on Tuesday in a court ruling that could force changes to the country’s spy laws.

Three judges at London’s Court of Appeal found that a sweeping data retention law, which allowed authorities to access people’s phone and email records, was not subject to adequate safeguards. The court ruled that access to the private data “should be restricted to the objective of fighting serious crime.” The court also said that such data should not be turned over to authorities until after a “prior review by a court or an independent administrative body.”

The case was originally brought by the Labour Member of Parliament Tom Watson following the introduction of the 2014 Data Retention and Investigatory Powers Act. That law expired in 2016 and has since been replaced by the Investigatory Powers Act, which expanded the government’s surveillance authority further, retroactively legalizing controversial spy tactics exposed in documents leaked by Edward Snowden. Human rights group Liberty, which represented Watson in the case, said Tuesday’s ruling meant parts of the Investigatory Powers Act – dubbed the “Snoopers’ Charter” by critics – would now need to be reformed.

“Yet again a U.K. court has ruled the government’s extreme mass surveillance regime unlawful,” said Martha Spurrier, Liberty’s director, in a statement. “This judgment tells ministers in crystal clear terms that they are breaching the public’s human rights. … When will the government stop bartering with judges and start drawing up a surveillance law that upholds our democratic freedoms?”

Watson said he was “proud to have played my part in safeguarding citizens’ fundamental rights.” He called on the government to “ensure that hundreds of thousands of people, many of whom are innocent victims or witnesses to crime, are protected by a system of independent approval for access to communications data.”

The Data Retention and Investigatory Powers Act forced telecommunications companies to store records on their customers’ emails and phone calls for 12 months. The Investigatory Powers Act broadened the data retention system by allowing the government to compel phone and internet companies to store not just email and phone records, but also logs showing the websites customers visited and the apps they used. Law enforcement agencies can then access this information without a court order or warrant for a broad range of reasons, not necessarily related to suspected criminal activity. They can obtain the data, for instance, if they judge it to be for the “purpose of protecting public health,” “in the interests of the economic well-being” of the U.K., or “for the purpose of assessing or collecting any tax, duty, levy or other imposition, contribution or charge payable to a government department.”

The Court of Appeal ruling is the latest in a series of blows for the U.K. government on surveillance. It partly reaffirms a December 2016 judgment in the European Union’s top court, which found that the British government’s data retention powers were “highly invasive” and exceeded “the limits of what is strictly necessary and cannot be considered to be justified, within a democratic society.” At least three other major legal cases challenging the country’s spy powers remain ongoing.

Ben Wallace, the U.K. government’s security minister, was dismissive of Tuesday’s decision. “This judgment relates to legislation which is no longer in force and … does not change the way in which law enforcement agencies can detect and disrupt crimes,” he said in a statement. Wallace claimed that the ruling would “not undermine the [data retention] regime” because the government had already acted preemptively in November by introducing safeguards that rein in police officers’ ability to self-authorize access to people’s private data. However, the significance of the ruling is that it will ensure the changes restricting police access to the data are bound into law and cannot be rolled back in future.

Critics believe more reforms are still required. They point out that the government has as of yet failed to address legal breaches identified in the earlier December 2016 European Union court ruling. That ruling stated that to be compliant with human rights law, the government must notify people – after investigations have been completed – if their data has been accessed and must also commit to keeping people’s private data within the EU. Spurrier described the government’s existing safeguards as “window-dressing for indiscriminate surveillance of the public.”

Top photo: A man chats on the phone during the auction at Tattersalls October Yearling Sale Book 1 on Oct. 4, 2016 in Newmarket, England.
FACEBOOK USERS, BY and large, are not very good at differentiating between what’s fact and what’s false. Many users will eagerly share both reliable news and the fake stuff without any hesitation. It happens because users either want the falsehoods to be received as true or simply can’t tell the difference. Rampant media illiteracy is the root cause of the fake news handwringing we’ve been dealing with since before the election and will be fretting over until the end of time (or the end of Facebook, whichever comes first). Today, Facebook honcho Mark Zuckerberg said he is setting out to fix this fundamental problem of digital media illiteracy — by putting more power in the hands of the illiterate.

In a new Facebook post today, Zuckerberg said he “asked our product teams to make sure we prioritize news that is trustworthy, informative, and local.” Why this has only become a priority in the company’s 14th year of existence is left unsaid. Zuckerberg admitted that “there’s too much sensationalism, misinformation and polarization in the world today,” and that his website “enables people to spread information faster than ever before.” As with the rest of Silicon Valley, Facebook is obsessed with the appearance of machine-like objectivity, and so Zuckerberg said figuring out which outlets deliberately package viral-ready falsehoods and which do not is a head-scratcher (spoiler: It isn’t):

The hard question we’ve struggled with is how to decide what news sources are broadly trusted in a world with so much division. We could try to make that decision ourselves, but that’s not something we’re comfortable with. We considered asking outside experts, which would take the decision out of our hands but would likely not solve the objectivity problem. Or we could ask you — the community — and have your feedback determine the ranking.

So, rather than relying on the subjectivity and biases of a team of outside experts, Facebook will rely on the subjectivity and biases of 2 billion people around the world. Specifically, Facebook said it will decide which media outlets are prioritized at least in part by just asking people which outlets they like:

As part of our ongoing quality surveys, we will now ask people whether they’re familiar with a news source and, if so, whether they trust that source. The idea is that some news organizations are only trusted by their readers or watchers, and others are broadly trusted across society even by those who don’t follow them directly. (We eliminate from the sample those who aren’t familiar with a source, so the output is a ratio of those who trust the source to those who are familiar with it.)

Facebook is either unaware of — or, more likely — unwilling to deal with the fact that people have rabid, tribalistic loyalties to certain outlets. Someone who enjoys sharing, say, the Daily Caller or InfoWars articles is going to, of course, say that these are trustworthy outlets. Otherwise, they’re admitting that they voluntarily consume and spread information that isn’t trustworthy, and we all think too highly of ourselves for that. According to a Facebook spokesperson, the surveys are meant to make sure “people can have more from their favorite sources and more from trusted sources.” Isn’t part of the Facebook information disaster that so many people count things like RedStateEagleMilitiaZoneDeepStateNews (or what have you) among their “favorite sources?” Should we be asking these people what’s trustworthy and what isn’t? Should they be deciding what will appear on your feed — or even their own — as reliable news?

LIKE MANY OTHER journalists, activists, and software developers I know, I carry my laptop everywhere while I’m traveling. It contains sensitive information; messaging app conversations, email, password databases, encryption keys, unreleased work, web browsers  logged into various accounts, and so on. My disk is encrypted, but all it takes to bypass this protection is for an attacker — a malicious hotel housekeeper, or “evil maid,” for example — to spend a few minutes physically tampering with it without my knowledge. If I come back and continue to use my compromised computer, the attacker could gain access to everything.

Edward Snowden and his friends have a solution. The NSA whistleblower and a team of collaborators have been working on a new open source Android app called Haven that you install on a spare smartphone, turning the device into a sort of sentry to watch over your laptop. Haven uses the smartphone’s many sensors — microphone, motion detector, light detector, and cameras — to monitor the room for changes, and it logs everything it notices. The first public beta version of Haven has officially been released; it’s available in the Play Store and on F-Droid, an open source app store for Android.

Snowden is helping to develop the software through a project he leads at the Freedom of the Press Foundation, which receives funding from The Intercept’s parent company. I sit on the FPF board with Snowden, am an FPF founder, and lent some help developing the app, including through nine months of testing. With that noted, I’ll be forthright about the product’s flaws below, and have solicited input for this article from people not involved in the project.

Also collaborating on Haven is the Guardian Project, a global collective of mobile security app developers.

Haven is an external solution to a problem computer makers traditionally attempted to handle from within their devices. Some laptops, for example, offer “secure boot” through a special tamper-resistant chip called a Trusted Platform Module, which tries to ensure that the computer’s bootloader code hasn’t been modified to be malicious. But there are various ways this could go wrong: there can be bugs in the code that does the verification, attackers could connive to get their code marked as trustworthy, or malicious code could be inserted after the bootloader. Some computer users have tried the low-tech solution of painting glitter nail polish on their laptop screws, creating a sort of seal that would be broken during a tampering attempt.

“Due to how current laptops, and probably most other computing devices, are made today, it is virtually impossible to systematically check later if the laptop has been compromised or not,” said Joanna Rutkowska, founder of the secure Qubes operating system, who invented the term “evil maid” in 2009 as part of her work as a security researcher.
In her first comments to the press, former Green Party presidential candidate Jill Stein said she will cooperate fully with the Senate Intelligence Committee’s investigation into “collusion with the Russians” during the 2016 campaign, and is currently searching for relevant documents. Stein denies holding any substantive communications with the Russian government or RT, its state-owned media property.
Stein says her involvement in the inquiry, first reported by BuzzFeed News, came as a surprise when her campaign was first contacted last month. After a subsequent dialogue between attorneys representing Stein and lawyers from the Senate Intelligence Committee, the former candidate received a formal request for cooperation. Although she says the possibility of testifying before Congress has not yet been broached, Stein says she would be “happy to do so” if asked.
Still, Stein clearly resents the Senate’s attention vis-à-vis  electoral interference and foreign meddling: “This smacks of the dangerous underbelly of these investigations. The extent to which they exercise overreach, politicizing, and sensationalism is a danger to democracy, especially in the current climate of all-out war on our First Amendment rights. This is not a time to be attacking the rights of political speech and political association.”
In the meantime, Stein’s defunct 2016 campaign is working to “produce all docs related to the inquiry into Russian interference” in accordance with the Intelligence Committee’s formal request, though Stein doesn’t “believe they’ve given us search terms,” as their respective attorneys are still “in the process of working that out.” Stein added that she’s unaware of a deadline for this document handover, but “we are trying to comply as quickly as we possibly can. … There are a number of people we have to contact that we’re not in touch with, and they have to search as well.”
It’s safe to assume the Intelligence Committee is interested in anything pertaining to Stein’s now-infamous attendance of an RT gala in Moscow, at which she was seated and photographed with Russian President Vladimir Putin and President Donald Trump’s former national security adviser Michael Flynn. Stein told The Intercept that as she has routinely appeared on RT and expects to hand over communications related to booking those TV segments and other “administrative” messages between her campaign and the Russian network, as well as “logistical” messages about the Moscow event. Stein also noted that before her Moscow visit, she had “requested to speak with either Putin or [Russian Foreign Minister Sergey] Lavrov, or someone in the Russian government, to be able to discuss our policies, because I was there to advance our agenda for peace and climate action and diplomacy and nuclear weapon abolition.”
Stein says this request was not granted. Stein also maintains that she declined to let RT pay for any portion of her trip to Moscow and further denies requesting or receiving any other assistance, monetary or otherwise, from RT, the Russian government, WikiLeaks, or the Trump campaign, adding that any dialogue or cooperation with Trump “would have been quite contrary to our values.”
When asked why she had attended the gala and sought an audience with Putin, she told The Intercept that “we sought contact with every powerful world leader we had access to,” and that the Russian government was of particular interest because of its involvement in the Syrian civil war. “We don’t have any reason to suspect that there was any backdoor communication,” Stein said. “We were very much focused on the substantive issues of the elections, and we avoid like the plague manipulations and machinations in order to make things happen behind the scenes.”
A spokesperson for Stein provided the following statement:








Responding to a Senate Select Committee on Intelligence request for documents pertaining to interference in the 2016 election, former Green Party Presidential candidate Jill Stein said she is cooperating by sharing all communications relevant to the committee’s mission. “We take seriously the issue of potential interference in our elections, as demonstrated by our continuing efforts to investigate the integrity of the 2016 election and examine our voting machines that are widely known to be vulnerable, but which still have not been examined for evidence of interference. To restore trust in our elections and democracy itself, we must safeguard our elections from all potential sources of interference, whether by foreign state actors or domestic political partisans, criminal networks, lone wolves, or private corporations – including those who control voting software.
Our campaign has observed the highest standards of transparency and integrity in our interactions with foreign nationals as well as Americans. Our communications with Russian individuals regarding an invitation to speak on international relations at the RT 10th anniversary media conference will confirm what we stated publicly at that time and since: that we did not accept any payment or even reimbursement for the trip, and that we made the trip with the goal of reaching an international audience and Russian officials with a message of Middle East peace, diplomacy, and cooperation against the urgent threat of climate change, consistent with long-standing Green principles and policies.
We strongly support legitimate inquiry into any illegal activity in our elections – including quid pro quo deals, money laundering, corruption and violation of campaign finance laws. At the same time, we caution against the politicization, sensationalism and collapse of journalistic standards that has plagued media coverage of the investigation. In the current climate of attacks on our civil liberties, with the emergence of censorship in social media and the press, criminalization of protest, militarization of police and massive expansion of the surveillance state, we must guard against the potential for these investigations to be used to intimidate and silence principled opposition to the political establishment.
















Stein said that she would release a more comprehensive statement about the investigation in the near future.








Top photo: Green Party presidential candidate Jill Stein speaks at a news conference on Fifth Avenue across the street from Trump Tower December 5, 2016 in New York City. 
THE U.K. GOVERNMENT is facing fresh calls to clarify its role in U.S. drone strikes after acknowledging that there are potentially hundreds of British spy agency personnel working inside a U.S.-controlled surveillance base that has played a key role in so-called targeted killings.

Earlier this month, British Minister of State for the Armed Forces Mark Lancaster disclosed to the U.K. Parliament that employees of eavesdropping agency Government Communications Headquarters, or GCHQ, are stationed at a remote base in the north of England called Menwith Hill. An unknown number of GCHQ employees are among 578 British civilians, military, and contractors at the site, Lancaster confirmed in a previously unreported written statement, alongside 627 Americans.

Questioned in 2013 about GCHQ’s presence at the base, the British government had insisted that it “would not comment on whether there are personnel working in intelligence” there – a position that appears to have changed with Lancaster’s admission, possibly unintentionally. His statement came in response to a Parliament member’s question about how many people are working at Menwith Hill. A spokesperson for the U.K. government’s Ministry of Defence declined to answer questions about whether the statement represented a policy shift.

Menwith Hill is the National Security Agency’s largest overseas surveillance facility, located near the small town of Harrogate in North Yorkshire. As The Intercept revealed in 2015, the base has been used to aid “a significant number of capture-kill operations” across the Middle East and North Africa, according to top-secret documents. The facility operates spy satellites used to pinpoint the locations of people on the ground below, and it is equipped with eavesdropping technology that can harvest data from more than 300 million emails and phone calls a day.
THE U.K. GOVERNMENT is facing fresh calls to clarify its role in U.S. drone strikes after acknowledging that there are potentially hundreds of British spy agency personnel working inside a U.S.-controlled surveillance base that has played a key role in so-called targeted killings.

Earlier this month, British Minister of State for the Armed Forces Mark Lancaster disclosed to the U.K. Parliament that employees of eavesdropping agency Government Communications Headquarters, or GCHQ, are stationed at a remote base in the north of England called Menwith Hill. An unknown number of GCHQ employees are among 578 British civilians, military, and contractors at the site, Lancaster confirmed in a previously unreported written statement, alongside 627 Americans.

Questioned in 2013 about GCHQ’s presence at the base, the British government had insisted that it “would not comment on whether there are personnel working in intelligence” there – a position that appears to have changed with Lancaster’s admission, possibly unintentionally. His statement came in response to a Parliament member’s question about how many people are working at Menwith Hill. A spokesperson for the U.K. government’s Ministry of Defence declined to answer questions about whether the statement represented a policy shift.

Menwith Hill is the National Security Agency’s largest overseas surveillance facility, located near the small town of Harrogate in North Yorkshire. As The Intercept revealed in 2015, the base has been used to aid “a significant number of capture-kill operations” across the Middle East and North Africa, according to top-secret documents. The facility operates spy satellites used to pinpoint the locations of people on the ground below, and it is equipped with eavesdropping technology that can harvest data from more than 300 million emails and phone calls a day.
BRITISH SPY AGENCIES are under scrutiny in a landmark court case challenging the legality of top-secret mass surveillance programs revealed in documents leaked by whistleblower Edward Snowden.

A panel of 10 judges at the European Court of Human Rights in Strasbourg, France, held a hearing Tuesday to examine the U.K. government’s large-scale electronic spying operations, following three separate challenges brought by a dozen human rights groups, including Amnesty International, Privacy International, the American Civil Liberties Union, Big Brother Watch, the Open Rights Group, and the Irish Council for Civil Liberties.

The case is the first of its kind to be heard by the court, which handles complaints related to violations of the European Convention on Human Rights, an international treaty by which the U.K. is still bound despite its vote last year to leave the European Union. The court’s judgments could have ramifications for future U.K. surveillance operations.

The human rights groups are arguing that British spy programs violate four key rights protected under the convention: the right to privacy; the right to a fair trial; the right to freedom of expression; and the right not to be discriminated against. They cite a 2015 ruling by a U.K. tribunal, which found that British eavesdropping agency Government Communications Headquarters, or GCHQ, had unlawfully spied on the communications of Amnesty International and the South Africa-based Legal Resources Centre.

Dinah Rose, a lawyer representing the human rights groups, acknowledged in court Tuesday that some serious security threats require the use of covert government surveillance. But, she added, “excessive and unaccountable state surveillance puts at risk the very core values of the free and democratic societies that terrorism seeks to undermine.”

The British government has insisted that it does not carry out “mass surveillance,” preferring instead to use the term “bulk surveillance,” which it says is necessary to discover previously unknown threats. Documents leaked by Snowden describe how GCHQ planned to carry out “population scale” surveillance; boasted that it had “massive access” to internet communications; and monitored more than 50 billion “events” about communications each day.

Government lawyer James Eadie told the court that using surveillance systems to collect and store communications is not itself a violation of privacy. Instead, he said, privacy is only violated when there is “sentient examination” of communications – in other words, when a human analyst reads or listens to individual messages or calls. This will be a key point of contention for the Strasbourg judges to consider.

Last year, a complaint filed in the case by 10 of the human rights groups named more than a dozen surveillance programs that allegedly violate rights and do not have adequate safeguards against abuse. Among them are GCHQ programs, such as KARMA POLICE, which was first exposed by The Intercept. KARMA POLICE was designed to allow the GCHQ to build “a web-browsing profile for every visible user on the internet.” The complaint also focuses on NSA-operated programs that have been shared with British spies, such as XKEYSCORE, a tool that can be used to sift through masses of emails, online chats, and virtually every other kind of internet data.

Nick Williams, Amnesty International’s senior legal counsel, said in an email that the case represented a “watershed moment for people’s privacy and freedom of expression” across the world. “The case concerns the U.K., but its significance is global. By bringing together human rights defenders and journalists from four different continents, it serves to highlight the dangers mass surveillance poses to the vital work of countless organisations and to individuals who expose human rights abuses and defend those at risk.”

Scarlet Kim, a legal officer with Privacy International, said in a statement: “For years, the U.K. Government has been intercepting the private communications and data of millions of people around the world. At the same time, it can access similarly enormous troves of information intercepted by the U.S. Government. These practices are unlawful and violate the fundamental rights of individuals across the world, assailing privacy and chilling thought and speech.”

A spokesperson for the U.K. government’s Home Office declined to comment on the specifics of the case, but said in a statement that British intelligence agencies “conduct their vital work within a strict legal and policy framework that applies rigorous safeguards and oversight mechanisms to ensure respect for human rights. We will vigorously defend the powers our agencies need to keep us all safe and secure.”

Top photo: City workers use smartphones inside in London on Oct. 30, 2017.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
AXON, THE WORLD’S largest vendor of police-worn body cameras, is moving into the business of capturing video taken by the public. In a survey emailed to law enforcement officials last month, the company formerly known as Taser International solicited naming ideas for its provisionally titled Public Evidence Product. According to the survey, the product will allow citizens to submit photos or video evidence of “a crime, suspicious activity, or event” to Evidence.com, the company’s cloud-based storage platform, to help agencies “in solving a crime or gathering a fuller point of view from the public.” Civil rights advocates interviewed by The Intercept were surprised to learn about the corporation’s latest initiative, seeing it as yet another untested effort to co-opt community oversight and privatize criminal justice.

“When police body cameras were initially established, it was because citizens were clamoring for police accountability,” explained Shahid Buttar, director of grassroots advocacy with the Electronic Frontier Foundation. “But we’ve seen how cameras have been more useful for police investigations than for accountability. This product realizes those dangers and takes them to a new dystopian level by crowdsourcing the collection of evidence and turning it over to law enforcement.”
CHAT LOGS OBTAINED from message boards used by neo-Nazis and other far-right groups show a concerted effort to compile private information on leftist enemies and circulate the data to encourage harassment or violence.

The messages were obtained by an anonymous source, who infiltrated and gained the trust of white nationalists and other right-wingers, and has been leaking the material to Unicorn Riot, a “decentralized media collective” that emerged from leftist protest movements.

The chat logs originate from various web discussion communities hosted by the provider Discord and closed to the public. The communities, which have names like “Vibrant Diversity,” “Ethnoserver,” “Safe Space 3,” “4th Reich,” and “Charlottesville 2.0,” range from having 36 users to 1,269 users. The most active, with nearly a quarter million messages over seven months, is “Vibrant Diversity,” a neo-Nazi community forum that includes a channel called “#oven,” where users share racist memes. The 4th Reich server, the second most active, has 130,000 messages over the course of four months and includes a channel called “#rare_hitlers,” where users share propaganda posters and other glorified media from Nazi Germany. The “Charlottesville 2.0” server, which contains 35,000 messages, is where the “Unite the Right” hate rally in Charlottesville was organized.

This article is based solely on chat logs from a community called “Pony Power” (Unicorn Riot published the logs yesterday). The Pony Power server has 50 users, and the chat logs contain just over 1,000 messages, posted over the course of 10 days and ranging in topic from far-right politics to advice about digital and operational security to debates about the legal limits of online behavior. The primary activity on the Pony Power server is posting private information, like names, photos, home addresses, and phone numbers of dozens of anti-fascist activists.

Victims of the outings, also known as “doxing,” described reactions ranging from terror to anger to annoyance, and have variously turned to friends and family for support and locked down their accounts. They said the Pony Power doxing campaign is just the latest in a series of online efforts by neo-Nazis and their allies to marginalize their opponents. The information compiled on Pony Power hasn’t yet been distributed to the larger right-wing extremist community. However, doxing efforts associated with prior online hate campaigns have forced targets to leave their homes in the face of death threats, rape threats, and other forms of harassment. And those attacks were mounted even before President Donald Trump came to power on the back of racist attacks against his predecessor, Mexicans, and Muslims, and before he embraced white nationalists and encouraged violence against protesters at campaign rallies.

People chatting on the Pony Power server spoke openly, as though behind closed doors, often using offensive slurs. So be warned, some of the following conversations are hard to stomach.

Scope of the harassment campaign

DURING THE 10-DAY span that the Pony Power chat logs cover, from August 17 to 27, so-called alt-right members collected private information from over 50 anti-fascist activists from the states of California, Florida, Illinois, Iowa, Maryland, Massachusetts, Minnesota, Missouri, Nebraska, North Carolina, South Dakota, Texas, Virginia, and Washington.

The information collected often included photographs, social media profiles, home address, phone numbers, email addresses, date of birth, driver license numbers, vehicle information, place of employment, and in one instance, a social security number. The justification for doxing normally put forward in Pony Power was that the targets were part of loosely structured far-left groups known as antifa, or anti-fascists, which has put up some of the most militant opposition to the far right; or they’re judged sympathetic to antifa; or they’ve been seen at protests deemed “communist” by the the far right.

The members of Pony Power often brainstorm methods to increase the effectiveness of their harassment campaigns. One user called “oxycolton” wrote, “We’ve had a lot of people dox antifags but it doesn’t hold,” apparently meaning that the information is lost, in part because they don’t yet have a database to keep track of everything.
CHAT LOGS OBTAINED from message boards used by neo-Nazis and other far-right groups show a concerted effort to compile private information on leftist enemies and circulate the data to encourage harassment or violence.

The messages were obtained by an anonymous source, who infiltrated and gained the trust of white nationalists and other right-wingers, and has been leaking the material to Unicorn Riot, a “decentralized media collective” that emerged from leftist protest movements.

The chat logs originate from various web discussion communities hosted by the provider Discord and closed to the public. The communities, which have names like “Vibrant Diversity,” “Ethnoserver,” “Safe Space 3,” “4th Reich,” and “Charlottesville 2.0,” range from having 36 users to 1,269 users. The most active, with nearly a quarter million messages over seven months, is “Vibrant Diversity,” a neo-Nazi community forum that includes a channel called “#oven,” where users share racist memes. The 4th Reich server, the second most active, has 130,000 messages over the course of four months and includes a channel called “#rare_hitlers,” where users share propaganda posters and other glorified media from Nazi Germany. The “Charlottesville 2.0” server, which contains 35,000 messages, is where the “Unite the Right” hate rally in Charlottesville was organized.

This article is based solely on chat logs from a community called “Pony Power” (Unicorn Riot published the logs yesterday). The Pony Power server has 50 users, and the chat logs contain just over 1,000 messages, posted over the course of 10 days and ranging in topic from far-right politics to advice about digital and operational security to debates about the legal limits of online behavior. The primary activity on the Pony Power server is posting private information, like names, photos, home addresses, and phone numbers of dozens of anti-fascist activists.

Victims of the outings, also known as “doxing,” described reactions ranging from terror to anger to annoyance, and have variously turned to friends and family for support and locked down their accounts. They said the Pony Power doxing campaign is just the latest in a series of online efforts by neo-Nazis and their allies to marginalize their opponents. The information compiled on Pony Power hasn’t yet been distributed to the larger right-wing extremist community. However, doxing efforts associated with prior online hate campaigns have forced targets to leave their homes in the face of death threats, rape threats, and other forms of harassment. And those attacks were mounted even before President Donald Trump came to power on the back of racist attacks against his predecessor, Mexicans, and Muslims, and before he embraced white nationalists and encouraged violence against protesters at campaign rallies.

People chatting on the Pony Power server spoke openly, as though behind closed doors, often using offensive slurs. So be warned, some of the following conversations are hard to stomach.

Scope of the harassment campaign

DURING THE 10-DAY span that the Pony Power chat logs cover, from August 17 to 27, so-called alt-right members collected private information from over 50 anti-fascist activists from the states of California, Florida, Illinois, Iowa, Maryland, Massachusetts, Minnesota, Missouri, Nebraska, North Carolina, South Dakota, Texas, Virginia, and Washington.

The information collected often included photographs, social media profiles, home address, phone numbers, email addresses, date of birth, driver license numbers, vehicle information, place of employment, and in one instance, a social security number. The justification for doxing normally put forward in Pony Power was that the targets were part of loosely structured far-left groups known as antifa, or anti-fascists, which has put up some of the most militant opposition to the far right; or they’re judged sympathetic to antifa; or they’ve been seen at protests deemed “communist” by the the far right.

The members of Pony Power often brainstorm methods to increase the effectiveness of their harassment campaigns. One user called “oxycolton” wrote, “We’ve had a lot of people dox antifags but it doesn’t hold,” apparently meaning that the information is lost, in part because they don’t yet have a database to keep track of everything.
WHEN CIVIL LIBERTIES advocates discuss the dangers of new policing technologies, they often point to sci-fi films like “RoboCop” and “Minority Report” as cautionary tales. In “RoboCop,” a massive corporation purchases Detroit’s entire police department. After one of its officers gets fatally shot on duty, the company sees an opportunity to save on labor costs by reanimating the officer’s body with sleek weapons, predictive analytics, facial recognition, and the ability to record and transmit live video.

Although intended as a grim allegory of the pitfalls of relying on untested, proprietary algorithms to make lethal force decisions, “RoboCop” has long been taken by corporations as a roadmap. And no company has been better poised than Taser International, the world’s largest police body camera vendor, to turn the film’s ironic vision into an earnest reality.

In 2010, Taser’s longtime vice president Steve Tuttle “proudly predicted” to GQ that once police can search a crowd for outstanding warrants using real-time face recognition, “every cop will be RoboCop.” Now Taser has announced that it will provide any police department in the nation with free body cameras, along with a year of free “data storage, training, and support.” The company’s goal is not just to corner the camera market, but to dramatically increase the video streaming into its servers.

With an estimated one-third of departments using body cameras, police officers have been generating millions of hours of video footage. Taser stores terabytes of such video on Evidence.com, in private servers, operated by Microsoft, to which police agencies must continuously subscribe for a monthly fee. Data from these recordings is rarely analyzed for investigative purposes, though, and Taser — which recently rebranded itself as a technology company and renamed itself “Axon” — is hoping to change that.

Taser has started to get into the business of making sense of its enormous archive of video footage by building an in-house “AI team.” In February, the company acquired a computer vision startup called Dextro and a computer vision team from Fossil Group Inc. Taser says the companies will allow agencies to automatically redact faces to protect privacy, extract important information, and detect emotions and objects — all without human intervention. This will free officers from the grunt work of manually writing reports and tagging videos, a Taser spokesperson wrote in an email. “Our prediction for the next few years is that the process of doing paperwork by hand will begin to disappear from the world of law enforcement, along with many other tedious manual tasks.” Analytics will also allow departments to observe historical patterns in behavior for officer training, the spokesperson added. “Police departments are now sitting on a vast trove of body-worn footage that gives them insight for the first time into which interactions with the public have been positive versus negative, and how individuals’ actions led to it.”

But looking to the past is just the beginning: Taser is betting that its artificial intelligence tools might be useful not just to determine what happened, but to anticipate what might happen in the future.

“We’ve got all of this law enforcement information with these videos, which is one of the richest treasure troves you could imagine for machine learning,” Taser CEO Rick Smith told PoliceOne in an interview about the company’s AI acquisitions. “Imagine having one person in your agency who would watch every single one of your videos — and remember everything they saw — and then be able to process that and give you the insight into what crimes you could solve, what problems you could deal with. Now, that’s obviously a little further out, but based on what we’re seeing in the artificial intelligence space, that could be within five to seven years.”

As video analytics and machine vision have made rapid gains in recent years, the future long dreaded by privacy experts and celebrated by technology companies is quickly approaching. No longer is the question whether artificial intelligence will transform the legal and lethal limits of policing, but how and for whose profits.

“Everyone refers to ‘Minority Report’ … about how they use facial recognition and iris recognition,” said Ron Kirk, director of the West Virginia Intelligence Fusion Center, which uses both technologies, in an interview with Vocativ. “I actually think that that is the way of the future.”
LAST YEAR, A RUSSIAN startup announced that it could scan the faces of people passing by Moscow’s thousands of CCTV cameras and pick out wanted criminals or missing persons. Unlike much face recognition technology — which runs stills from videos or photographs after the fact — NTechLab’s FindFace algorithm has achieved a feat that once only seemed possible in the science fictional universe of “Minority Report”: It can determine not just who someone is, but where they’ve been, where they’re going, and whether they have an outstanding warrant, immigration detainer, or unpaid traffic ticket.

For years, the development of real-time face recognition has been hampered by poor video resolution, the angles of bodies in motion, and limited computing power. But as systems begin to transcend these technical barriers, they are also outpacing the development of policies to constrain them. Civil liberties advocates fear that the rise of real-time face recognition alongside the growing number of police body cameras creates the conditions for a perfect storm of mass surveillance.

“The main concern is that we’re already pretty far along in terms of having this real-time technology, and we already have the cameras,” said Jake Laperruque, a fellow at the Constitution Project. “These cameras are small, hard to notice, and all over the place. That’s a pretty lethal combination for privacy unless we have reasonable rules on how they can be used together.”

This imminent reality has led several civil liberties groups to call on police departments and legislators to implement clear policies on camera footage retention, biometrics, and privacy. On Wednesday morning, the House Oversight Committee held a hearing on law enforcement’s use of facial recognition technology, where advocates emphasized the dangers of allowing advancements in real-time recognition to broaden surveillance powers. As Alvaro Bedoya, executive director of the Center on Privacy and Technology at Georgetown Law, told Congress, pairing the technology with body cameras, in particular, “will redefine the nature of public spaces.”

The integration of real-time face recognition with body-worn cameras is further along than lawmakers and citizens realize. A recent Justice Department-funded survey conducted by Johns Hopkins University found that at least nine out of 38 manufacturers of body cameras currently have facial recognition capacities or have built in an option for such technology to be used later.

Taser, which leads the market for body cameras, recently acquired two startups that will allow it to run video analytics on the footage the cameras collect, and Taser’s CEO has repeatedly emphasized the development of real-time applications, such as scanning videos for faces, objects, and suspicious activity. A spokesperson for NTechLab, which has pilot projects in 20 countries including the United States, China, the United Kingdom, and Turkey, told The Intercept that its high-performing algorithm is already compatible with body cameras.

Police see the appeal. The captain of the Las Vegas Police Department told Bloomberg in July that he envisions his officers someday patrolling the Strip with “real-time analysis” on their body cameras and an earpiece to tell them, “‘Hey, that guy you just passed 20 feet ago has an outstanding warrant.’”

At least five U.S. police departments, including those in Los Angeles and New York, have already purchased or looked into purchasing real-time face recognition for their CCTV cameras, according to a study of face recognition technology published by Bedoya and other researchers at Georgetown. Bedoya emphasized that it’s only a matter of time until the nation’s body-worn cameras will be hooked up to real-time systems. With 6,000 of the country’s 18,000 police agencies estimated to be using body cameras, the pairing would translate into hundreds of thousands of new, mobile surveillance cameras.

“For many of these systems, the inclusion of real-time face recognition is just a software update away,” said Harlan Yu, co-author of a report on body camera policies for Upturn, a technology think tank.

Civil liberties experts warn that just walking down the street in a major urban center could turn into an automatic law enforcement interaction. With the ability to glean information at a distance, officers would not need to justify a particular interaction or find probable cause for a search, stop, or frisk. Instead, everybody walking past a given officer on his patrol could be subject to a “perpetual line-up,” as the Georgetown study put it. In Ferguson, Missouri, where a Justice Department investigation showed that more than three-quarters of the population had outstanding warrants, real-time face searches could give police immense power to essentially arrest individuals at will. And in a city like New York, which has over 100 officers per square mile and plans to equip each one of them with body cameras by 2019, the privacy implications of turning every beat cop into a surveillance camera are enormous.

“The inclusion of face recognition really changes the nature and purpose of body cameras, and it changes what communities expect when they call for and pay for cameras with taxpayer dollars,” Yu said. “I think there’s a real fear in communities of color, where officers are already concentrated, that these body-worn cameras will become another tool for surveillance rather than a tool for accountability.”
THE FBI’S RAP BACK program is quietly transforming the way employers conduct background checks. While routine background checks provide employers with a one-time “snapshot” of their employee’s past criminal history, employers enrolled in federal and state Rap Back programs receive ongoing, real-time notifications and updates about their employees’ run-ins with law enforcement, including arrests at protests and charges that do not end up in convictions. (“Rap” is an acronym for Record of Arrest and Prosecution; “Back” is short for background.) Testifying before Congress about the program in 2015, FBI Director James Comey explained some limits of regular background checks: “People are clean when they first go in, then they get in trouble five years down the road [and] never tell the daycare about this.”

A majority of states already have their own databases that they use for background checks and have accessed in-state Rap Back programs since at least 2007; states and agencies now partnering with the federal government will be entering their data into the FBI’s Next Generation Identification database. The NGI database, widely considered to be the world’s largest biometric database, allows federal and state agencies to search more than 70 million civil fingerprints submitted for background checks alongside over 50 million prints submitted for criminal purposes. In July 2015, Utah became the first state to join the federal Rap Back program. Last April, aviation workers at Dallas-Ft. Worth Airport and Boston Logan International Airport began participating in a federal Rap Back pilot program for aviation employees. Two weeks ago, Texas submitted its first request to the federal criminal Rap Back system.

Rap Back has been advertised by the FBI as an effort to target individuals in “positions of trust,” such as those who work with children, the elderly, and the disabled. According to a Rap Back spokesperson, however, there are no formal limits as to “which populations of individuals can be enrolled in the Rap Back Service.” Civil liberties advocates fear that under Trump’s administration the program will grow with serious consequences for employee privacy, accuracy of records, and fair employment practices.
A BROAD COALITION of over 50 civil liberties groups delivered a letter to the Justice Department’s civil rights division Tuesday calling for an investigation into the expanding use of face recognition technology by police. “Safeguards to ensure this technology is being used fairly and responsibly appear to be virtually nonexistent,” the letter stated. The routine unsupervised use of face recognition systems, according to the dozens of signatories, threatens the privacy and civil liberties of millions — especially those of immigrants and people of color.

These civil rights groups were provided with advance copies of a watershed 150-page report detailing — in many cases for the first time — how local police departments across the country have been using facial recognition technology. Titled “The Perpetual Lineup,” the report, published Tuesday morning by the Georgetown Center on Privacy & Technology, reveals that police deploy face recognition technology in ways that are more widespread, advanced, and unregulated than anyone has previously reported.

“Face recognition is a powerful technology that requires strict oversight. But those controls by and large don’t exist today,” said Clare Garvie, one of the report’s co-authors. “With only a few exceptions, there are no laws governing police use of the technology, no standards ensuring its accuracy, and no systems checking for bias. It’s a wild west.”

Of the 52 agencies that acknowledged using face recognition in response to 106 records requests, the authors found that only one had obtained legislative approval before doing so. Government reports have long confirmed that millions of images of citizens are collected and stored in federal face recognition databases. Since at least 2002, civil liberties advocates have raised concerns that millions of drivers license photos of Americans who have never been arrested are being subject to facial searches — a practice that amounts to a perpetual digital lineup. This report augments such fears, demonstrating that at least one in four state or local law enforcement agencies have access to face recognition systems.

Among its findings, the report provides the most fine-grained detail to date on how exactly these face recognition systems might disproportionately impact African-Americans. “Face recognition systems are powerful — but they can also be biased,” the coalition’s letter explains. While one in two American adults have face images stored in at least one database, African-Americans are more likely than others to have their images captured and searched by face recognition systems.

In Virginia, for instance, the report shows how state police can search a mug shot database disproportionately populated with African-Americans, who are twice as likely to be arrested in the state. Not only are African-Americans more likely to be subject to searches, according to the report, but this overrepresentation puts them at greatest risk for a false match.

These errors could be compounded by the fact that some face recognition algorithms have been shown to misidentify African-Americans, women, and young people at unusually high rates. In a 2012 study co-authored by FBI experts, three algorithms that were tested performed between 5 and 10 percent worse on black faces than on white faces. And the overall accuracy of systems has been shown to decrease as a dataset expands. The Georgetown report interviewed two major facial recognition vendors which said that they did not test for racial basis, despite the fact that systems have been shown to be far from “race-blind.”

A slideshow on San Diego’s privacy policy obtained by the researchers reveals that people of color in the county are between 1.5 and 2.5 more likely to be targeted by its surveillance systems. San Diego County uses a mugshot-only system, and repeated studies have shown that African-Americans are twice as likely as white people to be arrested and searched by police.

A BROAD COALITION of over 50 civil liberties groups delivered a letter to the Justice Department’s civil rights division Tuesday calling for an investigation into the expanding use of face recognition technology by police. “Safeguards to ensure this technology is being used fairly and responsibly appear to be virtually nonexistent,” the letter stated. The routine unsupervised use of face recognition systems, according to the dozens of signatories, threatens the privacy and civil liberties of millions — especially those of immigrants and people of color.

These civil rights groups were provided with advance copies of a watershed 150-page report detailing — in many cases for the first time — how local police departments across the country have been using facial recognition technology. Titled “The Perpetual Lineup,” the report, published Tuesday morning by the Georgetown Center on Privacy & Technology, reveals that police deploy face recognition technology in ways that are more widespread, advanced, and unregulated than anyone has previously reported.

“Face recognition is a powerful technology that requires strict oversight. But those controls by and large don’t exist today,” said Clare Garvie, one of the report’s co-authors. “With only a few exceptions, there are no laws governing police use of the technology, no standards ensuring its accuracy, and no systems checking for bias. It’s a wild west.”

Of the 52 agencies that acknowledged using face recognition in response to 106 records requests, the authors found that only one had obtained legislative approval before doing so. Government reports have long confirmed that millions of images of citizens are collected and stored in federal face recognition databases. Since at least 2002, civil liberties advocates have raised concerns that millions of drivers license photos of Americans who have never been arrested are being subject to facial searches — a practice that amounts to a perpetual digital lineup. This report augments such fears, demonstrating that at least one in four state or local law enforcement agencies have access to face recognition systems.

Among its findings, the report provides the most fine-grained detail to date on how exactly these face recognition systems might disproportionately impact African-Americans. “Face recognition systems are powerful — but they can also be biased,” the coalition’s letter explains. While one in two American adults have face images stored in at least one database, African-Americans are more likely than others to have their images captured and searched by face recognition systems.

In Virginia, for instance, the report shows how state police can search a mug shot database disproportionately populated with African-Americans, who are twice as likely to be arrested in the state. Not only are African-Americans more likely to be subject to searches, according to the report, but this overrepresentation puts them at greatest risk for a false match.

These errors could be compounded by the fact that some face recognition algorithms have been shown to misidentify African-Americans, women, and young people at unusually high rates. In a 2012 study co-authored by FBI experts, three algorithms that were tested performed between 5 and 10 percent worse on black faces than on white faces. And the overall accuracy of systems has been shown to decrease as a dataset expands. The Georgetown report interviewed two major facial recognition vendors which said that they did not test for racial basis, despite the fact that systems have been shown to be far from “race-blind.”

A slideshow on San Diego’s privacy policy obtained by the researchers reveals that people of color in the county are between 1.5 and 2.5 more likely to be targeted by its surveillance systems. San Diego County uses a mugshot-only system, and repeated studies have shown that African-Americans are twice as likely as white people to be arrested and searched by police.


FEDERAL AGENTS from the Department of Homeland Security and the Justice Department used “a sophisticated cell phone cloning attack—the details of which remain classified—to intercept protesters’ phone communications” in Portland this summer, Ken Klippenstein reported this week in The Nation. Put aside for the moment that, if the report is true, federal agents conducted sophisticated electronic surveillance against American protesters, an alarming breach of constitutional rights. Do ordinary people have any hope of defending their privacy and freedom of assembly against threats like this?

Yes, they do. Here are two simple things you can do to help mitigate this type of threat:

As much as possible, and especially in the context of activism, use an encrypted messaging app like Signal — and get everyone you work with to use it too — to protect your SMS text messages, texting groups, and voice and video calls.
Prevent other people from using your SIM card by setting a SIM PIN on your phone. There are instructions on how to do this below.
How SIM Cloning Works

Without more details, it’s hard to be entirely sure what type of surveillance was used, but The Nation’s mention of “cell phone cloning” makes me think it was a SIM cloning attack. This involves duplicating a small chip used by virtually every cellphone to link itself to its owner’s phone number and account; this small chip is the subscriber identity module, more commonly known as SIM.

Here’s how SIM cloning would work:

First, the feds would need physical access to their target’s phone; for example, they could arrest their target at a protest, temporarily confiscating their phone.
Then they would pop out the SIM card from the phone, a process designed to be easy, since end users often have reasons to replace the card (such as traveling abroad and needing a local SIM card to access the local cellular network, or when switching cellular providers).
The feds would then copy their target’s SIM card data onto a blank SIM card (this presents some challenges, as I explain below), and then put the original SIM card back without their target knowing.

SIM cards contain a secret encryption key that is used to encrypt data between the phone and cellphone towers. They’re designed so that this key can be used (like when you receive a text or call someone) but so the key itself can’t be extracted.

But it’s still possible to extract the key from the SIM card, by cracking it. Older SIM cards used a weaker encryption algorithm and could be cracked quickly and easily, but newer SIM cards use stronger encryption and might take days or significantly longer to crack. It’s possible that this is why the details of the type of surveillance used in Portland “remain classified.” Do federal agencies know of a way to quickly extract encryption keys from SIM cards? (On the other hand, it’s also possible that “cell phone cloning” doesn’t describe SIM cloning at all but something else instead, like extracting files from the phone itself instead of data from the SIM card.)
Since May, as protesters around the country have marched against police brutality and in support of the Black Lives Matter movement, activists have spotted a recurring presence in the skies: mysterious planes and helicopters hovering overhead, apparently conducting surveillance on protesters. A press release from the Justice Department at the end of May revealed that the Drug Enforcement Agency and U.S. Marshals Service were asked by the Justice Department to provide unspecified support to law enforcement during protests. A few days later, a memo obtained by BuzzFeed News offered a little more insight on the matter; it revealed that shortly after protests began in various cities, the DEA had sought special authority from the Justice Department to covertly spy on Black Lives Matter protesters on behalf of law enforcement. 
Although the press release and memo didn’t say what form the support and surveillance would take, it’s likely that the two agencies were being asked to assist police for a particular reason. Both the DEA and the Marshals possess airplanes outfitted with so-called stingrays or dirtboxes: powerful technologies capable of tracking mobile phones or, depending on how they’re configured, collecting data and communications from mobile phones in bulk.



  
      





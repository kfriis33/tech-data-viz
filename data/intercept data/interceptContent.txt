Text
AFTER YEARS of backlash over controversial government work, Google technology will be used to aid the Trump administration’s efforts to fortify the U.S.-Mexico border, according to documents related to a federal contract.

In August, Customs and Border Protection accepted a proposal to use Google Cloud technology to facilitate the use of artificial intelligence deployed by the CBP Innovation Team, known as INVNT. Among other projects, INVNT is working on technologies for a new “virtual” wall along the southern border that combines surveillance towers and drones, blanketing an area with sensors to detect unauthorized entry into the country.

In 2018, Google faced internal turmoil over a contract with the Pentagon to deploy AI-enhanced drone image recognition solutions; the capability sparked employee concern that Google was becoming embroiled in work that could be used for lethal purposes and other human rights concerns. In response to the controversy, Google ended its involvement with the initiative, known as Project Maven, and established a new set of AI principles to govern future government contracts.
NEW YORK STARTUP Dataminr aggressively markets itself as a tool for public safety, giving institutions from local police to the Pentagon the ability to scan the entirety of Twitter using sophisticated machine-learning algorithms. But company insiders say their surveillance efforts were often nothing more than garden-variety racial profiling, powered not primarily by artificial intelligence but by a small army of human analysts conducting endless keyword searches.

In July, The Intercept reported that Dataminr, leveraging its status as an official “Twitter Partner,” surveilled the Black Lives Matter protests that surged across the country in the wake of the police killing of George Floyd. Dataminr’s services were initially designed to help hedge funds turn the first glimmers of breaking news on social media into market-beating trades, enabling something like a supercharged version of professional Twitter dashboard TweetDeck. They have since been adopted by media outlets, the military, police departments, and various other organizations seeking real-time alerts on chaos and strife.
NEW YORK STARTUP Dataminr aggressively markets itself as a tool for public safety, giving institutions from local police to the Pentagon the ability to scan the entirety of Twitter using sophisticated machine-learning algorithms. But company insiders say their surveillance efforts were often nothing more than garden-variety racial profiling, powered not primarily by artificial intelligence but by a small army of human analysts conducting endless keyword searches.

In July, The Intercept reported that Dataminr, leveraging its status as an official “Twitter Partner,” surveilled the Black Lives Matter protests that surged across the country in the wake of the police killing of George Floyd. Dataminr’s services were initially designed to help hedge funds turn the first glimmers of breaking news on social media into market-beating trades, enabling something like a supercharged version of professional Twitter dashboard TweetDeck. They have since been adopted by media outlets, the military, police departments, and various other organizations seeking real-time alerts on chaos and strife.
FACEBOOK CONTRACTORS TASKED with sifting through some of the most heinous and traumatizing content on the internet faced a new hurdle this week when they were told to return to company offices to do their work in person as a pandemic runs rampant around them. Audio obtained by The Intercept suggests that their employer, Accenture, is downplaying the risk of indoor exposure to Covid-19.

When the United States began a patchwork national lockdown in March, Facebook contractors, paid a relatively low hourly wage with few of the generous perks afforded to the company’s full-time staffers, began to feel even more acutely dispensable to the $750 billion company. Beginning this week, as first reported by The Verge, these contractors must now resume working in the same facilities that Facebook’s full-time can safely avoid, having been told that they’ll be permitted to work from home through July 2021. “Based on guidance from health and government experts, as well as decisions drawn from our internal discussions about these matters, we are allowing employees to continue voluntarily working from home until July 2021,” a Facebook spokesperson explained to Business Insider.

Facebook has said that the contractors in question, who must wade through so-called priority zero content encompassing the worst of child sexual abuse and graphic violence, can’t safely do this work from home. Three Facebook moderators employed through Accenture who spoke to The Intercept on the condition of anonymity, because they are not permitted to speak with the press, expressed a profound worry that the company, and their ultimate bosses at Facebook HQ, are once again ignoring their safety in the name of keeping the social network running smoothly.
BY 2013, the Obama administration had concluded that it could not charge WikiLeaks or Julian Assange with crimes related to publishing classified documents — documents that showed, among other things, evidence of U.S. war crimes in Iraq and Afghanistan — without criminalizing investigative journalism itself. President Barack Obama’s Justice Department called this the “New York Times problem,” because if WikiLeaks and Assange were criminals for publishing classified information, the New York Times would be just as guilty.

Five years later, in 2018, the Trump administration indicted Assange anyway. But, rather than charging him with espionage for publishing classified information, they charged him with conspiracy to commit a computer crime, later adding 17 counts of espionage in a superseding May 2019 indictment and expanding on those charges in another superseding indictment in June 2020.
BY 2013, the Obama administration had concluded that it could not charge WikiLeaks or Julian Assange with crimes related to publishing classified documents — documents that showed, among other things, evidence of U.S. war crimes in Iraq and Afghanistan — without criminalizing investigative journalism itself. President Barack Obama’s Justice Department called this the “New York Times problem,” because if WikiLeaks and Assange were criminals for publishing classified information, the New York Times would be just as guilty.

Five years later, in 2018, the Trump administration indicted Assange anyway. But, rather than charging him with espionage for publishing classified information, they charged him with conspiracy to commit a computer crime, later adding 17 counts of espionage in a superseding May 2019 indictment and expanding on those charges in another superseding indictment in June 2020.
WHEN 17-YEAR-OLD Kyle Rittenhouse killed two Black Lives Matter protesters (and wounded a third) in late August in Kenosha, Wisconsin, he instantly became a hero among white nationalist circles, in which the Second Amendment is sacrosanct.

On Wednesday, Rittenhouse and the members of the armed militias that supported him, including the Kenosha Guard and Boogaloo Bois, were named in a federal lawsuit brought under the post-Civil War Reconstruction amendments that aimed to establish Black equality.

The suit also names what it alleges was the militia groups’s most prominent enabler: Facebook. All of the parties, the complaint contends, helped deprive Kenosha Black Lives Matter protesters of their First Amendment right to assemble, thus violating the Black Lives Matter activists’ 14th Amendment right to equal protection.
IN AUGUST, 40 federal agents arrived in Memphis. Some were already on the ground by the time U.S. Attorney Michael Dunavant announced the onset of Operation Legend and the city became, along with St. Louis, the seventh to be targeted by the Justice Department’s heavy-handed initiative to reduce violent crime. Many of the agents are on temporary assignment, working in collaboration with police; nearly half will relocate by November. But they will leave behind a city flush with grant money for local police — and heightened surveillance capabilities.

In Memphis, organizers have long battled police surveillance. The fight came to a head in 2017, when a lawsuit against the city of Memphis revealed years of close surveillance of Black Lives Matter activists and union organizers. “We knew we were being watched and monitored and surveilled,” said Hunter Demster, an activist who was tracked on social media by MPD. The suit was successful, and in 2018, a federal judge ordered an independent monitor to oversee policing in the city. Now, activists there say that Operation Legend is a serious blow.
THE RISE OF the internet-connected home security camera has generally been a boon to police, as owners of these devices can (and frequently do) share footage with cops at the touch of a button. But according to a leaked FBI bulletin, law enforcement has discovered an ironic downside to ubiquitous privatized surveillance: The cameras are alerting residents when police show up to conduct searches.

A November 2019 “technical analysis bulletin” from the FBI provides an overview of “opportunities and challenges” for police from networked security systems like Amazon’s Ring and other “internet of things,” or IoT, devices. Marked unclassified but “law enforcement sensitive” and for official use only, the document was included as part of the BlueLeaks cache of material hacked from the websites of fusion centers and other law enforcement entities.


WHOEVER BROKE INTO 251 law enforcement websites and obtained the BlueLeaks trove of documents appears to have reused decades-old software for opening “backdoors” in web servers.

The use of the widely available backdoors provides evidence that the hacktivist who compromised the sensitive sites, including fusion centers linked to federal agencies, didn’t need to use sophisticated digital attack methods because the sites were not very secure.


WHILE DOCTORS AND politicians still struggle to convince Americans to take the barest of precautions against Covid-19 by wearing a mask, the Department of Homeland Security has an opposite concern, according to an “intelligence note” found among the BlueLeaks trove of law enforcement documents: Masks are breaking police facial recognition.

The rapid global spread and persistent threat of the coronavirus has presented an obvious roadblock to facial recognition’s similar global expansion. Suddenly everyone is covering their faces. Even in ideal conditions, facial recognition technologies often struggle with accuracy and have a particularly dismal track record when it comes to identifying faces that aren’t white or male. Some municipalities, startled by the civil liberties implications of inaccurate and opaque software in the hands of unaccountable and overly aggressive police, have begun banning facial recognition software outright. But the global pandemic may have inadvertently provided a privacy fix of its own — or for police, a brand new crisis.
AFTER FAILING TO PREVENT the terrorist attacks of September 11, 2001, the U.S. government realized it had an information sharing problem. Local, state and federal law enforcement agencies had their own separate surveillance databases that possibly could have prevented the attacks, but they didn’t communicate any of this information with each other. So Congress directed the newly formed Department of Homeland Security to form “fusion centers” across the country, collaborations between federal agencies like DHS and the FBI with state and local police departments, to share intelligence and prevent future terrorist attacks.

Yet in 2012 the Senate found that fusion centers have “not produced useful intelligence to support Federal counterterrorism efforts,” that the majority of the reports fusion centers produced had no connection to terrorism at all, and that the reports were low quality and often not about illegal activity. Fusion centers have also been criticized for privacy and civil liberties violations such as infiltrating and spying on anti-war activists.

Last month, the transparency collective Distributed Denial of Secrets published 269 gigabytes of law enforcement data on its website and using the peer-to-peer file sharing technology BitTorrent. The data, stolen from 251 different law enforcement websites by the hacktivist collective Anonymous, was mostly taken from fusion center websites (including many of those listed on DHS’s website), though some of the hacked websites were for local police departments, police training organizations, members-only associations for cops or retired FBI agents, and law enforcement groups specifically dedicated to investigating organized retail crime, drug trafficking, and working with industry.
NATIONWIDE PROTESTS AGAINST racist policing have brought new scrutiny onto big tech companies like Facebook, which is under boycott by advertisers over hate speech directed at people of color, and Amazon, called out for aiding police surveillance. But Microsoft, which has largely escaped criticism, is knee-deep in services for law enforcement, fostering an ecosystem of companies that provide police with software using Microsoft’s cloud and other platforms. The full story of these ties highlights how the tech sector is increasingly entangled in intimate, ongoing relationships with police departments.

Microsoft’s links to law enforcement agencies have been obscured by the company, whose public response to the outrage that followed the murder of George Floyd has focused on facial recognition software. This misdirects attention away from Microsoft’s own mass surveillance platform for cops, the Domain Awareness System, built for the New York Police Department and later expanded to Atlanta, Brazil, and Singapore. It also obscures that Microsoft has partnered with scores of police surveillance vendors who run their products on a “Government Cloud” supplied by the company’s Azure division and that it is pushing platforms to wire police field operations, including drones, robots, and other devices.
LEVERAGING CLOSE TIES to Twitter, controversial artificial intelligence startup Dataminr helped law enforcement digitally monitor the protests that swept the country following the killing of George Floyd, tipping off police to social media posts with the latest whereabouts and actions of demonstrators, according to documents reviewed by The Intercept and a source with direct knowledge of the matter.

The monitoring seems at odds with claims from both Twitter and Dataminr that neither company would engage in or facilitate domestic surveillance following a string of 2016 controversies. Twitter, up until recently a longtime investor in Dataminr alongside the CIA, provides the company with full access to a content stream known as the “firehose” — a rare privilege among tech firms and one that lets Dataminr, recently valued at over $1.8 billion, scan every public tweet as soon as its author hits send. Both companies denied that the protest monitoring meets the definition of surveillance.

A History of Police Work

Dataminr helps newsrooms, corporations, and governments around the world track crises with superhuman speed as they unfold across social media and the wider web. Through a combination of people and software, the company alerts organizations to chatter around global crises — wars, shootings, riots, disasters, and so forth — so that they’ll have a competitive edge as news is breaking. But the meaning of that competitive edge, the supercharged ability to filter out important events from the noise of hundreds of millions of tweets and posts across social media, will vary drastically based on the customer; the agenda of a newspaper using Dataminr to inform its breaking news coverage won’t be the same as the agendas of a bank or the FBI. It’s this latter category of Dataminr’s business, lucrative government work, that’s had the firm on the defensive in recent years.

In 2016, Twitter was forced to reckon with multiple reports that its platform was being used to enable domestic surveillance, including a Wall Street Journal report on Dataminr’s collaboration with American spy agencies in May; an American Civil Liberties Union report on Geofeedia, a Dataminr competitor, in October; and another ACLU investigation into Dataminr’s federal police surveillance work in December. The company sought to assure the public that attempts to monitor its users for purposes of surveillance were strictly forbidden under its rules, and that any violators would be kicked off the platform. For example, then-VP Chris Moody wrote in a company blog post that “using Twitter’s Public APIs or data products to track or profile protesters and activists is absolutely unacceptable and prohibited.” In a letter to the ACLU, Twitter public policy chief Colin Crowell similarly wrote that “the use of Twitter data for surveillance is strictly prohibited” and that “Datatminr’s product does not provide any government customers with … any form of surveillance.”

Twitter also said that Dataminr, one of its “official partners,” would “no longer support direct access by fusion centers” to information such as tweet locations; fusion centers are controversial facilities dedicated to sharing intelligence between the federal government and local police. Dataminr at the same time announced it would no longer provide a product for conducting geospatial analysis “to those supporting first reponse” and added that such clients did not have “direct firehose access.”

But based on interviews, public records requests, and company documents reviewed by The Intercept, Dataminr continues to enable what is essentially surveillance by U.S. law enforcement entities, contradicting its earlier assurances to the contrary, even if it remains within some of the narrow technical boundaries it outlined four years ago, like not providing direct firehose access, tweet geolocations, or certain access to fusion centers.

Dataminr relayed tweets and other social media content about the George Floyd and Black Lives Matter protests directly to police, apparently across the country. In so doing, it used to great effect its privileged access to Twitter data — despite current terms of service that explicitly bar software developers “from tracking, alerting, or monitoring sensitive events (such as protests, rallies, or community organizing meetings)” via Twitter.
IN THE RUN-UP to the 2020 election, former Vice President Joe Biden’s campaign is putting together a foreign policy team for a potential future administration. Among those described as being part of the team is Avril Haines, former deputy director of the CIA during the Obama administration. According to an NBC News report from last week, Haines has been tapped to work advising on policy, as well as lead the national security and foreign policy team.

In addition to her past national security work and impressive presence in the D.C. think tank world, Haines has in the past described herself as a former consultant for the controversial data-mining firm Palantir. Haines’s biography page at the Brookings Institute, where she is listed as a nonresident senior fellow, boasted of this affiliation until at least last week, when it suddenly no longer appeared on the page.

The nature of the consulting work that Haines did for Palantir is not clear. As of press time, requests for comment to her, the Biden campaign, Palantir, and Brookings were not answered. Prior to being removed from the Brookings page, the connection to the data-mining company was listed alongside a long list of other affiliations that were similarly pared down.
THE FEDERAL BUREAU of Investigation may be watching what you tweet and where people gather.

The federal law enforcement agency’s records show a growing focus on harnessing the latest private sector tools for mass surveillance, including recent contracts with companies that monitor social media posts and collect cellphone location data.

On June 9, after demonstrations around the country erupted over the police killing of George Floyd, the FBI signed an expedited agreement to extend its relationship with Dataminr, a company that monitors social media.

About a week prior, the agency modified an agreement it signed in February with Venntel, Inc., a Virginia technology firm that maps and sells the movements of millions of Americans. The company purchases bulk location data and sells it largely to government agencies.
WITH THE INK still drying on their landmark $52 million settlement with Facebook over trauma they suffered working for the company, many outsourced content moderators are now being told that they must view some of the most horrific and disturbing content on the internet for an extra 48 minutes per day, The Intercept has learned.

Following an unprecedented 2018 lawsuit by ex-Facebook content moderator Selena Scola, who said her daily exposure to depictions of rape, murder, and other gruesome acts caused her to develop post-traumatic stress disorder, Facebook agreed in early May to a $52 million settlement, paid out with $1,000 individual minimums to current and former contractors employed by outsourcing firms like Accenture. Following news of the settlement, Facebook spokesperson Drew Pusateri issued a statement reading, “We are grateful to the people who do this important work to make Facebook a safe environment for everyone. We’re committed to providing them additional support through this settlement and in the future.”

Less than a month after this breakthrough, however, Accenture management informed moderation teams that it had renegotiated its contract with Facebook, affecting at least hundreds of North American content workers who would now have to increase their exposure to exactly the sort of extreme content at the heart of the settlement, according to internal company communications reviewed by The Intercept and interviews with multiple affected workers.

The new hours were announced at the tail end of May and beginning of June via emails sent by Accenture management to the firm’s content moderation teams, including those responsible for reviewing Child Exploitation Imagery, or CEI, generally graphic depictions of sexually abused children, and Inappropriate Interactions with Children, or IIC, typically conversations in which adults message minors in an attempt to “groom” them for later sexual abuse or exchange sexually explicit images. The Intercept reviewed multiple versions of this email, apparently based off a template created by Accenture. It refers to the new contract between the two companies as the “Golden SoW,” short for “Statement of Work,” and its wording strongly suggests that stipulations in the renewed contract led to 48-minute increases in the so-called “Safety flows” that handle Facebook posts containing depictions of child abuse.
WITH THE INK still drying on their landmark $52 million settlement with Facebook over trauma they suffered working for the company, many outsourced content moderators are now being told that they must view some of the most horrific and disturbing content on the internet for an extra 48 minutes per day, The Intercept has learned.

Following an unprecedented 2018 lawsuit by ex-Facebook content moderator Selena Scola, who said her daily exposure to depictions of rape, murder, and other gruesome acts caused her to develop post-traumatic stress disorder, Facebook agreed in early May to a $52 million settlement, paid out with $1,000 individual minimums to current and former contractors employed by outsourcing firms like Accenture. Following news of the settlement, Facebook spokesperson Drew Pusateri issued a statement reading, “We are grateful to the people who do this important work to make Facebook a safe environment for everyone. We’re committed to providing them additional support through this settlement and in the future.”

Less than a month after this breakthrough, however, Accenture management informed moderation teams that it had renegotiated its contract with Facebook, affecting at least hundreds of North American content workers who would now have to increase their exposure to exactly the sort of extreme content at the heart of the settlement, according to internal company communications reviewed by The Intercept and interviews with multiple affected workers.

The new hours were announced at the tail end of May and beginning of June via emails sent by Accenture management to the firm’s content moderation teams, including those responsible for reviewing Child Exploitation Imagery, or CEI, generally graphic depictions of sexually abused children, and Inappropriate Interactions with Children, or IIC, typically conversations in which adults message minors in an attempt to “groom” them for later sexual abuse or exchange sexually explicit images. The Intercept reviewed multiple versions of this email, apparently based off a template created by Accenture. It refers to the new contract between the two companies as the “Golden SoW,” short for “Statement of Work,” and its wording strongly suggests that stipulations in the renewed contract led to 48-minute increases in the so-called “Safety flows” that handle Facebook posts containing depictions of child abuse.
DURING AN internal presentation at Facebook on Wednesday, the company debuted features for Facebook Workplace, an intranet-style chat and office collaboration product similar to Slack.

On Facebook Workplace, employees see a stream of content similar to a news feed, with automatically generated trending topics based on what people are posting about. One of the new tools debuted by Facebook allows administrators to remove and block certain trending topics among employees.

The presentation discussed the “benefits” of “content control.” And it offered one example of a topic employers might find it useful to blacklist: the word “unionize.”

SURVEILLANCE FIRMS around the world are licking their lips at a once-in-a-lifetime opportunity to cash in on the coronavirus by repositioning one of their most invasive products: the tracking bracelet.

Body monitors are associated with criminality and guilt in the popular imagination, the accessories of Wall Street crooks under house arrest and menace-to-society parolees. Unlike smartphones, de facto tracking devices in their own right, strapped-on trackers are expressly designed to be attached to the body and exist solely to report the user’s whereabouts and interactions to one or more third parties; they don’t play podcasts or tell you how many steps you took that day to sweeten the surveillance.

But a climate of perpetual bio-anxiety has paved the way for broader acceptance of carceral technologies, with a wave of companies trying to sell tracking accessories to business owners eager to reopen under the aegis of responsible social distancing and to governments hoping to keep a closer eye on people under quarantine.

EARLIER THIS MONTH, Facebook debuted its group video chat offering, Messenger Rooms, to a world under widespread pandemic lockdown, one that’s in large part replaced face-to-face meetings with streamed conversations. The chief beneficiary of this shift, Zoom, has spent months as a punching bag for privacy advocates, so Facebook was quick to assure users that it had “built Rooms with privacy in mind” and that “we don’t watch or listen to your audio or video calls.”

But today, well over a week after the rollout and nearly a month after Facebook announced and offered the privacy assurances about Messenger Rooms, it’s impossible to determine exactly what information will be collected about you and your life if you decide to use the product. The company’s public documentation of Messenger Rooms, including a post focused on privacy, offers very few details, although the privacy post promises, narrowly, that “audio and video from Rooms won’t be used to inform ads.” Facebook’s communications department spent weeks researching my questions about Messenger Rooms privacy, only to come back with few answers, and offering instead only links to a spate of vague policies that predate the product.
EARLIER THIS MONTH, Facebook debuted its group video chat offering, Messenger Rooms, to a world under widespread pandemic lockdown, one that’s in large part replaced face-to-face meetings with streamed conversations. The chief beneficiary of this shift, Zoom, has spent months as a punching bag for privacy advocates, so Facebook was quick to assure users that it had “built Rooms with privacy in mind” and that “we don’t watch or listen to your audio or video calls.”

But today, well over a week after the rollout and nearly a month after Facebook announced and offered the privacy assurances about Messenger Rooms, it’s impossible to determine exactly what information will be collected about you and your life if you decide to use the product. The company’s public documentation of Messenger Rooms, including a post focused on privacy, offers very few details, although the privacy post promises, narrowly, that “audio and video from Rooms won’t be used to inform ads.” Facebook’s communications department spent weeks researching my questions about Messenger Rooms privacy, only to come back with few answers, and offering instead only links to a spate of vague policies that predate the product.
BLUETOOTH HAS SPENT much of its life ignobly associated with crummy headphones, byzantine connection procedures, and car stereo systems that never quite seem to work right. Now this wireless technology concocted in the ’90s to help PCs and mobile phones communicate is being asked to step up and save the planet from a global pandemic. According to its two co-inventors, there could be some issues.

Named for the 10th century king Harald “Bluetooth” Gormsson, famous in Scandinavia for uniting (and Christianizing) the Danes, the humble, oft-derided wireless technology included in some form in nearly every portable device from the past decade and beyond is central to coronavirus contact tracing apps pushed by Apple, Google, and governments across the world. Banking on the standard’s ubiquity, and considerably improved reliability since the ’90s, these entities hope to turn billions of Bluetooth-enabled devices into an army of public health automatons that can map anyone who came into contact with someone who tests positive for Covid-19.
A WINDOW MAY soon open for banks and lenders to use robocalls during the coronavirus crisis. Backed by a push to provide consumers with economic relief, a more expansive exemption could lead to unsolicited debt collection and marketing.

A law passed in 1991 created a working definition of unsolicited robocalls. Last year, in response to outrage over hundreds of millions of unwanted robocalls, Congress passed a new law giving the Federal Communications Commission enhanced power to crack down on the practice with stepped-up penalties for spam callers. Generally, consumers can only receive lawful automated calls if they have opted in or provided their phone number for a financial service.

Now, the coronavirus crisis has sparked an unusual alliance of consumer advocates and the banking industry to come together to seek exemptions to the 1991 definition — and therefore the penalties that accompany enforcement. Together, the unlikely allies called for reinstating the limited use of automatic telephone dialing systems for prerecorded or artificial voice calls made without the consent of consumers.

Normally political foes, the National Consumer Law Center and the American Bankers Association came together to ask the FCC for an expedited, limited exemption to anti-robocalling regulations to allow for automated calls designed explicitly for financial relief. The exemption would allow alerts to inform consumers of loan modifications or payment forbearance options during the Covid-19 epidemic.
JAIL AND PRISON officials in at least three states are using software to scan inmate calls for mentions of the coronavirus, a move advocacy groups believe paves the way for abuse while raising stark questions about carceral health care.

The monitoring software was created by LEO Technologies, a Los Angeles company backed primarily by scandal-plagued Republican fundraiser Elliott Broidy. Known as Verus, it was first deployed several years ago to forestall suicide attempts, mine calls for investigative tips, and for a range of other purposes. In recent weeks, it has been marketed as a system “that can mitigate the effects of the COVID-19 pandemic across our nation’s jail and prison facilities” by alerting prison authorities to sickness-related conversations between inmates and the outside world.
A SMARTPHONE TRACKING firm helping Donald Trump clinch his 2020 presidential reelection recently told investors it’s identified a promising new profit opportunity: the global coronavirus pandemic.

Phunware is part of a vast galaxy of obscure advertising technology companies that help clients follow and target their customers — to “capitalize on users’ daily digital trail,” as Phunware’s site puts it. By embedding Phunware code in their app, a developer can easily glean detailed records of where a user goes and what they do, creating a rich behavioral history to sell on to others.
FACEBOOK INFORMATION TECHNOLOGY contractors have been told their physical presence is required to set up laptops for new hires and other remote employees, and have been given letters to carry on their commutes stating that they are helping to provide “essential services” amid the Covid-19 pandemic.

The contractors, who are employed through global staffing firm Astreya, serve Facebook offices across the country, including in the cities of New York; Austin, Texas; and, Menlo Park, California, all of which require nonessential workers to remain at home. Facebook is eager to get hundreds of laptops and phones set up and shipped to its newly remote workforce across the country, two contractors told The Intercept, a task delegated to them at the same time the social network has emphasized a companywide work-from-home initiative.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 

LEIA EM PORTUGUÊS 
LIKE OTHER tech firms scrambling in the face of the Covid-19 pandemic, Facebook is encouraging staff worldwide to work from home, part of a so-called social distancing strategy to slow the new coronavirus’s spread. But some in the social network’s army of contract workers, already often treated like second-class employees, have complained that they have no such luxury and are being asked to choose between their jobs and their health.

Discussions from Facebook’s internal employee forum reviewed by The Intercept reveal a state of confusion, fear, and resentment, with many precariously employed hourly contract workers stating that, contrary to statements to them from Facebook, they are barred by their actual employers from working from home, despite the technical feasibility and clear public health benefits of doing so.
IN 2013, U.S. Immigration and Customs Enforcement quietly began using a software tool to recommend whether people arrested over immigration violations should be let go after 48 hours or detained. The software’s algorithm supposedly pored over a variety of risk factors before outputting a decision.

A new lawsuit, however, filed by the New York Civil Liberties Union and Bronx Defenders, alleges that the algorithm doesn’t really make a decision, at least not one that can result in a detainee being released. Instead, the groups said, it’s an unconstitutional cudgel that’s been rigged to detain virtually everyone ICE’s New York Field Office brings in, even when the government itself believes they present a minimal threat to public safety.
IN 2013, U.S. Immigration and Customs Enforcement quietly began using a software tool to recommend whether people arrested over immigration violations should be let go after 48 hours or detained. The software’s algorithm supposedly pored over a variety of risk factors before outputting a decision.

A new lawsuit, however, filed by the New York Civil Liberties Union and Bronx Defenders, alleges that the algorithm doesn’t really make a decision, at least not one that can result in a detainee being released. Instead, the groups said, it’s an unconstitutional cudgel that’s been rigged to detain virtually everyone ICE’s New York Field Office brings in, even when the government itself believes they present a minimal threat to public safety.
A POLICE INVESTIGATOR in Spain is trying to solve a crime, but she only has an image of a suspect’s face, caught by a nearby security camera. European police have long had access to fingerprint and DNA databases throughout the 27 countries of the European Union and, in certain cases, the United States. But soon, that investigator may be able to also search a network of police face databases spanning the whole of Europe and the U.S.

According to leaked internal European Union documents, the EU could soon be creating a network of national police facial recognition databases. A report drawn up by the national police forces of 10 EU member states, led by Austria, calls for the introduction of EU legislation to introduce and interconnect such databases in every member state. The report, which The Intercept obtained from a European official who is concerned about the network’s development, was circulated among EU and national officials in November 2019. If previous data-sharing arrangements are a guide, the new facial recognition network will likely be connected to similar databases in the U.S., creating what privacy researchers are calling a massive transatlantic consolidation of biometric data.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 

THERE’S WIDESPREAD CONCERN that video cameras will use facial recognition software to track our every public move. Far less remarked upon — but every bit as alarming — is the exponential expansion of “smart” video surveillance networks.

Private businesses and homes are starting to plug their cameras into police networks, and rapid advances in artificial intelligence are investing closed-circuit television, or CCTV, networks with the power for total public surveillance. In the not-so-distant future, police forces, stores, and city administrators hope to film your every move — and interpret it using video analytics.

The rise of all-seeing smart camera networks is an alarming development that threatens civil rights and liberties throughout the world. Law enforcement agencies have a long history of using surveillance against marginalized communities, and studies show surveillance chills freedom of expression — ill effects that could spread as camera networks grow larger and more sophisticated.

To understand the situation we’re facing, we have to understand the rise of the video surveillance industrial complex — its history, its power players, and its future trajectory. It begins with the proliferation of cameras for police and security, and ends with a powerful new industry imperative: complete visual surveillance of public space.

Video Management Systems and Plug-in Surveillance Networks

In their first decades of existence, CCTV cameras were low-resolution analog devices that recorded onto tapes. Businesses or city authorities deployed them to film a small area of interest. Few cameras were placed in public, and the power to track people was limited: If police wanted to pursue a person of interest, they had to spend hours collecting footage by foot from nearby locations.

In the late 1990s, video surveillance became more advanced. A company called Axis Communications invented the first internet-enabled surveillance camera, which converted moving images to digital data. New businesses like Milestone Systems built Video Management Systems, or VMS, to organize video information into databases. VMS providers created new features like motion sensor technology that alerted guards when a person was caught on camera in a restricted area.

As time marched on, video surveillance spread. On one account, about 50 years ago, the United Kingdom had somewhere north of 60 permanent CCTV cameras installed nationwide. Today, the U.K. has over 6 million such devices, while the U.S. has tens of millions. According to marketing firm IHS Markit, 1 billion cameras will be watching the world by the end of 2021, with the United States rivaling China’s per person camera penetration rate. Police can now track people across multiple cameras from a command-and-control center, desktop, or smartphone.

While it is possible to link thousands of cameras in a VMS, it is also expensive. To increase the amount of CCTVs available, cities recently came up with a clever hack: encouraging businesses and residents to place privately owned cameras on their police network — what I call “plug-in surveillance networks.”
ON NOVEMBER 21, the Ukrainian business publication Vector published a genuine regional success story: An Amazon research lab in Kyiv, affiliated with the company’s Ring home security division, was receiving a “rebrand” makeover and a broader new role within the company. The office was already involved “in many other Amazon projects,” a lab manager told Vector. “We are no longer part of a small startup,” he said in Ukranian, “but a full-fledged R&D center working for one of the world’s largest corporations.”

Ring Ukraine has repeatedly drawn scrutiny and criticism over the past year. In November, five U.S. senators, in a letter to Amazon CEO Jeff Bezos, released the day before the Vector story, had raised concerns about the Kyiv office’s access to Ring home security footage and other private information and asked whether a foreign government could access the material as well. The letter cited an Intercept report that many employees in the office were provided blanket, inappropriate access to a web server containing customer video files. In response, Ring revealed that it had fired four employees over “access to Ring video data” that “exceeded what was necessary for their job functions.” The company did not say what, if anything, it was doing prevent such incidents in the future.
AS SEVEN University of Puerto Rico students prepare to face trial in February for participating in a nonviolent protest more than two years ago, documents released to their defense attorneys reveal that Facebook granted the island’s Justice Department access to a trove of private information from student news publications. The department’s sweeping search warrant was part of a hunt for crimes committed by members of the youth anti-austerity movement, and it has raised fears among civil liberties advocates of a return to a period of Puerto Rico’s history when police routinely targeted citizens for surveillance on the basis of their political interests.

It was April 2017, and for weeks, University of Puerto Rico students had been holding a school-wide strike protesting austerity policies that were poised to defund public services across the island to satisfy the government’s creditors. When the university’s governing board gathered on April 27 to discuss $241 million in budget cuts, the students demanded to be let in. The board refused, locking the doors to the building where the meeting was being held. But the students stormed in anyway, pushing past security.

The action unfolded in real time on Facebook, as three student media outlets, Diálogo UPR, Pulso Estudiantil UPR, and Centro de Comunicación Estudiantil, livestreamed the protest. The students surrounded the board members and shut down the meeting, demanding that the board sign a commitment to rejecting the budget cuts. The action, one of many that took place on campus and in the streets, was over within half an hour. A glass door, some furniture, and a lamp were allegedly broken or damaged. No one was injured, and no one was arrested. But the secretary of Puerto Rico’s Justice Department, now-Gov. Wanda Vázquez, pledged to investigate the incident and arrest lawbreakers.

Two weeks later, students who had assumed leadership roles in the wider strike received citations ordering them to appear in court. When they showed up, they were handcuffed, paraded before media crews, and charged with a host of crimes related to the boardroom protest, the most severe of which — rioting and burglary — were later dropped. The remaining charges, including violating the right to assemble, aggravated restriction of freedom, and violence or intimidation against a public authority, each carry between six months and three years in prison. The seven students go to trial on February 7.

How exactly Vázquez’s Justice Department determined which students to charge out of the dozens who participated in the protest has remained a mystery to defense attorneys. The lawyers’ suspicion: that the case isn’t about crimes committed in the boardroom that day, but rather an attempt to penalize the political activity of some of the most active student organizers. The seven facing trial were members of the student strikers’ negotiating committee as well as political organizations critical of the government.




JUST DAYS AFTER Facebook and one of its contractors, Accenture, sent teams responsible for content moderation back to their offices amid concerns about the coronavirus pandemic, one worker at the office tested positive for Covid-19, according to an internal email viewed by The Intercept.

According to a notification email sent to contractors working out of Accenture’s Facebook facility in Austin, Texas — where hourly contractors deal with the social media giant’s most graphic forms of violence and sexual abuse — the office has already been hit with a positive case. “We have learned that one of our people working at Facebook Domain 8 on the 12th floor has tested positive for COVID-19,” the email reads. “This individual was last in the office on 10/13, became symptomatic on 10/14 and received a positive test result on 10/16. Currently, this person is in self-quarantine.”
FEDERAL AGENTS from the Department of Homeland Security and the Justice Department used “a sophisticated cell phone cloning attack—the details of which remain classified—to intercept protesters’ phone communications” in Portland this summer, Ken Klippenstein reported this week in The Nation. Put aside for the moment that, if the report is true, federal agents conducted sophisticated electronic surveillance against American protesters, an alarming breach of constitutional rights. Do ordinary people have any hope of defending their privacy and freedom of assembly against threats like this?

Yes, they do. Here are two simple things you can do to help mitigate this type of threat:

As much as possible, and especially in the context of activism, use an encrypted messaging app like Signal — and get everyone you work with to use it too — to protect your SMS text messages, texting groups, and voice and video calls.
Prevent other people from using your SIM card by setting a SIM PIN on your phone. There are instructions on how to do this below.
How SIM Cloning Works

Without more details, it’s hard to be entirely sure what type of surveillance was used, but The Nation’s mention of “cell phone cloning” makes me think it was a SIM cloning attack. This involves duplicating a small chip used by virtually every cellphone to link itself to its owner’s phone number and account; this small chip is the subscriber identity module, more commonly known as SIM.

Here’s how SIM cloning would work:

First, the feds would need physical access to their target’s phone; for example, they could arrest their target at a protest, temporarily confiscating their phone.
Then they would pop out the SIM card from the phone, a process designed to be easy, since end users often have reasons to replace the card (such as traveling abroad and needing a local SIM card to access the local cellular network, or when switching cellular providers).
The feds would then copy their target’s SIM card data onto a blank SIM card (this presents some challenges, as I explain below), and then put the original SIM card back without their target knowing.

SIM cards contain a secret encryption key that is used to encrypt data between the phone and cellphone towers. They’re designed so that this key can be used (like when you receive a text or call someone) but so the key itself can’t be extracted.

But it’s still possible to extract the key from the SIM card, by cracking it. Older SIM cards used a weaker encryption algorithm and could be cracked quickly and easily, but newer SIM cards use stronger encryption and might take days or significantly longer to crack. It’s possible that this is why the details of the type of surveillance used in Portland “remain classified.” Do federal agencies know of a way to quickly extract encryption keys from SIM cards? (On the other hand, it’s also possible that “cell phone cloning” doesn’t describe SIM cloning at all but something else instead, like extracting files from the phone itself instead of data from the SIM card.)
Since May, as protesters around the country have marched against police brutality and in support of the Black Lives Matter movement, activists have spotted a recurring presence in the skies: mysterious planes and helicopters hovering overhead, apparently conducting surveillance on protesters. A press release from the Justice Department at the end of May revealed that the Drug Enforcement Agency and U.S. Marshals Service were asked by the Justice Department to provide unspecified support to law enforcement during protests. A few days later, a memo obtained by BuzzFeed News offered a little more insight on the matter; it revealed that shortly after protests began in various cities, the DEA had sought special authority from the Justice Department to covertly spy on Black Lives Matter protesters on behalf of law enforcement. 
Although the press release and memo didn’t say what form the support and surveillance would take, it’s likely that the two agencies were being asked to assist police for a particular reason. Both the DEA and the Marshals possess airplanes outfitted with so-called stingrays or dirtboxes: powerful technologies capable of tracking mobile phones or, depending on how they’re configured, collecting data and communications from mobile phones in bulk.

WHILE DOCTORS AND politicians still struggle to convince Americans to take the barest of precautions against Covid-19 by wearing a mask, the Department of Homeland Security has an opposite concern, according to an “intelligence note” found among the BlueLeaks trove of law enforcement documents: Masks are breaking police facial recognition.

The rapid global spread and persistent threat of the coronavirus has presented an obvious roadblock to facial recognition’s similar global expansion. Suddenly everyone is covering their faces. Even in ideal conditions, facial recognition technologies often struggle with accuracy and have a particularly dismal track record when it comes to identifying faces that aren’t white or male. Some municipalities, startled by the civil liberties implications of inaccurate and opaque software in the hands of unaccountable and overly aggressive police, have begun banning facial recognition software outright. But the global pandemic may have inadvertently provided a privacy fix of its own — or for police, a brand new crisis.
LEVERAGING CLOSE TIES to Twitter, controversial artificial intelligence startup Dataminr helped law enforcement digitally monitor the protests that swept the country following the killing of George Floyd, tipping off police to social media posts with the latest whereabouts and actions of demonstrators, according to documents reviewed by The Intercept and a source with direct knowledge of the matter.

The monitoring seems at odds with claims from both Twitter and Dataminr that neither company would engage in or facilitate domestic surveillance following a string of 2016 controversies. Twitter, up until recently a longtime investor in Dataminr alongside the CIA, provides the company with full access to a content stream known as the “firehose” — a rare privilege among tech firms and one that lets Dataminr, recently valued at over $1.8 billion, scan every public tweet as soon as its author hits send. Both companies denied that the protest monitoring meets the definition of surveillance.

A History of Police Work

Dataminr helps newsrooms, corporations, and governments around the world track crises with superhuman speed as they unfold across social media and the wider web. Through a combination of people and software, the company alerts organizations to chatter around global crises — wars, shootings, riots, disasters, and so forth — so that they’ll have a competitive edge as news is breaking. But the meaning of that competitive edge, the supercharged ability to filter out important events from the noise of hundreds of millions of tweets and posts across social media, will vary drastically based on the customer; the agenda of a newspaper using Dataminr to inform its breaking news coverage won’t be the same as the agendas of a bank or the FBI. It’s this latter category of Dataminr’s business, lucrative government work, that’s had the firm on the defensive in recent years.

In 2016, Twitter was forced to reckon with multiple reports that its platform was being used to enable domestic surveillance, including a Wall Street Journal report on Dataminr’s collaboration with American spy agencies in May; an American Civil Liberties Union report on Geofeedia, a Dataminr competitor, in October; and another ACLU investigation into Dataminr’s federal police surveillance work in December. The company sought to assure the public that attempts to monitor its users for purposes of surveillance were strictly forbidden under its rules, and that any violators would be kicked off the platform. For example, then-VP Chris Moody wrote in a company blog post that “using Twitter’s Public APIs or data products to track or profile protesters and activists is absolutely unacceptable and prohibited.” In a letter to the ACLU, Twitter public policy chief Colin Crowell similarly wrote that “the use of Twitter data for surveillance is strictly prohibited” and that “Datatminr’s product does not provide any government customers with … any form of surveillance.”

Twitter also said that Dataminr, one of its “official partners,” would “no longer support direct access by fusion centers” to information such as tweet locations; fusion centers are controversial facilities dedicated to sharing intelligence between the federal government and local police. Dataminr at the same time announced it would no longer provide a product for conducting geospatial analysis “to those supporting first reponse” and added that such clients did not have “direct firehose access.”

But based on interviews, public records requests, and company documents reviewed by The Intercept, Dataminr continues to enable what is essentially surveillance by U.S. law enforcement entities, contradicting its earlier assurances to the contrary, even if it remains within some of the narrow technical boundaries it outlined four years ago, like not providing direct firehose access, tweet geolocations, or certain access to fusion centers.

Dataminr relayed tweets and other social media content about the George Floyd and Black Lives Matter protests directly to police, apparently across the country. In so doing, it used to great effect its privileged access to Twitter data — despite current terms of service that explicitly bar software developers “from tracking, alerting, or monitoring sensitive events (such as protests, rallies, or community organizing meetings)” via Twitter.


CITIZEN, A MOBILE APP that alerts people to nearby emergencies, is testing the reintroduction of a controversial feature that lets users report crimes and incidents on their own by live streaming video.

Created by New York-based startup sp0n, Citizen first launched under the name “Vigilante” in 2016 in New York City, broadcasting alerts of 911 calls to users in the vicinity and allowing those users to send live video from incident scenes, comment on alerts, and report incidents on their own. In a splashy launch video with the hashtag #CrimeNoMore, several young men were depicted rushing to aid a woman who was chased by a menacing stranger; the video instructs users not to “interfere with the crime,” but then adds, “Good luck out there!” Vigilante was met with swift backlash from the public and police departments, and Apple soon pulled the app from its store. At that time, the New York Police Department issued a statement saying, “Crimes in progress should be handled by the NYPD and not a vigilante with a cell phone.”

Several months later, the app rebranded as Citizen, removed the incident reporting feature, and said it was shifting its focus to “safety” and “avoiding crime” — a far cry from its prior positioning.

Citizen’s return to public crime reporting has not been publicized, but is documented on the company’s user support website. The app’s latest version in Apple and Google’s app stores also includes the description: “Keep Your Community Safe: Report incidents right when they happen to protect the people around you.”
PRESIDENT DONALD TRUMP’S reelection effort has retained the services of a technology company that specializes in the mass collection of smartphone location data, which can be used to track voters for political targeting purposes.

Phunware, an Austin, Texas-based firm, announced the connection in a little-noticed press release in October, touting “new and existing customer wins including American Made Media Consultants,” the consulting firm set up this year by Trump campaign manager Brad Parscale to handle advertising services for a variety of official Trump reelection PACs. The release noted that the deal was signed in conjunction with the Trump-Pence 2020 reelection effort.

A growing subset of advertising firms rely on data brokers that use third-party apps — from popular mobile games to apps used for checking the weather, perfecting a selfie, and online banking — to harvest vast troves of information about potential voters. Phunware, in a section of its website, discusses the company’s ability to obtain GPS location data and the Wi-Fi network used by an individual, as well as user data that can infer an “individual’s gender, age, lifestyle preferences” — potential tools for identifying and influencing voters.

The company claims to offer a wide range of services based on user location data. Individuals who attend a political rally or protest can be identified as potential targets for ads, a technique known as geofencing. Location data can provide insights into how long a shopper spends at a particular clothing store, type of religious venues, or the night clubs they tend to frequent.

“Unfortunately Phunware does not comment on customer-specific data or information,” wrote Brent Brightwell, a spokesperson for Phunware, when contacted about the company’s work with Trump campaign. “Please contact the Trump reelection campaign directly should you have any questions about their activities or efforts.” The Trump campaign did not respond to a request for comment.
A FILMMAKER WORKING on a documentary that’s critical of U.S. policies. A writer who operates a pseudonymous Twitter account to evade an authoritarian regime in their home country. An activist who uses Facebook to organize protests at the U.S.-Mexico border.

These are the kinds of people who might not want U.S. immigration agents poring over their social media profiles before deciding whether they should be allowed into the country. Yet that’s exactly what the State Department now requires as part of the Trump administration’s “extreme vetting” of millions of visa applicants. As of May, people who need a visa to enter the U.S. have to disclose any social media handles they’ve used over the past five years on 20 platforms, from Instagram and Twitter to YouTube and Weibo (the Chinese microblogging service). If they don’t, their visas could be denied.
In partnership with
RING, AMAZON’S CRIMEFIGHTING surveillance camera division, has crafted plans to use facial recognition software and its ever-expanding network of home security cameras to create AI-enabled neighborhood “watch lists,” according to internal documents reviewed by The Intercept.

The planning materials envision a seamless system whereby a Ring owner would be automatically alerted when an individual deemed “suspicious” was captured in their camera’s frame, something described as a “suspicious activity prompt.”

It’s unclear who would have access to these neighborhood watch lists, if implemented, or how exactly they would be compiled, but the documents refer repeatedly to law enforcement, and Ring has forged partnerships with police departments throughout the U.S., raising the possibility that the lists could be used to aid local authorities. The documents indicate that the lists would be available in Ring’s Neighbors app, through which Ring camera owners discuss potential porch and garage security threats with others nearby.

Ring spokesperson Yassi Shahmiri told The Intercept that “the features described are not in development or in use and Ring does not use facial recognition technology,” but would not answer further questions.
SHUTTERSTOCK, THE WELL-KNOWN online purveyor of stock images and photographs, is the latest U.S. company to willingly support China’s censorship regime, blocking searches that might offend the country’s authoritarian government, The Intercept has learned.

The publicly traded company built a $639 million-per-year business on the strength of its vast — sometimes comically vast — catalog of images depicting virtually anything a blogger or advertiser could imagine. The company now does business in more than 150 countries. But in China, there is now a very small, very significant gap in Shutterstock’s offerings. In early September, Shutterstock engineers were given a new goal: The creation of a search blacklist that would wipe from query results images associated with keywords forbidden by the Chinese government. Under the new system, which The Intercept is told went into effect last month, anyone with a mainland Chinese IP address searching Shutterstock for “President Xi,” “Chairman Mao,” “Taiwan flag,” “dictator,” “yellow umbrella,” or “Chinese flag” will receive no results at all. Variations of these terms, including “umbrella movement” — the precursor to the mass pro-democracy protests currently gripping Hong Kong — are also banned.

Shutterstock’s decision to silently aid China’s censorship agenda comes at a time of heightened scrutiny into the relationship between corporate America and President Xi Jinping’s authoritarian regime. Household names like Apple, Blizzard Entertainment, the NBA, and Google have all garnered harsh criticism for letting the policy directives of the Communist Party of China, and the gilded promise of a billion customers, dictate company strategy. Deciding to censor is a particularly stark inversion of values for Shutterstock, which markets itself as an enabler of creative expression.

The photo company’s relationship with China dates back to at least 2014, when it struck a distribution deal with ZCool, a Chinese social network and portfolio site for visual artists. Last year Shutterstock announced a $15 million investment in ZCool, noting that owing to the partnership, “Shutterstock’s content now powers large technology platforms in China such as Tencent Social Ads,” an online advertising subsidiary of the tremendously popular Chinese internet conglomerate Tencent.

Shutterstock’s censorship feature appears to have been immediately controversial within the company, prompting more than 180 Shutterstock workers to sign a petition against the search blacklist and accuse the company of trading its values for access to the lucrative Chinese market. Chinese internet users already struggle to discuss even the tamest of taboo subjects; now, it seemed, the situation would get a little worse, with the aid of yet another willing American company.

“Yes, we’re a creative photo and video marketplace, but we are also an editorial news hub,” one Shutterstock employee told The Intercept. “Want to write a story about the protests in Hong Kong? They never existed. Want to write about Taiwan? It never existed. Xi Jinping is NOT a dictator because he specifically said so. This is dark shit.”

The text of the petition, provided to The Intercept, can be read in full below.

Shutterstock’s founder and CEO Jon Oringer replied to the petition several days later; those hoping for a change of heart were to be disappointed. Shutterstock’s pro-censorship compromise with the Chinese government was justified, Oringer argued, because to refuse to do business in China rather than help the country’s government expand its information control scheme would be the real act of craven corporate turpitude: “Do we make the majority of our content available to China’s 1.3 billion citizens or do we take away their ability to access it entirely? We ultimately believe, consistent with our brand promise, it is more valuable for storytellers to have access to our collection to creatively and impactfully tell their stories.” Shutterstock with a bespoke censorship feature was “more empowering” and “will better serve the people of China than the alternative,” Oringer continued.

Oringer’s company-wide response is also reproduced below.

Following Oringer’s letter and the implementation of the search term blacklist, some employees fear the use of censorship at the company will grow: “He offered no consolation in terms of what our actions will be when China requests to add an X number more search terms to the censorship list,” the Shutterstock staffer told The Intercept, “or if another country comes to us with a similar request. We are devastated.”
LEIA EM PORTUGUÊS 
IS THE HUMAN race approaching its demise? The question itself may sound hyperbolic — or like a throwback to the rapture and apocalypse. Yet there is reason to believe that such fears are no longer so overblown. The threat of climate change is forcing millions around the world to realistically confront a future in which their lives, at a minimum, look radically worse than they are today. At the same time, emerging technologies of genetic engineering and artificial intelligence are giving a small, technocratic elite the power to radically alter homo sapiens to the point where the species no longer resembles itself. Whether through ecological collapse or technological change, human beings are fast approaching a dangerous precipice.
INVESTIGATIVE JOURNALIST Thomas Peele remembers the day he learned that he and fellow reporters at the East Bay Times and the San Jose Mercury News won the Pulitzer Prize for their “relentless” coverage of the deadly Ghost Ship warehouse fire in Oakland, California.

Peele was stunned and, of course, elated that he and his team would be honored for their dogged work covering a tragedy that shook their region.

It was April 2017. One week later, MediaNews Group, owner of the two papers and scores of others around the country, announced plans to move the Bay Area’s copy desk work to Southern California, triggering 20 layoffs from a shrunken roster of fewer than 100 employees in the East Bay newsroom.

In a flash, almost a quarter of the award-winning editorial staff was gone.

“It was a kick in the teeth,” said Peele, who is an officer for the NewsGuild, the union that represents workers at the papers.

The chain’s owners and executives, he said, “didn’t even send a pizza.” But they did send layoff notices, along with “word that our papers would be edited 400 miles away by people we had no relationship with, and who had little knowledge about our region of the state. And we were basically told some stories wouldn’t be copy edited at all.”

“Pulitzer?” he asked. “What Pulitzer?’

It was all part of a growing trend of shifting tasks such as proofreading, copy editing, and page design to “hubs” that would save money. Meanwhile, the papers were profitable, not to mention Pulitzer-winning — but the hedge fund that owned their parent company wanted to skim more off the top.

Two years later, MediaNews Group, better known by its trade name Digital First Media, has decided that shutting down pressrooms, eliminating jobs, and concentrating design and printing into regional hubs hasn’t cut costs enough.

Now it’s outsourcing California news design to the Philippines, paying pennies on the dollar for work that once employed professionals who lived in the communities they served.

An employee in the production department of the design hub, who asked not to be named because of fears he’d be laid off, said Digital First once assured workers that the hubs were key to helping the papers survive. By his count, just before the Philippines contract went into effect in May, the company cut its Southern California design and copy editing pool by a half-dozen people — all on the heels of 65 layoffs across the company’s Southern California newspapers a year earlier.

“They said it was to save money,” he said. “So once we heard everything was being outsourced, we were confused, because we were supposed to be that.”

Computers Covering City Hall?

At the Denver Post, the company is pushing the envelope even further. In bargaining talks with union leaders this summer, Digital First pushed for the right to use artificial intelligence to cover high school sports. They also hope to allow computers to “gather and publish” municipal government news, including “local news stories from suburban communities, school districts and other governmental districts,” according to a company proposal obtained by The Intercept. Denver Post union official Tony Mulligan said the company has already selected a vendor and budgeted money for the prep sports transition.

Of course, artificial intelligence and data mining don’t automatically lead to job loss, and they can be useful tools in the hands of skilled journalists, said Ken Doctor, a media analyst with Nieman Lab.

“The problem is the tools are being used by those who are primarily looking at cost-cutting,” he said. “Actual journalism requires judgment.”

In the midst of this unending budget slashing, Digital First papers are making plenty of money. They netted profits of $159 million in fiscal year 2017, according to Doctor. In fact, Digital First may be the most profitable daily newspaper chain in the industry, earning an average 17 percent annual profit margin in a field where 7 or 8 percent is considered viable. Some of its papers have earned as much as 30 percent, Doctor reported.

In press releases and statements filed with the Securities and Exchange Commission this year, Digital First confirmed its “increased profitability,” which it said grew from 11.6 percent to 16.2 percent between fiscal years 2015 and 2018.

But apparently even “increased profitability” isn’t enough. That’s because Digital First is controlled by the New York hedge fund Alden Global Capital.

Since 2012, when the privately held firm took control of the chain’s hundreds of dailies and weeklies, Alden has treated them like a personal ATM, admitting in court documents that it siphoned hundreds of millions in cash from local papers across the country to gamble on unrelated businesses — many losing propositions — such the Fred’s Pharmacy chain, which on September 9 declared bankruptcy and announced that it was liquidating all its stores and assets. Other court filings show that it invested in Payless ShoeSource, which this year shuttered its more than 2,000 stores.

Under Alden’s stewardship, those companies have shuttered thousands of stores, eliminating a combined 22,000 jobs, while Alden executives and associates took in six-figure compensation packages.

This is a typical strategy for what are known as vulture funds, which push these companies into bankruptcy, preferring to make their money by selling off assets like real estate, extracting maximum profits by slashing payrolls, paying themselves handsomely by serving on the company’s boards, and charging a 2 percent management fee (which, in Alden’s case, adds up to $20 million a year).

Shuttering Newsrooms

I’ve seen firsthand what Alden can do to a company. I worked for one of its papers, the Monterey Herald, until 2015. (I now freelance as a reporter and editor for various media outlets, including websites published by the NewsGuild, the union that represents workers at 13 Digital First newspapers.)

At the Herald, it was bad enough when the hot water heater broke and never got fixed. Then employees had to put plants under the leaky roof to keep the break room from flooding. Then the presses were shut down, the building sold, and we moved into smaller rented digs.

But at least we had an office.

In 2013, Alden quietly set up a mysterious firm called Twenty Lake Holdings to sell off its newspapers’ real estate, which often includes historic buildings in prime downtown locations. In towns from Longmont, Colorado, to Pottstown, Pennsylvania, Alden has closed Digital First offices entirely, forcing reporters to work from other papers’ buildings, printing plants miles from the towns they cover — or from their homes, cars or coffee shops.

Bill Ross, executive director of the NewsGuild of Greater Philadelphia, said that in his region, three Pennsylvania newspapers have closed their offices — in Pottstown, Norristown, and West Chester. These are the papers that, according to Doctor, are earning that stunning 30 percent profit margin.

Evan Brandt, a veteran reporter at the Pottstown Mercury, now works in his attic, “up here with the Christmas decorations.”

Brandt said the biggest challenge is getting breaking news to his community. “The cops reporter doesn’t live in town, so that’s 20 minutes away.”

Local News Going Overseas

In Monrovia, 20 miles northeast of Los Angeles, a Digital First production hub of 33 full-time and three freelance employees puts together 12 daily papers that include the L.A. Daily News, the Orange County Register, the San Jose Mercury News, and a swath of smaller California papers, along with dozens of weekly papers.

But now, even that consolidation effort is apparently not saving enough. Since May, Alden has been shifting design work to the Philippines. Through an outsourcing company called AffinityX, more than 40 California weekly newspapers once designed in Monrovia are now produced in Manila.

In an August 23 email to staff, Digital First’s Southern California News Group managing editor Helayne Perry wrote that “AffinityX has taken over the design of our opinion pages.” There are “rumblings” that page two and local news sections will likely be next, the Monrovia design hub employee said.

Requests for comment to Digital First management went unanswered. An AffinityX spokesperson said company executives declined to comment.
THE FOREIGN INTELLIGENCE Surveillance Court found that the FBI may have violated the rights of potentially millions of Americans — including its own agents and informants — by improperly searching through information obtained by the National Security Agency’s mass surveillance program.

U.S. District Court Judge James E. Boasberg, who serves in the District of Columbia and the FISA court, made his sweeping and condemnatory assessment in October 2018 in a 138-page ruling, which was declassified by the U.S. government this week.
FACEBOOK CEO MARK ZUCKERBERG will be the sole witness to testify before the House Financial Services Committee on October 23 during a hearing on the company’s plans to launch its own cryptocurrency.

Members of the committee had been in talks over whether to allow Zuckerberg to skip the hearing and instead hear from Facebook COO Sheryl Sandberg, but several members pushed for the CEO to appear instead.

The hearing, titled “An Examination of Facebook and Its Impact on the Financial Services and Housing Sectors,” comes three months after Chair Maxine Waters and committee Democrats sent a letter to Facebook calling on the company to suspend plans to launch the cryptocurrency, Libra. The company is also working on a program called Calibra, which would function as a digital wallet for the cryptocurrency.
FACEBOOK CEO MARK ZUCKERBERG will be the sole witness to testify before the House Financial Services Committee on October 23 during a hearing on the company’s plans to launch its own cryptocurrency.

Members of the committee had been in talks over whether to allow Zuckerberg to skip the hearing and instead hear from Facebook COO Sheryl Sandberg, but several members pushed for the CEO to appear instead.

The hearing, titled “An Examination of Facebook and Its Impact on the Financial Services and Housing Sectors,” comes three months after Chair Maxine Waters and committee Democrats sent a letter to Facebook calling on the company to suspend plans to launch the cryptocurrency, Libra. The company is also working on a program called Calibra, which would function as a digital wallet for the cryptocurrency.
TWITTER HELPED TO promote Chinese government propaganda and disinformation about the country’s controversial internment camps in the Xinjiang region, a review of the company’s advertising records reveals.

The social media company today announced a policy change that would bar such promotion following an inquiry from The Intercept and an earlier controversy over similar propaganda related to demonstrations in Hong Kong.

In Xinjiang, a western province in China, the United Nations has estimated that 1 million ethnic minority Muslim Uighurs — including children, pregnant women, elderly people, and people with disabilities — have been detained under the pretext of fighting extremism. According to Human Rights Watch, Chinese authorities are “committing human rights abuses in Xinjiang on a scale unseen in the country in decades.”
GOOGLE IS SET to re-staff its Cairo office, which more or less went dormant in 2014, following the military coup that brought President Abdel Fattah el-Sisi to power in Egypt. The move comes against the backdrop of well-documented abuses by the Sisi government against dissidents and activists, which it facilitates using mass and targeted internet surveillance, and by blocking news, human rights, and blogging websites.

Google said it would begin recruiting full-time staff for the office after a meeting between Egyptian ministers and Google staff led by Google MENA head Lino Cattaruzzi, according to a June press release from the Egyptian government. The company also recently consulted with the Egyptian government on a data protection bill. And it is in talks to partner with the Egyptian government to expand its “Maharat min Google,” or “Skills From Google,” program, which has provided digital training for entrepreneurs through partner organizations over the past year. The expansion would be overseen by a government ministry.

Google’s renewed engagement with Egypt comes just a year after the company sparked outrage when The Intercept revealed that Google planned to develop a censored search engine for use in China, which it code-named Dragonfly. When Google had previously ended its search services in China in 2010, co-founder Sergey Brin referenced the government’s poor tolerance for dissent as a reason for the pullout. Executives say Dragonfly has been shelved, after harsh criticism from Google employees, advocacy groups, and the U.S. Congress.
NEARLY 1,500 MILES from the Menlo Park headquarters of Facebook, at a company outpost in Austin, Texas, moderators toil around the clock to screen and scrub some the most gruesome, hateful, and heinous posts that make their way onto the social network and its photo-sharing subsidiary, Instagram. They are required to view as many as 800 pieces of disturbing content in a single shift, and routinely turn to on-site counselors to help cope with the procession of stomach-turning images, videos, and text. But some members of this invisible army have complained, in a statement widely circulated within Facebook, that the outsourcing giant that officially employs them, Accenture, has repeatedly attempted to violate the confidentiality of these therapy sessions.

The moderators work from within a special section for outsourced staffers at Facebook Austin. The Texas outpost is designed to mimic the look and feel of the company’s famously opulent Silicon Valley digs, but Accenture workers say they’re reminded daily of their secondary status and denied perks, prestige, and basic respect. This second-class tier at Facebook, a sort of international shadow workforce, has been well documented in the media, from Manila to Arizona, and it’s not clear whether the company has done anything to address it beyond issuing defensive PR statements. Moderators in Austin say their job is a brutalizing slog and that Facebook remains largely indifferent to their struggles. Access to on-site counseling is one of the few bright points for this workforce.
Since June, people in China have been unable to read The Intercept, after the country’s government apparently banned our website, along with those of several other media organizations. Today, we are happy to announce a workaround that will allow people in China to circumvent the restrictions, access our full site, and continue to read our award-winning journalism.


THEY CALL IT the Silent Talker. It is a virtual policeman designed to strengthen Europe’s borders, subjecting travelers to a lie detector test before they are allowed to pass through customs.

Prior to your arrival at the airport, using your own computer, you log on to a website, upload an image of your passport, and are greeted by an avatar of a brown-haired man wearing a navy blue uniform.

“What is your surname?” he asks. “What is your citizenship and the purpose of your trip?” You provide your answers verbally to those and other questions, and the virtual policeman uses your webcam to scan your face and eye movements for signs of lying.

At the end of the interview, the system provides you with a QR code that you have to show to a guard when you arrive at the border. The guard scans the code using a handheld tablet device, takes your fingerprints, and reviews the facial image captured by the avatar to check if it corresponds with your passport. The guard’s tablet displays a score out of 100, telling him whether the machine has judged you to be truthful or not.

A person judged to have tried to deceive the system is categorized as “high risk” or “medium risk,” dependent on the number of questions they are found to have falsely answered. Our reporter — the first journalist to test the system before crossing the Serbian-Hungarian border earlier this year — provided honest responses to all questions but was deemed to be a liar by the machine, with four false answers out of 16 and a score of 48. The Hungarian policeman who assessed our reporter’s lie detector results said the system suggested that she should be subject to further checks, though these were not carried out.

Travelers who are deemed dangerous can be denied entry, though in most cases they would never know if the avatar test had contributed to such a decision. The results of the test are not usually disclosed to the traveler; The Intercept obtained a copy of our reporter’s test only after filing a data access request under European privacy laws.
LAST YEAR,GOOGLE faced internal revolt from many employees over its handling of Project Maven, a secretive contract between the company and the Department of Defense to use artificial intelligence to improve the military’s drone targeting capabilities. After a series of internal, worker-led protests and resignations following reporting by The Intercept and Gizmodo, the company said it would wind down the drone project and promised a more transparent approach to similar work in the future.

Now, a number of Google workers are voicing concerns that the Mountain View, California-based search giant is continuing to deploy cutting-edge AI technology to the Pentagon and law enforcement customers.


IT IS THE size of a small suitcase and can be placed discreetly in the back of a car. When the device is powered up, it begins secretly monitoring hundreds of cellphones in the vicinity, recording people’s private conversations and vacuuming up their text messages.

The device is one of several spy tools manufactured by a Chinese company called Semptian, which has supplied the equipment to authoritarian governments in the Middle East and North Africa, according to two sources with knowledge of the company’s operations.

As The Intercept first reported on Thursday, since 2015, Semptian has been using American technology to help build more powerful surveillance and censorship equipment, which it sells to governments under the guise of a front company called iNext.
AN AMERICAN ORGANIZATION founded by tech giants Google and IBM is working with a company that is helping China’s authoritarian government conduct mass surveillance against its citizens, The Intercept can reveal.

The OpenPower Foundation — a nonprofit led by Google and IBM executives with the aim of trying to “drive innovation” — has set up a collaboration between IBM, Chinese company Semptian, and U.S. chip manufacturer Xilinx. Together, they have worked to advance a breed of microprocessors that enable computers to analyze vast amounts of data more efficiently.

Shenzhen-based Semptian is using the devices to enhance the capabilities of internet surveillance and censorship technology it provides to human rights-abusing security agencies in China, according to sources and documents. A company employee said that its technology is being used to covertly monitor the internet activity of 200 million people.

Semptian, Google, and Xilinx did not respond to requests for comment. The OpenPower Foundation said in a statement that it “does not become involved, or seek to be informed, about the individual business strategies, goals or activities of its members,” due to antitrust and competition laws. An IBM spokesperson said that his company “has not worked with Semptian on joint technology development,” but declined to answer further questions. A source familiar with Semptian’s operations said that Semptian had worked with IBM through a collaborative cloud platform called SuperVessel, which is maintained by an IBM research unit in China.
JUST MONTHS BEFORE millions of its internal documents were stolen and dumped on the internet, the Tennessee-based surveillance company Perceptics was preparing to pitch New York’s transit authority on how it could help enforce impending “congestion pricing” rules, according to leaked documents reviewed by The Intercept. The pitch, as outlined in the files, went well beyond mere toll enforcement and into profiling New Yorkers’ travel patterns and companions, creating what experts describe as major privacy risks.

Congestion pricing, on the face of it, doesn’t seem like it would present a privacy risk — it’s a traffic policy, after all, not some new NYPD initiative. The plan is to essentially tax the cars that clog Manhattan’s streets and route the proceeds to public transportation, providing both a deterrent against and palliative for traffic. There won’t be any congestion pricing toll booths: The fee will be assessed automatically and electronically, potentially by photographing the license plates of passing cars and sending the plate owner a bill in the mail. This requires cameras running around the clock, dutifully recording every car that comes and goes. And this, Perceptics claims, is where the company truly shines.

According to an internal presentation released by the Perceptics hacker and reviewed by The Intercept, the company pitched New York’s Metropolitan Transportation Authority, or MTA, in February of this year on how Perceptics’ car-scanning camera arrays, already deployed and honed in areas like the Mexican border and an assortment of U.S. military installations, could help the MTA track down drivers. It’s unknown how the plan was received by the MTA, which administers public transit, bridges, and tolls for New York City and some of its surrounding suburbs, but leaked Perceptics emails show that the company shipped camera hardware to the MTA’s Bridges and Tunnels division for a live demonstration.

Perceptics did not respond to a request for comment. An MTA spokesperson told The Intercept that “all details are still to be determined” regarding congestion pricing enforcement.

The presentation document, titled “Smart Imaging Solutions for New York City Congestion Pricing,” makes clear that Perceptics wants to “produce vehicle-specific profiles” using cameras and “unique machine learning algorithms,” allowing the city to immediately recognize and build travel histories of every car in the congestion zone. Law enforcement and surveillance experts said the system described goes far beyond what would ever be necessary to mail scofflaws traffic tickets. Instead, it is an entirely new sort of surveillance apparatus that tracks deeply personal information like “customer travel patterns and travel consistency,” the number of passengers in the car, or “likely trip purpose,” and associates this information with a unique fingerprint of every vehicle that passes by Perceptics’ cameras.

Allie Bohm, a policy counsel with the New York Civil Liberties Union, described the Perceptics plan as an “incredibly privacy-invasive proposal” that “raises all sorts of associational and First Amendment concerns.” Bohm expressed particular alarm about the possibility of a congestion pricing enforcement system eventually feeding data into the NYPD’s existing surveillance regime. “The NYPD has fancied itself an intelligence agency for a very long time,” said Bohm. “These are folks who are pioneering some really, at best, questionable, and, at worst, alarming programs of surveillance and of drawing conclusions from innocuous behavior.”

The MTA will not deploy congestion pricing before 2021 and has yet to select a tolling vendor. But whether Perceptics wins a contract or not, its idea to bring to the heart of Manhattan military-grade surveillance technology — already provided to Saudi Special Forces and the Jordanian army, according to a Perceptics document — is an example of how something as innocuous-sounding as congestion pricing can turn into a surveillance sprawl.
IN APRIL 2018, Facebook CEO Mark Zuckerberg sat before members of both houses of Congress and told them his company respected the privacy of the roughly two billion people who use it. “Privacy” remained largely undefined throughout Zuckerberg’s televised flagellations, but he mentioned the concept more than two dozen times, including when he told the Senate’s Judiciary and Commerce committees, “We have a broader responsibility to protect people’s privacy even beyond” a consent decree from federal privacy regulators, and when he told the House Energy and Commerce Committee, “We believe that everyone around the world deserves good privacy controls.” A year later, Zuckerberg claimed in interviews and essays to have discovered the religion of personal privacy and vowed to rebuild the company in its image.

But only months after Zuckerberg first outlined his “privacy-focused vision for social networking” in a 3,000-word post on the social network he founded, his lawyers were explaining to a California judge that privacy on Facebook is nonexistent.

The courtroom debate, first reported by Law360, took place as Facebook tried to scuttle litigation from users upset that their personal data was shared without their knowledge with the consultancy Cambridge Analytica and later with advisers to Donald Trump’s campaign. The full transcript of the proceedings — which has been quoted from only briefly — reveal one of the most stunning examples of corporate doublespeak certainly in Facebook’s history.

Representing Facebook before U.S. District Judge Vince Chhabria was Orin Snyder of Gibson Dunn & Crutcher, who claimed that the plaintiffs’ charges of privacy invasion were invalid because Facebook users have no expectation of privacy on Facebook. The simple act of using Facebook, Snyder claimed, negated any user’s expectation of privacy:

There is no privacy interest, because by sharing with a hundred friends on a social media platform, which is an affirmative social act to publish, to disclose, to share ostensibly private information with a hundred people, you have just, under centuries of common law, under the judgment of Congress, under the SCA, negated any reasonable expectation of privacy.

An outside party can’t violate what you yourself destroyed, Snyder seemed to suggest. Snyder was emphatic in his description of Facebook as a sort of privacy anti-matter, going so far as to claim that “the social act of broadcasting your private information to 100 people negates, as a matter of law, any reasonable expectation of privacy.” You’d be hard-pressed to come up with a more elegant, concise description of Facebook than “the social act of broadcasting your private information” to people. So not only is it Facebook’s legal position that you’re not entitled to any expectation of privacy, but it’s your fault that the expectation went poof the moment you started using the site (or at least once you connected with 100 Facebook “friends”).

Judge Chhabria was skeptical of Snyder’s privacy nonexistence argument at times, which he rejected as treating personal privacy as a binary, “like either you have a full expectation of privacy, or you have no expectation of privacy at all,” the judge put it at one point. Chhabria continued with a relatable hypothetical:

If I share [information] with ten people, that doesn’t eliminate my expectation of privacy. It might diminish it, but it doesn’t eliminate it. And if I share something with ten people on the understanding that the entity that is helping me share it will not further disseminate it to a thousand companies, I don’t understand why I don’t have — why that’s not a violation of my expectation of privacy.

Snyder responded with an incredible metaphor for how Facebook sees your use of its services — legally, at least:

Let me give you a hypothetical of my own. I go into a classroom and invite a hundred friends. This courtroom. I invite a hundred friends, I rent out the courtroom, and I have a party. And I disclose — And I disclose something private about myself to a hundred people, friends and colleagues. Those friends then rent out a 100,000-person arena, and they rebroadcast those to 100,000 people. I have no cause of action because by going to a hundred people and saying my private truths, I have negated any reasonable expectation of privacy, because the case law is clear.

And there it is, in broad daylight: Using Facebook is a depressing party taking place in a courtroom, for some reason, that’s being simultaneously broadcasted to a 100,000-person arena on a sort of time delay. If you show up at the party, don’t be mad when your photo winds up on the Jumbotron. That is literally the company’s legal position.

Again and again, Snyder blames the targets of surveillance capitalism for their own surveillance:

This is why every parent says to their child, “Do not post it on Facebook if you don’t want to read about it tomorrow morning in the school newspaper,” or, as I tell my young associates if I were going to be giving them an orientation, “Do not put anything on social media that you don’t want to read in the Law Journal in the morning.” There is no expectation of privacy when you go on a social media platform, the purpose of which, when you are set to friends, is to share and communicate things with a large group of people, a hundred people.

At one point Chhabria asked, seemingly unable to believe Snyder’s argument himself, “If Facebook promises not to disseminate anything that you send to your hundred friends, and Facebook breaks that promise and disseminates your photographs to a thousand corporations, that would not be a serious privacy invasion?

Snyder didn’t blink: “Facebook does not consider that to be actionable, as a matter of law under California law.”

Facebook’s counsel did seem to concede one possibility for the existence of privacy on Facebook: someone who uses Facebook completely contrary to the way it’s designed and to the way it has always been marketed. “If you really want to be private,” Snyder proposed to the court, “there are people who have archival Facebook pages that are like their own private mausoleum. It’s only set to [be visible by] me, and it’s for the purpose of repository, you know, of your private information, and no one will ever see that.” So these are your possible valid legal statuses as a Facebook user: You’re either plugged into the 100,0000-person perpetual surveillance Coachella or living in a digital “mausoleum.” But if you ever decide to fling open the doors of your private data crypt and, say, share a little content on Facebook with friends, as the company has been pushing us for the past 13 years, Snyder says you’re out of luck:

Once you go to friends, the gig is over because you’ve just gone — taken a hundred people and pronounced your personal likes and dislikes. In fact, the very act of liking something and showing your friends that you like something is a non-private act. It’s the whole premise of Facebook and social media, is to render not private your likes, your dislikes, your expressions. When I tag someone in a photo, it’s to tell people, not keep private, that I’m sitting on a park bench with John Smith. So it’s the opposite of private when you do that.

Facebook’s stance that if one truly wants to keep something private, they should keep it far from Facebook is odd — odder, still, given the fact that the company publishes an extremely detailed privacy policy, perhaps only meant for those huddling in private mausoleums where such a principle still exists.

“Facebook was built to bring people closer together,” reads the start of the company’s “Privacy Principles.” “We help you connect with friends and family, discover local events and find groups to join.” Not mentioned is that if you do any of that, it’s Facebook’s official opinion that you’ve “negated” your claim to any privacy whatsoever. The list of principles reads like a bad joke after studying Snyder’s courtroom theorizing: “We design privacy into our products from the outset” seems hard to reconcile with “Once you go to friends, the gig is over.” It’s similarly hard to take “We give you control of your privacy” seriously after hearing, through Snyder, that because Facebook users “shared the information … you’ve lost control over the information and its subsequent disclosure.”
OPERATIVES AT A controversial cybersecurity firm working for the United Arab Emirates government discussed targeting The Intercept and breaching the computers of its employees, according to two sources, including a member of the hacking team who said they were present at a meeting to plan for such an attack.

The firm, DarkMatter, brought ex-National Security Agency hackers and other U.S. intelligence and military veterans together with Emirati analysts to compromise the computers of political dissidents at home and abroad, including American citizens, Reuters revealed in January. The news agency also reported that the FBI is investigating DarkMatter’s use of American hacking expertise and the possibility that it was wielded against Americans.

The campaign against dissidents and critics of the Emirati government, code-named Project Raven, began in Baltimore. A 2016 Intercept article by reporter Jenna McLaughlin revealed how the Maryland-based computer security firm CyberPoint assembled a team of Americans for a contract to hone UAE’s budding hacking and surveillance capabilities, leaving some recruits unsettled. Much of the CyberPoint team was later poached by DarkMatter, a firm with close ties to the Emirati government and headquartered just two floors from the Emirati equivalent of the NSA, the National Electronic Security Authority (which later became the Signals Intelligence Agency). One of McLaughlin’s sources described the episode as something of a “hostile takeover” by the UAE government. A subsequent story by McLaughlin  for Foreign Policy detailed how American spies at DarkMatter had been crucial in building the UAE’s intelligence apparatus. The NESA would go on to become Project Raven’s primary “client,” responsible for handing down groups and organizations to be targeted and compromised.
A MEMBER OF Project Veritas gave testimony in a federal court case indicating that the right-wing group, known for its undercover videos, violates Facebook policies designed to counter systematic deception by Russian troll farms and other groups. The deposition raises questions over whether Facebook will deter American operatives who use the platform to strategically deceive and damage political opponents as vigorously as it has Iranian and Russian propagandists. But is the company capable of doing so without just creating more problems?

Close observers of Veritas and Facebook, including one at a research lab that works with the social network, said the testimony shows the group is clearly violating policies against what Facebook refers to as “coordinated inauthentic behavior.” The company formally defined such behavior in a December 2018 video featuring its cybersecurity policy chief Nathaniel Gleicher, who said it “is when groups of pages or people work together to mislead others about who they are or what they’re doing.” The designation, Gleicher added, is applied by Facebook to a group not “because of the content they’re sharing” but rather only “because of their deceptive behavior.” That is, using Facebook to dupe people is all it takes to fit the company’s institutional definition of coordinated inauthentic behavior.

In practice, “coordinated inauthentic behavior” has become a sort of catchall label for untoward meddling on Facebook, snagging everyone from Burmese military officers to Russian meme spammers. But curbing such activity has become a very public crusade for Facebook in the wake of its prominent role as a platform for the spread of disinformation, propaganda, and outright hoaxes during the 2016 presidential campaign. This past January, Gleicher announced the removal of coordinated inauthentic behavior from Iran, which spread when operatives “coordinated with one another and used fake accounts to misrepresent themselves,” thus triggering a Facebook ban. Similarly, in a 2017 update on Facebook’s internal investigation into Russian online propaganda efforts, the company’s then-head of security Alex Stamos assured the world’s democracies the company was providing “technology improvements for detecting fake accounts,” including “changes to help us more efficiently detect and stop inauthentic accounts at the time they are being created.”

Throughout all of this, coordinated inauthentic behavior has remained more or less synonymous with “foreign actors” and “nation-states,” the cloak-and-dagger stuff of an increasingly militarized internet filled with enemies of the Western Democracy who seek to subvert it from abroad.

Project Veritas, a hybrid of an opposition research shop and a ranting YouTube channel, has taken pride in its ability to deceive since its creation in 2010. With conservative backers like Peter Thiel, the Koch brothers, and the Trump Foundation, the group and its founder James O’Keefe have worked relentlessly to target and malign individuals at institutions they deem leftist, whether it’s Planned Parenthood (reportedly targeted by O’Keefe posing as a young teen’s 23-year-old boyfriend), George Soros (the progressive philanthropist whose professional circle Veritas tried and spectacularly failed to infiltrate), or the Washington Post (whose reporter was offered a fake story on Alabama Senate candidate Roy Moore). O’Keefe has long attempted to position himself in the context of dogged, daring, traditional journalism, describing Veritas’s efforts as “investigative” reporting executed by “undercover journalists.” But his efforts are often executed by what the New Yorker has called “amateurish spies” — their efforts against the Post and Soros resembled a Three Stooges bit — and packaged with mendacious editing, duplicitous production, and outright lying, making Veritas’s audience as much a victim of its productions as the subjects. Debates over who or what is to be considered “real journalism” are almost always counterproductive and contrived, but Veritas stands out for the shamelessness with which it pursues nakedly partisan ends.

There is, of course, a proud tradition of undercover journalism executed unequivocally in the name of informing the public. Writers like Barbara Ehrenreich and Shane Bauer have taken jobs they were not otherwise interested in in order to reveal injustices in society’s margins, and some of the most damning details of the Cambridge Analytica scandal were exposed by a reporter with the UK’s Channel 4 posing as a foreign politician interested in the company’s services. This reporting involved lying, sure — or at least the withholding of true intent, and a willingness to let others deceive themselves — but only as a means to a truthful end. The distinction between these reporters and Veritas operatives may be that the end the latter group seeks, the final media product, is typically just another act of partisan misdirection that doesn’t withstand further scrutiny.

Neither Project Veritas nor Facebook commented for this story.

“Legend Building” by Project Veritas

Project Veritas has systematically deceived not just targets on the left and viewers on the right but Facebook users as well (their official page has over 200,000 followers) at a time when the company is publicly dedicated to fighting this sort of systemic duplicity. That’s a wrinkle that raises questions about Facebook’s commitment to rooting out coordinated inauthentic behavior closer to home — Thiel sits on the company’s board — not to mention Project Veritas’s presence on social media.
THE CHINESE GOVERNMENT appears to have launched a major new internet crackdown, blocking the country’s citizens from accessing The Intercept’s website and those of at least seven other Western news organizations.

On Friday, people in China began reporting that they could not access the websites of The Intercept, The Guardian, the Washington Post, HuffPost, NBC News, the Christian Science Monitor, the Toronto Star, and Breitbart News.

It is unclear exactly when the censorship came into effect or the reasons for it. But Tuesday marked the 30th anniversary of the Tiananmen Square massacre, and Chinese authorities have reportedly increased levels of online censorship to coincide with the event.

Charlie Smith, co-founder of GreatFire.org, an organization that monitors Chinese government internet censorship, said that the apparent crackdown on Western news sites represented a significant new development and described it as a “censorship Black Friday.”

“This frenzied activity could indicate that the authorities are accelerating their push to sever the link between Chinese citizens and any news source that falls outside of the influence of The Party,” said Smith, referencing the ruling Communist Party regime.

For years, China has blocked several Western news organizations after they have published stories that reflect negatively on the government. The New York Times, Bloomberg, the Wall Street Journal, and Reuters have all previously been censored, rendering their websites inaccessible in the country.

China operates an internet censorship system known as the Great Firewall, which uses filtering equipment to stop people in the country from accessing content published on banned websites that are operated outside China’s borders.

It is possible to circumvent the censorship using tools such as a virtual private network, or VPN. However, use of technology that bypasses the Great Firewall is banned — and people in the country who sell access to these services have been jailed.
THE MESSAGES ARRIVED suddenly and then he went quiet. “My identity is leaked,” he said. “I am worried about my safety.”

The Chinese dissident artist Badiucao had been busy preparing an exhibition in Hong Kong to celebrate Free Expression Week, a series of events organized by rights groups. His show was partly inspired by Google’s plan to build a censored search engine in China, and was set to include work that the artist had created skewering the U.S. tech giant for cooperating with the Communist Party regime’s suppression of internet freedom.

But just days before the exhibition was set to launch last year, at a high-profile event featuring members of Russian punk-activist group Pussy Riot, it was canceled by organizers. Badiucao had received threats from the Chinese government and soon went into hiding.

It was a nightmare scenario for the artist, one of China’s most prolific political satirists, who has never revealed his real name. Somehow, police in China had discovered who he was — and they were trying to track him down.
IN A FEDERAL lawsuit, the tech giant Oracle has provided new details to support its accusation that Amazon secretly negotiated a job offer with a then-Department of Defense official who helped shape the procurement process for a massive federal contract for which Amazon was a key bidder.

Amazon Web Services and Microsoft are now the two finalists to win the highly contested $10 billion contract for what is known as the Joint Enterprise Defense Infrastructure, or JEDI. The deal, one of the largest federal contracts in U.S. history, would pay one company to provide cloud computing services in support of Defense Department operations around the world.

But the contract has been hotly contested since the department began soliciting proposals last year. Two of Amazon’s competitors, IBM and Oracle, filed complaints with the Government Accountability Office saying that the winner-take-all process unfairly favored Amazon, which is seen as an industry leader in cloud computing. When its claim was rejected, Oracle sued the government in the U.S. Court of Federal Claims.

Since the court battle began in 2018, Oracle has aggressively lodged conflict-of-interest accusations involving a former DOD official named Deap Ubhi, who left the department in 2017 to take a job at Amazon. In a court motion filed on Friday, Oracle alleged that while Ubhi worked on the preliminary research for the JEDI program in the late summer and fall of 2017, he was also engaged in a secret job negotiation with Amazon for months, complete with salary discussions, offers of signing bonuses, and lucrative stock options.

The motion further alleges that Ubhi did not recuse himself from the JEDI program until weeks after verbally accepting a job offer from Amazon and that he continued to receive information about Amazon’s competitors and participate in meetings about technical requirements, despite a government regulation that forbids such conflicts of interest.

“Neither Ubhi nor [Amazon Web Services] disclosed the employment discussions or job offer to DOD — not when the employment discussions started, not when the informal job offer occurred, not when the formal offer occurred, and not even when Ubhi accepted the offer,” Oracle’s motion reads.

As America’s technology companies have continued to outpace the Pentagon, the Defense Department has looked to recruit talent from Silicon Valley to help enhance its information technology.

Ubhi is a venture capitalist and technology entrepreneur who worked for Amazon before his time in government. He took a job working on a Defense Department initiative aimed at collaborating with Silicon Valley to modernize the Pentagon’s information technology systems. After working as part of a four-person team to help shape the Pentagon JEDI procurement process, he left the department and returned to Amazon in November 2017.

A spokesperson for Amazon Web Services declined to comment and declined to make Ubhi available for an interview, citing ongoing litigation. Elissa Smith, a spokesperson for the Department of Defense, also told The Intercept that “we don’t comment on pending litigation.”

In a previous court filing, U.S. government lawyers accused Oracle of a “broad fishing expedition primarily [intended] to find support for its claim that the solicitation at issue is tainted by alleged conflicts of interest.”

According to Oracle’s motion on Friday, Ubhi began job negotiations with Amazon in August 2017, while he was working on the early stages of the JEDI program. Oracle claims says that “deep discussions” about employment began in late September and that Ubhi “verbally committed” to take the job on October 4. But according to the filing, Ubhi did not recuse himself until October 31, 2017. Oracle alleges that he continued to influence the program in the meantime.

Under the Procurement Integrity Act, government officials who are “contacted by a [contract] bidder about non-federal employment” have two options: They must either report the contact and reject the offer of employment or promptly recuse themselves from any contract proceedings.

“Contracts should be awarded fairly based on merit,” Mandy Smithberger, director of the Center for Defense Information at the Project on Government Oversight, told The Intercept. “The Procurement Integrity Act seeks to ensure that job offers and other financial conflicts of interest don’t influence that process.”

Last year, a Defense Department review found that “there were four instances where [department] individuals with potential financial conflicts of interest” had worked on the JEDI program, according to court records, but the Pentagon concluded that this hadn’t unfairly impacted the contracting process. Two follow-up reviews — one by the GAO in November 2018 and another by the Defense Department in April 2019 — came to similar conclusions.

The second Pentagon review came after the department said that it had received “new information” about Ubhi and would investigate it. According to Oracle’s motion on Friday, the “new information” came from a “belated submission from [Amazon]” to the DOD’s contracting officer that finally acknowledged the monthslong employment talks.

According to Oracle, Ubhi provided a “false narrative” to the contracting officer at the time of his recusal, saying that he was stepping away from the project because Amazon had offered to acquire a company that Ubhi had a stake in. That was a pretext to mask the fact he had been negotiating for months to obtain a job at the company, Oracle’s filing said.

The filing also alleges that between Ubhi’s verbal commitment to accept Amazon’s offer and his recusal from JEDI, he continued to participate in Pentagon meetings about the project’s technical requirements and to receive submissions from Amazon competitors. It also alleges that Ubhi downloaded material from a JEDI project Google Drive to his own laptop.

In its filings, Oracle has argued that Ubhi was instrumental in persuading the Pentagon to seek services from a single vendor — a decision widely seen to improve Amazon’s chances. Oracle cites workplace messages on the platform Slack in which Ubhi tries to persuade his colleagues to come around to that view, but the company does not cite any messages suggesting what his reasons or motive may have been.




AMONG THE MEGA-CORPORATIONS that surveil you, your cellphone carrier has always been one of the keenest monitors, in constant contact with the one small device you keep on you at almost every moment. A confidential Facebook document reviewed by The Intercept shows that the social network courts carriers, along with phone makers — some 100 different companies in 50 countries — by offering the use of even more surveillance data, pulled straight from your smartphone by Facebook itself.

Offered to select Facebook partners, the data includes not just technical information about Facebook members’ devices and use of Wi-Fi and cellular networks, but also their past locations, interests, and even their social groups. This data is sourced not just from the company’s main iOS and Android apps, but from Instagram and Messenger as well. The data has been used by Facebook partners to assess their standing against competitors, including customers lost to and won from them, but also for more controversial uses like racially targeted ads.

Some experts are particularly alarmed that Facebook has marketed the use of the information — and appears to have helped directly facilitate its use, along with other Facebook data — for the purpose of screening customers on the basis of likely creditworthiness. Such use could potentially run afoul of federal law, which tightly governs credit assessments.

Facebook said it does not provide creditworthiness services and that the data it provides to cellphone carriers and makers does not go beyond what it was already collecting for other uses.

Facebook’s cellphone partnerships are particularly worrisome because of the extensive surveillance powers already enjoyed by carriers like AT&T and T-Mobile: Just as your internet service provider is capable of watching the data that bounces between your home and the wider world, telecommunications companies have a privileged vantage point from which they can glean a great deal of information about how, when, and where you’re using your phone. AT&T, for example, states plainly in its privacy policy that it collects and stores information “about the websites you visit and the mobile applications you use on our networks.” Paired with carriers’ calling and texting oversight, that accounts for just about everything you’d do on your smartphone.

An Inside Look at “Actionable Insights”

You’d think that degree of continuous monitoring would be more than sufficient for a communications mammoth to operate its business — and perhaps for a while it was. But Facebook’s “Actionable Insights,” a corporate data-sharing program, suggests that even the incredible visibility telecoms have into your daily life isn’t enough — and Zuckerberg et al. can do them one better. Actionable Insights was announced last year in an innocuous, easy-to-miss post on Facebook’s engineering blog. The article, titled “Announcing tools to help partners improve connectivity,” strongly suggested that the program was primarily aimed at solving weak cellular data connections around the world. “To address this problem,” the post began, “we are building a diverse set of technologies, products, and partnerships designed to expand the boundaries of existing connectivity quality and performance, catalyze new market segments, and bring better access to the unconnected.” What sort of monster would stand against better access for the unconnected?

The blog post makes only a brief mention of Actionable Insights’ second, less altruistic purpose: “enabling better business decisions” through “analytics tools.” According to materials reviewed by The Intercept and a source directly familiar with the program, the real boon of Actionable Insights lies not in its ability to fix spotty connections, but to help chosen corporations use your personal data to buy more tightly targeted advertising.

The source, who discussed Actionable Insights on the condition of anonymity because they were not permitted to speak to the press, explained that Facebook has offered the service to carriers and phone makers ostensibly of free charge, with access to Actionable Insights granted as a sweetener for advertising relationships. According to the source, the underlying value of granting such gratis access to Actionable Insights in these cases isn’t simply to help better service cell customers with weak signals, but also to ensure that telecoms and phone makers keep buying  more and more carefully targeted Facebook ads. It’s exactly this sort of quasi-transactional data access that’s become a hallmark of Facebook’s business, allowing the company to plausibly deny that it ever sells your data while still leveraging it for revenue. Facebook may not be “selling” data through Actionable Insights in the most baldly literal sense of the word — there’s no briefcase filled with hard drives being swapped for one containing cash — but the relationship based on spending and monetization certainly fits the spirit of a sale. A Facebook spokesperson declined to answer whether the company charges for Actionable Insights access.

The confidential Facebook document provides an overview of Actionable Insights and espouses its benefits to potential corporate users. It shows how the program, ostensibly created to help improve underserved cellular customers, is pulling in far more data than how many bars you’re getting. According to one portion of the presentation, the Facebook mobile app harvests and packages eight different categories of information for use by over 100 different telecom companies in over 50 different countries around the world, including usage data from the phones of children as young as 13. These categories include use of video, demographics, location, use of Wi-Fi and cellular networks, personal interests, device information, and friend homophily, an academic term of art. A 2017 article on social media friendship from the Journal of the Society of Multivariate Experimental Psychology defined “homophily” in this context as “the tendency of nodes to form relations with those who are similar to themselves.” In other words, Facebook is using your phone to not only provide behavioral data about you to cellphone carriers, but about your friends as well.

From these eight categories alone, a third party could learn an extraordinary amount about patterns of users’ daily life, and although the document claims that the data collected through the program is “aggregated and anonymized,” academic studies have found time and again that so-called anonymized user data can be easily de-anonymized. Today, such claims of anonymization and aggregation are essentially boilerplate from companies who wager you’ll be comfortable with them possessing a mammoth trove of personal observations and behavioral predictions about your past and future if the underlying data is sufficiently neutered and grouped with your neighbor’s.

A Facebook spokesperson told The Intercept that Actionable Insights doesn’t collect any data from user devices that wasn’t already being collected anyway. Rather, this spokesperson said Actionable Insights repackages the data in novel ways useful to third-party advertisers in the telecom and smartphone industries.

Material reviewed by The Intercept show demographic information presented in a dashboard-style view, with maps showing customer locations at the county and city level. A Facebook spokesperson said they “didn’t think it goes more specific than zip code.” But armed with location data beamed straight from your phone, Facebook could technically provide customer location accurate to a range of several meters, indoors or out.

Targeting By Race and Likely Creditworthiness

Despite Facebook’s repeated assurances that user information is completely anonymized and aggregated, the Actionable Insights materials undermine this claim. One Actionable Insights case study from the overview document promotes how an unnamed North American cellular carrier had previously used its Actionable Insights access to target a specific, unnamed racial group. Facebook’s targeting of “multicultural affinity groups,” as the company formerly referred to race, was discontinued in 2017 after the targeting practice was widely criticized as potentially discriminatory.

Another case study described how Actionable Insights can be used to single out individual customers on the basis of creditworthiness. In this example, Facebook explained how one of its advertising clients, based outside the U.S., wanted to exclude individuals from future promotional offers on the basis of their credit. Using data provided through Actionable Insights, a Data Science Strategist, a role for which Facebook continues to hire, was able to generate profiles of customers with desirable and undesirable credit standings. The advertising client then used these profiles to target or exclude Facebook users who resembled these profiles.
AMONG THE MEGA-CORPORATIONS that surveil you, your cellphone carrier has always been one of the keenest monitors, in constant contact with the one small device you keep on you at almost every moment. A confidential Facebook document reviewed by The Intercept shows that the social network courts carriers, along with phone makers — some 100 different companies in 50 countries — by offering the use of even more surveillance data, pulled straight from your smartphone by Facebook itself.

Offered to select Facebook partners, the data includes not just technical information about Facebook members’ devices and use of Wi-Fi and cellular networks, but also their past locations, interests, and even their social groups. This data is sourced not just from the company’s main iOS and Android apps, but from Instagram and Messenger as well. The data has been used by Facebook partners to assess their standing against competitors, including customers lost to and won from them, but also for more controversial uses like racially targeted ads.

Some experts are particularly alarmed that Facebook has marketed the use of the information — and appears to have helped directly facilitate its use, along with other Facebook data — for the purpose of screening customers on the basis of likely creditworthiness. Such use could potentially run afoul of federal law, which tightly governs credit assessments.

Facebook said it does not provide creditworthiness services and that the data it provides to cellphone carriers and makers does not go beyond what it was already collecting for other uses.

Facebook’s cellphone partnerships are particularly worrisome because of the extensive surveillance powers already enjoyed by carriers like AT&T and T-Mobile: Just as your internet service provider is capable of watching the data that bounces between your home and the wider world, telecommunications companies have a privileged vantage point from which they can glean a great deal of information about how, when, and where you’re using your phone. AT&T, for example, states plainly in its privacy policy that it collects and stores information “about the websites you visit and the mobile applications you use on our networks.” Paired with carriers’ calling and texting oversight, that accounts for just about everything you’d do on your smartphone.

An Inside Look at “Actionable Insights”

You’d think that degree of continuous monitoring would be more than sufficient for a communications mammoth to operate its business — and perhaps for a while it was. But Facebook’s “Actionable Insights,” a corporate data-sharing program, suggests that even the incredible visibility telecoms have into your daily life isn’t enough — and Zuckerberg et al. can do them one better. Actionable Insights was announced last year in an innocuous, easy-to-miss post on Facebook’s engineering blog. The article, titled “Announcing tools to help partners improve connectivity,” strongly suggested that the program was primarily aimed at solving weak cellular data connections around the world. “To address this problem,” the post began, “we are building a diverse set of technologies, products, and partnerships designed to expand the boundaries of existing connectivity quality and performance, catalyze new market segments, and bring better access to the unconnected.” What sort of monster would stand against better access for the unconnected?

The blog post makes only a brief mention of Actionable Insights’ second, less altruistic purpose: “enabling better business decisions” through “analytics tools.” According to materials reviewed by The Intercept and a source directly familiar with the program, the real boon of Actionable Insights lies not in its ability to fix spotty connections, but to help chosen corporations use your personal data to buy more tightly targeted advertising.

The source, who discussed Actionable Insights on the condition of anonymity because they were not permitted to speak to the press, explained that Facebook has offered the service to carriers and phone makers ostensibly of free charge, with access to Actionable Insights granted as a sweetener for advertising relationships. According to the source, the underlying value of granting such gratis access to Actionable Insights in these cases isn’t simply to help better service cell customers with weak signals, but also to ensure that telecoms and phone makers keep buying  more and more carefully targeted Facebook ads. It’s exactly this sort of quasi-transactional data access that’s become a hallmark of Facebook’s business, allowing the company to plausibly deny that it ever sells your data while still leveraging it for revenue. Facebook may not be “selling” data through Actionable Insights in the most baldly literal sense of the word — there’s no briefcase filled with hard drives being swapped for one containing cash — but the relationship based on spending and monetization certainly fits the spirit of a sale. A Facebook spokesperson declined to answer whether the company charges for Actionable Insights access.

The confidential Facebook document provides an overview of Actionable Insights and espouses its benefits to potential corporate users. It shows how the program, ostensibly created to help improve underserved cellular customers, is pulling in far more data than how many bars you’re getting. According to one portion of the presentation, the Facebook mobile app harvests and packages eight different categories of information for use by over 100 different telecom companies in over 50 different countries around the world, including usage data from the phones of children as young as 13. These categories include use of video, demographics, location, use of Wi-Fi and cellular networks, personal interests, device information, and friend homophily, an academic term of art. A 2017 article on social media friendship from the Journal of the Society of Multivariate Experimental Psychology defined “homophily” in this context as “the tendency of nodes to form relations with those who are similar to themselves.” In other words, Facebook is using your phone to not only provide behavioral data about you to cellphone carriers, but about your friends as well.

From these eight categories alone, a third party could learn an extraordinary amount about patterns of users’ daily life, and although the document claims that the data collected through the program is “aggregated and anonymized,” academic studies have found time and again that so-called anonymized user data can be easily de-anonymized. Today, such claims of anonymization and aggregation are essentially boilerplate from companies who wager you’ll be comfortable with them possessing a mammoth trove of personal observations and behavioral predictions about your past and future if the underlying data is sufficiently neutered and grouped with your neighbor’s.

A Facebook spokesperson told The Intercept that Actionable Insights doesn’t collect any data from user devices that wasn’t already being collected anyway. Rather, this spokesperson said Actionable Insights repackages the data in novel ways useful to third-party advertisers in the telecom and smartphone industries.

Material reviewed by The Intercept show demographic information presented in a dashboard-style view, with maps showing customer locations at the county and city level. A Facebook spokesperson said they “didn’t think it goes more specific than zip code.” But armed with location data beamed straight from your phone, Facebook could technically provide customer location accurate to a range of several meters, indoors or out.

Targeting By Race and Likely Creditworthiness

Despite Facebook’s repeated assurances that user information is completely anonymized and aggregated, the Actionable Insights materials undermine this claim. One Actionable Insights case study from the overview document promotes how an unnamed North American cellular carrier had previously used its Actionable Insights access to target a specific, unnamed racial group. Facebook’s targeting of “multicultural affinity groups,” as the company formerly referred to race, was discontinued in 2017 after the targeting practice was widely criticized as potentially discriminatory.

Another case study described how Actionable Insights can be used to single out individual customers on the basis of creditworthiness. In this example, Facebook explained how one of its advertising clients, based outside the U.S., wanted to exclude individuals from future promotional offers on the basis of their credit. Using data provided through Actionable Insights, a Data Science Strategist, a role for which Facebook continues to hire, was able to generate profiles of customers with desirable and undesirable credit standings. The advertising client then used these profiles to target or exclude Facebook users who resembled these profiles.
IMAGINE YOU’RE HIKING through the woods near a border. Suddenly, you hear a mechanical buzzing, like a gigantic bee. Two quadcopters have spotted you and swoop in for a closer look. Antennae on both drones and on a nearby autonomous ground vehicle pick up the radio frequencies coming from the cell phone in your pocket. They send the signals to a central server, which triangulates your exact location and feeds it back to the drones. The robots close in.

Cameras and other sensors on the machines recognize you as human and try to ascertain your intentions. Are you a threat? Are you illegally crossing a border? Do you have a gun? Are you engaging in acts of terrorism or organized crime? The machines send video feeds to their human operator, a border guard in an office miles away, who checks the videos and decides that you are not a risk. The border guard pushes a button, and the robots disengage and continue on their patrol.

This is not science fiction. The European Union is financing a project to develop drones piloted by artificial intelligence and designed to autonomously patrol Europe’s borders. The drones will operate in swarms, coordinating and corroborating information among fleets of quadcopters, small fixed-wing airplanes, ground vehicles, submarines, and boats. Developers of the project, known as Roborder, say the robots will be able to identify humans and independently decide whether they represent a threat. If they determine that you may have committed a crime, they will notify border police.

President Donald Trump has used the specter of criminals crossing the southern border to stir nationalist political sentiment and energize his base. In Europe, two years after the height of the migration crisis that brought more than a million people to the continent, mostly from the Middle East and Africa, immigration remains a hot-button issue, even as the number of new arrivals has dropped. Political parties across the European Union are winning elections on anti-immigrant platforms and enacting increasingly restrictive border policies. Tech ethicists and privacy advocates worry that Roborder and projects like it outsource too much law enforcement work to nonhuman actors and could easily be weaponized against people in border areas.
IMAGINE YOU’RE HIKING through the woods near a border. Suddenly, you hear a mechanical buzzing, like a gigantic bee. Two quadcopters have spotted you and swoop in for a closer look. Antennae on both drones and on a nearby autonomous ground vehicle pick up the radio frequencies coming from the cell phone in your pocket. They send the signals to a central server, which triangulates your exact location and feeds it back to the drones. The robots close in.

Cameras and other sensors on the machines recognize you as human and try to ascertain your intentions. Are you a threat? Are you illegally crossing a border? Do you have a gun? Are you engaging in acts of terrorism or organized crime? The machines send video feeds to their human operator, a border guard in an office miles away, who checks the videos and decides that you are not a risk. The border guard pushes a button, and the robots disengage and continue on their patrol.

This is not science fiction. The European Union is financing a project to develop drones piloted by artificial intelligence and designed to autonomously patrol Europe’s borders. The drones will operate in swarms, coordinating and corroborating information among fleets of quadcopters, small fixed-wing airplanes, ground vehicles, submarines, and boats. Developers of the project, known as Roborder, say the robots will be able to identify humans and independently decide whether they represent a threat. If they determine that you may have committed a crime, they will notify border police.

President Donald Trump has used the specter of criminals crossing the southern border to stir nationalist political sentiment and energize his base. In Europe, two years after the height of the migration crisis that brought more than a million people to the continent, mostly from the Middle East and Africa, immigration remains a hot-button issue, even as the number of new arrivals has dropped. Political parties across the European Union are winning elections on anti-immigrant platforms and enacting increasingly restrictive border policies. Tech ethicists and privacy advocates worry that Roborder and projects like it outsource too much law enforcement work to nonhuman actors and could easily be weaponized against people in border areas.
PALANTIR, THE CIA-FUNDED data analysis company founded by billionaire Trump adviser Peter Thiel, provided software at the center of a 2017 operation targeting unaccompanied children and their families, newly released Homeland Security documents show.

The documents undercut prior statements from Palantir, in which the company tried to draw a clean line between the wing of ICE devoted strictly to deportations and the enforcement of immigration laws, and its $38 million contract with Homeland Security Investigations, or HSI, a component of ICE with a far broader criminal enforcement mandate. Asked about the contract renewal by the New York Times, a Palantir spokesperson stated:

“There are two major divisions of ICE with two distinct mandates: Homeland Security Investigations, or H.S.I., is responsible for cross-border criminal investigations. The other major directorate, Enforcement and Removal Operations, or E.R.O., is responsible for interior civil immigration enforcement, including deportation and detention of undocumented immigrants. We do not work for E.R.O.”

Documents obtained through Freedom of Information Act litigation and provided to The Intercept show that this claim, that Palantir software is strictly involved in criminal investigations as opposed to deportations, is false. The discrepancy between the private intelligence firm’s public assertion and the reality conveyed in the newly-released documents was first identified by Mijente, an advocacy organization that has closely tracked Palantir’s murky role in immigration enforcement. Far from detached support in “cross-border criminal investigations,” the materials released this week confirm the role Palantir technology played in facilitating hundreds of arrests, only a small fraction of which led to criminal prosecutions.
DURING A GROUP DINNER in a small town in Norway in 2015, at an international conference for investigative journalists, a Ukrainian reporter told me that he used both Gmail and Mail.ru, Russia’s most popular email provider. “Every time I write an email,” he said, “I have to decide if I want Obama to read it, or if I want Putin to read it.”

It may be hyperbolic to suggest that world leaders personally comb through individual email accounts, but the reporter’s point stands: When you use services like Gmail, Mail.ru, Facebook, Dropbox, Slack, or any other site that stores your data, they will hand your private information to governments when compelled to do so and in some cases, merely when asked. Last year, the Supreme Court ruled that the government usually needs a warrant to access private data held by third-party companies. But even with new legal protection, email remains all too easy for governments to quietly obtain. Many companies, like Facebook, have shared personal information even more widely, with private entities. When your personal data is stored on a company’s servers, as with the email in your Gmail account, there are no technical barriers to the host company sharing it when it sees fit.

Google provided private information to government agencies around the world more than 60,000 times in 2017, often turning over data from multiple Google accounts at once, according to its transparency report. And that doesn’t include over 100,000 Google accounts from which the company gave data in response to secret orders from the Foreign Intelligence Surveillance Court, a U.S. national security tribunal whose meetings and decisions are kept from the public. Mail.ru doesn’t provide a transparency report, but the situation is no doubt much worse in Russia: All Russian internet companies are required to retain data they collect about their users and to hand it to FSB, a Russian spy agency, if asked.
THE BERNIE SANDERS campaign kicked off its massive volunteer program this weekend by holding nearly 5,000 house parties across the country and unveiling a new organizing app that gives campaign supporters a way to share political information on friends, family, and neighbors. 

Sanders’s strategy to emerge from the crowded primary field revolves around energizing and empowering his army of supporters, and giving them easy-to-use tools in the hopes of expanding the electoral map in both the primary and general elections. More than 60,000 people attended the events, which took place in every state and more than 30 countries outside the U.S., according to the campaign.

Sanders, along with campaign manager Faiz Shakir and campaign co-chair Nina Turner, addressed supporters through a pre-recorded broadcast that was streamed at the parties. “So let’s do it, let’s run a historic grassroots campaign,” Sanders told supporters. “And when we do that, the 1 percent can spend all of the money that they want. We’re gonna beat them.”

The campaign’s new organizing tool, called BERN, helps volunteers track potential supporters and voters, allowing them to log the name and background of anyone they talk to, from friends and family members to a stranger on the street. The app will also help volunteers know how to participate in the Democratic primary or caucus in their state and register voters.

On friend-to-friend mode, supporters are asked to add the name, city, and state of everyone they know, information that is then matched to their voter record. The app also asks about the person’s level of support, union membership, and other candidates they might vote for.

Some critics have called the app invasive, arguing that the database of personal information could open non-supporters up to harassment. Though much of the information the app requests is publicly available, critics say that having the data neatly compiled — while not giving people a way to opt out of it — presents safety concerns.

The skepticism appears rooted in (hostility to Sanders and) a basic lack of familiarity with how campaigns work. Voter rolls are public, and the Democratic Party has long been aggregating additional information about voters to aid with fundraising and turnout operations, data that all major campaigns have access to. The difference is that the Sanders app democratizes the process with the goal of expanding the electorate, while the party operations are aimed at identifying existing supporters so they can be motivated to vote. The party data is generally available to campaign volunteers, but because Sanders lowers the bar to volunteering, more people will now have access to the data. The goal, though, is to get more people to vote for Sanders, not to attack Sanders opponents.

To that end, they’ll be relying heavily on supporters.“We don’t think, in the national office, that we have all of the answers,” Sanders said. “Trust me, we don’t. Every person out there knows your own community better than we do. Can you put on a concert, can you have a potluck event? Whatever it may be, bring people together. Develop a sense of community, reach out to people who might feel uncomfortable about being involved in politics.”

Sanders has a list of 1.1 million people who’ve pledged to volunteer so far, meaning roughly 6 percent showed up to a house party over the weekend. Sanders told them that the goal is to have volunteers engaging on social media in addition to the “old-fashioned stuff,” like knocking on doors and handing out literature. Unlike the typical political campaign, where volunteers work under the supervision of paid campaign staff, Sanders volunteers will be given the tools to help grow the movement at an exponential scale, free of the restraints of traditional top-down campaigns.

“And remember,” Sanders said. “It’s not Bernie! It’s us! Don’t forget that: Us! Us! Us!”

Sanders supporters, with and without previous organizing experience, gathered this weekend in libraries, living rooms, restaurants, and classrooms. They wore Bernie shirts, made Bernie signs, Bernie cookies, and Bernie cakes. Some groups even received a surprise phone call from the candidate himself.

In Oakland, Bay Area Muslims for Bernie held its party at a local Palestinian street food restaurant. A group of around 25 people, which included supporters from Egypt, Iran, Morocco, Yemen, Afghanistan, and around the United States, joined the organizing kickoff. “We even had refugees attend who cannot vote but still wanted to support and promote Bernie’s message,” Reyhaneh Rajabzadeh told The Intercept in a message.
AFTER YEARS OF ignoring the issue, lawmakers on Capitol Hill are suddenly engaged in a furious fight over enacting national legislation to establish basic online privacy rights for consumers. As with the crafting of much legislation dealing with complicated issues, legislators are relying on experts to help codify the consumer protections.

In a twist that is all too familiar in Washington, D.C., however, many of the groups that have positioned themselves as expert voices on consumer privacy are pushing for a bill that hews closely to tech industry interests. Lawmakers who are famously ignorant on technology issues are hearing largely from an army of industry lobbyists and experts funded by social media companies, online platforms, data brokers, advertisers, and telecommunication giants — the very same corporate interests that profit from the collection and sale of internet data.

Take the Center for Democracy and Technology, one of the most prominent privacy-centered Beltway think tanks. The group is considered to be well-respected among congressional staffers, routinely testifies before committees on privacy legislation, and is a prime mover in the national online privacy bill discussion.

Late last year, the organization circulated draft federal privacy legislation that would nullify major state-level regulations. In March, when the Senate Judiciary Committee held its first hearing of the session on how to formulate a federal consumer privacy standard, the center’s Privacy and Data Project Director Michelle Richardson testified.

The Center for Democracy and Technology is also awash in corporate money from the tech sector. Amazon, Verizon, and Google are among the corporate donors that each provide over $200,000 to the group. AT&T, Uber, and Twitter are also major donors.

Last Wednesday, the group hosted its annual gala, known as “Tech Prom,” which brought together lobbyists and government affairs officials from leading Silicon Valley and telecom firms. Facebook, Google, Amazon, and Microsoft purchased tables at the event and served as sponsors, a privilege that came in exchange for a $35,000 donation to the center.
IT IS A Chinese state-owned company that is implicated in disturbing human rights violations. But that has not stopped Hikvision from gaining a major foothold in the United Kingdom. Through a network of corporate partners, the Hangzhou-based security firm has supplied its surveillance cameras for use on the British parliamentary estate, as well as to police, hospitals, schools, and universities throughout the country, according to sources and procurement records.

Hikvision, whose technology the U.S. government recently banned federal agencies from purchasing, is generating millions of dollars in annual revenue selling its technology to British companies and organizations. At the same time, it has been helping to establish an oppressive surveillance state in the Xinjiang region of China, where the Uighur ethnic minorities have been held in secret internment camps.

British politicians are raising concerns about the technology — and are calling for an embargo on further purchases of it — on the grounds that Hikvision is complicit in human rights abuses and also represents a national security risk, as it is feared that Chinese intelligence agencies could potentially tap into camera feeds in sensitive locations. Some of the company’s cameras record audio and are connected to the internet, meaning that they can be monitored from anywhere in the world.

In January, the cameras were scheduled to be installed inside London’s Portcullis House, according to Adm. Lord Alan West, a member of the U.K. Parliament’s second chamber, the House of Lords. Portcullis House is an office building in Westminster used by more than 200 members of Parliament and 400 of their staff to carry out their daily work, which routinely involves discussion of confidential national security, economic, and foreign policy issues.

West told The Intercept that someone who was “concerned that this was happening” tipped him off about a contract that would equip the building with Hikvision surveillance equipment. He said he subsequently complained about the matter to authorities within the parliamentary estate.

“It seems to me to be extremely worrying — it’s rather like being able to get a Mata Hari into each office,” he said, referring to the Dutch exotic dancer who was accused of spying for Germany during World War I. “Are we sure we are happy with Chinese CCTV in members of Parliament’s offices, listening to what they say to their constituents, listening to what ministers say, filming the documents on their desks?”

A Parliament spokesperson denied the existence of a contract involving Hikvision and said that there was no plan to “install any additional cameras at Portcullis House this year.”

A source familiar with security on parts of the parliamentary estate, which, in addition to Portcullis House, consists of the Palace of Westminster, the Norman Shaw buildings, and Big Ben, told The Intercept that Hikvision’s equipment had “absolutely” been used there in the past. The source said they could not confirm whether any Hikvision cameras were currently active, as there are hundreds of cameras fitted both in and around all parliamentary and government buildings in the area.
HOW EXACTLY FACEBOOK decides who sees what is one of the great pieces of forbidden knowledge in the information age, hidden away behind nondisclosure agreements, trade secrecy law, and a general culture of opacity. New research from experts at Northeastern University, the University of Southern California, and the public-interest advocacy group Upturn doesn’t reveal how Facebook’s targeting algorithms work, but does show an alarming outcome: They appear to deliver certain ads, including for housing and employment, in a way that aligns with race and gender stereotypes — even when advertisers ask for the ads to be exposed a broad, inclusive audience.

There are two basic steps to advertising on Facebook. The first is taken by advertisers when they choose certain segments of the Facebook population to target: Canadian women who enjoy badminton and Weezer, lacrosse dads over 40 with an interest in white genocide, and so forth. The second is taken by Facebook, when it makes an ad show up on certain peoples’ screens, reconciling the advertiser’s targeting preferences with the flow of people through Facebook’s apps and webpages in a given period of time. Advertisers can see which audiences ended up viewing the ad, but are never permitted to know the underlying logic of how those precise audiences were selected.

The new research focuses on the second step of advertising on Facebook, the process of ad delivery, rather than on ad targeting. Essentially, the researchers created ads without any demographic target at all and watched where Facebook placed them. The results, said the researchers, were disturbing:

Critically, we observe significant skew in delivery along gender and racial lines for “real” ads for employment and housing opportunities despite neutral targeting parameters. Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive.

Rather than targeting a demographic niche, the researchers requested only that their ads reach Facebook users in the United States, leaving matters of ethnicity and gender entirely up to Facebook’s black box. As Facebook itself tells potential advertisers, “We try to show people the ads that are most pertinent to them.” What exactly does the company’s ad-targeting black box, left to its own devices, consider pertinent? Are Facebook’s ad-serving algorithms as prone to bias like so many others? The answer will not surprise you.

For one portion of the study, researchers ran ads for a wide variety of job listings in North Carolina, from janitors to nurses to lawyers, without any further demographic targeting options. With all other things being equal, the study found that “Facebook delivered our ads for jobs in the lumber industry to an audience that was 72% white and 90% men, supermarket cashier positions to an audience of 85% women, and jobs with taxi companies to a 75% black audience even though the target audience we specified was identical for all ads.” Ad displays for “artificial intelligence developer” listings also skewed white, while listings for secretarial work overwhelmingly found their way to female Facebook users.

Although Facebook doesn’t permit advertisers to view the racial composition of an ad’s viewers, the researchers said they were able to confidently infer these numbers by cross-referencing the indicators Facebook does provide, particularly regions where users live, which in some states can be cross-referenced with race data held in voter registration records.

In the case of housing ads — an area Facebook has already shown in the past has potential for discriminatory abuse — the results were also heavily skewed along racial lines. “In our experiments,” the researchers wrote, “Facebook delivered our broadly targeted ads for houses for sale to audiences of 75% white users, when ads for rentals were shown to a more demographically balanced audience.” In other cases, the study found that “Facebook delivered some of our housing ads to audiences of over 85% white users while they delivered other ads to over 65% Black users (depending on the content of the ad) even though the ads were targeted identically.”

Facebook appeared to algorithmically reinforce stereotypes even in the case of simple, rather boring stock photos, indicating that not only does Facebook automatically scan and classify images on the site as being more “relevant” to men or women, but changes who sees the ad based on whether it includes a picture of, say, a football or a flower. The research took a selection of stereotypically gendered images — a military scene and an MMA fight on the stereotypically male side, a rose as stereotypically female — and altered them so that they would be invisible to the human eye (marking the images as transparent “alpha” channels, in technical terms). They then used these invisible pictures in ads run without any gender-based targeting, yet found Facebook, presumably after analyzing the images with software, made retrograde, gender-based decisions on how to deliver them: Ads with stereotypical macho images were shown mostly to men, even though the men had no idea what they were looking at. The study concluded that “Facebook has an automated image classification mechanism in place that is used to steer different ads towards different subsets of the user population.” In other words, the bias was on Facebook’s end, not in the eye of the beholder.
HOW EXACTLY FACEBOOK decides who sees what is one of the great pieces of forbidden knowledge in the information age, hidden away behind nondisclosure agreements, trade secrecy law, and a general culture of opacity. New research from experts at Northeastern University, the University of Southern California, and the public-interest advocacy group Upturn doesn’t reveal how Facebook’s targeting algorithms work, but does show an alarming outcome: They appear to deliver certain ads, including for housing and employment, in a way that aligns with race and gender stereotypes — even when advertisers ask for the ads to be exposed a broad, inclusive audience.

There are two basic steps to advertising on Facebook. The first is taken by advertisers when they choose certain segments of the Facebook population to target: Canadian women who enjoy badminton and Weezer, lacrosse dads over 40 with an interest in white genocide, and so forth. The second is taken by Facebook, when it makes an ad show up on certain peoples’ screens, reconciling the advertiser’s targeting preferences with the flow of people through Facebook’s apps and webpages in a given period of time. Advertisers can see which audiences ended up viewing the ad, but are never permitted to know the underlying logic of how those precise audiences were selected.

The new research focuses on the second step of advertising on Facebook, the process of ad delivery, rather than on ad targeting. Essentially, the researchers created ads without any demographic target at all and watched where Facebook placed them. The results, said the researchers, were disturbing:

Critically, we observe significant skew in delivery along gender and racial lines for “real” ads for employment and housing opportunities despite neutral targeting parameters. Our results demonstrate previously unknown mechanisms that can lead to potentially discriminatory ad delivery, even when advertisers set their targeting parameters to be highly inclusive.

Rather than targeting a demographic niche, the researchers requested only that their ads reach Facebook users in the United States, leaving matters of ethnicity and gender entirely up to Facebook’s black box. As Facebook itself tells potential advertisers, “We try to show people the ads that are most pertinent to them.” What exactly does the company’s ad-targeting black box, left to its own devices, consider pertinent? Are Facebook’s ad-serving algorithms as prone to bias like so many others? The answer will not surprise you.

For one portion of the study, researchers ran ads for a wide variety of job listings in North Carolina, from janitors to nurses to lawyers, without any further demographic targeting options. With all other things being equal, the study found that “Facebook delivered our ads for jobs in the lumber industry to an audience that was 72% white and 90% men, supermarket cashier positions to an audience of 85% women, and jobs with taxi companies to a 75% black audience even though the target audience we specified was identical for all ads.” Ad displays for “artificial intelligence developer” listings also skewed white, while listings for secretarial work overwhelmingly found their way to female Facebook users.

Although Facebook doesn’t permit advertisers to view the racial composition of an ad’s viewers, the researchers said they were able to confidently infer these numbers by cross-referencing the indicators Facebook does provide, particularly regions where users live, which in some states can be cross-referenced with race data held in voter registration records.

In the case of housing ads — an area Facebook has already shown in the past has potential for discriminatory abuse — the results were also heavily skewed along racial lines. “In our experiments,” the researchers wrote, “Facebook delivered our broadly targeted ads for houses for sale to audiences of 75% white users, when ads for rentals were shown to a more demographically balanced audience.” In other cases, the study found that “Facebook delivered some of our housing ads to audiences of over 85% white users while they delivered other ads to over 65% Black users (depending on the content of the ad) even though the ads were targeted identically.”

Facebook appeared to algorithmically reinforce stereotypes even in the case of simple, rather boring stock photos, indicating that not only does Facebook automatically scan and classify images on the site as being more “relevant” to men or women, but changes who sees the ad based on whether it includes a picture of, say, a football or a flower. The research took a selection of stereotypically gendered images — a military scene and an MMA fight on the stereotypically male side, a rose as stereotypically female — and altered them so that they would be invisible to the human eye (marking the images as transparent “alpha” channels, in technical terms). They then used these invisible pictures in ads run without any gender-based targeting, yet found Facebook, presumably after analyzing the images with software, made retrograde, gender-based decisions on how to deliver them: Ads with stereotypical macho images were shown mostly to men, even though the men had no idea what they were looking at. The study concluded that “Facebook has an automated image classification mechanism in place that is used to steer different ads towards different subsets of the user population.” In other words, the bias was on Facebook’s end, not in the eye of the beholder.
SEN. ELIZABETH WARREN’S plan to break up tech giants Amazon, Google, Facebook, and Apple has given concentrated corporate power its most prominent political platform since the 1912 presidential election — and we’re still nearly a year away from the first round of primary voting. This tracks with the rising awareness of the corrosiveness of monopoly power generally and those tech giants specifically.

Whether such policy boldness means anything in a brand-obsessed political landscape will be determined when ballots are cast. But it is undeniably driving a policy discussion that the next Democratic presidential nominee, no matter who it is, will likely take up. In that context, the debate over Warren’s plan is critical, as it prefigures the trajectory of each and every challenge to corporate dominance.

First, many critiques will come from those with a direct stake in the outcome — in this case, Big Tech-funded individuals or organizations, which are so ubiquitous as to create an echo chamber. Second, the critiques will highlight the “radical” nature of the changes, setting them at odds with American history, even though Warren’s central proposal — to structurally separate business lines in an effort to eliminate anti-competitive conduct and foster competition — has a century-old pedigree. And third, we’ll be assured that the cure is worse than the disease, that Warren’s ideas would destroy everything from online shopping to the smartphone, a perspective that relies on deliberate misinterpretation.

This roadmap for discrediting policy solutions that confront power should be easy enough to spot by now, and will be employed long into the future. So it’s worth breaking down how it works.

The Manufacturing of Dissent

“The issue is not the size and current market dominance of these [tech] companies,” wrote the American Enterprise Institute’s Michael Strain for Bloomberg, in response to the Warren plan. “If anything, politicians should be celebrating these companies as crown jewels of the U.S. economy.”

Strain’s employer, AEI, is funded in part by Google, according to the company’s transparency page. This is not noted in Strain’s Bloomberg op-ed. But AEI and its writers have done several critical pieces about Warren’s proposal, as well a California privacy regulation that also imposes stricter rules on Big Tech. All of these opinion articles indirectly benefit one of AEI’s donors.

The episode points to a significant trend of writers and scholars opining on the Warren plan while conflicted by the overwhelming amounts of Big Tech cash that have infested Washington. Google’s list of organizations to whom it has donated is massive, and combined with Facebook and Amazon’s dominance of Washington, it’s hard to find anyone with a critical eye toward Big Tech regulation who doesn’t have something to disclose.

Rich Lowry of National Review unleashed a pack of industry talking points to explain how Big Tech “helps create a strong American society.” National Review takes Google money. Here’s a similar sentiment dragging the Warren plan on the pages of National Review, from a senior fellow at the Competitive Enterprise Institute, which also takes Google money. The American Action Forum seems to dislike the Warren plan; the group, well, takes Google money.

Geoffrey Manne and Alec Stapp condemn Warren for “wanting to turn the Internet into a sewer.” Manne’s organization, the International Center for Law and Economics, has taken a boatload of Google money; as of 2015, he had contributed to at least eight white papers commissioned or funded by Google that endorsed Google’s policy positions, in addition to being a frequent pro-Google commentator in news articles and congressional testimony. Stapp, before hooking up with Manne at ICLE, worked at the Mercatus Center at George Mason University, another recipient of Google funds. A former Manne co-author, Joshua Wright, worked at George Mason University and has been periodically on and off the Google payroll in between government work.

Manne and Stapp’s piece got the pile-on treatment on Twitter from representatives of the Google-funded Cato Institute and Niskanen Center; Stapp previously worked at Niskanen. Several venture capitalists who currently rely on Big Tech for exit strategies for their companies also gave the thumbs-up to the piece.

The Computer and Communications Industry Association, a trade group that includes Amazon, Facebook, and Google among its members, uses a subsidiary named Springboard to hurl critiques at regulatory tech policies. In addition to the aforementioned articles from AEI and National Review, Springboard points to the opinions of a partner at Andreessen Horowitz, an early investor in Facebook, and the CCIA’s own vice president for Law an Policy — which amounts to CCIA linking to itself as outside confirmation of its beliefs.

These linkages are virtually endless and show an incestuous network of academics, think-tankers, advocacy organizations, and trade groups, all of which happen to agree on every issue important to Big Tech. The money supports extending the prominence and megaphone of these organizations, and with nearly unlimited pocketbooks, it creates the impression of a tsunami of support for the industry.

A “Radical” Idea That’s Been Around for Over a Century

The core of Warren’s plan, which for now is just a proposal on Medium rather than legislation, involves what is known as “structural separation.” Companies with over $25 billion in annual global revenue that operate platforms — connectors between people, people and advertisers, or people and merchants — would not be allowed to both own the platform and also participate as a seller on that platform. The classic example would be Amazon’s marketplace, where Amazon also operates its own line of Amazon Basics, competing with its third-party sellers. Google’s ad exchange also competes on Google with ad tech companies, and would need to be spun off. The same would go for Google’s local search, which routinely deprioritizes recommendation sites like Yelp.

The idea is that these entities get preferential treatment from the platform they own, giving Basics, Google ad tech, and Google Search an unfair advantage and extending the platform’s dominance. Only the biggest companies would have to structurally separate; smaller platforms would still have to meet a standard of fair, reasonable, and nondiscriminatory treatment for participants on the platform and its users.

This forced divestiture of tech platforms’ other business lines has been described as radical. Manne and Stapp claim it will turn the internet into your sewer service — mainly because Warren uses the word “utility” to describe regulated platforms.

Jeff Bezos didn’t come up with the idea of owning a marketplace and using it to sell your own stuff at an unfair advantage against rivals. Reading Railroad, for example, became the largest company in the world by owning the rails that carried anthracite coal, as well as the coal mines along the route. Rival coal producers that wanted to use the lines got less favorable rates, fell behind, and got swallowed up by Reading Railroad.

Congress put a stop to it in 1906 by adopting the Hepburn Act, which prevented the railroads from carrying products that they owned. This forced the Reading Railroad to divest the P&R Coal and Iron Company, the subsidiary that owned the coal mines. Warren is merely following a long history of structural separation that began when Teddy Roosevelt was president.

Theater owners were not allowed to also produce and distribute films after the Supreme Court’s Paramount decision in 1948. Television networks were prevented from owning the programming they ran in prime time, under the Financial Interest and Syndication, or “fin-syn,” rules imposed by the Federal Communications Commission in 1970. In telecom, AT&T was heavily circumscribed and restricted to common-carrier telephone service, banning the company from capitalizing on innovations from Bell Labs and forcing compulsory licensing of those patents in 1956, which created the modern electronics industry. Banks were structurally separated between investment and deposit-taking commercial lines after the Glass-Steagall reforms. Rep. David Cicilline, D-R.I., the chair of the House Judiciary’s antitrust subcommittee, has analogized a structural separation in tech as a Glass-Steagall type of rule.

These structural separations have widespread goals: diversity, financial stability, decentralization of power, and innovation. “We owe the internet to structural separation,” said Harold Feld, a senior vice president with Public Knowledge, referring to the Carterfone decisions, where the FCC allowed people to connect their own devices, like a modem, to the telephone network. “Clearly this has a long and successful history in telecom.”

Some of these restrictions, like those on banks or television stations, have been dismantled. And there are cases of companies selling products in a store while also owning the store: Kirkland products at Costco are ubiquitous, for example. But as Lina Khan, scholar and staffer for Cicilline’s antitrust subcommittee, has pointed out, the key question is whether the platform, be it brick and mortar or digital, is creating a bottleneck by privileging its own products over rivals. And there’s a lot of evidence that Amazon in particular does just that, reacting to high-selling products by creating a generic version, and down-ranking the competitor in its search. Because half of all e-commerce is sold on Amazon, competitors have few alternatives but to sell in what feels like a rigged marketplace.

India has already instituted a Warren-like rule to prevent e-commerce platforms from selling their own products on the platform. “We should go back and understand the wisdom of that kind of separation,” said Peter Carstensen, a professor emeritus at the University of Wisconsin Law School. “We would never want the interstate highway system to be owned by Walmart. It simplifies the market functions if you separate them out.”

Another benefit to structural separation is the relative ease of regulation. Instead of well-paid economists fighting it out over what constitutes anti-competitive conduct or restraint of trade, large companies simply can’t compete with rivals on their own platform, because of the threat of market power.

The Warren plan sets a rather arbitrary number of $25 billion in annual revenue as the dividing line for that power, a kind of substitute for the technocratic determination. This has angered critics: Andy Kessler at the Wall Street Journal denied that antitrust law has anything to do with bigness.

The idea that John Sherman, author of the Sherman Antitrust Act, was not concerned with bigness would come as news to Sherman, who once said, “If we will not endure a king as a political power, we should not endure a king over the production, transportation, and sale of any of the necessaries of life.” Warren’s campaign sees the $25 billion figure as a clean way to assist regulators with pinpointing market dominance. “It has the benefit of a clear rule,” said one senior campaign adviser, who was not authorized to speak on the record. “We should presume if a company with over $25 billion in revenue is operating a marketplace, it has power and leverage.”

While agnostic on the specific dividing number, Feld gave Warren’s team credit “for trying to come up with something that makes sense.” Others are not as thrilled about it. But their arguments often misconstrue the Warren plan.

Assuming the Worst

Ben Thompson, a former Apple and Microsoft analyst who writes about the business of technology, had one of the sharpest critiques of the Warren proposal, and it starts with denying Warren’s claim on the history of technology. Warren has credited the Microsoft trial for creating space for the modern tech giants to emerge, something Thompson mocks. “Bing was not even launched until 2009, eight years after the Microsoft case was settled. MSN Search, its predecessor, did launch in 1998, but with licensed search results from Inktomi and AltaVista; Microsoft didn’t launch its own web crawler until 2005.”

This view neglects the politics of the U.S. trial against Microsoft, which put a dominant company under pressure and wary of extending that dominance into the then-emerging web services arena. As Gary Reback, who represented Netscape against Microsoft in the 1990s, has often said, including to me in a 2017 interview, “The trial is the remedy.” By exposing Microsoft’s machinations to the nation, it made the company gun-shy to choke off competition, Reback argues.

“The only way to get to Google was the Microsoft browser,” he said. “Microsoft could have put up a big red sign saying this site is unsafe. It could have killed Google in the cradle, but didn’t. The reason why, and this is from Microsoft people, is they had this public trial. It wasn’t worth it as a company.”

Feld concurred that Microsoft’s behavior changed after the public spotlight of the trial, and the kind of aggressive actions to shut down competitors largely stopped. You can apply this to IBM’s antitrust issues in the 1970s and ’80s opening space for Apple, and AOL’s forced interoperability of Instant Messenger in 2001 giving room to social media. “Big companies are sensitive to this stuff; after they’ve been burned, they do generally play it safe,” Feld said, noting that big cable hasn’t had such a spotlight and they managed to crush TiVo swiftly and completely. So while Thompson focuses on specific Microsoft business decisions, he ignores the political context.

Thompson also warns that applying the structural separation standard to Apple, as Warren confirmed in an interview at South by Southwest, would lead to smartphones shipped without any applications. “Was Apple breaking the law when they shipped the first iPhone with only first-party apps?” Thompson asks. “At what point did delivering an acceptable consumer experience out-of-the-box cross the line into abusing a dominant position? This argument may make sense in theory but it makes zero sense in reality.”

This argument also has zero bearing on what Warren’s talking about. Whether Apple is unfairly tying or bundling its own apps onto its phones at purchase is a question for existing antitrust laws — it was the question in the Microsoft case, in fact. “The ordinary rules apply in that case,” said the senior Warren adviser. “The key thing we’re talking about is the marketplace.”

Contrary to what critics have claimed, Apple would not have to divest from the App Store completely under Warren’s plan, nor would the security benefits of Apple managing what goes onto its phone wither away. Apple would merely be disallowed from selling its own apps next to competing ones. This would hardly destroy Apple, largely a phone and hardware manufacturer and not primarily an app-maker. It would allow competition on the platform.

Apple does have a legitimate antitrust problem with the App Store, as Thompson acknowledges. Spotify has complained to the European Union that Apple takes a 30 percent cut from all revenues from its iPhone app, while preventing it from emailing users directly or allowing upgrades. This indirectly benefits Apple Music, Spotify says. Apple has accused Spotify of using “misleading rhetoric” in its complaint.

Spotify wants changes to Apple’s conduct on the App Store — which is not only fair game for traditional antitrust that can identify anti-competitive impositions of market power, but is also part of Warren’s plan, which mandates fair and nondiscriminatory treatment to marketplace participants. And it shows how Warren is highlighting a consumer welfare issue: If Spotify has to absorb a 30 percent transfer of revenues to Apple for use of its iPhone customers, it likely has to raise prices, and it cannot offer services like upgrades directly.
SEN. ELIZABETH WARREN’S plan to break up tech giants Amazon, Google, Facebook, and Apple has given concentrated corporate power its most prominent political platform since the 1912 presidential election — and we’re still nearly a year away from the first round of primary voting. This tracks with the rising awareness of the corrosiveness of monopoly power generally and those tech giants specifically.

Whether such policy boldness means anything in a brand-obsessed political landscape will be determined when ballots are cast. But it is undeniably driving a policy discussion that the next Democratic presidential nominee, no matter who it is, will likely take up. In that context, the debate over Warren’s plan is critical, as it prefigures the trajectory of each and every challenge to corporate dominance.

First, many critiques will come from those with a direct stake in the outcome — in this case, Big Tech-funded individuals or organizations, which are so ubiquitous as to create an echo chamber. Second, the critiques will highlight the “radical” nature of the changes, setting them at odds with American history, even though Warren’s central proposal — to structurally separate business lines in an effort to eliminate anti-competitive conduct and foster competition — has a century-old pedigree. And third, we’ll be assured that the cure is worse than the disease, that Warren’s ideas would destroy everything from online shopping to the smartphone, a perspective that relies on deliberate misinterpretation.

This roadmap for discrediting policy solutions that confront power should be easy enough to spot by now, and will be employed long into the future. So it’s worth breaking down how it works.

The Manufacturing of Dissent

“The issue is not the size and current market dominance of these [tech] companies,” wrote the American Enterprise Institute’s Michael Strain for Bloomberg, in response to the Warren plan. “If anything, politicians should be celebrating these companies as crown jewels of the U.S. economy.”

Strain’s employer, AEI, is funded in part by Google, according to the company’s transparency page. This is not noted in Strain’s Bloomberg op-ed. But AEI and its writers have done several critical pieces about Warren’s proposal, as well a California privacy regulation that also imposes stricter rules on Big Tech. All of these opinion articles indirectly benefit one of AEI’s donors.

The episode points to a significant trend of writers and scholars opining on the Warren plan while conflicted by the overwhelming amounts of Big Tech cash that have infested Washington. Google’s list of organizations to whom it has donated is massive, and combined with Facebook and Amazon’s dominance of Washington, it’s hard to find anyone with a critical eye toward Big Tech regulation who doesn’t have something to disclose.

Rich Lowry of National Review unleashed a pack of industry talking points to explain how Big Tech “helps create a strong American society.” National Review takes Google money. Here’s a similar sentiment dragging the Warren plan on the pages of National Review, from a senior fellow at the Competitive Enterprise Institute, which also takes Google money. The American Action Forum seems to dislike the Warren plan; the group, well, takes Google money.

Geoffrey Manne and Alec Stapp condemn Warren for “wanting to turn the Internet into a sewer.” Manne’s organization, the International Center for Law and Economics, has taken a boatload of Google money; as of 2015, he had contributed to at least eight white papers commissioned or funded by Google that endorsed Google’s policy positions, in addition to being a frequent pro-Google commentator in news articles and congressional testimony. Stapp, before hooking up with Manne at ICLE, worked at the Mercatus Center at George Mason University, another recipient of Google funds. A former Manne co-author, Joshua Wright, worked at George Mason University and has been periodically on and off the Google payroll in between government work.

Manne and Stapp’s piece got the pile-on treatment on Twitter from representatives of the Google-funded Cato Institute and Niskanen Center; Stapp previously worked at Niskanen. Several venture capitalists who currently rely on Big Tech for exit strategies for their companies also gave the thumbs-up to the piece.

The Computer and Communications Industry Association, a trade group that includes Amazon, Facebook, and Google among its members, uses a subsidiary named Springboard to hurl critiques at regulatory tech policies. In addition to the aforementioned articles from AEI and National Review, Springboard points to the opinions of a partner at Andreessen Horowitz, an early investor in Facebook, and the CCIA’s own vice president for Law an Policy — which amounts to CCIA linking to itself as outside confirmation of its beliefs.

These linkages are virtually endless and show an incestuous network of academics, think-tankers, advocacy organizations, and trade groups, all of which happen to agree on every issue important to Big Tech. The money supports extending the prominence and megaphone of these organizations, and with nearly unlimited pocketbooks, it creates the impression of a tsunami of support for the industry.

A “Radical” Idea That’s Been Around for Over a Century

The core of Warren’s plan, which for now is just a proposal on Medium rather than legislation, involves what is known as “structural separation.” Companies with over $25 billion in annual global revenue that operate platforms — connectors between people, people and advertisers, or people and merchants — would not be allowed to both own the platform and also participate as a seller on that platform. The classic example would be Amazon’s marketplace, where Amazon also operates its own line of Amazon Basics, competing with its third-party sellers. Google’s ad exchange also competes on Google with ad tech companies, and would need to be spun off. The same would go for Google’s local search, which routinely deprioritizes recommendation sites like Yelp.

The idea is that these entities get preferential treatment from the platform they own, giving Basics, Google ad tech, and Google Search an unfair advantage and extending the platform’s dominance. Only the biggest companies would have to structurally separate; smaller platforms would still have to meet a standard of fair, reasonable, and nondiscriminatory treatment for participants on the platform and its users.

This forced divestiture of tech platforms’ other business lines has been described as radical. Manne and Stapp claim it will turn the internet into your sewer service — mainly because Warren uses the word “utility” to describe regulated platforms.

Jeff Bezos didn’t come up with the idea of owning a marketplace and using it to sell your own stuff at an unfair advantage against rivals. Reading Railroad, for example, became the largest company in the world by owning the rails that carried anthracite coal, as well as the coal mines along the route. Rival coal producers that wanted to use the lines got less favorable rates, fell behind, and got swallowed up by Reading Railroad.

Congress put a stop to it in 1906 by adopting the Hepburn Act, which prevented the railroads from carrying products that they owned. This forced the Reading Railroad to divest the P&R Coal and Iron Company, the subsidiary that owned the coal mines. Warren is merely following a long history of structural separation that began when Teddy Roosevelt was president.

Theater owners were not allowed to also produce and distribute films after the Supreme Court’s Paramount decision in 1948. Television networks were prevented from owning the programming they ran in prime time, under the Financial Interest and Syndication, or “fin-syn,” rules imposed by the Federal Communications Commission in 1970. In telecom, AT&T was heavily circumscribed and restricted to common-carrier telephone service, banning the company from capitalizing on innovations from Bell Labs and forcing compulsory licensing of those patents in 1956, which created the modern electronics industry. Banks were structurally separated between investment and deposit-taking commercial lines after the Glass-Steagall reforms. Rep. David Cicilline, D-R.I., the chair of the House Judiciary’s antitrust subcommittee, has analogized a structural separation in tech as a Glass-Steagall type of rule.

These structural separations have widespread goals: diversity, financial stability, decentralization of power, and innovation. “We owe the internet to structural separation,” said Harold Feld, a senior vice president with Public Knowledge, referring to the Carterfone decisions, where the FCC allowed people to connect their own devices, like a modem, to the telephone network. “Clearly this has a long and successful history in telecom.”

Some of these restrictions, like those on banks or television stations, have been dismantled. And there are cases of companies selling products in a store while also owning the store: Kirkland products at Costco are ubiquitous, for example. But as Lina Khan, scholar and staffer for Cicilline’s antitrust subcommittee, has pointed out, the key question is whether the platform, be it brick and mortar or digital, is creating a bottleneck by privileging its own products over rivals. And there’s a lot of evidence that Amazon in particular does just that, reacting to high-selling products by creating a generic version, and down-ranking the competitor in its search. Because half of all e-commerce is sold on Amazon, competitors have few alternatives but to sell in what feels like a rigged marketplace.

India has already instituted a Warren-like rule to prevent e-commerce platforms from selling their own products on the platform. “We should go back and understand the wisdom of that kind of separation,” said Peter Carstensen, a professor emeritus at the University of Wisconsin Law School. “We would never want the interstate highway system to be owned by Walmart. It simplifies the market functions if you separate them out.”

Another benefit to structural separation is the relative ease of regulation. Instead of well-paid economists fighting it out over what constitutes anti-competitive conduct or restraint of trade, large companies simply can’t compete with rivals on their own platform, because of the threat of market power.

The Warren plan sets a rather arbitrary number of $25 billion in annual revenue as the dividing line for that power, a kind of substitute for the technocratic determination. This has angered critics: Andy Kessler at the Wall Street Journal denied that antitrust law has anything to do with bigness.

The idea that John Sherman, author of the Sherman Antitrust Act, was not concerned with bigness would come as news to Sherman, who once said, “If we will not endure a king as a political power, we should not endure a king over the production, transportation, and sale of any of the necessaries of life.” Warren’s campaign sees the $25 billion figure as a clean way to assist regulators with pinpointing market dominance. “It has the benefit of a clear rule,” said one senior campaign adviser, who was not authorized to speak on the record. “We should presume if a company with over $25 billion in revenue is operating a marketplace, it has power and leverage.”

While agnostic on the specific dividing number, Feld gave Warren’s team credit “for trying to come up with something that makes sense.” Others are not as thrilled about it. But their arguments often misconstrue the Warren plan.

Assuming the Worst

Ben Thompson, a former Apple and Microsoft analyst who writes about the business of technology, had one of the sharpest critiques of the Warren proposal, and it starts with denying Warren’s claim on the history of technology. Warren has credited the Microsoft trial for creating space for the modern tech giants to emerge, something Thompson mocks. “Bing was not even launched until 2009, eight years after the Microsoft case was settled. MSN Search, its predecessor, did launch in 1998, but with licensed search results from Inktomi and AltaVista; Microsoft didn’t launch its own web crawler until 2005.”

This view neglects the politics of the U.S. trial against Microsoft, which put a dominant company under pressure and wary of extending that dominance into the then-emerging web services arena. As Gary Reback, who represented Netscape against Microsoft in the 1990s, has often said, including to me in a 2017 interview, “The trial is the remedy.” By exposing Microsoft’s machinations to the nation, it made the company gun-shy to choke off competition, Reback argues.

“The only way to get to Google was the Microsoft browser,” he said. “Microsoft could have put up a big red sign saying this site is unsafe. It could have killed Google in the cradle, but didn’t. The reason why, and this is from Microsoft people, is they had this public trial. It wasn’t worth it as a company.”

Feld concurred that Microsoft’s behavior changed after the public spotlight of the trial, and the kind of aggressive actions to shut down competitors largely stopped. You can apply this to IBM’s antitrust issues in the 1970s and ’80s opening space for Apple, and AOL’s forced interoperability of Instant Messenger in 2001 giving room to social media. “Big companies are sensitive to this stuff; after they’ve been burned, they do generally play it safe,” Feld said, noting that big cable hasn’t had such a spotlight and they managed to crush TiVo swiftly and completely. So while Thompson focuses on specific Microsoft business decisions, he ignores the political context.

Thompson also warns that applying the structural separation standard to Apple, as Warren confirmed in an interview at South by Southwest, would lead to smartphones shipped without any applications. “Was Apple breaking the law when they shipped the first iPhone with only first-party apps?” Thompson asks. “At what point did delivering an acceptable consumer experience out-of-the-box cross the line into abusing a dominant position? This argument may make sense in theory but it makes zero sense in reality.”

This argument also has zero bearing on what Warren’s talking about. Whether Apple is unfairly tying or bundling its own apps onto its phones at purchase is a question for existing antitrust laws — it was the question in the Microsoft case, in fact. “The ordinary rules apply in that case,” said the senior Warren adviser. “The key thing we’re talking about is the marketplace.”

Contrary to what critics have claimed, Apple would not have to divest from the App Store completely under Warren’s plan, nor would the security benefits of Apple managing what goes onto its phone wither away. Apple would merely be disallowed from selling its own apps next to competing ones. This would hardly destroy Apple, largely a phone and hardware manufacturer and not primarily an app-maker. It would allow competition on the platform.

Apple does have a legitimate antitrust problem with the App Store, as Thompson acknowledges. Spotify has complained to the European Union that Apple takes a 30 percent cut from all revenues from its iPhone app, while preventing it from emailing users directly or allowing upgrades. This indirectly benefits Apple Music, Spotify says. Apple has accused Spotify of using “misleading rhetoric” in its complaint.

Spotify wants changes to Apple’s conduct on the App Store — which is not only fair game for traditional antitrust that can identify anti-competitive impositions of market power, but is also part of Warren’s plan, which mandates fair and nondiscriminatory treatment to marketplace participants. And it shows how Warren is highlighting a consumer welfare issue: If Spotify has to absorb a 30 percent transfer of revenues to Apple for use of its iPhone customers, it likely has to raise prices, and it cannot offer services like upgrades directly.
IN SEPTEMBER 2017, Aileen Black wrote an email to her colleagues at Google. Black, who led sales to the U.S. government, worried that details of the company’s work to help the military guide lethal drones would become public through the Freedom of Information Act. “We will call tomorrow to reinforce the need to keep Google under the radar,” Black wrote.

According to a Pentagon memo signed last year, however, no one at Google needed worry: All 5,000 pages of documents about Google’s work on the drone effort, known as Project Maven, are barred from public disclosure, because they constitute “critical infrastructure security information.”

One government transparency advocate said the memo is part of a recent wave of federal decisions that  keep sensitive documents secret on that same basis — thus allowing agencies to quickly deny document requests.
In partnership with
In partnership with



GOOGLE EMPLOYEES HAVE carried out their own investigation into the company’s plan to launch a censored search engine for China and say they are concerned that development of the project remains ongoing, The Intercept can reveal.

Late last year, bosses moved engineers away from working on the controversial project, known as Dragonfly, and said that there were no current plans to launch it. However, a group of employees at the company was unsatisfied with the lack of information from leadership on the issue — and took matters into their own hands.

The group has identified ongoing work on a batch of code that is associated with the China search engine, according to three Google sources. The development has stoked anger inside Google offices, where many of the company’s 88,000 workforce previously protested against plans to launch the search engine, which was designed to censor broad categories of information associated with human rights, democracy, religion, and peaceful protest.
AFTER YEARS of backlash over controversial government work, Google technology will be used to aid the Trump administration’s efforts to fortify the U.S.-Mexico border, according to documents related to a federal contract.

In August, Customs and Border Protection accepted a proposal to use Google Cloud technology to facilitate the use of artificial intelligence deployed by the CBP Innovation Team, known as INVNT. Among other projects, INVNT is working on technologies for a new “virtual” wall along the southern border that combines surveillance towers and drones, blanketing an area with sensors to detect unauthorized entry into the country.

In 2018, Google faced internal turmoil over a contract with the Pentagon to deploy AI-enhanced drone image recognition solutions; the capability sparked employee concern that Google was becoming embroiled in work that could be used for lethal purposes and other human rights concerns. In response to the controversy, Google ended its involvement with the initiative, known as Project Maven, and established a new set of AI principles to govern future government contracts.
THE RISE OF the internet-connected home security camera has generally been a boon to police, as owners of these devices can (and frequently do) share footage with cops at the touch of a button. But according to a leaked FBI bulletin, law enforcement has discovered an ironic downside to ubiquitous privatized surveillance: The cameras are alerting residents when police show up to conduct searches.

A November 2019 “technical analysis bulletin” from the FBI provides an overview of “opportunities and challenges” for police from networked security systems like Amazon’s Ring and other “internet of things,” or IoT, devices. Marked unclassified but “law enforcement sensitive” and for official use only, the document was included as part of the BlueLeaks cache of material hacked from the websites of fusion centers and other law enforcement entities.
SURVEILLANCE FIRMS around the world are licking their lips at a once-in-a-lifetime opportunity to cash in on the coronavirus by repositioning one of their most invasive products: the tracking bracelet.

Body monitors are associated with criminality and guilt in the popular imagination, the accessories of Wall Street crooks under house arrest and menace-to-society parolees. Unlike smartphones, de facto tracking devices in their own right, strapped-on trackers are expressly designed to be attached to the body and exist solely to report the user’s whereabouts and interactions to one or more third parties; they don’t play podcasts or tell you how many steps you took that day to sweeten the surveillance.

But a climate of perpetual bio-anxiety has paved the way for broader acceptance of carceral technologies, with a wave of companies trying to sell tracking accessories to business owners eager to reopen under the aegis of responsible social distancing and to governments hoping to keep a closer eye on people under quarantine.
LEIA EM PORTUGUÊS 

LEIA EM PORTUGUÊS 
AS SEVEN University of Puerto Rico students prepare to face trial in February for participating in a nonviolent protest more than two years ago, documents released to their defense attorneys reveal that Facebook granted the island’s Justice Department access to a trove of private information from student news publications. The department’s sweeping search warrant was part of a hunt for crimes committed by members of the youth anti-austerity movement, and it has raised fears among civil liberties advocates of a return to a period of Puerto Rico’s history when police routinely targeted citizens for surveillance on the basis of their political interests.

It was April 2017, and for weeks, University of Puerto Rico students had been holding a school-wide strike protesting austerity policies that were poised to defund public services across the island to satisfy the government’s creditors. When the university’s governing board gathered on April 27 to discuss $241 million in budget cuts, the students demanded to be let in. The board refused, locking the doors to the building where the meeting was being held. But the students stormed in anyway, pushing past security.

The action unfolded in real time on Facebook, as three student media outlets, Diálogo UPR, Pulso Estudiantil UPR, and Centro de Comunicación Estudiantil, livestreamed the protest. The students surrounded the board members and shut down the meeting, demanding that the board sign a commitment to rejecting the budget cuts. The action, one of many that took place on campus and in the streets, was over within half an hour. A glass door, some furniture, and a lamp were allegedly broken or damaged. No one was injured, and no one was arrested. But the secretary of Puerto Rico’s Justice Department, now-Gov. Wanda Vázquez, pledged to investigate the incident and arrest lawbreakers.

Two weeks later, students who had assumed leadership roles in the wider strike received citations ordering them to appear in court. When they showed up, they were handcuffed, paraded before media crews, and charged with a host of crimes related to the boardroom protest, the most severe of which — rioting and burglary — were later dropped. The remaining charges, including violating the right to assemble, aggravated restriction of freedom, and violence or intimidation against a public authority, each carry between six months and three years in prison. The seven students go to trial on February 7.

How exactly Vázquez’s Justice Department determined which students to charge out of the dozens who participated in the protest has remained a mystery to defense attorneys. The lawyers’ suspicion: that the case isn’t about crimes committed in the boardroom that day, but rather an attempt to penalize the political activity of some of the most active student organizers. The seven facing trial were members of the student strikers’ negotiating committee as well as political organizations critical of the government.

SHUTTERSTOCK, THE WELL-KNOWN online purveyor of stock images and photographs, is the latest U.S. company to willingly support China’s censorship regime, blocking searches that might offend the country’s authoritarian government, The Intercept has learned.

The publicly traded company built a $639 million-per-year business on the strength of its vast — sometimes comically vast — catalog of images depicting virtually anything a blogger or advertiser could imagine. The company now does business in more than 150 countries. But in China, there is now a very small, very significant gap in Shutterstock’s offerings. In early September, Shutterstock engineers were given a new goal: The creation of a search blacklist that would wipe from query results images associated with keywords forbidden by the Chinese government. Under the new system, which The Intercept is told went into effect last month, anyone with a mainland Chinese IP address searching Shutterstock for “President Xi,” “Chairman Mao,” “Taiwan flag,” “dictator,” “yellow umbrella,” or “Chinese flag” will receive no results at all. Variations of these terms, including “umbrella movement” — the precursor to the mass pro-democracy protests currently gripping Hong Kong — are also banned.

Shutterstock’s decision to silently aid China’s censorship agenda comes at a time of heightened scrutiny into the relationship between corporate America and President Xi Jinping’s authoritarian regime. Household names like Apple, Blizzard Entertainment, the NBA, and Google have all garnered harsh criticism for letting the policy directives of the Communist Party of China, and the gilded promise of a billion customers, dictate company strategy. Deciding to censor is a particularly stark inversion of values for Shutterstock, which markets itself as an enabler of creative expression.

The photo company’s relationship with China dates back to at least 2014, when it struck a distribution deal with ZCool, a Chinese social network and portfolio site for visual artists. Last year Shutterstock announced a $15 million investment in ZCool, noting that owing to the partnership, “Shutterstock’s content now powers large technology platforms in China such as Tencent Social Ads,” an online advertising subsidiary of the tremendously popular Chinese internet conglomerate Tencent.

Shutterstock’s censorship feature appears to have been immediately controversial within the company, prompting more than 180 Shutterstock workers to sign a petition against the search blacklist and accuse the company of trading its values for access to the lucrative Chinese market. Chinese internet users already struggle to discuss even the tamest of taboo subjects; now, it seemed, the situation would get a little worse, with the aid of yet another willing American company.

“Yes, we’re a creative photo and video marketplace, but we are also an editorial news hub,” one Shutterstock employee told The Intercept. “Want to write a story about the protests in Hong Kong? They never existed. Want to write about Taiwan? It never existed. Xi Jinping is NOT a dictator because he specifically said so. This is dark shit.”

The text of the petition, provided to The Intercept, can be read in full below.

Shutterstock’s founder and CEO Jon Oringer replied to the petition several days later; those hoping for a change of heart were to be disappointed. Shutterstock’s pro-censorship compromise with the Chinese government was justified, Oringer argued, because to refuse to do business in China rather than help the country’s government expand its information control scheme would be the real act of craven corporate turpitude: “Do we make the majority of our content available to China’s 1.3 billion citizens or do we take away their ability to access it entirely? We ultimately believe, consistent with our brand promise, it is more valuable for storytellers to have access to our collection to creatively and impactfully tell their stories.” Shutterstock with a bespoke censorship feature was “more empowering” and “will better serve the people of China than the alternative,” Oringer continued.

Oringer’s company-wide response is also reproduced below.

Following Oringer’s letter and the implementation of the search term blacklist, some employees fear the use of censorship at the company will grow: “He offered no consolation in terms of what our actions will be when China requests to add an X number more search terms to the censorship list,” the Shutterstock staffer told The Intercept, “or if another country comes to us with a similar request. We are devastated.”
FEW PEOPLE HAD ever heard of Perceptics, a Tennessee-based subcontractor that sells license plate readers to U.S. Customs and Border Protection, before last month, when news emerged that the company had been hacked and that sensitive data — including images of license plates and drivers — had been released on the dark web.

The hack is just the sort of privacy breach that civil liberties advocates have long warned could come from massive government data collection, especially when it is contracted out to private firms. And it comes at a time when the CBP is under scrutiny for monitoring activists and journalists at the U.S.-Mexico border and airports.

Yet while photos of faces and license plates of some 100,000 U.S. drivers are now freely available online, the CEO of Perceptics, John Dalton, claimed in an email a few years ago that “CBP has none of the privacy concerns at the border that all agencies have inland.”

Writing to one of his company’s lobbyists in 2013, Dalton suggested that the border agency offered Perceptics an opportunity to make greater use of license plate images, stating, “Data mining and looking at traffic patterns/abnormalities are strong analytics for CBP, and could be for others.” Dalton appeared to be referring to the CBP’s relatively unfettered powers of search and seizure within 100 miles of the border. In contrast, for agencies other than CBP, “there is much concern with ACLU state level lawsuits and elsewhere around privacy issues, so this is a live challenge,” he wrote.
JUST MONTHS BEFORE millions of its internal documents were stolen and dumped on the internet, the Tennessee-based surveillance company Perceptics was preparing to pitch New York’s transit authority on how it could help enforce impending “congestion pricing” rules, according to leaked documents reviewed by The Intercept. The pitch, as outlined in the files, went well beyond mere toll enforcement and into profiling New Yorkers’ travel patterns and companions, creating what experts describe as major privacy risks.

Congestion pricing, on the face of it, doesn’t seem like it would present a privacy risk — it’s a traffic policy, after all, not some new NYPD initiative. The plan is to essentially tax the cars that clog Manhattan’s streets and route the proceeds to public transportation, providing both a deterrent against and palliative for traffic. There won’t be any congestion pricing toll booths: The fee will be assessed automatically and electronically, potentially by photographing the license plates of passing cars and sending the plate owner a bill in the mail. This requires cameras running around the clock, dutifully recording every car that comes and goes. And this, Perceptics claims, is where the company truly shines.

According to an internal presentation released by the Perceptics hacker and reviewed by The Intercept, the company pitched New York’s Metropolitan Transportation Authority, or MTA, in February of this year on how Perceptics’ car-scanning camera arrays, already deployed and honed in areas like the Mexican border and an assortment of U.S. military installations, could help the MTA track down drivers. It’s unknown how the plan was received by the MTA, which administers public transit, bridges, and tolls for New York City and some of its surrounding suburbs, but leaked Perceptics emails show that the company shipped camera hardware to the MTA’s Bridges and Tunnels division for a live demonstration.

Perceptics did not respond to a request for comment. An MTA spokesperson told The Intercept that “all details are still to be determined” regarding congestion pricing enforcement.

The presentation document, titled “Smart Imaging Solutions for New York City Congestion Pricing,” makes clear that Perceptics wants to “produce vehicle-specific profiles” using cameras and “unique machine learning algorithms,” allowing the city to immediately recognize and build travel histories of every car in the congestion zone. Law enforcement and surveillance experts said the system described goes far beyond what would ever be necessary to mail scofflaws traffic tickets. Instead, it is an entirely new sort of surveillance apparatus that tracks deeply personal information like “customer travel patterns and travel consistency,” the number of passengers in the car, or “likely trip purpose,” and associates this information with a unique fingerprint of every vehicle that passes by Perceptics’ cameras.

Allie Bohm, a policy counsel with the New York Civil Liberties Union, described the Perceptics plan as an “incredibly privacy-invasive proposal” that “raises all sorts of associational and First Amendment concerns.” Bohm expressed particular alarm about the possibility of a congestion pricing enforcement system eventually feeding data into the NYPD’s existing surveillance regime. “The NYPD has fancied itself an intelligence agency for a very long time,” said Bohm. “These are folks who are pioneering some really, at best, questionable, and, at worst, alarming programs of surveillance and of drawing conclusions from innocuous behavior.”

The MTA will not deploy congestion pricing before 2021 and has yet to select a tolling vendor. But whether Perceptics wins a contract or not, its idea to bring to the heart of Manhattan military-grade surveillance technology — already provided to Saudi Special Forces and the Jordanian army, according to a Perceptics document — is an example of how something as innocuous-sounding as congestion pricing can turn into a surveillance sprawl.
A MEMBER OF Project Veritas gave testimony in a federal court case indicating that the right-wing group, known for its undercover videos, violates Facebook policies designed to counter systematic deception by Russian troll farms and other groups. The deposition raises questions over whether Facebook will deter American operatives who use the platform to strategically deceive and damage political opponents as vigorously as it has Iranian and Russian propagandists. But is the company capable of doing so without just creating more problems?

Close observers of Veritas and Facebook, including one at a research lab that works with the social network, said the testimony shows the group is clearly violating policies against what Facebook refers to as “coordinated inauthentic behavior.” The company formally defined such behavior in a December 2018 video featuring its cybersecurity policy chief Nathaniel Gleicher, who said it “is when groups of pages or people work together to mislead others about who they are or what they’re doing.” The designation, Gleicher added, is applied by Facebook to a group not “because of the content they’re sharing” but rather only “because of their deceptive behavior.” That is, using Facebook to dupe people is all it takes to fit the company’s institutional definition of coordinated inauthentic behavior.

In practice, “coordinated inauthentic behavior” has become a sort of catchall label for untoward meddling on Facebook, snagging everyone from Burmese military officers to Russian meme spammers. But curbing such activity has become a very public crusade for Facebook in the wake of its prominent role as a platform for the spread of disinformation, propaganda, and outright hoaxes during the 2016 presidential campaign. This past January, Gleicher announced the removal of coordinated inauthentic behavior from Iran, which spread when operatives “coordinated with one another and used fake accounts to misrepresent themselves,” thus triggering a Facebook ban. Similarly, in a 2017 update on Facebook’s internal investigation into Russian online propaganda efforts, the company’s then-head of security Alex Stamos assured the world’s democracies the company was providing “technology improvements for detecting fake accounts,” including “changes to help us more efficiently detect and stop inauthentic accounts at the time they are being created.”

Throughout all of this, coordinated inauthentic behavior has remained more or less synonymous with “foreign actors” and “nation-states,” the cloak-and-dagger stuff of an increasingly militarized internet filled with enemies of the Western Democracy who seek to subvert it from abroad.

Project Veritas, a hybrid of an opposition research shop and a ranting YouTube channel, has taken pride in its ability to deceive since its creation in 2010. With conservative backers like Peter Thiel, the Koch brothers, and the Trump Foundation, the group and its founder James O’Keefe have worked relentlessly to target and malign individuals at institutions they deem leftist, whether it’s Planned Parenthood (reportedly targeted by O’Keefe posing as a young teen’s 23-year-old boyfriend), George Soros (the progressive philanthropist whose professional circle Veritas tried and spectacularly failed to infiltrate), or the Washington Post (whose reporter was offered a fake story on Alabama Senate candidate Roy Moore). O’Keefe has long attempted to position himself in the context of dogged, daring, traditional journalism, describing Veritas’s efforts as “investigative” reporting executed by “undercover journalists.” But his efforts are often executed by what the New Yorker has called “amateurish spies” — their efforts against the Post and Soros resembled a Three Stooges bit — and packaged with mendacious editing, duplicitous production, and outright lying, making Veritas’s audience as much a victim of its productions as the subjects. Debates over who or what is to be considered “real journalism” are almost always counterproductive and contrived, but Veritas stands out for the shamelessness with which it pursues nakedly partisan ends.

There is, of course, a proud tradition of undercover journalism executed unequivocally in the name of informing the public. Writers like Barbara Ehrenreich and Shane Bauer have taken jobs they were not otherwise interested in in order to reveal injustices in society’s margins, and some of the most damning details of the Cambridge Analytica scandal were exposed by a reporter with the UK’s Channel 4 posing as a foreign politician interested in the company’s services. This reporting involved lying, sure — or at least the withholding of true intent, and a willingness to let others deceive themselves — but only as a means to a truthful end. The distinction between these reporters and Veritas operatives may be that the end the latter group seeks, the final media product, is typically just another act of partisan misdirection that doesn’t withstand further scrutiny.

Neither Project Veritas nor Facebook commented for this story.

“Legend Building” by Project Veritas

Project Veritas has systematically deceived not just targets on the left and viewers on the right but Facebook users as well (their official page has over 200,000 followers) at a time when the company is publicly dedicated to fighting this sort of systemic duplicity. That’s a wrinkle that raises questions about Facebook’s commitment to rooting out coordinated inauthentic behavior closer to home — Thiel sits on the company’s board — not to mention Project Veritas’s presence on social media.
IN A FEDERAL lawsuit, the tech giant Oracle has provided new details to support its accusation that Amazon secretly negotiated a job offer with a then-Department of Defense official who helped shape the procurement process for a massive federal contract for which Amazon was a key bidder.

Amazon Web Services and Microsoft are now the two finalists to win the highly contested $10 billion contract for what is known as the Joint Enterprise Defense Infrastructure, or JEDI. The deal, one of the largest federal contracts in U.S. history, would pay one company to provide cloud computing services in support of Defense Department operations around the world.

But the contract has been hotly contested since the department began soliciting proposals last year. Two of Amazon’s competitors, IBM and Oracle, filed complaints with the Government Accountability Office saying that the winner-take-all process unfairly favored Amazon, which is seen as an industry leader in cloud computing. When its claim was rejected, Oracle sued the government in the U.S. Court of Federal Claims.

Since the court battle began in 2018, Oracle has aggressively lodged conflict-of-interest accusations involving a former DOD official named Deap Ubhi, who left the department in 2017 to take a job at Amazon. In a court motion filed on Friday, Oracle alleged that while Ubhi worked on the preliminary research for the JEDI program in the late summer and fall of 2017, he was also engaged in a secret job negotiation with Amazon for months, complete with salary discussions, offers of signing bonuses, and lucrative stock options.

The motion further alleges that Ubhi did not recuse himself from the JEDI program until weeks after verbally accepting a job offer from Amazon and that he continued to receive information about Amazon’s competitors and participate in meetings about technical requirements, despite a government regulation that forbids such conflicts of interest.

“Neither Ubhi nor [Amazon Web Services] disclosed the employment discussions or job offer to DOD — not when the employment discussions started, not when the informal job offer occurred, not when the formal offer occurred, and not even when Ubhi accepted the offer,” Oracle’s motion reads.

As America’s technology companies have continued to outpace the Pentagon, the Defense Department has looked to recruit talent from Silicon Valley to help enhance its information technology.

Ubhi is a venture capitalist and technology entrepreneur who worked for Amazon before his time in government. He took a job working on a Defense Department initiative aimed at collaborating with Silicon Valley to modernize the Pentagon’s information technology systems. After working as part of a four-person team to help shape the Pentagon JEDI procurement process, he left the department and returned to Amazon in November 2017.

A spokesperson for Amazon Web Services declined to comment and declined to make Ubhi available for an interview, citing ongoing litigation. Elissa Smith, a spokesperson for the Department of Defense, also told The Intercept that “we don’t comment on pending litigation.”

In a previous court filing, U.S. government lawyers accused Oracle of a “broad fishing expedition primarily [intended] to find support for its claim that the solicitation at issue is tainted by alleged conflicts of interest.”

According to Oracle’s motion on Friday, Ubhi began job negotiations with Amazon in August 2017, while he was working on the early stages of the JEDI program. Oracle claims says that “deep discussions” about employment began in late September and that Ubhi “verbally committed” to take the job on October 4. But according to the filing, Ubhi did not recuse himself until October 31, 2017. Oracle alleges that he continued to influence the program in the meantime.

Under the Procurement Integrity Act, government officials who are “contacted by a [contract] bidder about non-federal employment” have two options: They must either report the contact and reject the offer of employment or promptly recuse themselves from any contract proceedings.

“Contracts should be awarded fairly based on merit,” Mandy Smithberger, director of the Center for Defense Information at the Project on Government Oversight, told The Intercept. “The Procurement Integrity Act seeks to ensure that job offers and other financial conflicts of interest don’t influence that process.”

Last year, a Defense Department review found that “there were four instances where [department] individuals with potential financial conflicts of interest” had worked on the JEDI program, according to court records, but the Pentagon concluded that this hadn’t unfairly impacted the contracting process. Two follow-up reviews — one by the GAO in November 2018 and another by the Defense Department in April 2019 — came to similar conclusions.

The second Pentagon review came after the department said that it had received “new information” about Ubhi and would investigate it. According to Oracle’s motion on Friday, the “new information” came from a “belated submission from [Amazon]” to the DOD’s contracting officer that finally acknowledged the monthslong employment talks.

According to Oracle, Ubhi provided a “false narrative” to the contracting officer at the time of his recusal, saying that he was stepping away from the project because Amazon had offered to acquire a company that Ubhi had a stake in. That was a pretext to mask the fact he had been negotiating for months to obtain a job at the company, Oracle’s filing said.

The filing also alleges that between Ubhi’s verbal commitment to accept Amazon’s offer and his recusal from JEDI, he continued to participate in Pentagon meetings about the project’s technical requirements and to receive submissions from Amazon competitors. It also alleges that Ubhi downloaded material from a JEDI project Google Drive to his own laptop.

In its filings, Oracle has argued that Ubhi was instrumental in persuading the Pentagon to seek services from a single vendor — a decision widely seen to improve Amazon’s chances. Oracle cites workplace messages on the platform Slack in which Ubhi tries to persuade his colleagues to come around to that view, but the company does not cite any messages suggesting what his reasons or motive may have been.
In partnership with

FORMER GOOGLE CEO Eric Schmidt has defended the company’s plan to build a censored version of its search engine in China.

In an interview with the BBC on Monday, Schmidt said that he wasn’t involved in decisions to build the censored search platform, code-named Dragonfly. But he insisted that there were “many benefits” to working with China and said he was an advocate of operating in the country because he believed that it could “help change China to be more open.”

As The Intercept first revealed in August, Google developed a prototype of the censored search engine that was designed to remove content that China’s ruling Communist Party regime deems sensitive. The search engine would have blacklisted thousands of words and phrases, including terms such as “human rights,” “student protest,” and “Nobel Prize” in Mandarin.

The revelations prompted a wave of protests inside and outside of Google, with employees, activists, and prominent lawmakers demanding an end to the project. Google subsequently stated that it had ceased work on Dragonfly and moved employees to new projects.
LAST YEAR, A coalition of privacy advocates and child psychologists warned against putting an Amazon Alexa speaker anywhere near your child on the fairly reasonable grounds that developing minds shouldn’t befriend always-on surveillance devices, no matter how cute the packaging. Now, a group of privacy researchers, attorneys, and U.S. senators are calling on the Federal Trade Commission to investigate Amazon’s alleged violations of COPPA, a law protecting the littlest users of all.

COPPA, the Children’s Online Privacy Protection Act, regulates how companies can collect and use data on users who might have trouble spelling “privacy,” let alone understand it enough to consent to relinquishing it. COPPA is the reason why so many sites, like Facebook, simply don’t allow children under 13 to sign up. Amazon, on the other hand, decided to court children for its data collection business, releasing the Amazon Echo Dot Kids Edition, an always-listening “smart speaker” that retains all of the functions of its adult counterpart, but tucks them inside a candy-colored shell. The kiddo speaker also adds child-specific features, like the ability to have Amazon’s virtual assistant Alexa read your child a story in her disembodied robo-voice, or play child-geared content from sources like Cartoon Network and Nickelodeon.

A new complaint drafted by the Campaign for a Commercial-Free Childhood, the consumer privacy group Center for Digital Democracy, and Georgetown University’s Institute for Public Representation says that Amazon is committing a litany of COPPA violations through the Echo Dot Kids Edition, and calls on the FTC to investigate.

Amazon’s COPPA violations, according to the complaint, include failure to provide parental notice and obtain parental consent for online services related to the kids’ Echo Dot, failure to tell parents that they have a right to review personal information submitted by their child, and failure to provide parents a way to delete such information or opt out of its collection.

Across 96 pages, the complaint gets more specific, offering examples of how Amazon dodges, obscures, and otherwise neglects its duties to parents. Given that the attorneys who drafted the complaint were confused by Amazon’s byzantine policies, it’s hard to imagine average parents faring much better:

Even if parents were for some reason motivated to seek out the website version of the Children’s Privacy Disclosure, the hyperlinked Privacy Notice is long, confusingly written, and contains a lot of unrelated material. It is unclear what, if any, parts apply to the Echo Dot Kids Edition, and some of the information seems to contradict the Children’s Privacy Disclosure. For example, the Privacy Notice discloses that Amazon collects “search term and search result information from some searches conducted through the Web search features offered by our subsidiary, Alexa Internet,” but it does not say whether it does so when a child is using the Echo Dot Kids Edition.

Perhaps most troubling is what the complaint says about Amazon’s treatment of child voice recordings, which aren’t supposed to be stored indefinitely, per recent FTC guidance: “The answer is clear: No, the company can’t keep it. Under Section 312.10 of COPPA, you’re allowed to retain children’s personal information ‘for only as long as is reasonably necessary to fulfill the purpose for which the information was collected.'”

But Amazon takes a different approach, the complaint explains: “In response to a Congressional inquiry about how long it keeps recordings and other information collected from children, however, Amazon responded: ‘Voice recordings are retained for the parent’s review until the parent deletes them.'” In other words, Amazon is keeping a child’s Amazon queries stored indefinitely, not “for only as long as is reasonably necessary.”

GOOGLE EXECUTIVES ARE carrying out a secret internal assessment of work on a censored search engine for China, The Intercept has learned.

A small group of top managers at the internet giant are conducting a “performance review” of the controversial effort to build the search platform, known as Dragonfly, which was designed to blacklist information about human rights, democracy, religion, and peaceful protest.

Performance reviews at Google are undertaken annually to evaluate employees’ output and development. They are usually carried out in an open, peer review-style process: Workers grade each other’s projects and the results are then assessed by management, who can reward employees with promotion if they are deemed ready to progress at the company.

In the case of Dragonfly, however, the peer review aspect has been removed, subverting the normal procedure. In a move described as highly unusual by two Google sources, executives set up a separate group of closed “review committees,” comprised of senior managers who had all previously been briefed about the China search engine.

ACROSS THE WORLD, the reputation of elites and their institutions is in free fall. A flood of online information has given the public unprecedented access to elite individuals in politics, media, academia, science, business, and an array of other fields. Thanks to tools like social media, the activist public has greater proximity to its supposed mandarin class than ever before. What this newfound intimacy has revealed has not always been flattering. Many of those who had been held up as elites in their fields have, upon closer examination by the public, been revealed as mediocre, incompetent, buffoonish, and, in some cases, possibly unhinged. At the same time, the public, for all its passion, has also revealed itself to be vulnerable to conspiracy theories, disinformation, and outbreaks of hysteria.
FOLLOWING MONTHS OF protests from its employees, Google announced last summer that it would not renew its contract with the military on Project Maven, an initiative to use artificial intelligence to improve the targeting and surveillance capabilities of drones on the battlefield.

In an email sent this week by Kent Walker, Google’s senior vice president for global affairs, the Silicon Valley giant appeared to hedge on its commitment to fully cut ties with the drone initiative.


ON MARCH 17, 2016, Ring CEO Jamie Siminoff emailed out a company-wide declaration of war. The message, under the subject line “Going to war,” made two things clear to the home surveillance company’s hundreds of employees: Everyone was getting free camouflage-print T-shirts (“They look awesome,” assured Siminoff), and the company’s new mission was to use consumer electronics to fight crime. “We are going to war with anyone that wants to harm a neighborhood,” Siminoff wrote — and indeed Ring made it easier for police and worried neighbors to get their hands on footage from Ring home cameras. Internal documents and video reviewed by The Intercept show why this merging of private tech business and public law enforcement has troubling privacy implications.

This first declaration of startup militancy — which Siminoff would later refer to as “Ring War I” or simply “RW1” — would be followed by more, equally clumsy attempts at corporate galvanization, some aimed at competitors or lackluster customer support. But the RW1 email is striking in how baldly it lays out the priorities and values of Ring, a company now owned by Amazon and facing strident criticism over its mishandling of customer data, as previously reported by The Intercept and The Information.

Ring and Siminoff, who still leads the company, haven’t been shy about their focus on crime-fighting. In fact, Ring’s emphasis not only on personal peace of mind, but also active crime-fighting has been instrumental in differentiating its cloud-connected doorbell and household surveillance gear from those made by its competitors. Ring products come with access to a social app called Neighbors that allows customers to not just to keep tabs on their own property, but also to share information about suspicious-looking individuals and alleged criminality with the rest of the block. In other words, Ring’s cameras aren’t just for keeping tabs on your own stoop or garage — they work to create a private-sector security bubble around entire residential areas, a neighborhood watch for the era of the so-called smart home.


SHOSHANA ZUBOFF’S “The Age of Surveillance Capitalism” is already drawing comparisons to seminal socioeconomic investigations like Rachel Carson’s “Silent Spring” and Karl Marx’s “Capital.” Zuboff’s book deserves these comparisons and more: Like the former, it’s an alarming exposé about how business interests have poisoned our world, and like the latter, it provides a framework to understand and combat that poison. But “The Age of Surveillance Capitalism,” named for the now-popular term Zuboff herself coined five years ago, is also a masterwork of horror. It’s hard to recall a book that left me as haunted as Zuboff’s, with its descriptions of the gothic algorithmic daemons that follow us at nearly every instant of every hour of every day to suck us dry of metadata. Even those who’ve made an effort to track the technology that tracks us over the last decade or so will be chilled to their core by Zuboff, unable to look at their surroundings the same way.
A NEW WEBSITE exposes the extent to which Apple cooperates with Chinese government internet censorship, blocking access to Western news sources, information about human rights and religious freedoms, and privacy-enhancing apps that would circumvent the country’s pervasive online surveillance regime.

The new site, AppleCensorship.com, allows users to check which apps are not accessible to people in China through Apple’s app store, indicating those that have been banned. It was created by researchers at GreatFire.org, an organization that monitors Chinese government internet censorship.

In late 2017, Apple admitted to U.S. senators that it had removed from its app store in China more than 600 “virtual private network” apps that allow users to evade censorship and online spying. But the company never disclosed which specific apps it removed — nor did it reveal other services it had pulled from its app store at the behest of China’s authoritarian government.

In addition to the hundreds of VPN apps, Apple is currently preventing its users in China from downloading apps from news organizations, including the New York Times, Radio Free Asia, Tibetan News, and Voice of Tibet. It is also blocking censorship circumvention tools like Tor and Psiphon; Google’s search app and Google Earth; an app called Bitter Winter, which provides information about human rights and religious freedoms in China; and an app operated by the Central Tibetan Authority, which provides information about Tibetan human rights and social issues.

Some bans – such as those of certain VPN apps and the Times – have received media coverage in the past, but many never generate news headlines. Charlie Smith, a co-founder of GreatFire.org, told The Intercept that the group was motivated to launch the website because “Apple provides little transparency into what it censors in its app store. Most developers find out their app has been censored after they see a drop in China traffic and try to figure out if there is a problem. We wanted to bring transparency to what they are censoring.”
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
GOOGLE IS FACING a new campaign of global protests over its plan to launch a censored version of its search engine in China.

On Friday, a coalition of Chinese, Tibetan, Uighur, and human rights groups organized demonstrations outside Google’s offices in the U.S., U.K., Canada, India, Mexico, Chile, Argentina, Sweden, Switzerland, and Denmark.
THE “SMART HOME” of the 21st century isn’t just supposed to be a monument to convenience, we’re told, but also to protection, a Tony Stark-like bubble of vigilant algorithms and internet-connected sensors working ceaselessly to watch over us. But for some who’ve welcomed in Amazon’s Ring security cameras, there have been more than just algorithms watching through the lens, according to sources alarmed by Ring’s dismal privacy practices.

Ring has a history of lax, sloppy oversight when it comes to deciding who has access to some of the most precious, intimate data belonging to any person: a live, high-definition feed from around — and perhaps inside — their house. The company has marketed its line of miniature cameras, designed to be mounted as doorbells, in garages, and on bookshelves, not only as a means of keeping tabs on your home while you’re away, but of creating a sort of privatized neighborhood watch, a constellation of overlapping camera feeds that will help police detect and apprehend burglars (and worse) as they approach. “Our mission to reduce crime in neighborhoods has been at the core of everything we do at Ring,” founder and CEO Jamie Siminoff wrote last spring to commemorate the company’s reported $1 billion acquisition payday from Amazon, a company with its own recent history of troubling facial recognition practices. The marketing is working; Ring is a consumer hit and a press darling.

Despite its mission to keep people and their property secure, the company’s treatment of customer video feeds has been anything but, people familiar with the company’s practices told The Intercept. Beginning in 2016, according to one source, Ring provided its Ukraine-based research and development team virtually unfettered access to a folder on Amazon’s S3 cloud storage service that contained every video created by every Ring camera around the world. This would amount to an enormous list of highly sensitive files that could be easily browsed and viewed. Downloading and sharing these customer video files would have required little more than a click. The Information, which has aggressively covered Ring’s security lapses, reported on these practices last month.

At the time the Ukrainian access was provided, the video files were left unencrypted, the source said, because of Ring leadership’s “sense that encryption would make the company less valuable,” owing to the expense of implementing encryption and lost revenue opportunities due to restricted access. The Ukraine team was also provided with a corresponding database that linked each specific video file to corresponding specific Ring customers.

LEIA EM PORTUGUÊS 
GOOGLE CEO SUNDAR PICHAI came under fire from lawmakers on Tuesday over the company’s secretive plan to launch a censored search engine in China.

During a hearing held by the House Judiciary Committee, Pichai faced sustained questions over the China plan, known as Dragonfly, which would blacklist broad categories of information about democracy, human rights, and peaceful protest.

The hearing began with an opening statement from Rep. Kevin McCarthy, R-Calif., who said launching a censored search engine in China would “strengthen China’s system of surveillance and repression.” McCarthy questioned whether it was the role of American companies to be “instruments of freedom or instruments of control.”

Pichai read prepared remarks, stating “even as we expand into new markets, we never forget our American roots.” He added: “I lead this company without political bias and work to ensure that our products continue to operate that way. To do otherwise would go against our core principles and our business interests.”
GOOGLE IS FACING a renewed wave of criticism from human rights groups over its controversial plan to launch a censored search engine in China.

A coalition of more than 60 leading groups from countries across the world have joined forces to blast the internet giant for failing to address concerns about the secretive China project, known as Dragonfly. They come from countries including China, the United States, the United Kingdom, Argentina, Bolivia, Chile, France, Kazakhstan, Mexico, Norway, Pakistan, Palestine, Romania, Syria, Tibet, and Vietnam.

A prototype for the censored search engine was designed to blacklist broad categories of information about human rights, democracy, and peaceful protest. It would link Chinese users’ searches to their personal cellphone number and store people’s search records inside the data centers of a Chinese company in Beijing or Shanghai, which would be accessible to China’s authoritarian Communist Party government.

If the plan proceeds, “there is a real risk that Google would directly assist the Chinese government in arresting or imprisoning people simply for expressing their views online, making the company complicit in human rights violations,” the human rights groups wrote in a letter that will be sent to Google’s leadership on Tuesday.

The letter highlights mounting anger and frustration within the human rights community that Google has rebuffed concerns about Dragonfly, concerns that have been widely raised both inside and outside the company since The Intercept first revealed the plan in August. The groups say in their 900-word missive that Google’s China strategy is “reckless,” piling pressure on CEO Sundar Pichai, who is due to appear Tuesday before the House Judiciary Committee, where he will likely face questions on Dragonfly.
GOOGLE IS FACING a renewed wave of criticism from human rights groups over its controversial plan to launch a censored search engine in China.

A coalition of more than 60 leading groups from countries across the world have joined forces to blast the internet giant for failing to address concerns about the secretive China project, known as Dragonfly. They come from countries including China, the United States, the United Kingdom, Argentina, Bolivia, Chile, France, Kazakhstan, Mexico, Norway, Pakistan, Palestine, Romania, Syria, Tibet, and Vietnam.

A prototype for the censored search engine was designed to blacklist broad categories of information about human rights, democracy, and peaceful protest. It would link Chinese users’ searches to their personal cellphone number and store people’s search records inside the data centers of a Chinese company in Beijing or Shanghai, which would be accessible to China’s authoritarian Communist Party government.

If the plan proceeds, “there is a real risk that Google would directly assist the Chinese government in arresting or imprisoning people simply for expressing their views online, making the company complicit in human rights violations,” the human rights groups wrote in a letter that will be sent to Google’s leadership on Tuesday.

The letter highlights mounting anger and frustration within the human rights community that Google has rebuffed concerns about Dragonfly, concerns that have been widely raised both inside and outside the company since The Intercept first revealed the plan in August. The groups say in their 900-word missive that Google’s China strategy is “reckless,” piling pressure on CEO Sundar Pichai, who is due to appear Tuesday before the House Judiciary Committee, where he will likely face questions on Dragonfly.
LEIA EM PORTUGUÊS 
LAST MONTH, A famed hacker who has been serving a 10-year prison sentence since 2012 was accused by a guard at a federal detention center of “minor assault,” landing the so-called hacktivist in solitary confinement, according to advocates. The guard at Michigan’s Federal Correctional Institute-Milan made the accusation against Jeremy Hammond — the activist associated with hacking groups Anonymous and LulzSec and best know for hacking private intelligence firm Stratfor and leaking documents to WikiLeaks — on either November 19 or 20. Hammond has been held in solitary confinement ever since, according to the Jeremy Hammond Support Network.

The guard claims that Hammond hit him with a door, “stood his ground,” and pushed his shoulder into the guard. The head of Hammond’s support network said the prison guard’s account is an overblown. “Jeremy says that he was exiting his unit through a door that has no windows and could not see the guard on the other side, and as he’s exiting, bumped the guard with the door,” Grace North told The Intercept. “The guard immediately grabbed Jeremy and threw him up against the wall and dragged him down to solitary, with no handcuffs, without calling for backup, which is against prison protocol, and Jeremy has been there ever since.”

North’s version of events also portrays the guard as overly aggressive: After the guard was hit with the door, North said, he asked Hammond if he “wanted to go.”
LAST MONTH, A famed hacker who has been serving a 10-year prison sentence since 2012 was accused by a guard at a federal detention center of “minor assault,” landing the so-called hacktivist in solitary confinement, according to advocates. The guard at Michigan’s Federal Correctional Institute-Milan made the accusation against Jeremy Hammond — the activist associated with hacking groups Anonymous and LulzSec and best know for hacking private intelligence firm Stratfor and leaking documents to WikiLeaks — on either November 19 or 20. Hammond has been held in solitary confinement ever since, according to the Jeremy Hammond Support Network.

The guard claims that Hammond hit him with a door, “stood his ground,” and pushed his shoulder into the guard. The head of Hammond’s support network said the prison guard’s account is an overblown. “Jeremy says that he was exiting his unit through a door that has no windows and could not see the guard on the other side, and as he’s exiting, bumped the guard with the door,” Grace North told The Intercept. “The guard immediately grabbed Jeremy and threw him up against the wall and dragged him down to solitary, with no handcuffs, without calling for backup, which is against prison protocol, and Jeremy has been there ever since.”

North’s version of events also portrays the guard as overly aggressive: After the guard was hit with the door, North said, he asked Hammond if he “wanted to go.”

THE SECRECY SURROUNDING the work was unheard of at Google. It was not unusual for planned new products to be closely guarded ahead of launch. But this time was different. The objective, code-named Dragonfly, was to build a search engine for China that would censor broad categories of information about human rights, democracy, and peaceful protest.

In February 2017, during one of the first group meetings about Dragonfly at Google’s Mountain View headquarters in California, some of those present were left stunned by what they heard. Senior executives disclosed that the search system’s infrastructure would be reliant upon a Chinese partner company with data centers likely in Beijing or Shanghai.

Locating core parts of the search system on the Chinese mainland meant that people’s search records would be easily accessible to China’s authoritarian government, which has broad surveillance powers that it routinely deploys to target activists, journalists, and political opponents.

Yonatan Zunger, then a 14-year veteran of Google and one of the leading engineers at the company, was among a small group who had been asked to work on Dragonfly. He was present at some of the early meetings and said he pointed out to executives managing the project that Chinese people could be at risk of interrogation or detention if they were found to have used Google to seek out information banned by the government.

Scott Beaumont, Google’s head of operations in China and one of the key architects of Dragonfly, did not view Zunger’s concerns as significant enough to merit a change of course, according to four people who worked on the project. Beaumont and other executives then shut out members of the company’s security and privacy team from key meetings about the search engine, the four people said, and tried to sideline a privacy review of the plan that sought to address potential human rights abuses.

Zunger — who left his position at Google last year — is one of the four people who spoke to The Intercept for this story. He is the first person with direct involvement in Dragonfly to go on the record about the project. The other three who spoke to The Intercept are still employed by Google and agreed to share information on the condition of anonymity because they were not authorized to talk to the media. Their accounts provide extraordinary insight into how Google bosses worked to suppress employee criticism of the censored search engine and reveal deep fractures inside the company over the China plan dating back almost two years.

Google’s leadership considered Dragonfly so sensitive that they would often communicate only verbally about it and would not take written notes during high-level meetings to reduce the paper trail, two sources said. Only a few hundred of Google’s 88,000 workforce were briefed about the censorship plan. Some engineers and other staff who were informed about the project were told that they risked losing their jobs if they dared to discuss it with colleagues who were themselves not working on Dragonfly.

“They [leadership] were determined to prevent leaks about Dragonfly from spreading through the company,” said a current Google employee with knowledge of the project. “Their biggest fear was that internal opposition would slow our operations.”
LEIA EM PORTUGUÊS 
AMNESTY INTERNATIONAL HAS announced a new protest campaign calling on Google to cancel its controversial plan to launch a censored search engine in China.

The human rights group on Monday launched a petition against the search engine and said that on Tuesday, it will stage demonstrations outside Google offices in the United States, the United Kingdom, Australia, Canada, Germany, Hong Kong, the Netherlands, and Spain. Google’s plan for China would “irreparably damage internet users’ trust in the tech company,” Amnesty said in statement, and “would set a dangerous precedent for tech companies enabling rights abuses by governments.”

As The Intercept first reported in August, Google secretly developed the censored search engine as part of a project code-named Dragonfly. It was designed to blacklist words and phrases such as “human rights,” “Nobel Prize,” and “student protest.” The search platform would link Chinese users’ search records to their cellphone numbers and share people’s search histories with a Chinese partner company. The search records would in turn be accessible to China’s authoritarian government, which has broad surveillance and data-seizing powers that it routinely uses to identify and arrest activists and critics.

“This is a watershed moment for Google,” said Joe Westby, Amnesty International’s researcher on technology and human rights. “As the world’s No. 1 search engine, it should be fighting for an internet where information is freely accessible to everyone, not backing the Chinese government’s dystopian alternative.”
NINE HUMAN RIGHTS and civil liberties organizations sent a letter to the U.S. Justice Department today objecting to a potential agreement between the United States and the United Kingdom that would give British law enforcement broad access to data held by U.S. technology companies.

The possible agreement stems from the Clarifying Lawful Overseas Use of Data Act, or CLOUD Act, for which Justice Department officials have lobbied since 2016 and which President Donald Trump signed into law in March.

In addition to requiring American tech companies to provide data on U.S. citizens when served with a warrant, the CLOUD Act allows for so-called executive agreements between the president and foreign governments. These agreements, the first of which would be with the United Kingdom, would empower foreign law enforcement agencies to order U.S. tech companies to produce data about individual users without a warrant, so long as the search target is not a U.S. citizen or resident.

The Electronic Frontier Foundation, one of the organizations that signed the letter of protest, has described a possible scenario for how a U.K. police service might obtain data under the CLOUD Act: “London investigators want the private Slack messages of a Londoner they suspect of bank fraud. The London police could go directly to Slack, a U.S. company, to request and collect those messages. The London police would receive no prior judicial review for this request. The London police could avoid notifying U.S. law enforcement about this request. The London police would not need a probable cause warrant for this collection.”

But this form of international data-sharing could put Americans’ privacy at risk and expose citizens to potential Fourth Amendment abuses, critics say.

While the CLOUD Act requires that foreign police services not “intentionally target a United States person or a person located in the United States,” the law does not stop foreign police agencies from receiving communications of U.S. citizens or residents. Using the Electronic Frontier Foundation’s example of a Londoner communicating on Slack, any communications between the targeted British citizen and Americans would also be turned over to London police.

“The phrase ‘intentionally target’ creates a large loophole; people in the U.S. and U.S. persons overseas could easily get caught in the dragnet,” said Sarah St.Vincent, an investigator with Human Rights Watch, another signatory to the Justice Department letter. Although such so-called minimization procedures are ostensibly in place to prevent foreign governments from ensnaring U.S. users, St.Vincent told The Intercept that she rejects the notion that they “should be reassuring to anyone,” as “procedures are not laws,” but rather safeguards. “I don’t see any mechanism in here to ensure that those are strictly applied and inspected,” St.Vincent added.

The CLOUD Act also leaves open the possibility that a foreign police agency could obtain, without a warrant, incriminating communications from a U.S. citizen, which could then be shared with U.S. law enforcement. Data obtained in this way could not be used as evidence in a U.S. court, because its collection would violate Fourth Amendment protections. But local, state, or federal law enforcement agencies could reacquire the communications after obtaining a warrant — a controversial law enforcement practice known as “parallel construction.”

Federal law enforcement agencies, including the FBI and the Drug Enforcement Administration, already use parallel construction to launder information acquired from the warrantless wiretapping programs exposed by National Security Agency whistleblower Edward Snowden. In November 2017, The Intercept reported how the FBI used parallel construction to enter information first obtained through the government’s mass surveillance programs into evidence in terrorism trials. In these cases, prosecutors did not disclose to the courts that investigators had obtained the evidence from warrantless surveillance and then re-obtained it using legitimate warrants.

A Human Rights Watch report released in January documented how the DEA set up a unit called the Special Operations Division to receive raw intelligence from the NSA and disseminate leads to field agents. Agents on the ground were instructed to conceal the source of their information and find other ways to justify searches and broader investigations.

“The CLOUD Act would specifically allow the U.K. authorities to pass data belonging to U.S. persons back to the U.S. authorities if it ‘relates to significant harm, or the threat thereof, to the United States or United States persons’ — quite a significant loophole,” St.Vincent said. “The U.S. authorities can’t deliberately set up this end run around the Fourth Amendment themselves, but they’re free to sit back and receive whatever the U.K. sees fit to share.”

DESPITE THE ACT’S worrying implications for user privacy, the American tech vanguard has embraced it. In February, Google, Apple, Facebook, Microsoft, and Oath (formerly Yahoo) wrote to four U.S. senators detailing their support for the legislation, claiming that CLOUD “reflects a growing consensus in favor of protecting Internet users,” and “would be notable progress to protect consumers’ rights and would reduce conflicts of law.” In September, Reuters reported that Apple was building an “online tool” that would allow police around the world to more easily request the company’s user data.

Only after the CLOUD Act was passed did Microsoft, one of the law’s early boosters, address questions of personal privacy and offer assurances that the framework would not be abused. In a September blog post, Microsoft president and top lawyer Brad Smith announced “six principles that have driven, and will continue to drive, our advocacy as governments reform their laws and negotiate international agreements,” including a “universal right” for users to be notified if their data is accessed, and the ability to “challenge unlawful and inappropriate demands for user data.” Even so, Smith’s post states unequivocally that Microsoft believes the “passage of the CLOUD Act created the foundation for a new generation of international agreements that allows governments to engage with each other to create lasting rules to protect privacy.” Other tech firms have simply remained silent.
IS IT POSSIBLE to tell whether someone is a criminal just from looking at their face or listening to the sound of their voice? The idea may seem ludicrous, like something out of science fiction — Big Brother in “1984” detects any unconscious look “that carried with it the suggestion of abnormality” — and yet, some companies have recently begun to answer this question in the affirmative. AC Global Risk, a startup founded in 2016, claims to be able to determine your level of “risk” as an employee or an asylum-seeker based not on what you say, but how you say it.

The California-based company offers an automated screening system known as a Remote Risk Assessment, or RRA. Here’s how it works: Clients of AC Global Risk help develop automated, yes-or-no interview questions. The group of people selected for a given screening then answer these simple questions in their native language during a 10-minute interview that can be conducted over the phone. The RRA then measures the characteristics of their voice to produce an evaluation report that scores each individual on a spectrum from low to high risk. CEO Alex Martin has said that the company’s proprietary risk analysis can “forever change for the better how human risk is measured.”

AC Global Risk, which boasts the consulting firm of Robert Gates, Condoleezza Rice, and Stephen Hadley on its advisory board, has advertised contracts with the U.S. Special Operations Command in Afghanistan, the Ugandan Wildlife Authority, and the security teams at Palantir, Apple, Facebook, and Google, among others. The extensive use of risk screening in these and other markets, Martin has said, has proven that it is “highly accurate, scalable, cost-effective, and capable of high throughput.” AC Global Risk claims that its RRA system can simultaneously process hundreds of individuals anywhere in the world. Now, in response to President Donald Trump’s calls for the “extreme vetting” of immigrants, the company has pitched itself as the ultimate solution for “the monumental refugee crisis the U.S. and other countries are currently experiencing.”

It’s a proposal that would seem to appeal to the U.S. Department of Homeland Security. The DHS has already funded research to develop similar AI technology for the border. The program, known as the Automated Virtual Agent for Truth Assessments in Real-Time, or AVATAR, used artificial intelligence to measure changes in the voice, posture, and facial gestures of travelers in order to flag those who appeared untruthful or seemed to pose a potential risk. In 2012 it was tested by volunteers at the U.S.-Mexico border. The European Union has also funded research into technology that would reduce “the workload and subjective errors caused by human agents.”

Some of the leading experts in vocal analytics, algorithmic bias, and machine learning find the trend toward digital polygraph tests troubling, pointing to the faulty methodology of companies like AC Global Risk. “There is some information in dynamic changes in the voice and they’re detecting it. This is perfectly plausible,” explained Alex Todorov, a Princeton University psychologist who studies the science of social perception and first impressions. “But the question is, How unambiguous is this information at detecting the category of people they’ve defined as risky? There is always ambiguity in these kinds of signals.”

Over the past year, the American Civil Liberties Union and others have reported that Border Patrol agents have been seizing people from Greyhound buses based on their appearance or accent. Because Customs and Border Protection agents already use information about how someone speaks or looks as a pretext to search individuals in the 100-mile border zone, or to deny individuals entry to the U.S., experts fear that vocal emotion detection software could make such biases routine, pervasive, and seemingly “objective.”

AC Global Risk declined to respond to repeated requests for comment for this article. The company also did not respond to a list of detailed questions about how the technology works. In public appearances, however, Martin has claimed that the company’s proprietary analytical processes can determine someone’s risk level with greater than 97 percent accuracy. (AVATAR, meanwhile, claims an accuracy rate of between 60 and 70 percent.) Several leading audiovisual experts who reviewed AC Global Risk’s publicly available materials for The Intercept used the word “bullshit” or “bogus” to describe the company’s claims. “From an ethical point of view, it’s very dubious and shady to give the impression that recognizing deception from only the voice can be done with any accuracy,” said Björn Schuller, a professor at the University of Augsburg who has led the field’s major academic challenge event to advance the state of the art in vocal emotion detection. “Anyone who says they can do this should themselves be seen as a risk.”
LEIA EM PORTUGUÊS 
IN THE VIDEO game aisle of a Walmart Supercenter, Eric, 43, is refreshing his phone. The Superman logo on his T-shirt has been reworked into a hammer and sickle. He’s waiting to hear back from a stranger based, as far as he knows, in Kenya. “He just offered to pay $400,” Eric says. “I don’t feel 100 percent on him, but I don’t have anything real to base it on.”

Eric is a currency broker. But he doesn’t work on Wall Street, or at the Chicago Mercantile Exchange, or with any financial institution. He’s one of the thousands of Americans who have built a career out of the bitcoin phenomenon, whether out of necessity or a sense of entrepreneurship. It’s enabled Eric to stop toiling as an information technology engineer for software companies and set his own hours. “Never in my life had I thought I’d work for myself,” Eric explains.

His operation mainly consists of a phone and a couple of apps. But it also requires daily visits to his version of the trading floor: Walmart. Eric visits big box stores so often that he knows the self-checkout machines (his preferred method of payment) better than the clerks who work there. He goes to Walmart to buy gift cards, which serve as a medium of exchange and a means of protection from being duped by his clients. But the whole scheme relies on exploiting Walmart’s rather lax standard on gift card purchases. Eric believes, through his up-close experience, that these lax standards have been facilitating rampant gift card fraud, which has risen to epidemic levels as of late.

Eric will end up netting $12 on this transaction, but he did it at a discount, to make sure he had a customer. He wanted to make sure he could demonstrate to a reporter precisely how it’s done, on the condition that his real name not be used. The goal, he said, was to expose vulnerabilities in Walmart’s gift card policy, so that the company would take action. On Tuesday, Walmart did just that, sort of, announcing new rules in the way it treats such cards, upending a world that Eric offered The Intercept an invitation to explore.

The new policy will make it difficult for Eric to make ends meet, but he’s glad it happened. “I’m a socialist in a capitalist world,” he explains.

ERIC BECAME FASCINATED with bitcoin while using it for offshore gambling sites. That led him to his trading business, which he’s operated for over three years.

Here’s how it works: Eric sells bitcoin to buyers all over the world through a peer-to-peer marketplace called Paxful, charging a premium for the cryptocurrency. When Eric started his business a couple of years ago, he could charge a 100 percent markup — selling $300 in bitcoin for $600 — because buyers expected the cryptocurrency’s price to elevate, and had few options to obtain it. Nowadays, with more volatility and more competition among traders, that has dropped to around 30 percent. Paxful takes a small slice of each transaction on their service, usually about 1 percent.

In exchange for the bitcoin, the buyers typically swap gift cards, which are less traceable than dollars or yuan and, thanks to globalization, broadly useful all over the world. No physical card changes hands; Eric will get the number from the gift card and load it into an app called GiftMe, which generates a barcode that can be used at a checkout counter.

Sometimes the transaction stops there: Eric can use a gift card to buy essentials. But repeated trading requires an endless stream of bitcoin, more than he could mine or purchase on his own. So he has to use the gift cards to acquire more bitcoin, making his money on the spread between what he’s charged to buy it and what he charges to sell it.

As an intermediate step, Eric uses the gift cards he receives to buy other gift cards. This is important for a couple of reasons. First, certain gift cards are desirable to resellers overseas. Cards for electronics are especially attractive — iTunes, PlayStation, and Steam in particular. Those can be sold to customers in Brazil, Pakistan, and elsewhere. Even particular denominations are seen as attractive, because they can be more easily sold on the street. “If I take all these $10 cards, they won’t have any in stock for a month,” Eric says, rummaging through the gift card displays, explaining why he’s willing to travel 45 minutes to Walmarts in his area, just looking for the right cards.

Buying new gift cards also performs a kind of asset protection function for Eric. Scams are commonplace in the gift card trading world. Eric tries to engage in due diligence with his buyers, checking their prior transactions on Paxful. (If there are disputes about transactions, Paxful will help settle them with a court-like process.) He delays the release of bitcoin to his clients until he spends every last cent physically in a store, to ensure that the transaction is legitimate. But exchanging gift cards for gift cards adds to his security. “It’s so I have a card that no one but me and whoever I’m selling it to has seen,” he says. “I do try to avoid getting burned, but also try to avoid being part of burning someone else.”

Armed with fresh cards, Eric then sells them to accumulate more bitcoin. Again, the cards don’t actually change hands; Eric sends the codes through WhatsApp or another messaging system. The margin between what Eric charges for bitcoin and what others charge him represents his profit. Eric, because of how long he’s been in the business, has become adept at finding the best spreads, whereas his customers just want their bitcoin. On his trade with the Kenyan for the $400 gift card, he charged a 28 percent markup, and flipped the cards for bitcoin at a 25 percent markup. He walked away with a 3 percent margin, or $12.

If Eric can replicate that, he can make a living wage; some days, he’ll sell as much as $12,000 in bitcoin, which can translate to $300 or $400. Eric estimates that thousands of people exist in this informal economy, whether selling gift cards on the street, to pawn shops, or through online exchanges. Illicitly acquired gift cards have even been used to pay for opioids.

Technically speaking, reselling gift cards violates the terms of service. Everything else Eric does merely involves trading one legal thing of value for another. But he is very open about the nature of the world he traffics in. “I might sound to you like a scammer,” he says. “Don’t worry, I won’t take it the wrong way.”

The prevalence of illicit activity with gift cards, which invites a crackdown, has posed a threat to his career, with the only saving grace being the relative indifference at Walmart.

WALMART WAS NOT originally Eric’s favorite store to carry out his business. “I was on a first-name basis with everybody at Best Buy and Target for a long time,” he says. But over the summer, he went to ring up an order at a self-checkout counter at Target, and discovered that the company no longer allowed people to buy gift cards with other gift cards. “It changed overnight. The people at the stores were blindsided.” Days later, the same thing happened with Best Buy.

The companies, which do a lot of business in gift cards, had good reason to restrict purchases. A Federal Trade Commission bulletin in May warned Americans of an epidemic of gift card fraud. Specifically, the FTC highlighted callers claiming to be with the IRS or a family member and asking for payments in gift cards. Unsuspecting victims then buy gift cards and hand over the codes. Scammers can use them to either buy goods and services, or flip them in the resale market while simultaneously draining them of funds, making money twice on the same card.

This is just one way gift cards can lead to fraud. “Another way would be hacking accounts: People get the account numbers and take the balance off them,” said Joyce Carter, a vice president with Member Access Pacific, which manages gift and other card programs for businesses and credit unions. This can be done manually, by stealing cards at stores or memorizing card numbers at the sale display, or through more sophisticated means, like computer-aided phishing for card numbers or counterfeiting. There is rich variety in the scams.

Buying gift cards with other gift cards launders fraudulent or illegally obtained card codes into new, legitimate ones. It’s a common method for scam artists. “That type of fraud is happening a lot,” said Carter. According to the National Retail Federation, organized retail crime, which includes but is not limited to gift card fraud, costs retailers $726,351 for every $1 billion in sales.

Though retailers have traditionally not been on the hook for gift card fraud, that liability has shifted somewhat with the advent of chip readers at retail outlets. In particular, merchants that allow customers to swipe cards instead of reading them with chips face chargebacks from issuers on counterfeit transactions. Because of these new rules, retailers started to require that all gift card purchases be made in cash.

In a statement, Target spokesperson Danielle Schumann said the company “takes a comprehensive approach to preventing gift card scams that includes partnerships with law enforcement, technology enhancements and team member training.” That includes barring gift card purchases with other gift cards, as laid out on the Target website. Best Buy did not respond to inquiries, but their website notes that gift card purchases are limited to $500, and Eric’s experience indicates that they no longer let customers make gift-card-for-gift-card swaps.

Eric claimed to me that Walmart, prior to this week, had no restrictions on purchasing gift cards with gift cards. And indeed, I watched him pull it off.

After Eric got the $400 Walmart gift card code from the bitcoin buyer in Kenya, he pasted it into the GiftMe app, generating a bar code. He picked up seven $10 PlayStation cards, and a handful of $20 and $30 cards from PlayStation and Steam. We went to pay at the counter, and the sales clerk methodically scanned and activated the stack of gift cards. Eric was able to pay with the $400 Walmart gift card he’d purchased seconds earlier from someone halfway around the world. He didn’t have to show ID or the physical card. “You have a nice day,” Eric said to the clerk upon leaving.

Eric paid for the gift card used for his purchases. But it could just as easily have been a counterfeit, or a clone, or a stolen card, laundered through Walmart’s transaction into something legitimate. Back in the parking lot, Eric told me, “I walk out of Walmart all the time thinking that someone like me, with less scruples, could be walking out, too, without getting hassled.”

When I asked Carter about Walmart’s practices, she responded that the retailer should be more mindful of fraud. “If I’m buying more than five or six gift cards, [Walmart] might want to think about security,” she said.

Though Walmart has relatively more liability for gift card shenanigans, Carter said the burden remains disproportionately placed on consumer victims, and in particular the issuers that allow their card to be sold at Walmart. “They’re the ones taking the most risk. Most of the time the retailer doesn’t have anything to lose when it comes to fraud.”

Eric’s other interactions with Walmart left him skeptical that the company has much interest in preventing gift card fraud. Recently, a scammer tried to sell him two $1,000 Walmart gift cards, and when he checked them, he saw they were already being spent down online. “These were online orders that hadn’t shipped yet; Walmart could stop the orders to stop the rip-off,” he said. “They had no interest. I got all the way to a supervisor, and he said, ‘We don’t involve ourselves in a consumer’s personal business.’ But it’s fraud! They’re being used to facilitate fraud.”

In other words, Eric says, Walmart views gift cards as cash, and it doesn’t go around wondering whether purchases at their stores are made with stolen dollars. This is true of most retailers. “To them, revenue is revenue,” Eric said. “They very specifically don’t give a fuck about crimes involving their store if they’re not directly liable or directly hurt.” It can also be difficult to solve those crimes, putting a Walmart employee in the position of adjudicating which owner of the $1,000 gift card has the legal right to use it.

In October, Walmart spokesperson Randy Hargrove said in a statement to The Intercept, “We take this issue seriously and have measures designed to help guard against these types of crimes. Like many retailers, we are looking at this issue, the controls we have in place, and we are continuously working to enhance our gift card program to better serve and protect customers.”

It turns out that Walmart was more involved in reforming its practices than I knew. After a year-long investigation by the attorneys general of Pennsylvania and New York, Walmart, Best Buy, and Target announced new nationwide policies to deal with gift cards. Reuters reported that the changes would lead to “prohibiting the redemption of store-branded gift cards for other gift cards” — exactly what Eric wanted to happen.

That’s not quite right. As Pennsylvania Attorney General Josh Shapiro detailed in a press release, the main changes included limits to the monetary value that gift cards are sold for, and how much money can be loaded onto a gift card. As for trading gift cards for gift cards, there are restrictions on the purchase of iTunes, Steam, or Google Play gift cards, some of the main ones sold into the black market.

 
In partnership with
AT THE BEGINNING of October, Amazon was quietly issued a patent that would allow its virtual assistant Alexa to decipher a user’s physical characteristics and emotional state based on their voice. Characteristics, or “voice features,” like language accent, ethnic origin, emotion, gender, age, and background noise would be immediately extracted and tagged to the user’s data file to help deliver more targeted advertising.

The algorithm would also consider a customer’s physical location — based on their IP address, primary shipping address, and browser settings — to help determine their accent. Should Amazon’s patent become a reality, or if accent detection is already possible, it would introduce questions of surveillance and privacy violations, as well as possible discriminatory advertising, experts said.

The civil rights issues raised by the patent are similar to those around facial recognition, another technology Amazon has used as an anchor of its artificial intelligence strategy, and one that it controversially marketed to law enforcement. Like facial recognition, voice analysis underlines how existing laws and privacy safeguards simply aren’t capable of protecting users from new categories of data collection — or government spying, for that matter. Unlike facial recognition, voice analysis relies not on cameras in public spaces, but microphones inside smart speakers in our homes. It also raises its own thorny issues around advertising that targets or excludes certain groups of people based on derived characteristics like nationality, native language, and so on (the sort of controversy that Facebook has stumbled into again and again).
AT THE BEGINNING of October, Amazon was quietly issued a patent that would allow its virtual assistant Alexa to decipher a user’s physical characteristics and emotional state based on their voice. Characteristics, or “voice features,” like language accent, ethnic origin, emotion, gender, age, and background noise would be immediately extracted and tagged to the user’s data file to help deliver more targeted advertising.

The algorithm would also consider a customer’s physical location — based on their IP address, primary shipping address, and browser settings — to help determine their accent. Should Amazon’s patent become a reality, or if accent detection is already possible, it would introduce questions of surveillance and privacy violations, as well as possible discriminatory advertising, experts said.

The civil rights issues raised by the patent are similar to those around facial recognition, another technology Amazon has used as an anchor of its artificial intelligence strategy, and one that it controversially marketed to law enforcement. Like facial recognition, voice analysis underlines how existing laws and privacy safeguards simply aren’t capable of protecting users from new categories of data collection — or government spying, for that matter. Unlike facial recognition, voice analysis relies not on cameras in public spaces, but microphones inside smart speakers in our homes. It also raises its own thorny issues around advertising that targets or excludes certain groups of people based on derived characteristics like nationality, native language, and so on (the sort of controversy that Facebook has stumbled into again and again).
SO MUCH ATTENTION in the midterm elections this year has focused on the gubernatorial race in Georgia between Republican Brian Kemp and Democrat Stacey Abrams that the race for secretary of state, the office Kemp is vacating, has gone largely ignored.

It’s arguably the more important race, since this is the office that will control the state’s voter registration database and any purges made to the voter roll going forward. Equally important, it’s the office that will be responsible for programming all of the state’s currently paperless voting machines that can’t be audited, though Georgia will be looking to replace these machines with an undetermined model next year. Both of these factors could make Georgia a hotbed for voter suppression tactics and vote-counting integrity in the 2020 presidential elections, experts said.
APPARENTLY FUELED BY anti-Semitism and the bogus narrative that outside forces are scheming to exterminate the white race, Robert Bowers murdered 11 Jewish congregants as they gathered inside their Pittsburgh synagogue, federal prosecutors allege. But despite long-running international efforts to debunk the idea of a “white genocide,” Facebook was still selling advertisers the ability to market to those with an interest in that myth just days after the bloodshed.

Earlier this week, The Intercept was able to select “white genocide conspiracy theory” as a pre-defined “detailed targeting” criterion on the social network to promote two articles to an interest group that Facebook pegged at 168,000 users large and defined as “people who have expressed an interest or like pages related to White genocide conspiracy theory.” The paid promotion was approved by Facebook’s advertising wing. After we contacted the company for comment, Facebook promptly deleted the targeting category, apologized, and said it should have never existed in the first place.

Our reporting technique was the same as one used by the investigative news outlet ProPublica to report, just over one year ago, that in addition to soccer dads and Ariana Grande fans, “the world’s largest social network enabled advertisers to direct their pitches to the news feeds of almost 2,300 people who expressed interest in the topics of ‘Jew hater,’ ‘How to burn jews,’ or, ‘History of “why jews ruin the world.”’” The report exposed how little Facebook was doing to vet marketers, who pay the company to leverage personal information and inclinations in order to gain users’ attention — and who provide the foundation for its entire business model. At the time, ProPublica noted that Facebook “said it would explore ways to fix the problem, such as limiting the number of categories available or scrutinizing them before they are displayed to buyers.” Rob Leathern, a Facebook product manager, assured the public, “We know we have more work to do, so we’re also building new guardrails in our product and review processes to prevent other issues like this from happening in the future.”

Leathern’s “new guardrails” don’t seem to have prevented Facebook from manually approving our ad buy the same day it was submitted, despite its explicit labeling as “White Supremacy – Test.”
GOOGLE CEO SUNDAR PICHAI has refused to answer a list of questions from U.S. lawmakers about the company’s secretive plan for a censored search engine in China.

In a letter newly obtained by The Intercept, Pichai told a bipartisan group of six senators that Google could have “broad benefits inside and outside of China,” but said he could not share details about the censored search engine because it “remains unclear” whether the company “would or could release a search service” in the country.

Pichai’s letter contradicts the company’s search engine chief, Ben Gomes, who informed staff during a private meeting that the company was aiming to release the platform in China between January and April 2019. Gomes told employees working on the Chinese search engine that they should get it ready to be “brought off the shelf and quickly deployed.”

According to sources and confidential Google documents, the search engine for China, codenamed Dragonfly, was designed to comply with the strict censorship regime imposed by China’s ruling Communist Party. It would restrict people’s access to broad categories of information, blacklisting phrases like “human rights,” “student protest,” and “Nobel Prize.”

The Chinese platform was designed to link people’s searches to their phone number, track their location, and then share that data with a Chinese partner company. This would make it easy to track individual users’ searches, raising concerns that any person in China using Google to seek out information banned by the government could be at risk of interrogation or detention if security agencies were to obtain copies of their search records.
WHILE THE WORLD is grappling with the apparent grisly murder of Saudi dissident and Washington Post journalist Jamal Khashoggi, the Saudi government decided to announce a new band of influential Western allies, some plucked from the uppermost echelon of Silicon Valley, who would serve on an advisory board for Neom, the Saudi government’s improbable, exorbitant plan to build a “mega city” in the desert.

But almost as soon as his participation was revealed, Sam Altman, head of famed venture capital firm Y Combinator, announced that he is “suspending” his role with Neom, while two others on the star-studded list denied that they were participating.

Altman, along with legendary tech investor Marc Andreessen, notorious Uber founder (and ousted ex-CEO) Travis Kalanick, IDEO CEO Tim Brown, and Dan Doctoroff of Sidewalk Labs, a subsidiary of Google-owner Alphabet, were among those listed as members of the new board. Given that the United States itself is now forced into a momentarily uncomfortable spot given its longtime affection for and deep political ties to the Saudis, this was a less than ideal time for Americans to come out as friends of the kingdom.
A NEW REPORT from the U.S. Government Accountability Office brings both good and bad news. For governments around the world that might like to sabotage America’s military technology, the good news is that this would be all too easy to do: Testers at the Department of Defense “routinely found mission-critical cyber vulnerabilities in nearly all weapon systems that were under development” over a five-year period, the report said. For Americans, the bad news is that up until very recently, no one seemed to care enough to fix these security holes.

In 1991, the report noted, the U.S. National Research Council warned that “system disruptions will increase” as the use of computers and networks grows and as adversaries attack them. The Pentagon more or less ignored this and at least five subsequent warnings on the subject, according to the GAO, and hasn’t made a serious effort to safeguard the vast patchwork of software that controls planes, ships, missiles, and other advanced ordnance against hackers.

The sweeping report drew on nearly 30 years of published research, including recent assessments of the cybersecurity of specific weapon systems, as well as interviews with personnel from the Department of Defense, the National Security Agency, and weapons-testing bodies. It covered a broad span of American weapons, examining systems at all of the service branches and in space.

The report found that “mission-critical cyber vulnerabilities” cropped up routinely during weapons development and that test teams “easily” took over real systems without detection “using relatively simple tools and techniques,” exploiting “basic issues such as poor password management and unencrypted communications.” Testers could also download and delete data, in one cases exfiltrating 100 gigabytes of material, and could tap into operators’ terminals, in one instance popping up computer dialogs asking the operators “to insert two quarters to continue.” But a malicious attacker could pull off much worse than jokes about quarters, warns the GAO: “In one case, the test team took control of the operators’ terminals. They could see, in real-time, what the operators were seeing on their screens and could manipulate the system.”

Posing as surrogates for, say, Russian or Chinese military hackers, testers sometimes found easy victories. “In some cases,” the GAO found, “simply scanning a system caused parts of the system to shut down,” while one “test team was able to guess an administrator password in nine seconds.” The testers found embarrassing, elementary screw-ups of the sort that would get a middle school computer lab administrator in trouble, to say nothing of someone safeguarding lethal weapon systems. For example, “multiple weapon systems used commercial or open source software, but did not change the default password when the software was installed, which allowed test teams to look up the password on the Internet.”
“WE HAVE TO be focused on what we want to enable,” said Ben Gomes, Google’s search engine chief. “And then when the opening happens, we are ready for it.”

It was Wednesday, July 18, and Gomes was addressing a team of Google employees who were working on a secretive project to develop a censored search engine for China, which would blacklist phrases like “human rights,” “student protest,” and “Nobel Prize.”

“You have taken on something extremely important to the company,” Gomes declared, according to a transcript of his comments obtained by The Intercept. “I have to admit it has been a difficult journey. But I do think a very important and worthwhile one. And I wish ourselves the best of luck in actually reaching our destination as soon as possible.”

Gomes joked about the unpredictability of President Donald Trump and groaned about the ongoing trade war between the U.S. and China, which has slowed down Google’s negotiations with Communist Party officials in Beijing, whose approval Google requires to launch the censored search engine.
“WE HAVE TO be focused on what we want to enable,” said Ben Gomes, Google’s search engine chief. “And then when the opening happens, we are ready for it.”

It was Wednesday, July 18, and Gomes was addressing a team of Google employees who were working on a secretive project to develop a censored search engine for China, which would blacklist phrases like “human rights,” “student protest,” and “Nobel Prize.”

“You have taken on something extremely important to the company,” Gomes declared, according to a transcript of his comments obtained by The Intercept. “I have to admit it has been a difficult journey. But I do think a very important and worthwhile one. And I wish ourselves the best of luck in actually reaching our destination as soon as possible.”

Gomes joked about the unpredictability of President Donald Trump and groaned about the ongoing trade war between the U.S. and China, which has slowed down Google’s negotiations with Communist Party officials in Beijing, whose approval Google requires to launch the censored search engine.
AFTER CALIFORNIA PASSED the most sweeping online privacy law in the nation this summer, big tech went back to the state legislature to weaken it. While that effort fizzled before the end of the state’s legislative session, a more insidious strategy emerged this week: going around California and appealing to Congress.

Alastair Mactaggart, who led the California effort, told The Intercept that a Wednesday hearing in Congress left him concerned that Congress might pre-empt the state legislation at the behest of giant tech firms.

“Tech has had zero regulation,” Mactaggart said in an interview. “For them it’s been this Wild West of being able to monetize information any which way. They will pull out all the stops to try to get back to where they were.”

Mactaggart, a Bay Area real estate developer, became an unlikely activist when he bankrolled a ballot measure that, among other things, would require tech companies to reveal what personal information they collected on users, allow users to opt out of the sale of their data to third parties, and impose fines for data breaches. Tech firms fought it vigorously, but were hampered by a series of scandals like Facebook’s release of data to Cambridge Analytica, and widespread popular support for some limits on persistent surveillance.

Tech firms, fearing being locked into a policy they could only change at the ballot, encouraged the state legislature to get involved. The California House and Senate passed a law substantially similar to the initiative, with unanimous support in both chambers. Gov. Jerry Brown signed it in June.

The law doesn’t take effect until January 2020, and big tech’s hope was to water it down before that date. Industry representatives went to Congress this week after failing to get California lawmakers to narrow the definition of “selling” data, which would have rendered some of the protections of the law moot.

The Senate Commerce Committee held a hearing on Wednesday where leaders of six tech and telecom companies — Amazon, Google, Apple, Twitter, AT&T, and Charter Communications — endorsed a federal consumer privacy standard. In their conception, this would pre-empt any state data protection laws.

It’s an eerie echo of the federal pre-emption that Wall Street banks received and used to great effect during the run-up to the housing bubble. In 2002, Georgia passed an anti-predatory lending law, and both the Office of Thrift Supervision and the Office of the Comptroller of the Currency ruled that banks they regulated simply did not have to comply with it. This created a chilling effect, as states declined to crack down on the rampant fraud in the mortgage industry. And the financial crisis was the result.

At the hearing, the companies criticized the California statute, in particular its definition of “personal information,” which they consider overbroad. Amazon vice president Andrew DeVore called the statute “confusing and difficult to comply with,” adding that it “may actually undermine important privacy-protective practices.” Alone among the witnesses, Apple’s Bud Tribble did say that while federal legislation should pre-empt state law, it must also “meet the bar of protecting consumers meaningfully.” However, he did not elaborate on what that protection should entail.

EIGHT YEARS AGO, when Google announced that it would pull out of China, the company released a statement explaining that the Chinese government had been “crystal clear” that “self-censorship is a non-negotiable legal requirement” for operating in the country.

But now that the firm is considering a relaunch in China with a new search platform, an initiative codenamed Project Dragonfly, Google is hedging on whether they believe the Chinese government censors its citizens.

Keith Enright, Google’s chief privacy officer, discussed the China project under questioning from several senators during a commerce committee hearing on Wednesday.

“In your opinion, does China engage in censoring its citizens?” asked Sen. Ted Cruz, R-Texas, during the hearing.

“As the privacy representative of Google,” said Enright, “I am not sure I have an informed opinion on that question.”
OMNIPRESENT FACIAL RECOGNITION has become a golden goose for law enforcement agencies around the world. In the United States, few are as eager as the Department of Homeland Security. American airports are currently being used as laboratories for a new tool that would automatically scan your face — and confirm your identity with U.S. Customs and Border Protection — as you prepare to board a flight, despite the near-unanimous objections from privacy advocates and civil libertarians, who call such scans invasive and pointless.

According to a new report on the Biometric Entry-Exit Program by DHS itself, we can add another objection: Your flight could be late.

Although the new report, published by Homeland Security’s Office of the Inspector General, is overwhelmingly supportive in its evaluation of airport-based biometric surveillance — the practice of a computer detecting your face and pairing it with everything else in the system — the agency notes some hurdles from a recent test code-named “Sprint 8.” Among them, the report notes with palpable frustration, was that airlines insist on letting their passengers depart on time, rather than subjecting them to a Homeland Security surveillance prototype plagued by technical issues and slowdowns:

Demanding flight departure schedules posed other operational problems that significantly hampered biometric matching of passengers during the pilot in 2017. Typically, when incoming flights arrived behind schedule, the time allotted for boarding departing flights was reduced. In these cases, CBP allowed airlines to bypass biometric processing in order to save time. As such, passengers could proceed with presenting their boarding passes to gate agents without being photographed and biometrically matched by CBP first. We observed this scenario at the Atlanta Hartsfield-Jackson International Airport when an airline suspended the biometric matching process early to avoid a flight delay. This resulted in approximately 120 passengers boarding the flight without biometric confirmation.
A SCIENTIST WHO quit Google over its plan to build a censored search engine in China has told U.S. senators that some company employees may have “actively subverted” an internal privacy review of the system.

Jack Poulson resigned from Google in August after The Intercept reported that a group of the internet giant’s staffers was secretly working on a search engine for China that would remove content about subjects such as human rights, democracy, peaceful protest, and religion. “I view our intent to capitulate to censorship and surveillance demands in exchange for access to the Chinese market as a forfeiture of our values and governmental negotiating position across the globe,” Poulson told his bosses.

Now, Poulson has sent a letter to members of the Senate Committee on Commerce, Science, and Transportation ahead of a hearing on Wednesday at which Keith Enright, Google’s chief privacy officer, is scheduled to appear. Despite a major internal and external backlash over a period of almost two months, Google has so far refused to publicly address questions about its China censorship plan, code-named Dragonfly. The appearance of Enright on Capitol Hill is likely to be the first time a representative of the company is forced to provide answers about the project.

In his letter to the senators, Poulson said that there has been “a pattern of unethical and unaccountable decision making from company leadership” at Google. He called on the lawmakers to pressure Enright to respond to concerns raised by 14 leading human rights groups, who said in late August that Dragonfly could result in Google “directly contributing to, or [becoming] complicit in, human rights violations.”
THE UNITED NATIONS accidentally published passwords, internal documents, and technical details about websites when it misconfigured popular project management service Trello, issue tracking app Jira, and office suite Google Docs.

The mistakes made sensitive material available online to anyone with the proper link, rather than only to specific users who should have access. Affected data included credentials for a U.N. file server, the video conferencing system at the U.N.’s language school, and a web development environment for the U.N.’s Office for the Coordination of Humanitarian Affairs. Security researcher Kushagra Pathak discovered the accidental leak and notified the U.N. about what he found a little over a month ago. As of today, much of the material appears to have been taken down.

In an online chat, Pathak said he found the sensitive information by running searches on Google. The searches, in turn, produced public Trello pages, some of which contained links to the public Google Docs and Jira pages.

Trello projects are organized into “boards” that contain lists of tasks called “cards.” Boards can be public or private. After finding one public Trello board run by the U.N., Pathak found additional public U.N. boards by using “tricks like by checking if the users of one Trello board are also active on some other boards and so on.” One U.N. Trello board contained links to an issue tracker hosted on Jira, which itself contained even more sensitive information. Pathak also discovered links to documents hosted on Google Docs and Google Drive that were configured to be accessible to anyone who knew their web addresses. Some of these documents contained passwords.
FACEBOOK’S TOTAL INABILITY to keep itself from being a convenient tool for genocidal incitement in Myanmar has been well-covered, now a case study in how a company with such immense global power can so completely fail to use it for good. But a new report released this week by the United Nations fact-finding mission in Myanmar, where calls for the slaughter of Muslims have enjoyed all the convenience of a modern Facebook signal boost, makes clear just how unprepared the company was for its role in an ethnic massacre.

In a recent New Yorker profile of Facebook founder and CEO Mark Zuckerberg, he responds to his company’s role in the crisis — which the U.N. has described as “determining” — with all the urgency and guilt of a botched restaurant order: “I think, fundamentally, we’ve been slow at the same thing in a number of areas, because it’s actually the same problem. But, yeah, I think the situation in Myanmar is terrible.” Zuckerberg added that the company needs to “move from what is fundamentally a reactive model” when it comes to blocking content that’s fueled what the U.N. described last year as a “textbook example of ethnic cleansing.”

The new report reveals just how broken this “reactive model” truly is.

According to the 479-page document, and as flagged in a broader Guardian story this week, “the Mission itself experienced a slow and ineffective response from Facebook when it used the standard reporting mechanism to alert the company to a post targeting a human rights defender for his alleged cooperation with the Mission.” What follows is the most clear-cut imaginable violation of Facebook’s rules, followed by the most abject failure to enforce them when it mattered most:

The post described the individual as a “national traitor”, consistently adding the adjective “Muslim”. It was shared and re-posted over 1,000 times. Numerous comments to the post explicitly called for the person to be killed, in unequivocal terms: “Beggar-dog species. As long as we are feeling sorry for them, our country is not at peace. These dogs need to be completely removed.” “If this animal is still around, find him and kill him. There needs to be government officials in NGOs.” “Wherever they are, Muslim animals don’t know to be faithful to the country.” “He is a Muslim. Muslims are dogs and need to be shot.” “Don’t leave him alive. Remove his whole race. Time is ticking.” The Mission reported this post to Facebook on four occasions; in each instance the response received was that the post was examined but “doesn’t go against one of [Facebook’s] specific Community Standards”. The Mission subsequently sent a message to an official Facebook email account about the matter but did not receive a response. The post was finally removed several weeks later but only through the support of a contact at Facebook, not through the official channel. Several months later, however, the Mission found at least 16 re-posts of the original post still circulating on Facebook. In the weeks and months after the post went online, the human rights defender received multiple death threats from Facebook users, warnings from neighbours, friends, taxi drivers and other contacts that they had seen his photo and the posts on Facebook, and strong suggestions that the post was an early warning. His family members were also threatened. The Mission has seen many similar cases where individuals, usually human rights defenders or journalists, become the target of an online hate campaign that incites or threatens violence.
GOOGLE BOSSES HAVE forced employees to delete a confidential memo circulating inside the company that revealed explosive details about a plan to launch a censored search engine in China, The Intercept has learned.

The memo, authored by a Google engineer who was asked to work on the project, disclosed that the search system, codenamed Dragonfly, would require users to log in to perform searches, track their location — and share the resulting history with a Chinese partner who would have “unilateral access” to the data.

The memo was shared earlier this month among a group of Google employees who have been organizing internal protests over the censored search system, which has been designed to remove content that China’s authoritarian Communist Party regime views as sensitive, such as information about democracy, human rights, and peaceful protest.

According to three sources familiar with the incident, Google leadership discovered the memo and were furious that secret details about the China censorship were being passed between employees who were not supposed to have any knowledge about it. Subsequently, Google human resources personnel emailed employees who were believed to have accessed or saved copies of the memo and ordered them to immediately delete it from their computers. Emails demanding deletion of the memo contained “pixel trackers” that notified human resource managers when their messages had been read, recipients determined.
GOOGLE BUILT A prototype of a censored search engine for China that links users’ searches to their personal phone numbers, thus making it easier for the Chinese government to monitor people’s queries, The Intercept can reveal.

The search engine, codenamed Dragonfly, was designed for Android devices, and would remove content deemed sensitive by China’s ruling Communist Party regime, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest.

Previously undisclosed details about the plan, obtained by The Intercept on Friday, show that Google compiled a censorship blacklist that included terms such as “human rights,” “student protest,” and “Nobel Prize” in Mandarin.

Leading human rights groups have criticized Dragonfly, saying that it could result in the company “directly contributing to, or [becoming] complicit in, human rights violations.” A central concern expressed by the groups is that, beyond the censorship, user data stored by Google on the Chinese mainland could be accessible to Chinese authorities, who routinely target political activists and journalists.

Sources familiar with the project said that prototypes of the search engine linked the search app on a user’s Android smartphone with their phone number. This means individual people’s searches could be easily tracked – and any user seeking out information banned by the government could potentially be at risk of interrogation or detention if security agencies were to obtain the search records from Google.
GOOGLE BUILT A prototype of a censored search engine for China that links users’ searches to their personal phone numbers, thus making it easier for the Chinese government to monitor people’s queries, The Intercept can reveal.

The search engine, codenamed Dragonfly, was designed for Android devices, and would remove content deemed sensitive by China’s ruling Communist Party regime, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest.

Previously undisclosed details about the plan, obtained by The Intercept on Friday, show that Google compiled a censorship blacklist that included terms such as “human rights,” “student protest,” and “Nobel Prize” in Mandarin.

Leading human rights groups have criticized Dragonfly, saying that it could result in the company “directly contributing to, or [becoming] complicit in, human rights violations.” A central concern expressed by the groups is that, beyond the censorship, user data stored by Google on the Chinese mainland could be accessible to Chinese authorities, who routinely target political activists and journalists.

Sources familiar with the project said that prototypes of the search engine linked the search app on a user’s Android smartphone with their phone number. This means individual people’s searches could be easily tracked – and any user seeking out information banned by the government could potentially be at risk of interrogation or detention if security agencies were to obtain the search records from Google.
LINKNYC KIOSKS HAVE become a familiar eyesore to New Yorkers. Over 1,600 of these towering, nine-and-a-half-foot monoliths — their double-sided screens festooned with ads and fun facts — have been installed across the city since early 2016. Mayor Bill de Blasio has celebrated their ability to provide “the fastest and largest municipal Wi-Fi network in the world” as “a critical step toward a more equal, open, and connected city for every New Yorker, in every borough.” Anyone can use the kiosks’ Android tablets to search for directions and services; they are also equipped with charging stations, 911 buttons, and phones for free domestic calls.

But even as the kiosks have provided important services to connect New Yorkers, they may also represent a troubling expansion of the city’s surveillance network, potentially connecting every borough to a new level of invasive monitoring. Each kiosk has three cameras, 30 sensors, and heightened sight lines for viewing above crowds.

Since plans for LinkNYC were first unveiled, journalists, residents, and civil liberties experts have raised concerns that the internet kiosks might be storing sensitive data about its users and possibly tracking their movements. For the last two years, the American Civil Liberties Union, Electronic Frontier Foundation, and a small but vocal group of activists — including ReThink LinkNYC, a grassroots anti-surveillance group, and the anonymous Stop LinkNYC coalition — have highlighted the kiosk’s potential to track locations, collect personal information, and fuel mass surveillance.

Now an undergraduate researcher has discovered indications in LinkNYC code — accidentally made public on the internet — that LinkNYC may be actively planning to track users’ locations.
LINKNYC KIOSKS HAVE become a familiar eyesore to New Yorkers. Over 1,600 of these towering, nine-and-a-half-foot monoliths — their double-sided screens festooned with ads and fun facts — have been installed across the city since early 2016. Mayor Bill de Blasio has celebrated their ability to provide “the fastest and largest municipal Wi-Fi network in the world” as “a critical step toward a more equal, open, and connected city for every New Yorker, in every borough.” Anyone can use the kiosks’ Android tablets to search for directions and services; they are also equipped with charging stations, 911 buttons, and phones for free domestic calls.

But even as the kiosks have provided important services to connect New Yorkers, they may also represent a troubling expansion of the city’s surveillance network, potentially connecting every borough to a new level of invasive monitoring. Each kiosk has three cameras, 30 sensors, and heightened sight lines for viewing above crowds.

Since plans for LinkNYC were first unveiled, journalists, residents, and civil liberties experts have raised concerns that the internet kiosks might be storing sensitive data about its users and possibly tracking their movements. For the last two years, the American Civil Liberties Union, Electronic Frontier Foundation, and a small but vocal group of activists — including ReThink LinkNYC, a grassroots anti-surveillance group, and the anonymous Stop LinkNYC coalition — have highlighted the kiosk’s potential to track locations, collect personal information, and fuel mass surveillance.

Now an undergraduate researcher has discovered indications in LinkNYC code — accidentally made public on the internet — that LinkNYC may be actively planning to track users’ locations.
FACEBOOK CHIEF OPERATING officer Sheryl Sandberg draped herself in the star-spangled banner of American principles before today’s Senate Select Intelligence Committee hearing on social media. Sandberg proclaimed that democratic values of free expression were integral to the company’s conscience. “We would only operate in a country where we could do so in keeping with our values,” she went on. Either this was a lie told under oath, or Facebook has some pretty lousy values.
LEADING HUMAN RIGHTS groups are calling on Google to cancel its plan to launch a censored version of its search engine in China, which they said would violate the freedom of expression and privacy rights of millions of internet users in the country.

A coalition of 14 organizations — including Amnesty International, Human Rights Watch, Reporters Without Borders, Access Now, the Committee to Protect Journalists, the Electronic Frontier Foundation, the Center for Democracy and Technology, PEN International, and Human Rights in China — issued the demand Tuesday in an open letter addressed to the internet giant’s CEO, Sundar Pichai. The groups said the censored search engine represents “an alarming capitulation by Google on human rights” and could result in the company “directly contributing to, or [becoming] complicit in, human rights violations.”

The letter is the latest major development in an ongoing backlash over the censored search platform, code-named Dragonfly, which was first revealed by The Intercept earlier this month. The censored search engine would remove content that China’s ruling Communist Party regime views as sensitive, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to confidential Google documents.

Google launched a censored search engine in China in 2006, but ceased operating the service in the country in 2010, citing Chinese government efforts to limit free speech, block websites, and hack Google’s computer systems. The open letter released Tuesday asks Google to reaffirm the commitment it made in 2010 to no longer provide censored search in China.
ON AUGUST 13, FACEBOOK shut down the English-language page of Telesur, blocking access for roughly half a million followers of the leftist media network until it was abruptly reinstated two days later. Facebook has provided three different explanations for the temporary disappearing, all contradicting one another, and not a single one making sense.

Telesur was created by Venezuela’s then-President Hugo Chávez in 2005 and co-funded by hemispheric neighbors Cuba, Bolivia, Nicaragua, and Uruguay — Argentina pulled support for the web and cable property in 2016. As a state-owned media property, it exists somewhere on the same continuum as RT and Al Jazeera, though like the former, Telesur has been criticized as a nakedly partisan governmental mouthpiece, and like the latter, it does engage in real news reporting. But putting aside questions of bias and agenda, Telesur does seem to exist on a separate plane than, say, Infowars, which exists primarily to peddle its particular, patently false genre of right-wing paranoia fan fiction packaged as news (and brain pills), as opposed to some garden-variety political agenda. Unlike RT, Telesur hasn’t been singled-out for a role in laundering disinformation for military intelligence purposes, nor is it a hoax factory, à la Alex Jones.

So it was unexpected when Telesur English blinked out of existence on the 13th, and even stranger when Facebook struggled to explain its own actions. At the time of its suspension, Telesur received this boilerplate message from Facebook:

Hello,

Your Page “teleSUR English” has been removed for violating our Terms of Use. A Facebook Page is a distinct presence used solely for business or promotional purposes. Among other things, Pages that are hateful, threatening or obscene are not allowed. We also take down Pages that attack an individual or group, or that are set up by an unauthorised individual. If your Page was removed for any of the above reasons, it will not be reinstated. Continued misuse of Facebook’s features could result in the permanent loss of your account.

The Facebook Team

Later that day, a Facebook customer support agent told the network that the suspension appeared to be due to a technical glitch — a go-to explanation for the company — rather than a violation of the company’s Terms of Use, adding that the issue was “under analysis by the engineering department.”
GOOGLE BOSSES HAVE broken their silence on the company’s plan to launch a censored search engine in China amid mounting internal protests over the project.

On Thursday, CEO Sundar Pichai admitted to employees during an all-hands meeting that the censorship project – code-named Dragonfly – had been “in an exploration stage for quite a while now,” according to two sources who heard his remarks. Pichai emphasized his belief that Google should return to China, but claimed that the company was “not close to launching a search product in China.” Facing employee criticism for shrouding Dragonfly in secrecy, Pichai vowed that “we’ll definitely be transparent as we get closer to actually having a plan of record.”

Google co-founder Sergey Brin also spoke at Thursday’s meeting — and remarkably stated that he knew nothing about Dragonfly until The Intercept exposed it earlier this month. Back in 2006, Google launched a censored search engine in China. But four years later, in March 2010, it pulled the service out of the country, citing Chinese government efforts to limit free speech, block websites, and hack Google’s computer systems. At that time, Brin was a vocal opponent of the censorship. During Thursday’s meeting, Brin told Google employees that Dragonfly would have “certain trade-offs” but said the process was “slow-going and complicated.”

Both Pichai and Brin’s remarks to Google employees raise a number of questions. Pichai’s attempt to portray Dragonfly as an “exploratory” project contradicts internal Google documents and statements issued by senior Google officials on Dragonfly that The Intercept has seen.
GOOGLE EMPLOYEES ARE demanding answers from the company’s leadership amid growing internal protests over plans to launch a censored search engine in China.

Staff inside the internet giant’s offices have agreed that the censorship project raises “urgent moral and ethical issues” and have circulated a letter saying so, calling on bosses to disclose more about the company’s work in China, which they say is shrouded in too much secrecy, according to three sources with knowledge of the matter.

The internal furor began after The Intercept earlier this month revealed details about the censored search engine, which would remove content that China’s authoritarian government views as sensitive, such as information about political dissidents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, leaked Google documents disclosed. The search platform is to be launched via an Android app, pending approval from Chinese officials.

The censorship plan – code-named Dragonfly – was not widely known within Google. Prior to its public exposure, only a few hundred of Google’s 88,000 employees had been briefed about the project – around 0.35 percent of the total workforce. When the news spread through the company’s offices across the world, many employees expressed anger and confusion.

Now, a letter has been circulated among staff calling for Google’s leadership to recognize that there is a “code yellow” situation – a kind of internal alert that signifies a crisis is unfolding. The letter suggests that the Dragonfly initiative violates an internal Google artificial intelligence ethical code, which says that the company will not build or deploy technologies “whose purpose contravenes widely accepted principles of international law and human rights.”
BY MISCONFIGURING PAGES on Trello, a popular project management website, the governments of the United Kingdom and Canada exposed to the entire internet details of software bugs and security plans, as well as passwords for servers, official internet domains, conference calls, and an event-planning system.

The U.K. government also exposed a small quantity of code for running a government website, as well as a limited number of emails. All told, between the two governments, a total of 50 Trello pages, known on the site as “boards,” were published on the open web and indexed by Google.

The computer researcher who found the sensitive material, Kushagra Pathak, had disclosed just this past April a wide swath of additional private data exposed to the public on Trello, which is widely used by software developers, among others. That earlier disclosure revealed how, on dozens of public Trello boards run by various organizations and individuals, the information available included email and social media credentials, as well as specific information on unfixed bugs and security vulnerabilities. Pathak even found an NGO sharing login details to a donor management software database, which in turn contained, he said, personally identifiable information and financial records on donors. In both the April and new security research, the sensitive data on Trello was tracked down starting with a simple Google query.

The data exposures underscore how easy it has become to improperly leak sensitive data in the era of cloud computing. More broadly, they show how the use and development of software has become a complex endeavor, involving a wide range of independent online systems, and how this complexity itself represents a security risk, encouraging users and developers to take shortcuts intended to cut through the morass. Tools like Trello can help master the tangle of development in a safe and constructive way, but can also be misused.
BLACK HAT HAS established its reputation as a world-famous hacker conference by drawing attention to the complex problems in cybersecurity that no one else has solved, or even noticed. For the last two decades, its packed discussions, known as briefings, have made headlines by featuring highly technical experts revealing previously unknown security vulnerabilities. In recent years, hackers have demonstrated their ingenuity in overcoming a smart gun’s protections, tampering with voting machines, and shutting down critical city infrastructure.

But last week, for the first time in Black Hat’s history, the conference invited speakers to address gender discrimination, sexual assault, mental health, and substance abuse. The conference’s inaugural Community Track briefings provided a window into problems in the cybersecurity world that have long been hidden in plain sight. At the Mandala Bay Convention Center in Las Vegas, certified rape crisis counselors spoke alongside engineers and emergency physicians about some of the challenges facing hackers as people.

Many leading cybersecurity conferences, such as Black Hat, Def Con, and RSA, have seemed reluctant to outgrow their beginnings as boys’ clubs, even as their attendees have become more professional and diverse. Over the last decade, journalists, hackers, and advocates have documented a range of abusive incidents at these events. Earlier this year, I spoke to two dozen women who worked in cybersecurity, many of whom had reported incidents of harassment only to be dismissed or ignored by organizers of these events. Some said that the systemic nature of sexism at these annual events felt like a feature, not a bug. In this landscape, Black Hat’s Community Track — along with an expanded range of initiatives to support working mothers, survivors of sexual assault, queer hackers, and recovering alcoholics, among others — represented a welcome step.

Countering Stigma and Silence

Cybersecurity is, by all accounts, an emotionally demanding field. In a briefing on burnout, depression, and suicide in the hacker community, Christian Dameff, a physician, and Jay Radcliffe, a security researcher, explained the unique stressors that often accompany jobs in the information security sector, such as social isolation and abnormal sleep schedules. They cited an Information Systems Security Association study from 2018, in which 68 percent of respondents described work-life balance as a major problem. Contributing to this, they said, was a talent shortage that increased demands on an already overworked staff. The field’s self-image of strength and toughness, Radcliffe said, could also serve to further isolate employees from seeking help.

In her talk on addiction in infosec, Jamie Tomasello, an engineer at Duo Security, detailed the relationship between stress and alcoholism. She described the particular ways in which the imperative to drink overlapped with career opportunities — and an occasionally toxic conference culture. “I built rapport, trust, and respect while drinking,” she said. “I was included in conversations and projects that I wouldn’t have been in without that glass in hand.” As a recovering alcoholic, she noted, it could be difficult to attend conferences like Black Hat that were fueled by networking and afterparties at bars. She offered alternatives for managers and companies hoping to organize more inclusive events for employees struggling with alcoholism, and praised the introduction of sobriety meetings. Employee wellness programs, she stressed, needed “to extend beyond health, food, gym memberships.”

In their respective talks on the importance of neurodiversity, Joe Slowik, a veteran with post-traumatic stress disorder who now works in network defense, and Rhett Greenhagen, a senior security researcher for McAfee’s Advanced Programs Group who has Asperger’s, each echoed this call for empathy. Slowik said that he had “rage-submitted” his talk, “Demystifying PTSD in Information Security,” to the conference after coming across an article that failed to distinguish between burnout, high stress, and an actual PTSD diagnosis. He pushed back against a “one-size-fits-all” approach to dealing with survivors of sexual and military trauma. Alienation, depression, and disengagement were common symptoms, he said, and he described his daily work as giving him his confidence back. “Don’t shun, ignore, or pity. Engage,” he advised those who might work with colleagues with PTSD.

Greenhagen described the ways in which being a person with Asperger’s gave him an interest in pattern recognition — “It is extremely hard for us to not solve a puzzle,” he noted — and a major leg up as a network security analyst. While the evidence is chiefly anecdotal, it is suspected that there is a prevalence of hackers on the autism spectrum. But for all the pleasures of the demanding work, Greenhagen also acknowledged some serious downsides to working on a team. Sensory distractions and small talk interfered with his ability to do his job — an experience that was echoed by hackers with an autism spectrum disorder diagnosis who took part in an informal survey conducted by Stacy Thayer, a psychologist who spoke alongside Greenhagen. “I don’t think I’ll ever have a normal social interaction with other co-workers,” Greenhagen said. “Either there were people who absolutely adored me, even if they found stupid crap I did hilarious. Or there were people who couldn’t stand me. What made it livable was that it wasn’t a huge percentage. I had more people stand up for me and realize I have shortcomings.”

The briefings focused on mental health were by turns moving and vexing. Some of the men emphasized soul-baring, engaging their captive audience in a personal story, at the expense of skill-building. Race was notably absent as a topic of discussion. So too were the ways in which diagnoses such as alcoholism, PTSD, burnout, and Asperger’s might differently affect people across genders and identities. Given the graphic nature of the discussions about suicide in the PTSD and burnout talks, trigger warnings would have been prudent. But it was precisely the elementary nature of some of these discussions that testified to their novelty in the community — and hence their necessity.

GOOGLE’S FORMER HEAD of free expression issues in Asia has slammed the internet giant’s plan to launch a censored search engine in China, calling it a “stupid move” that would violate widely held human rights principles.

As The Intercept first reported last week, Google has been quietly developing a search platform for China that would remove content that China’s authoritarian government views as sensitive, such as information about political opponents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to internal Google documents.

Lokman Tsui, Google’s head of free expression for Asia and the Pacific between 2011 and 2014, read the leaked censorship plans and said he was disturbed by the details. “This is just a really bad idea, a stupid, stupid move,” he told The Intercept in an interview. “I feel compelled to speak out and say that this is not right.”
GOOGLE’S FORMER HEAD of free expression issues in Asia has slammed the internet giant’s plan to launch a censored search engine in China, calling it a “stupid move” that would violate widely held human rights principles.

As The Intercept first reported last week, Google has been quietly developing a search platform for China that would remove content that China’s authoritarian government views as sensitive, such as information about political opponents, free speech, democracy, human rights, and peaceful protest. It would “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to internal Google documents.

Lokman Tsui, Google’s head of free expression for Asia and the Pacific between 2011 and 2014, read the leaked censorship plans and said he was disturbed by the details. “This is just a really bad idea, a stupid, stupid move,” he told The Intercept in an interview. “I feel compelled to speak out and say that this is not right.”
A BIPARTISAN GROUP of six U.S. senators is demanding that Google CEO Sundar Pichai explain the company’s plan to launch a censored version of its search engine in China.

Since spring 2017, the internet giant has been developing a censored Android search app to launch in the country as part of a secretive project code-named Dragonfly, The Intercept revealed on Wednesday. The app would manipulate search results in accordance with strict censorship rules in China that are mandated by the ruling Communist Party regime, which restricts people’s access to information about political opponents, free speech, democracy, human rights, and peaceful protest. The censored Google search has been designed to “blacklist sensitive queries” so that “no results will be shown” at all when people enter certain words or phrases, according to internal Google documents.

In a letter sent to Pichai on Friday, the six lawmakers called the Google plan “deeply troubling” and said that it “risks making Google complicit in human rights abuses related to China’s rigorous censorship regime.” The letter was led by Sen. Marco Rubio, R-Fla., and also signed by Sens. Mark Warner, D-Va., Tom Cotton, R-Ark., Ron Wyden, D-Ore., Cory Gardner, R-Colo., and Robert Menendez, D-N.J.

The senators write: “It is a coup for the Chinese government and Communist Party to force Google—the biggest search engine in the world—to comply with their onerous censorship requirements, and sets a worrying precedent for other companies seeking to do business in China without compromising their core values.”
GOOGLE BOSSES WERE scrambling to contain leaks and internal anger on Wednesday after the company’s confidential plan to launch a censored version of its search engine in China was revealed by The Intercept.

Just a few hundred of Google’s massive 88,000-strong workforce had been briefed on the project prior to the revelations, which triggered a wave of disquiet that spread through the internet giant’s offices across the world.

Company managers responded by swiftly trying to shut down employees’ access to any documents that contained information about the China censorship project, according to Google insiders who witnessed the backlash.

“Everyone’s access to documents got turned off, and is being turned on [on a] document-by-document basis,” said one source. “There’s been total radio silence from leadership, which is making a lot of people upset and scared. … Our internal meme site and Google Plus are full of talk, and people are a.n.g.r.y.”

On a message board forum for Google employees, one staff member posted a link to The Intercept’s story alongside a note saying that they and two other members of their team had been asked to work on the Chinese censorship project, code-named Dragonfly.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
THE LATEST TECHNOLOGIES promise cops the ability to whip out a smartphone, take a snapshot of a passerby, and instantly learn if that person is in an immigration or gang database.

A federal broadband program, designed after 9/11 to improve first responder communication during emergencies, will enhance this sort of capability and integrate it into an internet “super highway” built specifically for police and public safety. The program, called FirstNet, is already expanding the surveillance options available to law enforcement agencies across the country.

According to publicly available documents, as well as interviews with program participants, stakeholders, and government researchers, FirstNet will help agencies like U.S. Customs and Border Protection communicate with local police, deliver more information to officers’ hands, accelerate the nascent law enforcement app industry, and provide public safety agencies with new privileges and powers over AT&T’s commercial broadband network.

The program will also hasten these agencies’ migration from public radio frequencies to encrypted broadband networks, potentially eliminating one resource that local newsrooms and citizens have historically relied upon to monitor police and first responders.

FirstNet is a public-private partnership that creates a dedicated lane for public safety agencies within AT&T’s existing broadband network. As of January, all U.S. states had opted in to FirstNet, meaning that they agreed not to build their own competing broadband lanes for law enforcement and public safety. Then, in March, AT&T announced that FirstNet’s core — the infrastructure that isolates police traffic from the commercial network — had become operational at last.

“It’s like having a super highway that only public safety can use,” the company wrote in a press release.

Why FirstNet?

Part of FirstNet’s mission is to create a virtual space that allows any federal, state or local law enforcement or public safety agency to communicate seamlessly with any other. Therefore, convincing as many agencies as possible to sign up for the program is key to its success.

FirstNet recently pitched U.S. Customs and Border Protection to convince the agency to subscribe to the network. In a white paper, FirstNet claims its services will provide CBP access to “photographs, real-time audio/video feeds, and databases from other state, local, or Federal agencies … to aid in the identification and apprehension of terrorists, undocumented aliens, and smugglers.” These capabilities would be offered “in times of crisis or simply day-to-day operations.”

In the pitch, FirstNet also promises to help agents “connect to critical databases to identify whether detained persons have been previously apprehended for violating immigration law by quickly and efficiently collecting biographic (e.g., name, date of birth, place of birth) and biometric information (e.g., 10-print fingerprints, photo image), which are submitted remotely to said databases.” The document also promotes FirstNet’s support of other data-heavy technologies, such as live video streaming from drones.

AT&T and FirstNet did not respond to questions about whether CBP or any other federal agency has subscribed to the program. (A recent press release indicates that some federal agencies are currently using the system, but it does not name them.) CBP did not respond to requests for comment.

Local law enforcement officials are well-aware of the new capabilities that FirstNet is offering their departments. Domingo Herraiz, programs director at the International Association of Chiefs of Police, is excited about the heightened access to federal data FirstNet promises. Herraiz told The Intercept that FirstNet will place information from fusion centers, which enable criminal intelligence-sharing between government agencies, at the fingertips of local officers. “You could have gang databases,” he said. “It’s not there [on officers’ phones] today, but it will be.”

A “Private Tunnel” for Law Enforcement and First Responders

The concept behind FirstNet — a broadband network dedicated to public safety — was inspired by the National Commission on Terrorist Attacks Upon the United States (the so-called 9/11 Commission). Its 2004 report determined that streamlined communication between different agencies and jurisdictions could have saved lives in the aftermath of the attacks on the World Trade Center. The report blamed the use of separate radio frequencies by police and firefighters for the deaths of firefighters who didn’t get the message to evacuate before the north tower collapsed.
SILICON VALLEY FIRMS seeking lucrative business opportunities with the Pentagon face a range of obstacles, not least the morally fraught choice of enabling a military led by President Donald Trump with the latest technological solutions. Enter a group of former high-level officials from the Obama administration, who are helping to bridge the divide between tech firms and the Defense Department through a new company called WestExec Advisors.

“Think Scowcroft Group, Kissinger, RiceHadleyGates, Albright, but my generation,” explained Michèle Flournoy, referencing the myriad of consulting firms founded by former top national security and foreign policy officials, during an interview on Capitol Hill. Flournoy herself is the former under secretary of defense for policy and one of the co-founders of WestExec Advisors.

The sort of initiatives WestExec is posturing itself to spearhead, however, have grown controversial. In recent months, Google has faced an internal rebellion over its work with the Defense Department to deploy cutting-edge artificial intelligence technology for drone warfare, part of a Pentagon initiative known as Project Maven. The internal uprising led to Google executives announcing last month that the firm would not renew the military contract when it expires next year. And WestExec has found itself at the center of the storm, with the consultancy’s officials deeply involved with the project and wading into the media firestorm that was set off by the Google employees’ objections.
ON FRIDAY, Special Counsel Robert Mueller, as part of his investigation into interference with the 2016 presidential election, charged 12 Russian military intelligence officers with conducting “large-scale cyber operations to interfere with the 2016 U.S. presidential election.” The indictment contains a surprising amount of technical information about alleged Russian cyberattacks against a range of U.S. political targets, including the Democratic Congressional Campaign Committee, the Democratic National Committee, members of Hillary Clinton’s presidential campaign, the Illinois (probably) State Board of Elections, and an American election vendor, apparently VR Systems, and its government customers.

While the indictment only describes the U.S. government’s charges in this case, the specific technical evidence presented is compelling and paints by far the most detailed and plausible picture yet of what exactly occurred in 2016.

It also sheds light on what the U.S. government is capable of doing when it investigates cyberattacks, as well as how Russia’s Main Intelligence Directorate of the General Staff, or GRU, allegedly conducted the attacks — which it denies — and what operational security mistakes they made. Here are what I find to be the most compelling takeaways from the indictment.

AMID HIGH-PROFILE Supreme Court rulings like the lifeline given to the practice of gerrymandering, the endorsement of Trump’s Muslim travel ban, the gutting of public sector unions, and the defense of bakers who don’t want to serve gay people, the case of Ohio v. American Express may get overlooked.

But it could prove to be one of the most consequential rulings of the decade, serving as a broad immunity cloak for Silicon Valley giants — and perhaps others — in their quest to utterly dominate the global political economy.

Which is weird, because it was a case about credit card fees.

In a 5-4 ruling along party lines on Monday, the court, featuring Justice Neil Gorsuch rather than Judge Merrick Garland, ruled that American Express did not violate antitrust laws when it wrote into its contracts with retailers that they could not offer discounts or enticements to get customers to use other forms of payment.
IN THE FACE of the Trump administration’s neglect and indifference toward the reunification of the thousands of immigrant families it has forcibly separated, some lawmakers, activists, and celebrities have called for the use of DNA testing, along with other biometrics, as a means to return some 3,700 children to their parents. So far, at least two direct-to-consumer genealogy companies have heeded those calls. MyHeritage and 23andMe have both offered to donate DNA sampling kits for the purposes of verifying kinship.

“In light of the humanitarian tragedy that has taken place, in which children have been separated from their parents, we have decided to rise to the challenge and take the lead in helping these families,” Gilad Japhet, founder and CEO of MyHeritage, said in a press statement. The company is offering 5,000 free DNA tests for separated parents and children, and it plans to distribute the kits through government agencies and NGOs. Both MyHeritage and 23andMe have said that its DNA results will be processed confidentially and not shared with any third parties.
LOBBYISTS FOR THE largest technology and telecommunications firms have only three days to prevent the California Consumer Privacy Act, or CCPA, a ballot initiative that would usher in the strongest consumer privacy standards in the country, from going before state voters this November.

The initiative allows consumers to opt out of the sale and collection of their personal data, and vastly expands the definition of personal information to include geolocation, biometrics, and browsing history. The initiative also allows consumers to pursue legal action for violations of the law.

The idea that Californians might gain sweeping new privacy rights has spooked Silicon Valley, internet service providers, and other industries that increasingly rely on data collection, leading to a lobbying push to defeat the initiative before it gains traction. Their best hope may be to convince the sponsors of the initiative, including San Francisco real estate developer Alastair Mactaggart, to pull the proposal in exchange for compromise privacy legislation, AB 375, which would achieve some of the same goals of the initiative. Lawmakers behind the legislation, led by state Assembly Member Ed Chau, D-Monterey Park, and state Sen. Robert Hertzberg, D-Van Nuys, have promised to swiftly pass their bill this week if sponsors withdraw the CCPA.

Emails obtained by The Intercept reveal that tech giants are fighting behind the scenes to water down the privacy legislation, hoping to prevent an expensive and potentially losing ballot fight this year.

Andrea Deveau, a lobbyist for TechNet, a trade group for Google, Facebook, and other tech companies, has continually updated an ad-hoc business lobbying coalition formed to defeat the CCPA. In an update sent on Sunday evening, Deveau provided a “compilation of feedback re: the most problematic aspects of AB 375.”

In her update, she listed a vast array of changes lobbyists are still seeking, including a rewrite of the privacy law’s description of what counts as personal information, changes to the conditions under which a consumer can seek legal action, the preservation of arbitration clauses in consumer contracts, and the removal of the mandate that firms display a button on their homepage giving consumers a clear way of opting out of data collection, among other changes.
LAST WEEK, INTERPOL held a final project review of its speaker identification system, a four-year, 10 million euro project that has recently come to completion. The Speaker Identification Integrated Project, what they call SiiP, marks a major development in the international expansion of voice biometrics for law enforcement uses — and raises red flags when it comes to privacy.

Speaker identification works by taking samples of a known voice, capturing its unique and behavioral features, and then turning these features into an algorithmic template that’s known as a voice print or voice model. With enough voice prints and samples collected in its global audio database, Interpol’s speaker identification system will be able to upload an unknown voice and, regardless of the language it is speaking, match it to a list of likely candidates. SiiP’s database allow uploads and downloads of samples from 192 law enforcement agencies across the world.

SiiP will join Interpol’s existing fingerprint and face databases, and its key advantage will be to facilitate a quick identification process — say, of a kidnapper making a phone call — even in the absence of other identifiers. The platform also boasts the ability to filter voice samples by gender, age, language, and accent. When the audio recordings are taken from similar acoustical environments, accuracy rates can be extremely high.
IN A LANDMARK privacy decision, the Supreme Court ruled 5-4 on Friday that police must get a warrant in order to obtain your cellphone’s location data over an extended period of time.

The decision is a major victory for privacy advocates, who have long argued that the law has failed to keep pace with the amount of intrusive data we voluntarily hand over to private companies.

Chief Justice John Roberts joined the liberal justices on the court, declaring that even though the data is held by a third party, the government still needs a warrant to obtain it.

“We decline to grant the state unrestricted access to a wireless carrier’s database of physical location information,” said Roberts, writing for the majority. “In light of the deeply revealing nature of [cell-site location information], its depth, breadth, and comprehensive reach, and the inescapable and automatic nature of its collection, the fact that such information is gathered by a third party does not make it any less deserving of Fourth Amendment protection.”

The court made the ruling in the case of Timothy Carpenter, who was convicted in 2013 of robbing Radio Shack and T-Mobile stores in Michigan and Ohio. In order to build their case, the FBI obtained 127 days’ worth of location information for Carpenter’s cellphone – almost 12,900 location points – which they used to place him at the scene of the robberies.
IN A LANDMARK privacy decision, the Supreme Court ruled 5-4 on Friday that police must get a warrant in order to obtain your cellphone’s location data over an extended period of time.

The decision is a major victory for privacy advocates, who have long argued that the law has failed to keep pace with the amount of intrusive data we voluntarily hand over to private companies.

Chief Justice John Roberts joined the liberal justices on the court, declaring that even though the data is held by a third party, the government still needs a warrant to obtain it.

“We decline to grant the state unrestricted access to a wireless carrier’s database of physical location information,” said Roberts, writing for the majority. “In light of the deeply revealing nature of [cell-site location information], its depth, breadth, and comprehensive reach, and the inescapable and automatic nature of its collection, the fact that such information is gathered by a third party does not make it any less deserving of Fourth Amendment protection.”

The court made the ruling in the case of Timothy Carpenter, who was convicted in 2013 of robbing Radio Shack and T-Mobile stores in Michigan and Ohio. In order to build their case, the FBI obtained 127 days’ worth of location information for Carpenter’s cellphone – almost 12,900 location points – which they used to place him at the scene of the robberies.
EARLIER THIS YEAR, it was reported that Elliott Broidy, previously known for his conviction in a state bribery case and his role as a top Donald Trump fundraiser, proffered meetings with the president to foreign regimes who were also potential clients of his defense firm Circinus. Little is known about Circinus, but purported company documents obtained by The Intercept contain plans to peddle social media surveillance software to repressive regimes.

The Circinus website paints the contractor as a red-blooded defender of U.S. national security: “Are you a patriot determined to keep our country — both government and private industry — safe?” its careers page reads. Circinus’s executive roster boasts experience in U.S. special forces, Homeland Security, and military intelligence. But the documents, a series of pitch decks, indicate that the company was prepared to sell what’s described as a suite of sophisticated internet-mining tools to the governments of Cyprus, Romania, Tunisia, and the United Arab Emirates, touting the ability to detect and identify online “detractors.” The recent histories of Tunisia and the UAE are rife with human rights abuses, including crackdowns against political dissent.
THE INTERCEPT AND European media partners are joining with The Signals Network, a whistleblower support organization, to solicit information from those who want to speak out against data-related malpractice in the technology sector.

Through this collaboration, The Intercept will work with German newspaper Die Zeit, French news website Mediapart, British newspaper The Daily Telegraph, and the global news platform WikiTribune to obtain and investigate information regarding the abuse of personal data — whether by a social network, health care company, government, marketing firm, or any other of the myriad entities that collect and process such data on an enormous scale. The Signals Network, a nonprofit founded by French media executive Gilles Raymond, said on its website that it can, in “selected … appropriate cases,” offer support to whistleblowers, including legal aid, psychological counseling, and safe shelter in case of physical threats.

As part of the effort, the news partners will share a contact number on the secure messaging platform Signal and share tips submitted to a set of email addresses.

Sources that come forward will have access to not just the newsrooms of the above news organizations, but the chance to reach their combined audiences of over 46 million readers in three languages. Those interested in contacting the consortium can find more details here, and should read these guidelines for submitting information.
THE INTERCEPT AND European media partners are joining with The Signals Network, a whistleblower support organization, to solicit information from those who want to speak out against data-related malpractice in the technology sector.

Through this collaboration, The Intercept will work with German newspaper Die Zeit, French news website Mediapart, British newspaper The Daily Telegraph, and the global news platform WikiTribune to obtain and investigate information regarding the abuse of personal data — whether by a social network, health care company, government, marketing firm, or any other of the myriad entities that collect and process such data on an enormous scale. The Signals Network, a nonprofit founded by French media executive Gilles Raymond, said on its website that it can, in “selected … appropriate cases,” offer support to whistleblowers, including legal aid, psychological counseling, and safe shelter in case of physical threats.

As part of the effort, the news partners will share a contact number on the secure messaging platform Signal and share tips submitted to a set of email addresses.

Sources that come forward will have access to not just the newsrooms of the above news organizations, but the chance to reach their combined audiences of over 46 million readers in three languages. Those interested in contacting the consortium can find more details here, and should read these guidelines for submitting information.
CAMILLE TUUTTI CAN’T remember all the times she’s been harassed. A prominent information technology journalist and editor, Tuutti feels that her friendly and outgoing personality — a necessity in her line of work — has often been misinterpreted by men in her field as an invitation for inappropriate behavior, especially at top cybersecurity conferences, where binge drinking is encouraged. Drunk men have often put their arms around her and her colleagues. She has been asked out “a million times.” Someone tried to kiss her the first time she met him.

This April, at RSA, a leading cybersecurity conference held in San Francisco, she was walking the showroom with a male colleague when a male stranger asked her what she was wearing to bed. She noticed, too, that vendors at the show assumed that she didn’t know what she was talking about and that her colleague did. And despite organizers’ previous attempts to implement a dress code, many of the booths featured “booth babes” — scantily clad models hired to attract men to vendors’ wares. “It was so tone-deaf, especially in 2018 and especially in the wake of #MeToo,” Tuutti said.

The casual sexism Tuutti encountered at RSA is not atypical of big-league hacker and cybersecurity conferences. While there are no precise statistics available about harassment at these events, anecdotal reports like Tuutti’s have been widespread and documented for years.

The Intercept spoke to nearly two dozen women across the industry who recounted experiences ranging from uncomfortable to traumatic at conferences such as Def Con and Black Hat, held each year in Las Vegas, and RSA, held worldwide. The women who spoke to The Intercept had encountered a variety of offenses, from suggestive commentary and drunken come-ons to groping and assault. Some of the women, among whom are renowned journalists, CEOs, diversity advocates, and hackers, said that even if their own status had shielded them from some of the worst behavior, they had all heard troubling stories from younger colleagues, peers, and friends.
GOOGLE EXECUTIVES ANNOUNCED to company staff this morning that the tech giant won’t renew its contract to work on Project Maven, the controversial Pentagon program designed to provide the military with artificial intelligence technology used to help drone operators identify images on the battlefield. Google will continue work on the project through March 2019, according to multiple people with knowledge of the announcement, but once the 18-month contract concludes, it will not be renewed.

The company, however, has not committed to forego signing other military contracts dealing with artificial intelligence, according to multiple people with knowledge of the decision. Google declined to comment for this story.
JUST DAYS BEFORE the 2016 presidential election, hackers identified by the National Security Agency as working for Russia attempted to breach American voting systems. Among their specific targets were the computers of state voting officials, which they had hoped to compromise with malware-laden emails, according to an intelligence report published previously by The Intercept.

Now we know what those emails looked like.

An image of the malicious email, provided to The Intercept in response to a public records request in North Carolina, reveals precisely how hackers, who the NSA believed were working for Russian military intelligence, impersonated a Florida-based e-voting vendor and attempted to trick its customers into opening malware-packed Microsoft Word files.
JUST DAYS BEFORE the 2016 presidential election, hackers identified by the National Security Agency as working for Russia attempted to breach American voting systems. Among their specific targets were the computers of state voting officials, which they had hoped to compromise with malware-laden emails, according to an intelligence report published previously by The Intercept.

Now we know what those emails looked like.

An image of the malicious email, provided to The Intercept in response to a public records request in North Carolina, reveals precisely how hackers, who the NSA believed were working for Russian military intelligence, impersonated a Florida-based e-voting vendor and attempted to trick its customers into opening malware-packed Microsoft Word files.
A WASHINGTON-BASED ADVOCACY organization that purports to be a voice for startup tech companies is actually a sock puppet for Google, according to a report released Wednesday that details numerous links between the two.

According to the report, startup advocacy group Engine has at least seven former Google employees and consultants on its board of directors and advisory board. Its three founders all previously worked at Google; they founded a startup incubator that Google eventually bought. Google has given Engine an undisclosed amount of funding over the past five years. The two share a lobbying firm called S-3 Group that has worked for both Engine and Google. The initial launch party for Engine in 2011 had attendees RSVP to a Google email address, which is reserved primarily for employees, unlike the Gmail address that is offered to the public.

On numerous issues, from patent reform, anti-piracy efforts, and high-skilled immigration, to the recent changes to Section 230 of the Communications Decency Act, Engine’s advocacy and Google’s stated policy preferences are in alignment, the report explains. Google even funded a research paper that Engine later released.

“Public officials need to be aware that this so-called startup advocacy group is really in bed with Silicon Valley’s foremost D.C. influence machine, whose interests are often in conflict with those of disruptive entrepreneurs,” said Daniel Stevens of the Campaign for Accountability, which released the report. There are no clean hands here: The Campaign for Accountability gets major funding from Oracle, a chief antagonist to Google.

A Google spokesperson said the company is “happy to support Engine’s work” to represent the views of startups in Washington policy debates. “While we often agree on policy matters, Engine is an independent organization just like the other groups we support,” the spokesperson said.

Google publicly discloses its funding support for Engine on its website, “in contrast to the Campaign for Accountability, which declines to list its corporate funders and has been instrumental in Oracle’s long-running legal grudge against Google,” the spokesperson said.

Ken Gleuck, a senior vice president in Oracle’s Washington office, said after publication that Google’s charge was off-base and that Oracle had nothing to do with the report. “Before reading your story, Oracle had no idea Engine even existed, nor did we have any knowledge or involvement with this report,” he said. “While we are flattered, Google should not assume we are behind every bad story about Google. We’d run out of 20 percent time if all we did was out Google front groups. Are we also responsible for the Red Wedding and plastic straws?”
OFFICIALS AT THE Lockport, New York, school district have purchased face recognition technology as part of a purported effort to prevent school shootings. Starting in September, all 10 of Lockport District’s school buildings, just north of Buffalo, will be outfitted with a surveillance system that can identify faces and objects. The software, known as Aegis, was developed by SN Technologies Corp., a Canadian biometrics firm that specifically advertises to schools. It can be used to alert officials to whenever sex offenders, suspended students, fired employees, suspected gang members, or anyone else placed on a school’s “blacklist” enters the premises. Aegis also sends alerts any time one of the “top 10” most popular guns used in school shootings appears in view of a camera.

The district is spending most of its recent $4 million state “Smart School” grant on these and other enhancements to its security systems, including bullet-proof greeter windows and a mass notification system, according to the Niagra Gazette. “We always have to be on our guard. We can’t let our guard down,” Lockport Superintendent Michelle T. Bradley told the Buffalo News. “For the Board of Education and the Lockport City School District, this is the No. 1 priority: school security.”
Update: Since this article was published, GPGTools released version 2018.2 which appears to successfully mitigate the OpenPGP EFAIL attack for macOS High Sierra users. If you use macOS High Sierra, Apple Mail, and GPGTools, it should be safe to use PGP again if you update to the latest version of everything. If you use an older version of macOS, GPGTools is still vulnerable.


IT’S BEEN NEARLY two weeks since a group of European researchers published a paper describing “EFAIL,” a set of critical software vulnerabilities that allow encrypted email messages to be stolen from within the inbox. And developers of email clients and encryption plug-ins are still scrambling to come up with a permanent fix.

Apple Mail is the email client that comes free with every Mac computer, and an open source project called GPGTools allows Apple Mail to smoothly encrypt and decrypt messages using the 23-year-old PGP standard. The day the EFAIL paper was published, GPGTools instructed users to workaround EFAIL by changing a setting in Apple Mail to disable loading remote content:
In partnership with
WHAT’S THE BEST way to keep adults from questioning the use of a deeply problematic product? Get them started when they’re too young to question anything. Amazon has a new addition to its line of voice-commanded artificial intelligence Alexa assistants, marketed for use by children as young as 5 years old, who can barely grasp a box of juice, let alone digital privacy. Now, a coalition of children’s privacy and psychology advocates are warning parents away from Amazon’s latest, cutest device, saying it could normalize surveillance and harm children’s mental development.

The Echo Dot for kids is functionally identical to the Echo Dot for adults, except that it’s brightly colored and inexplicably costs $30 more than the grown-up version. Cosmetics aside, Echo Dot is still an AI-powered microphone that listens constantly for an activation keyword, relays a user’s voice to remote servers where it is analyzed and processed opaquely, and then responds to an increasingly long list of commands; on its packaging, Amazon highlights commands like “tell me a story” and “start SpongeBob.” Dot for kids will not only perpetually listen to and entertain your children, but attempt to teach them manners in your stead: “Alexa even provides positive feedback when kids ask questions and remember to say ‘please,'” says Amazon.
LEIA EM PORTUGUÊS 
DIGITAL SECURITY SPECIALISTS like me get some version of this question all the time: “I think my laptop may have been infected with malware. Can you check?”

We dread this sort of query because modern computer exploits are as complex, clever, and hard to reason about as modern computers — particularly if someone has the ability to physically access your device, as is routinely the case with laptops, especially when traveling. So while it’s definitely possible to detect certain types of tampering, it isn’t always trivial. And even in controlled environments, it’s impossible to give a laptop a clean bill of health with full confidence – it’s always possible that it was tampered with in a way you did not think to check.

The issue of tampering is particularly relevant for human rights workers, activists, journalists, and software developers, all of whom hold sensitive data sought by powerful potential attackers. People in these vocations are often keenly aware of the security of their laptops while traveling – after all, laptops store critical secrets like communication with sources, lists of contacts, password databases, and encryption keys used to vouch for source code you write, or to give you access to remote servers.
LEIA EM PORTUGUÊS 
AFTER WATCHING the Facebook founder and CEO’s 48-hour trip to Capitol Hill, there are two possible conclusions: either Mark Zuckerberg deliberately misled Congress, or Mark Zuckerberg knows very little about his own company. Both are bad.

Again and again, before both Senate and House committees, Zuckerberg pleaded ignorance about the company he created and has controlled for 14 years. Zuckerberg wasn’t dodging questions about obscure corners of the company or corporate minutiae, but the most plainly fundamental aspects of Facebook’s business and privacy policies. Rather than the congressional beatdown many had expected, the most striking aspect of Zuckerberg’s testimony wasn’t his painful apologias or excuse-spinning, but his ability to spend nearly 10 hours saying almost nothing. The hearings may prove to be a sea change moment for Facebook and the greater data-mining industrial complex, but it would be hard to say the public learned much of anything.

When Sen. Kamala Harris asked Zuckerberg, on the subject of Cambridge Analytica, whether the company had any conversations about whether to inform the 87 million users affected, the CEO replied, “I don’t know if there were any conversations at Facebook overall because I wasn’t in a lot of them,” and finally “I don’t remember a conversation like that.”

When asked by Sen. Maria Cantwell whether Facebook employees had helped with Cambridge Analytica’s work: “Senator, I don’t know.”

When asked about the role of Palantir, a data-mining defense contractor co-founded by Facebook board member and early Zuckerberg ally Peter Thiel: “I’m not really that familiar with what Palantir does.”

Zuckerberg acted similarly confused when asked whether Facebook does things it openly says it does on its own website. When Sen. Roger Wicker asked Zuckerberg if he could confirm whether “Facebook can track a user’s internet browsing activity, even after that user has logged off of the Facebook platform,” the CEO replied, “Senator — I — I want to make sure I get this accurate, so it would probably be better to have my team follow up afterwards.”

The answer is categorically, unequivocally yes, according to Facebook.com: “If you’re logged out or don’t have a Facebook account and visit a website with the Like button or another social plugin, your browser sends us a more limited set of info.”

When Sen. Roy Blunt asked Zuckerberg whether Facebook tracks users across devices (say, from their iPhone to their iPad), he replied that he was “not sure of the answer to that question.”

Meanwhile, on Facebook.com:
FOUNDED IN 1936, the Advertising Research Foundation “has been the standard-bearer for unbiased quality in research on advertising, media and marketing,” according to its website, and works to spread “unifying standards and best practices” throughout the ad industry. Last year, the ARF presented Cambridge Analytica with its highest honor.

The current scandal engulfing both Facebook and Cambridge Analytica, the shadowy British political consultancy that exfiltrated and exploited 50 million profiles from the social network, centers mostly around how the data was acquired, not how it was used. This is due in part to the fact that before the pilfered profile revelations, Cambridge Analytica enjoyed the praise of its peers in the marketing world, which viewed it as a band of innovators.
Because of errors inserted during editing, the original version of this article contained the mistaken assertion that ICE used private Facebook data to track unauthorized immigrants. The story has been corrected. See below for full correction.

CAMBRIDGE ANALYTICA MAY have had access to the personal information of tens of millions of unwitting Americans, but a genuine debate has emerged about whether the company had the sophistication to put that data effectively to use on behalf of Donald Trump’s presidential campaign.

But one other organization that has ready access to Facebook’s trove of personal data has a much better track record of using such information effectively: U.S. Immigration and Customs Enforcement.

ICE, the federal agency tasked with Trump’s program of mass deportation, uses backend Facebook data to locate and track suspects, according to a string of emails and documents obtained by The Intercept through a public records request. The hunt for one particular suspect provides a rare window into how ICE agents use social media and powerful data analytics tools to find targets.
LEIA EM PORTUGUÊS 
ON WEDNESDAY, HOUSE Democrats on the Intelligence Committee released a memo laying out the steps they would have taken had they been in charge of the Trump-Russia investigation — and steps they may take if and when they gain subpoena power by taking over the House of Representatives in November.

The possibility became much more likely on Tuesday night, as Democrats pulled off an upset victory in a Pennsylvania special election for a congressional seat that President Donald Trump had carried by nearly 20 points, and that Democrats hadn’t even bothered to contest in the last two cycles. If the pattern holds, Democrats have a strong chance of picking up the nearly two-dozen seats they need to win the House — and the subpoena power that comes with it.

Down on Page 20 of the memo is a pair of ideas that could put Congress on a collision course with privacy advocates in Silicon Valley. “Apple: The Committee should seek records reflecting downloaded encrypted messaging apps for certain key individuals,” the memo suggests. “The Committee should likewise issue a subpoena to WhatsApp for messages exchanged between key witnesses of interest.”

The committee said that it would also seek to find out “all messaging applications that Mr. [Jared] Kushner used during the campaign as well as the presidential transition, including but not limited to SMS, iMessage, Whatsapp, Facebook Messenger, Signal, Slack, Instagram, and Snapchat.”

The committee may also consider adding ProtonMail, the encrypted email service, to that list. One White House staffer, Ryan P. McAvoy, jotted his ProtonMail passwords and his address on a piece of White House stationery and left it at a bus stop near the White House. A source found it there and provided it to The Intercept, which confirmed its authenticity. (McAvoy did not respond to requests for comment.)

Following publication of this story, Irina Marcopol, a spokesperson for ProtonMail, forwarded a statement from the company’s CEO, Andy Yen. “Don’t be a password idiot,” Yen suggested. “In other words, don’t be this guy.”
Yen went on to note that whether a White House staffer is using encrypted or unencrypted email isn’t the important question, at least from the perspective of retention of records. The use of any personal email account for official business would run afoul of that.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
GOOGLE HAS QUIETLY secured a contract to work on the Defense Department’s new algorithmic warfare initiative, providing assistance with a pilot project to apply its artificial intelligence solutions to drone targeting.

The military contract with Google is routed through a Northern Virginia technology staffing company called ECS Federal, obscuring the relationship from the public.

The contract, first reported Tuesday by Gizmodo, is part of a rapid push by the Pentagon to deploy state-of-the-art artificial intelligence technology to improve combat performance.

Google, which has made strides in applying its proprietary deep learning tools to improve language translation, and vision recognition, has a cross-team collaboration within the company to work on the AI drone project.

The team, The Intercept has learned, is working to develop deep learning technology to help drone analysts interpret the vast image data vacuumed up from the military’s fleet of 1,100 drones to better target bombing strikes against the Islamic State.

The race to adopt cutting-edge AI technology was announced in April 2017 by then-Deputy Defense Secretary Robert Work, who unveiled an ambitious plan called the Algorithmic Warfare Cross-Functional Team, code-named Project Maven. The initiative, Work wrote in an agency-wide memo, is designed to “accelerate DoD’s integration of big data and machine learning” and “turn the enormous volume of data available to DoD into actionable intelligence and insights at speed.”

The first phase of Project Maven, which incorporates multiple teams from across the Defense Department, is an effort to automate the identification and classification of images taken by drones — cars, buildings, people — providing analysts with increased ability to make informed decisions on the battlefield.
FOR A MOMENT, it seemed the hackers had slipped up and exposed their identities. It was the summer of 2013, and European investigators were looking into an unprecedented breach of Belgium’s telecommunications infrastructure. They believed they were on the trail of the people responsible. But it would soon become clear that they were chasing ghosts – fake names that had been invented by British spies.

The hack had targeted Belgacom, Belgium’s largest telecommunications provider, which serves millions of people across Europe. The company’s employees had noticed their email accounts were not receiving messages. On closer inspection, they made a startling discovery: Belgacom’s internal computer systems had been infected with one of the most advanced pieces of malware security experts had ever seen.

As The Intercept reported in 2014, the hack turned out to have been perpetrated by U.K. surveillance agency Government Communications Headquarters, better known as GCHQ. The British spies hacked into Belgacom employees’ computers and then penetrated the company’s internal systems. In an eavesdropping mission called “Operation Socialist,” GCHQ planted bugs inside the most sensitive parts of Belgacom’s networks and tapped into communications processed by the company.
ON A THURSDAY afternoon in November 2015, a light snow was falling outside the windows of the Ecuadorian embassy in London, despite the relatively warm weather, and Julian Assange was inside, sitting at his computer and pondering the upcoming 2016 presidential election in the United States.

In little more than a year, WikiLeaks would be engulfed in a scandal over how it came to publish internal emails that damaged Hillary Clinton’s presidential campaign, and the extent to which it worked with Russian hackers or Donald Trump’s campaign to do so. But in the fall of 2015, Trump was polling at less than 30 percent among Republican voters, neck-and-neck with neurosurgeon Ben Carson, and Assange spoke freely about why WikiLeaks wanted Clinton and the Democrats to lose the election.

“We believe it would be much better for GOP to win,” he typed into a private Twitter direct message group to an assortment of WikiLeaks’ most loyal supporters on Twitter. “Dems+Media+liberals woudl then form a block to reign in their worst qualities,” he wrote. “With Hillary in charge, GOP will be pushing for her worst qualities., dems+media+neoliberals will be mute.” He paused for two minutes before adding, “She’s a bright, well connected, sadistic sociopath.”
THE U.K. GOVERNMENT’S mass surveillance powers were deemed unlawful on Tuesday in a court ruling that could force changes to the country’s spy laws.

Three judges at London’s Court of Appeal found that a sweeping data retention law, which allowed authorities to access people’s phone and email records, was not subject to adequate safeguards. The court ruled that access to the private data “should be restricted to the objective of fighting serious crime.” The court also said that such data should not be turned over to authorities until after a “prior review by a court or an independent administrative body.”

The case was originally brought by the Labour Member of Parliament Tom Watson following the introduction of the 2014 Data Retention and Investigatory Powers Act. That law expired in 2016 and has since been replaced by the Investigatory Powers Act, which expanded the government’s surveillance authority further, retroactively legalizing controversial spy tactics exposed in documents leaked by Edward Snowden. Human rights group Liberty, which represented Watson in the case, said Tuesday’s ruling meant parts of the Investigatory Powers Act – dubbed the “Snoopers’ Charter” by critics – would now need to be reformed.

“Yet again a U.K. court has ruled the government’s extreme mass surveillance regime unlawful,” said Martha Spurrier, Liberty’s director, in a statement. “This judgment tells ministers in crystal clear terms that they are breaching the public’s human rights. … When will the government stop bartering with judges and start drawing up a surveillance law that upholds our democratic freedoms?”

Watson said he was “proud to have played my part in safeguarding citizens’ fundamental rights.” He called on the government to “ensure that hundreds of thousands of people, many of whom are innocent victims or witnesses to crime, are protected by a system of independent approval for access to communications data.”

The Data Retention and Investigatory Powers Act forced telecommunications companies to store records on their customers’ emails and phone calls for 12 months. The Investigatory Powers Act broadened the data retention system by allowing the government to compel phone and internet companies to store not just email and phone records, but also logs showing the websites customers visited and the apps they used. Law enforcement agencies can then access this information without a court order or warrant for a broad range of reasons, not necessarily related to suspected criminal activity. They can obtain the data, for instance, if they judge it to be for the “purpose of protecting public health,” “in the interests of the economic well-being” of the U.K., or “for the purpose of assessing or collecting any tax, duty, levy or other imposition, contribution or charge payable to a government department.”

The Court of Appeal ruling is the latest in a series of blows for the U.K. government on surveillance. It partly reaffirms a December 2016 judgment in the European Union’s top court, which found that the British government’s data retention powers were “highly invasive” and exceeded “the limits of what is strictly necessary and cannot be considered to be justified, within a democratic society.” At least three other major legal cases challenging the country’s spy powers remain ongoing.

Ben Wallace, the U.K. government’s security minister, was dismissive of Tuesday’s decision. “This judgment relates to legislation which is no longer in force and … does not change the way in which law enforcement agencies can detect and disrupt crimes,” he said in a statement. Wallace claimed that the ruling would “not undermine the [data retention] regime” because the government had already acted preemptively in November by introducing safeguards that rein in police officers’ ability to self-authorize access to people’s private data. However, the significance of the ruling is that it will ensure the changes restricting police access to the data are bound into law and cannot be rolled back in future.

Critics believe more reforms are still required. They point out that the government has as of yet failed to address legal breaches identified in the earlier December 2016 European Union court ruling. That ruling stated that to be compliant with human rights law, the government must notify people – after investigations have been completed – if their data has been accessed and must also commit to keeping people’s private data within the EU. Spurrier described the government’s existing safeguards as “window-dressing for indiscriminate surveillance of the public.”

Top photo: A man chats on the phone during the auction at Tattersalls October Yearling Sale Book 1 on Oct. 4, 2016 in Newmarket, England.
FACEBOOK USERS, BY and large, are not very good at differentiating between what’s fact and what’s false. Many users will eagerly share both reliable news and the fake stuff without any hesitation. It happens because users either want the falsehoods to be received as true or simply can’t tell the difference. Rampant media illiteracy is the root cause of the fake news handwringing we’ve been dealing with since before the election and will be fretting over until the end of time (or the end of Facebook, whichever comes first). Today, Facebook honcho Mark Zuckerberg said he is setting out to fix this fundamental problem of digital media illiteracy — by putting more power in the hands of the illiterate.

In a new Facebook post today, Zuckerberg said he “asked our product teams to make sure we prioritize news that is trustworthy, informative, and local.” Why this has only become a priority in the company’s 14th year of existence is left unsaid. Zuckerberg admitted that “there’s too much sensationalism, misinformation and polarization in the world today,” and that his website “enables people to spread information faster than ever before.” As with the rest of Silicon Valley, Facebook is obsessed with the appearance of machine-like objectivity, and so Zuckerberg said figuring out which outlets deliberately package viral-ready falsehoods and which do not is a head-scratcher (spoiler: It isn’t):

The hard question we’ve struggled with is how to decide what news sources are broadly trusted in a world with so much division. We could try to make that decision ourselves, but that’s not something we’re comfortable with. We considered asking outside experts, which would take the decision out of our hands but would likely not solve the objectivity problem. Or we could ask you — the community — and have your feedback determine the ranking.

So, rather than relying on the subjectivity and biases of a team of outside experts, Facebook will rely on the subjectivity and biases of 2 billion people around the world. Specifically, Facebook said it will decide which media outlets are prioritized at least in part by just asking people which outlets they like:

As part of our ongoing quality surveys, we will now ask people whether they’re familiar with a news source and, if so, whether they trust that source. The idea is that some news organizations are only trusted by their readers or watchers, and others are broadly trusted across society even by those who don’t follow them directly. (We eliminate from the sample those who aren’t familiar with a source, so the output is a ratio of those who trust the source to those who are familiar with it.)

Facebook is either unaware of — or, more likely — unwilling to deal with the fact that people have rabid, tribalistic loyalties to certain outlets. Someone who enjoys sharing, say, the Daily Caller or InfoWars articles is going to, of course, say that these are trustworthy outlets. Otherwise, they’re admitting that they voluntarily consume and spread information that isn’t trustworthy, and we all think too highly of ourselves for that. According to a Facebook spokesperson, the surveys are meant to make sure “people can have more from their favorite sources and more from trusted sources.” Isn’t part of the Facebook information disaster that so many people count things like RedStateEagleMilitiaZoneDeepStateNews (or what have you) among their “favorite sources?” Should we be asking these people what’s trustworthy and what isn’t? Should they be deciding what will appear on your feed — or even their own — as reliable news?

LIKE MANY OTHER journalists, activists, and software developers I know, I carry my laptop everywhere while I’m traveling. It contains sensitive information; messaging app conversations, email, password databases, encryption keys, unreleased work, web browsers  logged into various accounts, and so on. My disk is encrypted, but all it takes to bypass this protection is for an attacker — a malicious hotel housekeeper, or “evil maid,” for example — to spend a few minutes physically tampering with it without my knowledge. If I come back and continue to use my compromised computer, the attacker could gain access to everything.

Edward Snowden and his friends have a solution. The NSA whistleblower and a team of collaborators have been working on a new open source Android app called Haven that you install on a spare smartphone, turning the device into a sort of sentry to watch over your laptop. Haven uses the smartphone’s many sensors — microphone, motion detector, light detector, and cameras — to monitor the room for changes, and it logs everything it notices. The first public beta version of Haven has officially been released; it’s available in the Play Store and on F-Droid, an open source app store for Android.

Snowden is helping to develop the software through a project he leads at the Freedom of the Press Foundation, which receives funding from The Intercept’s parent company. I sit on the FPF board with Snowden, am an FPF founder, and lent some help developing the app, including through nine months of testing. With that noted, I’ll be forthright about the product’s flaws below, and have solicited input for this article from people not involved in the project.

Also collaborating on Haven is the Guardian Project, a global collective of mobile security app developers.

Haven is an external solution to a problem computer makers traditionally attempted to handle from within their devices. Some laptops, for example, offer “secure boot” through a special tamper-resistant chip called a Trusted Platform Module, which tries to ensure that the computer’s bootloader code hasn’t been modified to be malicious. But there are various ways this could go wrong: there can be bugs in the code that does the verification, attackers could connive to get their code marked as trustworthy, or malicious code could be inserted after the bootloader. Some computer users have tried the low-tech solution of painting glitter nail polish on their laptop screws, creating a sort of seal that would be broken during a tampering attempt.

“Due to how current laptops, and probably most other computing devices, are made today, it is virtually impossible to systematically check later if the laptop has been compromised or not,” said Joanna Rutkowska, founder of the secure Qubes operating system, who invented the term “evil maid” in 2009 as part of her work as a security researcher.
In her first comments to the press, former Green Party presidential candidate Jill Stein said she will cooperate fully with the Senate Intelligence Committee’s investigation into “collusion with the Russians” during the 2016 campaign, and is currently searching for relevant documents. Stein denies holding any substantive communications with the Russian government or RT, its state-owned media property.
Stein says her involvement in the inquiry, first reported by BuzzFeed News, came as a surprise when her campaign was first contacted last month. After a subsequent dialogue between attorneys representing Stein and lawyers from the Senate Intelligence Committee, the former candidate received a formal request for cooperation. Although she says the possibility of testifying before Congress has not yet been broached, Stein says she would be “happy to do so” if asked.
Still, Stein clearly resents the Senate’s attention vis-à-vis  electoral interference and foreign meddling: “This smacks of the dangerous underbelly of these investigations. The extent to which they exercise overreach, politicizing, and sensationalism is a danger to democracy, especially in the current climate of all-out war on our First Amendment rights. This is not a time to be attacking the rights of political speech and political association.”
In the meantime, Stein’s defunct 2016 campaign is working to “produce all docs related to the inquiry into Russian interference” in accordance with the Intelligence Committee’s formal request, though Stein doesn’t “believe they’ve given us search terms,” as their respective attorneys are still “in the process of working that out.” Stein added that she’s unaware of a deadline for this document handover, but “we are trying to comply as quickly as we possibly can. … There are a number of people we have to contact that we’re not in touch with, and they have to search as well.”
It’s safe to assume the Intelligence Committee is interested in anything pertaining to Stein’s now-infamous attendance of an RT gala in Moscow, at which she was seated and photographed with Russian President Vladimir Putin and President Donald Trump’s former national security adviser Michael Flynn. Stein told The Intercept that as she has routinely appeared on RT and expects to hand over communications related to booking those TV segments and other “administrative” messages between her campaign and the Russian network, as well as “logistical” messages about the Moscow event. Stein also noted that before her Moscow visit, she had “requested to speak with either Putin or [Russian Foreign Minister Sergey] Lavrov, or someone in the Russian government, to be able to discuss our policies, because I was there to advance our agenda for peace and climate action and diplomacy and nuclear weapon abolition.”
Stein says this request was not granted. Stein also maintains that she declined to let RT pay for any portion of her trip to Moscow and further denies requesting or receiving any other assistance, monetary or otherwise, from RT, the Russian government, WikiLeaks, or the Trump campaign, adding that any dialogue or cooperation with Trump “would have been quite contrary to our values.”
When asked why she had attended the gala and sought an audience with Putin, she told The Intercept that “we sought contact with every powerful world leader we had access to,” and that the Russian government was of particular interest because of its involvement in the Syrian civil war. “We don’t have any reason to suspect that there was any backdoor communication,” Stein said. “We were very much focused on the substantive issues of the elections, and we avoid like the plague manipulations and machinations in order to make things happen behind the scenes.”
A spokesperson for Stein provided the following statement:








Responding to a Senate Select Committee on Intelligence request for documents pertaining to interference in the 2016 election, former Green Party Presidential candidate Jill Stein said she is cooperating by sharing all communications relevant to the committee’s mission. “We take seriously the issue of potential interference in our elections, as demonstrated by our continuing efforts to investigate the integrity of the 2016 election and examine our voting machines that are widely known to be vulnerable, but which still have not been examined for evidence of interference. To restore trust in our elections and democracy itself, we must safeguard our elections from all potential sources of interference, whether by foreign state actors or domestic political partisans, criminal networks, lone wolves, or private corporations – including those who control voting software.
Our campaign has observed the highest standards of transparency and integrity in our interactions with foreign nationals as well as Americans. Our communications with Russian individuals regarding an invitation to speak on international relations at the RT 10th anniversary media conference will confirm what we stated publicly at that time and since: that we did not accept any payment or even reimbursement for the trip, and that we made the trip with the goal of reaching an international audience and Russian officials with a message of Middle East peace, diplomacy, and cooperation against the urgent threat of climate change, consistent with long-standing Green principles and policies.
We strongly support legitimate inquiry into any illegal activity in our elections – including quid pro quo deals, money laundering, corruption and violation of campaign finance laws. At the same time, we caution against the politicization, sensationalism and collapse of journalistic standards that has plagued media coverage of the investigation. In the current climate of attacks on our civil liberties, with the emergence of censorship in social media and the press, criminalization of protest, militarization of police and massive expansion of the surveillance state, we must guard against the potential for these investigations to be used to intimidate and silence principled opposition to the political establishment.
















Stein said that she would release a more comprehensive statement about the investigation in the near future.








Top photo: Green Party presidential candidate Jill Stein speaks at a news conference on Fifth Avenue across the street from Trump Tower December 5, 2016 in New York City. 
THE U.K. GOVERNMENT is facing fresh calls to clarify its role in U.S. drone strikes after acknowledging that there are potentially hundreds of British spy agency personnel working inside a U.S.-controlled surveillance base that has played a key role in so-called targeted killings.

Earlier this month, British Minister of State for the Armed Forces Mark Lancaster disclosed to the U.K. Parliament that employees of eavesdropping agency Government Communications Headquarters, or GCHQ, are stationed at a remote base in the north of England called Menwith Hill. An unknown number of GCHQ employees are among 578 British civilians, military, and contractors at the site, Lancaster confirmed in a previously unreported written statement, alongside 627 Americans.

Questioned in 2013 about GCHQ’s presence at the base, the British government had insisted that it “would not comment on whether there are personnel working in intelligence” there – a position that appears to have changed with Lancaster’s admission, possibly unintentionally. His statement came in response to a Parliament member’s question about how many people are working at Menwith Hill. A spokesperson for the U.K. government’s Ministry of Defence declined to answer questions about whether the statement represented a policy shift.

Menwith Hill is the National Security Agency’s largest overseas surveillance facility, located near the small town of Harrogate in North Yorkshire. As The Intercept revealed in 2015, the base has been used to aid “a significant number of capture-kill operations” across the Middle East and North Africa, according to top-secret documents. The facility operates spy satellites used to pinpoint the locations of people on the ground below, and it is equipped with eavesdropping technology that can harvest data from more than 300 million emails and phone calls a day.
THE U.K. GOVERNMENT is facing fresh calls to clarify its role in U.S. drone strikes after acknowledging that there are potentially hundreds of British spy agency personnel working inside a U.S.-controlled surveillance base that has played a key role in so-called targeted killings.

Earlier this month, British Minister of State for the Armed Forces Mark Lancaster disclosed to the U.K. Parliament that employees of eavesdropping agency Government Communications Headquarters, or GCHQ, are stationed at a remote base in the north of England called Menwith Hill. An unknown number of GCHQ employees are among 578 British civilians, military, and contractors at the site, Lancaster confirmed in a previously unreported written statement, alongside 627 Americans.

Questioned in 2013 about GCHQ’s presence at the base, the British government had insisted that it “would not comment on whether there are personnel working in intelligence” there – a position that appears to have changed with Lancaster’s admission, possibly unintentionally. His statement came in response to a Parliament member’s question about how many people are working at Menwith Hill. A spokesperson for the U.K. government’s Ministry of Defence declined to answer questions about whether the statement represented a policy shift.

Menwith Hill is the National Security Agency’s largest overseas surveillance facility, located near the small town of Harrogate in North Yorkshire. As The Intercept revealed in 2015, the base has been used to aid “a significant number of capture-kill operations” across the Middle East and North Africa, according to top-secret documents. The facility operates spy satellites used to pinpoint the locations of people on the ground below, and it is equipped with eavesdropping technology that can harvest data from more than 300 million emails and phone calls a day.
BRITISH SPY AGENCIES are under scrutiny in a landmark court case challenging the legality of top-secret mass surveillance programs revealed in documents leaked by whistleblower Edward Snowden.

A panel of 10 judges at the European Court of Human Rights in Strasbourg, France, held a hearing Tuesday to examine the U.K. government’s large-scale electronic spying operations, following three separate challenges brought by a dozen human rights groups, including Amnesty International, Privacy International, the American Civil Liberties Union, Big Brother Watch, the Open Rights Group, and the Irish Council for Civil Liberties.

The case is the first of its kind to be heard by the court, which handles complaints related to violations of the European Convention on Human Rights, an international treaty by which the U.K. is still bound despite its vote last year to leave the European Union. The court’s judgments could have ramifications for future U.K. surveillance operations.

The human rights groups are arguing that British spy programs violate four key rights protected under the convention: the right to privacy; the right to a fair trial; the right to freedom of expression; and the right not to be discriminated against. They cite a 2015 ruling by a U.K. tribunal, which found that British eavesdropping agency Government Communications Headquarters, or GCHQ, had unlawfully spied on the communications of Amnesty International and the South Africa-based Legal Resources Centre.

Dinah Rose, a lawyer representing the human rights groups, acknowledged in court Tuesday that some serious security threats require the use of covert government surveillance. But, she added, “excessive and unaccountable state surveillance puts at risk the very core values of the free and democratic societies that terrorism seeks to undermine.”

The British government has insisted that it does not carry out “mass surveillance,” preferring instead to use the term “bulk surveillance,” which it says is necessary to discover previously unknown threats. Documents leaked by Snowden describe how GCHQ planned to carry out “population scale” surveillance; boasted that it had “massive access” to internet communications; and monitored more than 50 billion “events” about communications each day.

Government lawyer James Eadie told the court that using surveillance systems to collect and store communications is not itself a violation of privacy. Instead, he said, privacy is only violated when there is “sentient examination” of communications – in other words, when a human analyst reads or listens to individual messages or calls. This will be a key point of contention for the Strasbourg judges to consider.

Last year, a complaint filed in the case by 10 of the human rights groups named more than a dozen surveillance programs that allegedly violate rights and do not have adequate safeguards against abuse. Among them are GCHQ programs, such as KARMA POLICE, which was first exposed by The Intercept. KARMA POLICE was designed to allow the GCHQ to build “a web-browsing profile for every visible user on the internet.” The complaint also focuses on NSA-operated programs that have been shared with British spies, such as XKEYSCORE, a tool that can be used to sift through masses of emails, online chats, and virtually every other kind of internet data.

Nick Williams, Amnesty International’s senior legal counsel, said in an email that the case represented a “watershed moment for people’s privacy and freedom of expression” across the world. “The case concerns the U.K., but its significance is global. By bringing together human rights defenders and journalists from four different continents, it serves to highlight the dangers mass surveillance poses to the vital work of countless organisations and to individuals who expose human rights abuses and defend those at risk.”

Scarlet Kim, a legal officer with Privacy International, said in a statement: “For years, the U.K. Government has been intercepting the private communications and data of millions of people around the world. At the same time, it can access similarly enormous troves of information intercepted by the U.S. Government. These practices are unlawful and violate the fundamental rights of individuals across the world, assailing privacy and chilling thought and speech.”

A spokesperson for the U.K. government’s Home Office declined to comment on the specifics of the case, but said in a statement that British intelligence agencies “conduct their vital work within a strict legal and policy framework that applies rigorous safeguards and oversight mechanisms to ensure respect for human rights. We will vigorously defend the powers our agencies need to keep us all safe and secure.”

Top photo: City workers use smartphones inside in London on Oct. 30, 2017.
LEIA EM PORTUGUÊS 
LEIA EM PORTUGUÊS 
AXON, THE WORLD’S largest vendor of police-worn body cameras, is moving into the business of capturing video taken by the public. In a survey emailed to law enforcement officials last month, the company formerly known as Taser International solicited naming ideas for its provisionally titled Public Evidence Product. According to the survey, the product will allow citizens to submit photos or video evidence of “a crime, suspicious activity, or event” to Evidence.com, the company’s cloud-based storage platform, to help agencies “in solving a crime or gathering a fuller point of view from the public.” Civil rights advocates interviewed by The Intercept were surprised to learn about the corporation’s latest initiative, seeing it as yet another untested effort to co-opt community oversight and privatize criminal justice.

“When police body cameras were initially established, it was because citizens were clamoring for police accountability,” explained Shahid Buttar, director of grassroots advocacy with the Electronic Frontier Foundation. “But we’ve seen how cameras have been more useful for police investigations than for accountability. This product realizes those dangers and takes them to a new dystopian level by crowdsourcing the collection of evidence and turning it over to law enforcement.”
CHAT LOGS OBTAINED from message boards used by neo-Nazis and other far-right groups show a concerted effort to compile private information on leftist enemies and circulate the data to encourage harassment or violence.

The messages were obtained by an anonymous source, who infiltrated and gained the trust of white nationalists and other right-wingers, and has been leaking the material to Unicorn Riot, a “decentralized media collective” that emerged from leftist protest movements.

The chat logs originate from various web discussion communities hosted by the provider Discord and closed to the public. The communities, which have names like “Vibrant Diversity,” “Ethnoserver,” “Safe Space 3,” “4th Reich,” and “Charlottesville 2.0,” range from having 36 users to 1,269 users. The most active, with nearly a quarter million messages over seven months, is “Vibrant Diversity,” a neo-Nazi community forum that includes a channel called “#oven,” where users share racist memes. The 4th Reich server, the second most active, has 130,000 messages over the course of four months and includes a channel called “#rare_hitlers,” where users share propaganda posters and other glorified media from Nazi Germany. The “Charlottesville 2.0” server, which contains 35,000 messages, is where the “Unite the Right” hate rally in Charlottesville was organized.

This article is based solely on chat logs from a community called “Pony Power” (Unicorn Riot published the logs yesterday). The Pony Power server has 50 users, and the chat logs contain just over 1,000 messages, posted over the course of 10 days and ranging in topic from far-right politics to advice about digital and operational security to debates about the legal limits of online behavior. The primary activity on the Pony Power server is posting private information, like names, photos, home addresses, and phone numbers of dozens of anti-fascist activists.

Victims of the outings, also known as “doxing,” described reactions ranging from terror to anger to annoyance, and have variously turned to friends and family for support and locked down their accounts. They said the Pony Power doxing campaign is just the latest in a series of online efforts by neo-Nazis and their allies to marginalize their opponents. The information compiled on Pony Power hasn’t yet been distributed to the larger right-wing extremist community. However, doxing efforts associated with prior online hate campaigns have forced targets to leave their homes in the face of death threats, rape threats, and other forms of harassment. And those attacks were mounted even before President Donald Trump came to power on the back of racist attacks against his predecessor, Mexicans, and Muslims, and before he embraced white nationalists and encouraged violence against protesters at campaign rallies.

People chatting on the Pony Power server spoke openly, as though behind closed doors, often using offensive slurs. So be warned, some of the following conversations are hard to stomach.

Scope of the harassment campaign

DURING THE 10-DAY span that the Pony Power chat logs cover, from August 17 to 27, so-called alt-right members collected private information from over 50 anti-fascist activists from the states of California, Florida, Illinois, Iowa, Maryland, Massachusetts, Minnesota, Missouri, Nebraska, North Carolina, South Dakota, Texas, Virginia, and Washington.

The information collected often included photographs, social media profiles, home address, phone numbers, email addresses, date of birth, driver license numbers, vehicle information, place of employment, and in one instance, a social security number. The justification for doxing normally put forward in Pony Power was that the targets were part of loosely structured far-left groups known as antifa, or anti-fascists, which has put up some of the most militant opposition to the far right; or they’re judged sympathetic to antifa; or they’ve been seen at protests deemed “communist” by the the far right.

The members of Pony Power often brainstorm methods to increase the effectiveness of their harassment campaigns. One user called “oxycolton” wrote, “We’ve had a lot of people dox antifags but it doesn’t hold,” apparently meaning that the information is lost, in part because they don’t yet have a database to keep track of everything.
CHAT LOGS OBTAINED from message boards used by neo-Nazis and other far-right groups show a concerted effort to compile private information on leftist enemies and circulate the data to encourage harassment or violence.

The messages were obtained by an anonymous source, who infiltrated and gained the trust of white nationalists and other right-wingers, and has been leaking the material to Unicorn Riot, a “decentralized media collective” that emerged from leftist protest movements.

The chat logs originate from various web discussion communities hosted by the provider Discord and closed to the public. The communities, which have names like “Vibrant Diversity,” “Ethnoserver,” “Safe Space 3,” “4th Reich,” and “Charlottesville 2.0,” range from having 36 users to 1,269 users. The most active, with nearly a quarter million messages over seven months, is “Vibrant Diversity,” a neo-Nazi community forum that includes a channel called “#oven,” where users share racist memes. The 4th Reich server, the second most active, has 130,000 messages over the course of four months and includes a channel called “#rare_hitlers,” where users share propaganda posters and other glorified media from Nazi Germany. The “Charlottesville 2.0” server, which contains 35,000 messages, is where the “Unite the Right” hate rally in Charlottesville was organized.

This article is based solely on chat logs from a community called “Pony Power” (Unicorn Riot published the logs yesterday). The Pony Power server has 50 users, and the chat logs contain just over 1,000 messages, posted over the course of 10 days and ranging in topic from far-right politics to advice about digital and operational security to debates about the legal limits of online behavior. The primary activity on the Pony Power server is posting private information, like names, photos, home addresses, and phone numbers of dozens of anti-fascist activists.

Victims of the outings, also known as “doxing,” described reactions ranging from terror to anger to annoyance, and have variously turned to friends and family for support and locked down their accounts. They said the Pony Power doxing campaign is just the latest in a series of online efforts by neo-Nazis and their allies to marginalize their opponents. The information compiled on Pony Power hasn’t yet been distributed to the larger right-wing extremist community. However, doxing efforts associated with prior online hate campaigns have forced targets to leave their homes in the face of death threats, rape threats, and other forms of harassment. And those attacks were mounted even before President Donald Trump came to power on the back of racist attacks against his predecessor, Mexicans, and Muslims, and before he embraced white nationalists and encouraged violence against protesters at campaign rallies.

People chatting on the Pony Power server spoke openly, as though behind closed doors, often using offensive slurs. So be warned, some of the following conversations are hard to stomach.

Scope of the harassment campaign

DURING THE 10-DAY span that the Pony Power chat logs cover, from August 17 to 27, so-called alt-right members collected private information from over 50 anti-fascist activists from the states of California, Florida, Illinois, Iowa, Maryland, Massachusetts, Minnesota, Missouri, Nebraska, North Carolina, South Dakota, Texas, Virginia, and Washington.

The information collected often included photographs, social media profiles, home address, phone numbers, email addresses, date of birth, driver license numbers, vehicle information, place of employment, and in one instance, a social security number. The justification for doxing normally put forward in Pony Power was that the targets were part of loosely structured far-left groups known as antifa, or anti-fascists, which has put up some of the most militant opposition to the far right; or they’re judged sympathetic to antifa; or they’ve been seen at protests deemed “communist” by the the far right.

The members of Pony Power often brainstorm methods to increase the effectiveness of their harassment campaigns. One user called “oxycolton” wrote, “We’ve had a lot of people dox antifags but it doesn’t hold,” apparently meaning that the information is lost, in part because they don’t yet have a database to keep track of everything.
WHEN CIVIL LIBERTIES advocates discuss the dangers of new policing technologies, they often point to sci-fi films like “RoboCop” and “Minority Report” as cautionary tales. In “RoboCop,” a massive corporation purchases Detroit’s entire police department. After one of its officers gets fatally shot on duty, the company sees an opportunity to save on labor costs by reanimating the officer’s body with sleek weapons, predictive analytics, facial recognition, and the ability to record and transmit live video.

Although intended as a grim allegory of the pitfalls of relying on untested, proprietary algorithms to make lethal force decisions, “RoboCop” has long been taken by corporations as a roadmap. And no company has been better poised than Taser International, the world’s largest police body camera vendor, to turn the film’s ironic vision into an earnest reality.

In 2010, Taser’s longtime vice president Steve Tuttle “proudly predicted” to GQ that once police can search a crowd for outstanding warrants using real-time face recognition, “every cop will be RoboCop.” Now Taser has announced that it will provide any police department in the nation with free body cameras, along with a year of free “data storage, training, and support.” The company’s goal is not just to corner the camera market, but to dramatically increase the video streaming into its servers.

With an estimated one-third of departments using body cameras, police officers have been generating millions of hours of video footage. Taser stores terabytes of such video on Evidence.com, in private servers, operated by Microsoft, to which police agencies must continuously subscribe for a monthly fee. Data from these recordings is rarely analyzed for investigative purposes, though, and Taser — which recently rebranded itself as a technology company and renamed itself “Axon” — is hoping to change that.

Taser has started to get into the business of making sense of its enormous archive of video footage by building an in-house “AI team.” In February, the company acquired a computer vision startup called Dextro and a computer vision team from Fossil Group Inc. Taser says the companies will allow agencies to automatically redact faces to protect privacy, extract important information, and detect emotions and objects — all without human intervention. This will free officers from the grunt work of manually writing reports and tagging videos, a Taser spokesperson wrote in an email. “Our prediction for the next few years is that the process of doing paperwork by hand will begin to disappear from the world of law enforcement, along with many other tedious manual tasks.” Analytics will also allow departments to observe historical patterns in behavior for officer training, the spokesperson added. “Police departments are now sitting on a vast trove of body-worn footage that gives them insight for the first time into which interactions with the public have been positive versus negative, and how individuals’ actions led to it.”

But looking to the past is just the beginning: Taser is betting that its artificial intelligence tools might be useful not just to determine what happened, but to anticipate what might happen in the future.

“We’ve got all of this law enforcement information with these videos, which is one of the richest treasure troves you could imagine for machine learning,” Taser CEO Rick Smith told PoliceOne in an interview about the company’s AI acquisitions. “Imagine having one person in your agency who would watch every single one of your videos — and remember everything they saw — and then be able to process that and give you the insight into what crimes you could solve, what problems you could deal with. Now, that’s obviously a little further out, but based on what we’re seeing in the artificial intelligence space, that could be within five to seven years.”

As video analytics and machine vision have made rapid gains in recent years, the future long dreaded by privacy experts and celebrated by technology companies is quickly approaching. No longer is the question whether artificial intelligence will transform the legal and lethal limits of policing, but how and for whose profits.

“Everyone refers to ‘Minority Report’ … about how they use facial recognition and iris recognition,” said Ron Kirk, director of the West Virginia Intelligence Fusion Center, which uses both technologies, in an interview with Vocativ. “I actually think that that is the way of the future.”
LAST YEAR, A RUSSIAN startup announced that it could scan the faces of people passing by Moscow’s thousands of CCTV cameras and pick out wanted criminals or missing persons. Unlike much face recognition technology — which runs stills from videos or photographs after the fact — NTechLab’s FindFace algorithm has achieved a feat that once only seemed possible in the science fictional universe of “Minority Report”: It can determine not just who someone is, but where they’ve been, where they’re going, and whether they have an outstanding warrant, immigration detainer, or unpaid traffic ticket.

For years, the development of real-time face recognition has been hampered by poor video resolution, the angles of bodies in motion, and limited computing power. But as systems begin to transcend these technical barriers, they are also outpacing the development of policies to constrain them. Civil liberties advocates fear that the rise of real-time face recognition alongside the growing number of police body cameras creates the conditions for a perfect storm of mass surveillance.

“The main concern is that we’re already pretty far along in terms of having this real-time technology, and we already have the cameras,” said Jake Laperruque, a fellow at the Constitution Project. “These cameras are small, hard to notice, and all over the place. That’s a pretty lethal combination for privacy unless we have reasonable rules on how they can be used together.”

This imminent reality has led several civil liberties groups to call on police departments and legislators to implement clear policies on camera footage retention, biometrics, and privacy. On Wednesday morning, the House Oversight Committee held a hearing on law enforcement’s use of facial recognition technology, where advocates emphasized the dangers of allowing advancements in real-time recognition to broaden surveillance powers. As Alvaro Bedoya, executive director of the Center on Privacy and Technology at Georgetown Law, told Congress, pairing the technology with body cameras, in particular, “will redefine the nature of public spaces.”

The integration of real-time face recognition with body-worn cameras is further along than lawmakers and citizens realize. A recent Justice Department-funded survey conducted by Johns Hopkins University found that at least nine out of 38 manufacturers of body cameras currently have facial recognition capacities or have built in an option for such technology to be used later.

Taser, which leads the market for body cameras, recently acquired two startups that will allow it to run video analytics on the footage the cameras collect, and Taser’s CEO has repeatedly emphasized the development of real-time applications, such as scanning videos for faces, objects, and suspicious activity. A spokesperson for NTechLab, which has pilot projects in 20 countries including the United States, China, the United Kingdom, and Turkey, told The Intercept that its high-performing algorithm is already compatible with body cameras.

Police see the appeal. The captain of the Las Vegas Police Department told Bloomberg in July that he envisions his officers someday patrolling the Strip with “real-time analysis” on their body cameras and an earpiece to tell them, “‘Hey, that guy you just passed 20 feet ago has an outstanding warrant.’”

At least five U.S. police departments, including those in Los Angeles and New York, have already purchased or looked into purchasing real-time face recognition for their CCTV cameras, according to a study of face recognition technology published by Bedoya and other researchers at Georgetown. Bedoya emphasized that it’s only a matter of time until the nation’s body-worn cameras will be hooked up to real-time systems. With 6,000 of the country’s 18,000 police agencies estimated to be using body cameras, the pairing would translate into hundreds of thousands of new, mobile surveillance cameras.

“For many of these systems, the inclusion of real-time face recognition is just a software update away,” said Harlan Yu, co-author of a report on body camera policies for Upturn, a technology think tank.

Civil liberties experts warn that just walking down the street in a major urban center could turn into an automatic law enforcement interaction. With the ability to glean information at a distance, officers would not need to justify a particular interaction or find probable cause for a search, stop, or frisk. Instead, everybody walking past a given officer on his patrol could be subject to a “perpetual line-up,” as the Georgetown study put it. In Ferguson, Missouri, where a Justice Department investigation showed that more than three-quarters of the population had outstanding warrants, real-time face searches could give police immense power to essentially arrest individuals at will. And in a city like New York, which has over 100 officers per square mile and plans to equip each one of them with body cameras by 2019, the privacy implications of turning every beat cop into a surveillance camera are enormous.

“The inclusion of face recognition really changes the nature and purpose of body cameras, and it changes what communities expect when they call for and pay for cameras with taxpayer dollars,” Yu said. “I think there’s a real fear in communities of color, where officers are already concentrated, that these body-worn cameras will become another tool for surveillance rather than a tool for accountability.”
THE FBI’S RAP BACK program is quietly transforming the way employers conduct background checks. While routine background checks provide employers with a one-time “snapshot” of their employee’s past criminal history, employers enrolled in federal and state Rap Back programs receive ongoing, real-time notifications and updates about their employees’ run-ins with law enforcement, including arrests at protests and charges that do not end up in convictions. (“Rap” is an acronym for Record of Arrest and Prosecution; “Back” is short for background.) Testifying before Congress about the program in 2015, FBI Director James Comey explained some limits of regular background checks: “People are clean when they first go in, then they get in trouble five years down the road [and] never tell the daycare about this.”

A majority of states already have their own databases that they use for background checks and have accessed in-state Rap Back programs since at least 2007; states and agencies now partnering with the federal government will be entering their data into the FBI’s Next Generation Identification database. The NGI database, widely considered to be the world’s largest biometric database, allows federal and state agencies to search more than 70 million civil fingerprints submitted for background checks alongside over 50 million prints submitted for criminal purposes. In July 2015, Utah became the first state to join the federal Rap Back program. Last April, aviation workers at Dallas-Ft. Worth Airport and Boston Logan International Airport began participating in a federal Rap Back pilot program for aviation employees. Two weeks ago, Texas submitted its first request to the federal criminal Rap Back system.

Rap Back has been advertised by the FBI as an effort to target individuals in “positions of trust,” such as those who work with children, the elderly, and the disabled. According to a Rap Back spokesperson, however, there are no formal limits as to “which populations of individuals can be enrolled in the Rap Back Service.” Civil liberties advocates fear that under Trump’s administration the program will grow with serious consequences for employee privacy, accuracy of records, and fair employment practices.
A BROAD COALITION of over 50 civil liberties groups delivered a letter to the Justice Department’s civil rights division Tuesday calling for an investigation into the expanding use of face recognition technology by police. “Safeguards to ensure this technology is being used fairly and responsibly appear to be virtually nonexistent,” the letter stated. The routine unsupervised use of face recognition systems, according to the dozens of signatories, threatens the privacy and civil liberties of millions — especially those of immigrants and people of color.

These civil rights groups were provided with advance copies of a watershed 150-page report detailing — in many cases for the first time — how local police departments across the country have been using facial recognition technology. Titled “The Perpetual Lineup,” the report, published Tuesday morning by the Georgetown Center on Privacy & Technology, reveals that police deploy face recognition technology in ways that are more widespread, advanced, and unregulated than anyone has previously reported.

“Face recognition is a powerful technology that requires strict oversight. But those controls by and large don’t exist today,” said Clare Garvie, one of the report’s co-authors. “With only a few exceptions, there are no laws governing police use of the technology, no standards ensuring its accuracy, and no systems checking for bias. It’s a wild west.”

Of the 52 agencies that acknowledged using face recognition in response to 106 records requests, the authors found that only one had obtained legislative approval before doing so. Government reports have long confirmed that millions of images of citizens are collected and stored in federal face recognition databases. Since at least 2002, civil liberties advocates have raised concerns that millions of drivers license photos of Americans who have never been arrested are being subject to facial searches — a practice that amounts to a perpetual digital lineup. This report augments such fears, demonstrating that at least one in four state or local law enforcement agencies have access to face recognition systems.

Among its findings, the report provides the most fine-grained detail to date on how exactly these face recognition systems might disproportionately impact African-Americans. “Face recognition systems are powerful — but they can also be biased,” the coalition’s letter explains. While one in two American adults have face images stored in at least one database, African-Americans are more likely than others to have their images captured and searched by face recognition systems.

In Virginia, for instance, the report shows how state police can search a mug shot database disproportionately populated with African-Americans, who are twice as likely to be arrested in the state. Not only are African-Americans more likely to be subject to searches, according to the report, but this overrepresentation puts them at greatest risk for a false match.

These errors could be compounded by the fact that some face recognition algorithms have been shown to misidentify African-Americans, women, and young people at unusually high rates. In a 2012 study co-authored by FBI experts, three algorithms that were tested performed between 5 and 10 percent worse on black faces than on white faces. And the overall accuracy of systems has been shown to decrease as a dataset expands. The Georgetown report interviewed two major facial recognition vendors which said that they did not test for racial basis, despite the fact that systems have been shown to be far from “race-blind.”

A slideshow on San Diego’s privacy policy obtained by the researchers reveals that people of color in the county are between 1.5 and 2.5 more likely to be targeted by its surveillance systems. San Diego County uses a mugshot-only system, and repeated studies have shown that African-Americans are twice as likely as white people to be arrested and searched by police.

A BROAD COALITION of over 50 civil liberties groups delivered a letter to the Justice Department’s civil rights division Tuesday calling for an investigation into the expanding use of face recognition technology by police. “Safeguards to ensure this technology is being used fairly and responsibly appear to be virtually nonexistent,” the letter stated. The routine unsupervised use of face recognition systems, according to the dozens of signatories, threatens the privacy and civil liberties of millions — especially those of immigrants and people of color.

These civil rights groups were provided with advance copies of a watershed 150-page report detailing — in many cases for the first time — how local police departments across the country have been using facial recognition technology. Titled “The Perpetual Lineup,” the report, published Tuesday morning by the Georgetown Center on Privacy & Technology, reveals that police deploy face recognition technology in ways that are more widespread, advanced, and unregulated than anyone has previously reported.

“Face recognition is a powerful technology that requires strict oversight. But those controls by and large don’t exist today,” said Clare Garvie, one of the report’s co-authors. “With only a few exceptions, there are no laws governing police use of the technology, no standards ensuring its accuracy, and no systems checking for bias. It’s a wild west.”

Of the 52 agencies that acknowledged using face recognition in response to 106 records requests, the authors found that only one had obtained legislative approval before doing so. Government reports have long confirmed that millions of images of citizens are collected and stored in federal face recognition databases. Since at least 2002, civil liberties advocates have raised concerns that millions of drivers license photos of Americans who have never been arrested are being subject to facial searches — a practice that amounts to a perpetual digital lineup. This report augments such fears, demonstrating that at least one in four state or local law enforcement agencies have access to face recognition systems.

Among its findings, the report provides the most fine-grained detail to date on how exactly these face recognition systems might disproportionately impact African-Americans. “Face recognition systems are powerful — but they can also be biased,” the coalition’s letter explains. While one in two American adults have face images stored in at least one database, African-Americans are more likely than others to have their images captured and searched by face recognition systems.

In Virginia, for instance, the report shows how state police can search a mug shot database disproportionately populated with African-Americans, who are twice as likely to be arrested in the state. Not only are African-Americans more likely to be subject to searches, according to the report, but this overrepresentation puts them at greatest risk for a false match.

These errors could be compounded by the fact that some face recognition algorithms have been shown to misidentify African-Americans, women, and young people at unusually high rates. In a 2012 study co-authored by FBI experts, three algorithms that were tested performed between 5 and 10 percent worse on black faces than on white faces. And the overall accuracy of systems has been shown to decrease as a dataset expands. The Georgetown report interviewed two major facial recognition vendors which said that they did not test for racial basis, despite the fact that systems have been shown to be far from “race-blind.”

A slideshow on San Diego’s privacy policy obtained by the researchers reveals that people of color in the county are between 1.5 and 2.5 more likely to be targeted by its surveillance systems. San Diego County uses a mugshot-only system, and repeated studies have shown that African-Americans are twice as likely as white people to be arrested and searched by police.


FEDERAL AGENTS from the Department of Homeland Security and the Justice Department used “a sophisticated cell phone cloning attack—the details of which remain classified—to intercept protesters’ phone communications” in Portland this summer, Ken Klippenstein reported this week in The Nation. Put aside for the moment that, if the report is true, federal agents conducted sophisticated electronic surveillance against American protesters, an alarming breach of constitutional rights. Do ordinary people have any hope of defending their privacy and freedom of assembly against threats like this?

Yes, they do. Here are two simple things you can do to help mitigate this type of threat:

As much as possible, and especially in the context of activism, use an encrypted messaging app like Signal — and get everyone you work with to use it too — to protect your SMS text messages, texting groups, and voice and video calls.
Prevent other people from using your SIM card by setting a SIM PIN on your phone. There are instructions on how to do this below.
How SIM Cloning Works

Without more details, it’s hard to be entirely sure what type of surveillance was used, but The Nation’s mention of “cell phone cloning” makes me think it was a SIM cloning attack. This involves duplicating a small chip used by virtually every cellphone to link itself to its owner’s phone number and account; this small chip is the subscriber identity module, more commonly known as SIM.

Here’s how SIM cloning would work:

First, the feds would need physical access to their target’s phone; for example, they could arrest their target at a protest, temporarily confiscating their phone.
Then they would pop out the SIM card from the phone, a process designed to be easy, since end users often have reasons to replace the card (such as traveling abroad and needing a local SIM card to access the local cellular network, or when switching cellular providers).
The feds would then copy their target’s SIM card data onto a blank SIM card (this presents some challenges, as I explain below), and then put the original SIM card back without their target knowing.

SIM cards contain a secret encryption key that is used to encrypt data between the phone and cellphone towers. They’re designed so that this key can be used (like when you receive a text or call someone) but so the key itself can’t be extracted.

But it’s still possible to extract the key from the SIM card, by cracking it. Older SIM cards used a weaker encryption algorithm and could be cracked quickly and easily, but newer SIM cards use stronger encryption and might take days or significantly longer to crack. It’s possible that this is why the details of the type of surveillance used in Portland “remain classified.” Do federal agencies know of a way to quickly extract encryption keys from SIM cards? (On the other hand, it’s also possible that “cell phone cloning” doesn’t describe SIM cloning at all but something else instead, like extracting files from the phone itself instead of data from the SIM card.)
Since May, as protesters around the country have marched against police brutality and in support of the Black Lives Matter movement, activists have spotted a recurring presence in the skies: mysterious planes and helicopters hovering overhead, apparently conducting surveillance on protesters. A press release from the Justice Department at the end of May revealed that the Drug Enforcement Agency and U.S. Marshals Service were asked by the Justice Department to provide unspecified support to law enforcement during protests. A few days later, a memo obtained by BuzzFeed News offered a little more insight on the matter; it revealed that shortly after protests began in various cities, the DEA had sought special authority from the Justice Department to covertly spy on Black Lives Matter protesters on behalf of law enforcement. 
Although the press release and memo didn’t say what form the support and surveillance would take, it’s likely that the two agencies were being asked to assist police for a particular reason. Both the DEA and the Marshals possess airplanes outfitted with so-called stingrays or dirtboxes: powerful technologies capable of tracking mobile phones or, depending on how they’re configured, collecting data and communications from mobile phones in bulk.


